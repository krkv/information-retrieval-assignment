<?xml version="1.0" encoding="UTF-8"?>
<results>
  <item>
    <title>Email Spam Filtering: A Systematic Review</title>
	<abstract>Spam is information crafted to be delivered to a large number of recipients, in spite of their wishes. A spam filter is an automated tool to recognize spam so as to prevent its delivery. The purposes of spam and spam filters are diametrically opposed: spam is effective if it evades filters, while a filter is effective if it recognizes spam. The circular nature of these definitions, along with their appeal to the intent of sender and recipient make them difficult to formalize. A typical email user has a working definition no more formal than "I know it when I see it." Yet, current spam filters are remarkably effective, more effective than might be expected given the level of uncertainty and debate over a formal definition of spam, more effective than might be expected given the state-of-the-art information retrieval and machine learning methods for seemingly similar problems. But are they effective enough? Which are better? How might they be improved? Will their effectiveness be compromised by more cleverly crafted spam?

We survey current and proposed spam filtering techniques with particular emphasis on how well they work. Our primary focus is spam filtering in email; Similarities and differences with spam filtering in other communication and storage media — such as instant messaging and the Web — are addressed peripherally. In doing so we examine the definition of spam, the user's information requirements and the role of the spam filter as one component of a large and complex information universe. Well-known methods are detailed sufficiently to make the exposition self-contained, however, the focus is on considerations unique to spam. Comparisons, wherever possible, use common evaluation measures, and control for differences in experimental setup. Such comparisons are not easy, as benchmarks, measures, and methods for evaluating spam filters are still evolving. We survey these efforts, their results and their limitations. In spite of recent advances in evaluation methodology, many uncertainties (including widely held but unsubstantiated beliefs) remain as to the effectiveness of spam filtering techniques and as to the validity of spam filter evaluation methods. We outline several uncertainties and propose experimental methods to address them.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A survey of learning-based techniques of email spam filtering</title>
	<abstract>Email spam is one of the major problems of the today's Internet, bringing financial damage to companies and annoying individual users. Among the approaches developed to stop spam, filtering is an important and popular one. In this paper we give an overview of the state of the art of machine learning applications for spam filtering, and of the ways of evaluation and comparison of different filtering methods. We also provide a brief description of other branches of anti-spam protection and discuss the use of various approaches in commercial and non-commercial anti-spam software solutions.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Relaxing feature selection in spam filtering by using case-based reasoning systems</title>
	<abstract>This paper presents a comparison between two alternative strategies for addressing feature selection on a well known case-based reasoning spam filtering system called SPAMHUNTING. We present the usage of the k more predictive features and a percentage-based strategy for the exploitation of our amount of information measure. Finally, we confirm the idea that the percentage feature selection method is more adequate for spam filtering domain.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Enhancing scalability in anomaly-based email spam filtering</title>
	<abstract>Spam has become an important problem for computer security because it is a channel for the spreading of threats such as computer viruses, worms and phishing. Currently, more than 85% of received emails are spam. Historical approaches to combat these messages, including simple techniques such as sender blacklisting or the use of email signatures, are no longer completely reliable. Many solutions utilise machine-learning approaches trained using statistical representations of the terms that usually appear in the emails. However, these methods require a time-consuming training step with labelled data. Dealing with the situation where the availability of labelled training instances is limited slows down the progress of filtering systems and offers advantages to spammers. In a previous work, we presented the first spam filtering method based on anomaly detection that reduces the necessity of labelling spam messages and only employs the representation of legitimate emails. We showed that this method achieved high accuracy rates detecting spam while maintaining a low false positive rate and reducing the effort produced by labelling spam. In this paper, we enhance that system applying a data reduction algorithm to the labelled dataset, finding similarities among legitimate emails and grouping them to form consistent clusters that reduce the amount of needed comparisons. We show that this improvement reduces drastically the processing time, while maintaining detection and false positive rates stable.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>
Contributions to the study of SMS spam filtering: new collection and results</title>
	<abstract>The growth of mobile phone users has lead to a dramatic increasing of SMS spam messages. In practice, fighting mobile phone spam is difficult by several factors, including the lower rate of SMS that has allowed many users and service providers to ignore the issue, and the limited availability of mobile phone spam-filtering software. On the other hand, in academic settings, a major handicap is the scarcity of public SMS spam datasets, that are sorely needed for validation and comparison of different classifiers. Moreover, as SMS messages are fairly short, content-based spam filters may have their performance degraded. In this paper, we offer a new real, public and non-encoded SMS spam collection that is the largest one as far as we know. Moreover, we compare the performance achieved by several established machine learning methods. The results indicate that Support Vector Machine outperforms other evaluated classifiers and, hence, it can be used as a good baseline for further comparison.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>On enhancing the performance of spam mail filtering system using semantic enrichment</title>
	<abstract>With the explosive growth of the Internet, e-mails are regarded as one of the most important methods to send e-mails as a substitute for traditional communications As e-mail has become a major mean of communication in the Internet age, exponentially growing spam mails have been raised as a main problem As a result of this problem, researchers have suggested many methodologies to solve it Especially, Bayesian classifier-based systems show high performances to filter spam mail and many commercial products available However, they have several problems First, it has a cold start problem, that is, training phase has to be done before execution of the system The system must be trained about spam and non-spam mail Second, its cost for filtering spam mail is higher than rule-based systems Last problem, we focus on, is that the filtering performance is decreased when E-mail has only a few terms which represent its contents To solve this problem, we suggest spam mail filtering system using concept indexing and Semantic Enrichment For the performance evaluation, we compare our experimental results with those of Bayesian classifier which is widely used in spam mail filtering The experimental result shows that the proposed system has improved performance in comparison with Bayesian classifier respectively.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Clustering ensemble for spam filtering</title>
	<abstract>One of the main problems that modern e-mail systems face is the management of the high degree of spam or junk mail they recieve. Those systems are expected to be able to distinguish between legitimate mail and spam; in order to present the final user as much interesting information as possible. This study presents a novel hybrid intelligent system using both unsupervised and supervised learning that can be easily adapted to be used in an individual or collaborative system. The system divides the spam filtering problem into two stages: firstly it divides the input data space into different similar parts. Then it generates several simple classifiers that are used to classify correctly messages that are contained in one of the parts previously determined. That way the efficiency of each classifier increases, as they can specialize in separate the spam from certain types of related messages. The hybrid system presented has been tested with a real e-mail data base and a comparison of its results with those obtained from other common classification methods is also included. This novel hybrid technique proves to be effective in the problem under study.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Detecting and filtering instant messaging spam: a global and personalized approach</title>
	<abstract>While Instant Message (IM) is gaining its popularity it is exposed to increasingly severe security threats. A serious problem is IM spam (spim) that is unsolicited commercial messages sent via IM messengers. Unlike email spam (unsolicited bulk e-mails), which has been a serious security issue for a long time and a number of techniques have been proposed to cope with, spim has not received adequate attention from the research community yet, and traditional spam filtering techniques are not directly applicable to spim due to its presence information and real time nature.

In this paper, we present a new architecture for detecting and filtering spim. With the unique infrastructure of IM systems spim detection and filtering can be achieved not only at the client (receiver) side - for a personalized filtering - but also at the server side and various IM gateways - for a global filtering. Our technique integrates a number of mature spam defending techniques with modifications for IM applications, such as Black/White List, collaborative feedback based filtering, content-based technique, and challenge-response based filtering. We also design and implement new techniques for efficient spim detection and filtering, including filtering methods based on IM sending rate, content based spim defending techniques, fingerprint vector based filtering, text comparison filtering, and Bayesian filtering. We provide an analysis of their performances based on experimental results.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An Approach to Image Spam Filtering Based on Base64 Encoding and N-Gram Feature Extraction</title>
	<abstract>As compared with text spam, the image spam is a variant which is invented to escape from traditional text-based spam classification and filtering. Various approaches to image spam filtering have been proposed with respective advantages and drawbacks in terms of time cost and efficiency. In this paper, we propose a new approach based on Base64 encoding of image files and $n$-gram technique for feature extraction. By transforming normal images into Base64 presentation, we try to extract features of an image with $n$-gram technique. With these features we train an SVM (support vector machine) which shows effectiveness and efficiency in detecting spam images from legitimate images. With an online shared personal corpus of images as the input, experimental results show that our approach, in comparison with some of the existing methods of feature extraction, can achieve very high performance for image spam classification in terms of some basic measures such as accuracy, precision, and recall. Moreover, our approach shows its practicability by taking less running time for image spam classification in comparison to other methods.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Periodic Topic Mining from Massive Amounts of Data</title>
	<abstract>Social media keeps growing and providing us with rich sources of information to understand our everyday lives, customs, and culture in the form of periodic topics. This paper proposes a method of detecting periodic topics based on autocorrelation using the time series of the document frequencies of keywords. To deal with the massive amount of data collected from social media, this method is implemented using Hadoop, which is an open-source framework for distributed processing and data storage. The implementation is evaluated in comparison with a relational database management system. Using this method, this paper analyzes blogs, news sites, and spam as information sources which serve as social and cultural indicators. Data is collected from Japanese blogs and news sites, and spam blogs are then separated from legitimate blogs using a spam filtering system. Distribution periods of keywords within each information source and weekly keywords are then extracted, and the characteristics of each information source are illustrated in terms of distribution and keywords. The results obtained using this extraction method indicate that periodic blog topics tend to be TV programs, hobbies, and social events; periodic news topics tend to be political and economic events; and periodic topics in spam tend to be automatically copied-and-pasted e-mail newsletters and affiliate offers.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A methodology for comparing classifiers that allow the control of bias</title>
	<abstract>This paper presents False Positive-Critical Classifiers Comparison a new technique for pairwise comparison of classifiers that allow the control of bias. An evaluation of Naïve Bayes, k-Nearest Neighbour and Support Vector Machine classifiers has been carried out on five datasets containing unsolicited and legitimate e-mail messages to confirm the advantage of the technique over Receiver Operating Characteristic curves. The evaluation results suggest that the technique may be useful for choosing the better classifier when the ROC curves do not show comprehensive differences, as well as to prove that the difference between two classifiers is not significant, when ROC suggests that it might be. Spam filtering is a typical application for such a comparison tool, as it requires a classifier to be biased toward negative prediction and to have some upper limit on the rate of false positives. Finally the particular evaluation summary is presented, which confirms that Support Vector Machines out-perform other methods in most cases, while the Naïve Bayes classifier works well in a narrow, but relevant range of false positive rate.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Searching for Interacting Features for Spam Filtering</title>
	<abstract>In this paper, we introduce a novel feature selection method--INTERACT to select relevant words of emails for spam email filtering, i.e. classifying an email as spam or legitimate. Four traditional feature selection methods in text categorization domain, Information Gain, Gain Ratio, Chi Squared, and ReliefF, are also used for performance comparison. Three classifiers, Support Vector Machine (SVM), Naïve Bayes and a novel classifier--Locally Weighted learning with Naïve Bayes (LWNB) are discussed in this paper. Four popular datasets are employed as the benchmark corpora in our experiments to examine the capabilities of these five feature selection methods and the three classifiers. In our simulations, we discover that the LWNB improves the Naïve Bayes and gain higher prediction results by learning local models, and its performance is sometimes better than that of the SVM. Our study also shows the INTERACT can result in better performances of classifiers than the other four traditional methods for the spam email filtering.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Computing a Comprehensible Model for Spam Filtering</title>
	<abstract>In this paper, we describe the application of the Desicion Tree Boosting (DTB) learning model to spam email filtering.This classification task implies the learning in a high dimensional feature space. So, it is an example of how the DTB algorithm performs in such feature space problems. In [1], it has been shown that hypotheses computed by the DTB model are more comprehensible that the ones computed by another ensemble methods. Hence, this paper tries to show that the DTB algorithm maintains the same comprehensibility of hypothesis in high dimensional feature space problems while achieving the performance of other ensemble methods. Four traditional evaluation measures (precision, recall, F1 and accuracy) have been considered for performance comparison between DTB and others models usually applied to spam email filtering. The size of the hypothesis computed by a DTB is smaller and more comprehensible than the hypothesis computed by Adaboost and Naïve Bayes.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A survey of emerging approaches to spam filtering</title>
	<abstract>From just an annoying characteristic of the electronic mail epoch, spam has evolved into an expensive resource and time-consuming problem. In this survey, we focus on emerging approaches to spam filtering built on recent developments in computing technologies. These include peer-to-peer computing, grid computing, semantic Web, and social networks. We also address a number of perspectives related to personalization and privacy in spam filtering. We conclude that, while important advancements have been made in spam filtering in recent years, high performance approaches remain to be explored due to the large scale of the problem.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Time-efficient spam e-mail filtering using n-gram models</title>
	<abstract>In this paper, we propose spam e-mail filtering methods having high accuracies and low time complexities. The methods are based on the n-gram approach and a heuristics which is referred to as the first n-words heuristics. We develop two models, a class general model and an e-mail specific model, and test the methods under these models. The models are then combined in such a way that the latter one is activated for the cases the first model falls short. Though the approach proposed and the methods developed are general and can be applied to any language, we mainly apply them to Turkish, which is an agglutinative language, and examine some properties of the language. Extensive tests were performed and success rates about 98% for Turkish and 99% for English were obtained. It has been shown that the time complexities can be reduced significantly without sacrificing performance.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A survey and experimental evaluation of image spam filtering techniques</title>
	<abstract>In their arms race against developers of spam filters, spammers have recently introduced the image spam trick to make the analysis of emails' body text ineffective. It consists in embedding the spam message into an attached image, which is often randomly modified to evade signature-based detection, and obfuscated to prevent text recognition by OCR tools. Detecting image spam turns out to be an interesting instance of the problem of content-based filtering of multimedia data in adversarial environments, which is gaining increasing relevance in several applications and media. In this paper we give a comprehensive survey and categorisation of computer vision and pattern recognition techniques proposed so far against image spam, and make an experimental analysis and comparison of some of them on real, publicly available data sets.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Enhanced Topic-based Vector Space Model for semantics-aware spam filtering</title>
	<abstract>Spam has become a major issue in computer security because it is a channel for threats such as computer viruses, worms and phishing. More than 85% of received e-mails are spam. Historical approaches to combat these messages including simple techniques such as sender blacklisting or the use of e-mail signatures, are no longer completely reliable. Currently, many solutions feature machine-learning algorithms trained using statistical representations of the terms that usually appear in the e-mails. Still, these methods are merely syntactic and are unable to account for the underlying semantics of terms within the messages. In this paper, we explore the use of semantics in spam filtering by representing e-mails with a recently introduced Information Retrieval model: the enhanced Topic-based Vector Space Model (eTVSM). This model is capable of representing linguistic phenomena using a semantic ontology. Based upon this representation, we apply several well-known machine-learning models and show that the proposed method can detect the internal semantics of spam messages.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Collaborative spam filtering with heterogeneous agents</title>
	<abstract>Spam continues to generate great interest both among academicians and practitioners. Many spam filtering techniques have made considerable progress in recent years. The predominant approaches include data mining methods and machine learning methods. Researchers have largely focused on either one of the approaches since a unified framework is still lacking. To fill the gap in the literature, this paper inherits the credit-assignment problem by proposing a collaborative learning framework that could credit or blame each selected heterogeneous technique. The results of this study indicate that the collaborative learning framework is simple and comprehensible. In addition, we found that the framework offers a principle solution to combine heterogeneous individual technique to collaborative filtering for anti-spam problems.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A comparative performance study of feature selection methods for the anti-spam filtering domain</title>
	<abstract>In this paper we analyse the strengths and weaknesses of the mainly used feature selection methods in text categorization when they are applied to the spam problem domain. Several experiments with different feature selection methods and content-based filtering techniques are carried out and discussed. Information Gain, χ2-text, Mutual Information and Document Frequency feature selection methods have been analysed in conjunction with Naïve Bayes, boosting trees, Support Vector Machines and ECUE models in different scenarios. From the experiments carried out the underlying ideas behind feature selection methods are identified and applied for improving the feature selection process of SpamHunting, a novel anti-spam filtering software able to accurate classify suspicious e-mails.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Adaptive e-mails intention finding system based on words social networks</title>
	<abstract>Although many anti-spam techniques have been proposed till date, a foolproof solution for overcoming spam has not been found yet. Spammers still spread spam by using invariant intentions such as advertising and phishing; these intentions are difficult to detect using signature-based or content-based spam filters. In this study, we have proposed an adaptive e-mail intention finding system based on the E-mail Word Social Network (EWSN) that can detect the e-mails' intention and can adaptively and continually learn. EWSN is a data structure used for profiling a user's intentions through solicited and unsolicited e-mails. The EWSNs are constructed on the basis of the information in the user's mailbox and the expanded social relations of words obtained via search engines on the World Wide Web. Unlike previous approaches of spam filters, our system only requires a small amount of training data and it can be trained through feedback incrementally. Experimental quantitative results demonstrate that the misclassification rate, precision rate, and recall rate are better than several content-based filtering methods using a limited amount of training data. The quantitative results also demonstrate that the proposed method has good detection ability in the case of novel spam e-mail detection, without constantly updating the pattern of novel spam e-mails. The proposed method - capable of intention profiling and continual adaptation - is robust for detecting spam e-mails.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Applying cost-sensitive multiobjective genetic programming to feature extraction for spam e-mail filtering</title>
	<abstract>In this paper we apply multiobjective genetic programming to the cost-sensitive classification task of labelling spam e-mails. We consider three publicly-available spam corpora and make comparison with both support vector machines and naïve Bayes classifiers, both of which are held to perform well on the spam filtering problem. We find that for the high cost ratios of practical interest, our cost-sensitive multiobjective genetic programming gives the best results across a range of performance measures.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A comparative study for content-based dynamic spam classification using four machine learning algorithms</title>
	<abstract>The growth of email users has resulted in the dramatic increasing of the spam emails during the past few years. In this paper, four machine learning algorithms, which are Naive Bayesian (NB), neural network (NN), support vector machine (SVM) and relevance vector machine (RVM), are proposed for spam classification. An empirical evaluation for them on the benchmark spam filtering corpora is presented. The experiments are performed based on different training set size and extracted feature size. Experimental results show that NN classifier is unsuitable for using alone as a spam rejection tool. Generally, the performances of SVM and RVM classifiers are obviously superior to NB classifier. Compared with SVM, RVM is shown to provide the similar classification result with less relevance vectors and much faster testing time. Despite the slower learning procedure, RVM is more suitable than SVM for spam classification in terms of the applications that require low complexity.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Spam email filtering using network-level properties</title>
	<abstract>Spam is serious problem that affects email users (e.g. phishing attacks, viruses and time spent reading unwanted messages). We propose a novel spam email filtering approach based on network-level attributes (e.g. the IP sender geographic coordinates) that are more persistent in time when compared to message content. This approach was tested using two classifiers, Naive Bayes (NB) and Support Vector Machines (SVM), and compared against bag-of-words models and eight blacklists. Several experiments were held with recent collected legitimate (ham) and non legitimate (spam) messages, in order to simulate distinct user profiles from two countries (USA and Portugal). Overall, the network-level based SVM model achieved the best discriminatory performance. Moreover, preliminary results suggests that such method is more robust to phishing attacks.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Cooperative anti-spam system based on multilayer agents</title>
	<abstract>Spam is unsolicited bulk email which is extremely annoying to the recipients and the ISPs. However, most of the traditional spam filtering methods commonly neglect the bulk character of spam. This paper proposes a model of cooperative anti-spam system based on multilayer agents. We compared our model to the state-of-the-art and found that our model achieved better performance and robustness on several known corpora.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An empirical study of three machine learning methods for spam filtering</title>
	<abstract>The increasing volumes of unsolicited bulk e-mail (also known as spam) are bringing more annoyance for most Internet users. Using a classifier based on a specific machine-learning technique to automatically filter out spam e-mail has drawn many researchers' attention. This paper is a comparative study the performance of three commonly used machine learning methods in spam filtering. On the other hand, we try to integrate two spam filtering methods to obtain better performance. A set of systematic experiments has been conducted with these methods which are applied to different parts of an e-mail. Experiments show that using the header only can achieve satisfactory performance, and the idea of integrating disparate methods is a promising way to fight spam.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Anomaly Detection in Dynamic Systems Using Weak Estimators</title>
	<abstract>Anomaly detection involves identifying observations that deviate from the normal behavior of a system. One of the ways to achieve this is by identifying the phenomena that characterize “normal” observations. Subsequently, based on the characteristics of data learned from the “normal” observations, new observations are classified as being either “normal” or not. Most state-of-the-art approaches, especially those which belong to the family of parameterized statistical schemes, work under the assumption that the underlying distributions of the observations are stationary. That is, they assume that the distributions that are learned during the training (or learning) phase, though unknown, are not time-varying. They further assume that the same distributions are relevant even as new observations are encountered. Although such a “stationarity” assumption is relevant for many applications, there are some anomaly detection problems where stationarity cannot be assumed. For example, in network monitoring, the patterns which are learned to represent normal behavior may change over time due to several factors such as network infrastructure expansion, new services, growth of user population, and so on. Similarly, in meteorology, identifying anomalous temperature patterns involves taking into account seasonal changes of normal observations. Detecting anomalies or outliers under these circumstances introduces several challenges. Indeed, the ability to adapt to changes in nonstationary environments is necessary so that anomalous observations can be identified even with changes in what would otherwise be classified as “normal” behavior. In this article we propose to apply a family of weak estimators for anomaly detection in dynamic environments. In particular, we apply this theory to spam email detection. Our experimental results demonstrate that our proposal is both feasible and effective for the detection of such anomalous emails.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Density-based spam detector</title>
	<abstract>The volume of mass unsolicited electronic mail, often known as spam, has recently increased enormously and has become a serious threat to not only the Internet but also to society. This paper proposes a new spam detection method which uses document space density information. Although it requires extensive e-mail traffic to acquire the necessary information, an unsupervised learning engine with a short white list can achieve a 98% recall rate and 100% precision. A direct-mapped cache method contributes handling of over 13,000 e-mails per second. Experimental results, which were conducted using over 50 million actual e-mails of traffic, are also reported in this paper.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Extracting user preference from Web browsing behaviour for spam filtering</title>
	<abstract>We focus on user behaviour that most e-mail users browse the web. In this paper, we attempt to exploit user preference extracted from the behaviour in a spam filtering method. The method reduces troublesome maintenance of the filter, since it keeps track of user preference as ham words in background. The ham words are used to determine whether a received e-mail is a ham or not. The method can detect some spams which are hard to classify correctly by existing Bayesian filters. We show that a combination of a Bayesian filter and our method reduces the number of false negatives.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>e-mail authentication system: a spam filtering for smart senders</title>
	<abstract>In the present time, the electronic mail is the most popular communication method of people around the world. However, the increasing of electronic mails is the beginning of a malevolent aftereffect e-mail. The transgressors have developed a set of codes that is the electronic bomb of the electronic mails passing through the communication line. As a result, various protection methods and software have been implemented. However, miscellaneous problems caused by spams still remain. Therefore, this paper proposed a novel method, called EMAS, to certify delivered mails. The benefits obtained from this system do not only solve the spam problem, but also untie the indirect effects from spams that no other methods have been missed.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Practical learning from one-sided feedback</title>
	<abstract>In many data mining applications, online labeling feedback is only available for examples which were predicted to belong to the positive class. Such applications includespam filtering in the case where users never checkemails marked "spam", document retrieval where users cannotgive relevance feedback on unretrieved documents,and online advertising where user behavior cannot beobserved for unshown advertisements. One-sided feedback can cripple the performance of classical mistake-driven online learners such as Perceptron. Previous work under the Apple Tasting framework showed how to transform standard online learners into successful learners from one sided feedback. However, we find in practice that this transformation may request more labels than necessary to achieve strong performance. In this paper,we employ two active learning methods which reduce the number of labels requested in practice. One method is the use of Label Efficient active learning. The other method,somewhat surprisingly, is the use of margin-based learners without modification, which we show combines implicit active learning and a greedy strategy to managing the exploration exploitation tradeoff. Experimental results show that these methods can be significantly more effective in practice than those using the Apple Tasting transformation, even on minority class problems.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Content-based mobile spam classification using stylistically motivated features</title>
	<abstract>The feature of brevity in mobile phone messages makes it difficult to distinguish lexical patterns to identify spam. This paper proposes a novel approach to spam classification of extremely short messages using not only lexical features that reflect the content of a message but new stylistic features that indicate the manner in which the message is written. Experiments on two mobile phone message collections in two different languages show that the approach outperforms previous content-based approaches significantly, regardless of language.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Fuzzy logic for e-mail spam deduction</title>
	<abstract>In this Information era, most of the communication is happening through e-mails. Many emails contain web spam, as the transaction through this internet is affected by Passive attacks and Active attacks. Recently, several algorithms are developed partially for detecting spam emails. Therefore there is a need for improving the performance of the spam-detecting algorithm. In this proposed work fuzzy rules are defined and applied to all incoming emails. Based on the result of the various rules against user attitude, input email is classified as spam or ham. This method outperforms the existing spam-detecting algorithms in terms of accuracy and user friendly.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An intelligent technique to detect file formats and e-mail spam</title>
	<abstract>With the everyday increasing importance of privacy, security, and wise use of computational resources, the corresponding technologies are increasingly being faced with the problem of file type detection. In digital forensic, there are numerous file formats in use. Criminals have started using either non-standard file formats or changing extensions of files while storing or transmitting them over a network. This makes recovering data out of these files difficult. An extension to the file name with the file type is stored in the disk directory, but when a file is deleted, the entry for the file in the directory may be overwritten and hence quite difficult to identify its type which is serious issue in computer forensics. But if the fragment of file has its header information containing type identifying information the mentioned problem may be solved. But it is difficult to identify the type of fragment from the middle or if the header information is deleted or unavailable the identification becomes more complex. This paper focuses on identifying the file types addressing the various scenarios of file type being changed by the malicious user to send some confidential or sensitive information by changing the file type (say.exe banned by Gmail can be converted to any acceptable format and sent across). E-mail spam has become an epidemic problem that can negatively affect the usability of electronic mail as a communication means. Besides wasting users' time and effort to scan and delete the massive amount of junk e-mails received, it consumes network bandwidth and storage space, slows down email servers, and provides a medium to distribute harmful and/or offensive content. Inspired by the success of fuzzy similarity in text classification and document retrieval, the approach investigates its effectiveness in filtering spam based on the textual content of e-mail messages.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>BSPNN: boosted subspace probabilistic neural network for email security</title>
	<abstract>In the modern age of Internet connectivity, advanced information systems have accumulated huge volumes of data. Such fast growing, tremendous amount of data, collected and stored in large databases has far exceeded our human ability to comprehend without proper tools. There has been a great deal of research conducted to explore the potential applications of Machine Learning technologies in Security Informatics. This article studies the Network Security Detection problems in which predictive models are constructed to detect network security breaches such as spamming. Due to overwhelming volume of data, complexity and dynamics of computer networks and evolving cyber threats, current security systems suffer limited performance with low detection accuracy and high number of false alarms. To address such performance issues, a novel Machine Learning algorithm, namely Boosted Subspace Probabilistic Neural Network (BSPNN), has been proposed which combines a Radial Basis Function Neural Network with an innovative diversity-based ensemble learning framework. Extensive empirical analyses suggested that BSPNN achieved high detection accuracy with relatively small computational complexity compared with other conventional detection methods.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>BioDR: Semantic indexing networks for biomedical document retrieval</title>
	<abstract>In Biomedical research, retrieving documents that match an interesting query is a task performed quite frequently. Typically, the set of obtained results is extensive containing many non-interesting documents and consists in a flat list, i.e., not organized or indexed in any way. This work proposes BioDR, a novel approach that allows the semantic indexing of the results of a query, by identifying relevant terms in the documents. These terms emerge from a process of Named Entity Recognition that annotates occurrences of biological terms (e.g. genes or proteins) in abstracts or full-texts. The system is based on a learning process that builds an Enhanced Instance Retrieval Network (EIRN) from a set of manually classified documents, regarding their relevance to a given problem. The resulting EIRN implements the semantic indexing of documents and terms, allowing for enhanced navigation and visualization tools, as well as the assessment of relevance for new documents.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Geotag propagation in social networks based on user trust model</title>
	<abstract>In the past few years sharing photos within social networks has become very popular. In order to make these huge collections easier to explore, images are usually tagged with representative keywords such as persons, events, objects, and locations. In order to speed up the time consuming tag annotation process, tags can be propagated based on the similarity between image content and context. In this paper, we present a system for efficient geotag propagation based on a combination of object duplicate detection and user trust modeling. The geotags are propagated by training a graph based object model for each of the landmarks on a small tagged image set and finding its duplicates within a large untagged image set. Based on the established correspondences between these two image sets and the reliability of the user, tags are propagated from the tagged to the untagged images. The user trust modeling reduces the risk of propagating wrong tags caused by spamming or faulty annotation. The effectiveness of the proposed method is demonstrated through a set of experiments on an image database containing various landmarks.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Adversarial Web Search</title>
	<abstract>Web search engines have become indispensable tools for finding content. As the popularity of the Web has increased, the efforts to exploit the Web for commercial, social, or political advantage have grown, making it harder for search engines to discriminate between truthful signals of content quality and deceptive attempts to game search engines' rankings. This problem is further complicated by the open nature of the Web, which allows anyone to write and publish anything, and by the fact that search engines must analyze ever-growing numbers of Web pages. Moreover, increasing expectations of users, who over time rely on Web search for information needs related to more aspects of their lives, further deepen the need for search engines to develop effective counter-measures against deception.

In this monograph, we consider the effects of the adversarial relationship between search systems and those who wish to manipulate them, a field known as "Adversarial Information Retrieval". We show that search engine spammers create false content and misleading links to lure unsuspecting visitors to pages filled with advertisements or malware. We also examine work over the past decade or so that aims to discover such spamming activities to get spam pages removed or their effect on the quality of the results reduced.

Research in Adversarial Information Retrieval has been evolving over time, and currently continues both in traditional areas (e.g., link spam) and newer areas, such as click fraud and spam in social media, demonstrating that this conflict is far from over.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Identifying Web Spam with the Wisdom of the Crowds</title>
	<abstract>Combating Web spam has become one of the top challenges for Web search engines. State-of-the-art spam-detection techniques are usually designed for specific, known types of Web spam and are incapable of dealing with newly appearing spam types efficiently. With user-behavior analyses from Web access logs, a spam page-detection algorithm is proposed based on a learning scheme. The main contributions are the following. (1) User-visiting patterns of spam pages are studied, and a number of user-behavior features are proposed for separating Web spam pages from ordinary pages. (2) A novel spam-detection framework is proposed that can detect various kinds of Web spam, including newly appearing ones, with the help of the user-behavior analysis. Experiments on large-scale practical Web access log data show the effectiveness of the proposed features and the detection framework.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Application of genetic optimized artificial immune system and neural networks in spam detection</title>
	<abstract>Spam is a serious universal problem which causes problems for almost all computer users. This issue not only affects normal users of the internet, but also causes a big problem for companies and organizations since it costs a huge amount of money in lost productivity, wasting users' time and network bandwidth. There are many studies on spam indicates that spam costs organizations billions of dollars yearly. This work presents a lot of modification on a machine learning method inspired by the human immune system called artificial immune system (AIS) which is a new emerging method that still needs more investigations and demonstrations. Core modifications were applied on the standard AIS with the aid of the Genetic Algorithm (GA). Also Artificial Neural Network (ANN) for spam detection is applied in a new manner. SpamAssassin corpus is used in all our simulations. In standard AIS several user defined parameters are used such as culling of old lymphocytes. Genetic optimized AIS is used to present culling time instead of using user defined value. Also, a new idea to check antibodies in AIS is introduced. This would make the system able to accept types of messages that were previously considered as spam. The idea is accomplished by introducing a new issue which we call ''rebuild time''. Moreover, an adaptive weighting of lymphocytes is used to modify selection opportunities for different gene fragments. In this work also, core modifications on ANN neurons are applied; these modifications allow neurons to be changed over time replacing useless layers. This approach is called Continuous Learning Approach Artificial Neural Network, CLA_ANN. The final results are compared and analyzed. Results show that both systems, optimized spam detection using GA and spam detection using ANN, achieved promising scores comparable to standard AIS and other known methods.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Socio-technical defense against voice spamming</title>
	<abstract>Voice over IP (VoIP) is a key enabling technology for migration of circuit-switched PSTN (Public Switched Telephone Network) architectures to packet-based networks. One problem of the present VoIP networks is filtering spam calls referred to as SPIT (Spam over Internet Telephony). Unlike spam in e-mail systems, VoIP spam calls have to be identified in real time. Many of the techniques devised for e-mail spam detection rely upon content analysis, and in the case of VoIP, it is too late to analyze the content (voice) as the user would have already attended the call. Therefore, the real challenge is to block a spam call before the telephone rings. In addition, we believe it is imperative that spam filters integrate human behavioral aspects to gauge the legitimacy of voice calls. We know that, when it comes to receiving or rejecting a voice call, people use the social meaning of trust, reputation, friendship of the calling party and their own mood. In this article, we describe a multi-stage, adaptive spam filter based on presence (location, mood, time), trust, and reputation to detect spam in voice calls. In particular, we describe a closed-loop feedback control between different stages to decide whether an incoming call is spam. We further propose formalism for voice-specific trust and reputation analysis. We base this formal model on a human intuitive behavior for detecting spam based on the called party's direct and indirect relationships with the calling party. No VoIP corpus is available for testing the detection mechanism. Therefore, for verifying the detection accuracy, we used a laboratory setup of several soft-phones, real IP phones and a commercial-grade proxy server that receives and processes incoming calls. We experimentally validated the proposed filtering mechanisms by simulating spam calls and measured the filter's accuracy by applying the trust and reputation formalism. We observed that, while the filter blocks a second spam call from a spammer calling from the same end IP host and domain, the filter needs only a maximum of three calls---even in the case when spammer moves to a new host and domain. Finally, we present a detailed sensitivity analysis for examining the influence of parameters such as spam volume and network size on the filter's accuracy.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Understanding the network-level behavior of spammers</title>
	<abstract>This paper studies the network-level behavior of spammers, including: IP address ranges that send the most spam, common spamming modes (e.g., BGP route hijacking, bots), how persistent across time each spamming host is, and characteristics of spamming botnets. We try to answer these questions by analyzing a 17-month trace of over 10 million spam messages collected at an Internet "spam sinkhole", and by correlating this data with the results of IP-based blacklist lookups, passive TCP fingerprinting information, routing information, and botnet "command and control" traces.We find that most spam is being sent from a few regions of IP address space, and that spammers appear to be using transient "bots" that send only a few pieces of email over very short periods of time. Finally, a small, yet non-negligible, amount of spam is received from IP addresses that correspond to short-lived BGP routes, typically for hijacked prefixes. These trends suggest that developing algorithms to identify botnet membership, filtering email messages based on network-level properties (which are less variable than email content), and improving the security of the Internet routing infrastructure, may prove to be extremely effective for combating spam.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Review: A review of machine learning approaches to Spam filtering</title>
	<abstract>In this paper, we present a comprehensive review of recent developments in the application of machine learning algorithms to Spam filtering, focusing on both textual- and image-based approaches. Instead of considering Spam filtering as a standard classification problem, we highlight the importance of considering specific characteristics of the problem, especially concept drift, in designing new filters. Two particularly important aspects not widely recognized in the literature are discussed: the difficulties in updating a classifier based on the bag-of-words representation and a major difference between two early naive Bayes models. Overall, we conclude that while important advancements have been made in the last years, several aspects remain to be explored, especially under more realistic evaluation settings.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Workload models of spam and legitimate e-mails</title>
	<abstract>This article presents an extensive characterization of a spam-infected e-mail workload. The study aims at identifying and quantifying the characteristics that significantly distinguish spam from non-spam (i.e., legitimate) traffic, assessing the impact of spam on the aggregate traffic, providing data for creating synthetic workload models, and drawing insights into more effective spam detection techniques. Our analysis reveals significant differences in the spam and non-spam workloads. We conjecture that these differences are consequence of the inherently different mode of operation of the e-mail senders. Whereas legitimate e-mail transmissions are driven by social bilateral relationships, spam transmissions are a unilateral spammer-driven action.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Trusting spam reporters: A reporter-based reputation system for email filtering</title>
	<abstract>Spam is a growing problem; it interferes with valid email and burdens both email users and service providers. In this work, we propose a reactive spam-filtering system based on reporter reputation for use in conjunction with existing spam-filtering techniques. The system has a trust-maintenance component for users, based on their spam-reporting behavior. The challenge that we consider is that of maintaining a reliable system, not vulnerable to malicious users, that will provide early spam-campaign detection to reduce the costs incurred by users and systems. We report on the utility of a reputation system for spam filtering that makes use of the feedback of trustworthy users. We evaluate our proposed framework, using actual complaint feedback from a large population of users, and validate its spam-filtering performance on a collection of real email traffic over several weeks. To test the broader implication of the system, we create a model of the behavior of malicious reporters, and we simulate the system under various assumptions using a synthetic dataset.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A spam rejection scheme during SMTP sessions based on layer-3 e-mail classification</title>
	<abstract>This paper proposes a scheme that rejects spam e-mails during their simple mail transfer protocol (SMTP) sessions. This scheme utilizes a layer-3 e-mail classification technique, which allows e-mail classes to be estimated before the end of SMTP sessions at receiving e-mail servers. We analyze the proposed scheme using discrete-time Markov chain analysis under varying e-mail traffic loads and service capacities. This paper also considers the effects of e-mail retransmission and illegal spam relaying by zombie systems on the performance of the proposed scheme. Results from our analysis show that e-mail server loading decreases by using the proposed technique. This allows the reduction in non-spam e-mail queuing delays and loss probability. Our scheme also protects e-mail servers from being overloaded by spam traffic and, if performed collectively over the Internet, it is capable of performing outbound spam control.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Targeting spam control on middleboxes: Spam detection based on layer-3 e-mail content classification</title>
	<abstract>This paper proposes a spam detection technique, at the packet level (layer 3), based on classification of e-mail contents. Our proposal targets spam control implementations on middleboxes. E-mails are first pre-classified (pre-detected) for spam on a per-packet basis, without the need for reassembly. This, in turn, allows fast e-mail class estimation (spam detection) at receiving e-mail servers to support more effective spam handling on both inbound and outbound (relayed) e-mails. In this paper, the naive Bayes classification technique is adapted to support both pre-classification and fast e-mail class estimation, on a per-packet basis. We focus on evaluating the accuracy of spam detection at layer 3, considering the constraints on processing byte-streams over the network, including packet re-ordering, fragmentation, overlapped bytes, and different packet sizes. Results show that the proposed layer-3 classification technique gives less than 0.5% false positive, which approximately equals the performance attained at layer 7. This shows that classifying e-mails at the packet level could differentiate non-spam from spam with high confidence for a viable spam control implementation on middleboxes.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Suspended accounts in retrospect: an analysis of twitter spam</title>
	<abstract>In this study, we examine the abuse of online social networks at the hands of spammers through the lens of the tools, techniques, and support infrastructure they rely upon. To perform our analysis, we identify over 1.1 million accounts suspended by Twitter for disruptive activities over the course of seven months. In the process, we collect a dataset of 1.8 billion tweets, 80 million of which belong to spam accounts. We use our dataset to characterize the behavior and lifetime of spam accounts, the campaigns they execute, and the wide-spread abuse of legitimate web services such as URL shorteners and free web hosting. We also identify an emerging marketplace of illegitimate programs operated by spammers that include Twitter account sellers, ad-based URL shorteners, and spam affiliate programs that help enable underground market diversification.

Our results show that 77% of spam accounts identified by Twitter are suspended within on day of their first tweet. Because of these pressures, less than 9% of accounts form social relationships with regular Twitter users. Instead, 17% of accounts rely on hijacking trends, while 52% of accounts use unsolicited mentions to reach an audience. In spite of daily account attrition, we show how five spam campaigns controlling 145 thousand accounts combined are able to persist for months at a time, with each campaign enacting a unique spamming strategy. Surprisingly, three of these campaigns send spam directing visitors to reputable store fronts, blurring the line regarding what constitutes spam on social networks.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Awarded Best Paper! - Scalable Centralized Bayesian Spam Mitigation with Bogofilter</title>
	<abstract>Bayesian content filters gained popular acclaim when they were put forward in 2002 by Paul Graham as a potential long-term solution for the spam problem. They have since fallen from the limelight, however, due to perceived attack vulnerabilities inherent to all content-based filters as well as real and imagined vulnerabilities specific to Bayesian filters. It has also been assumed that Bayesian filters would be problematic to implement in centralized or large environments due to wordlist management issues. This paper revisits the effectiveness of Bayesian filters as a sustainable singular spam solution for mid- to large-sized environments through a real-world study of the deployment and operation of the Bogofilter Robinson-Fisher Bayesian classification utility in a production mail environment servicing thousands of accounts. Our implementation strategy and methodology as well as our results are described in detail so that they can be evaluated and replicated if desired. Other filtering methodologies which were previously implemented in this environment are also discussed for comparison purposes, though they have since been removed from production due primarily to lack of need. Bayesian classification has been able to solve the spam problem for this user population for the present and observable future, with a single wordlist, and with no secondary spam filtering techniques employed. Significantly, only two business-related legitimate messages have been reported as blocked due to filter misclassification since Bogofilter was deployed.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Thwarting E-mail Spam Laundering</title>
	<abstract>Laundering e-mail spam through open-proxies or compromised PCs is a widely-used trick to conceal real spam sources and reduce spamming cost in the underground e-mail spam industry. Spammers have plagued the Internet by exploiting a large number of spam proxies. The facility of breaking spam laundering and deterring spamming activities close to their sources, which would greatly benefit not only e-mail users but also victim ISPs, is in great demand but still missing. In this article, we reveal one salient characteristic of proxy-based spamming activities, namely packet symmetry, by analyzing protocol semantics and timing causality. Based on the packet symmetry exhibited in spam laundering, we propose a simple and effective technique, DBSpam, to online detect and break spam laundering activities inside a customer network. Monitoring the bidirectional traffic passing through a network gateway, DBSpam utilizes a simple statistical method, Sequential Probability Ratio Test, to detect the occurrence of spam laundering in a timely manner. To balance the goals of promptness and accuracy, we introduce a noise-reduction technique in DBSpam, after which the laundering path can be identified more accurately. Then DBSpam activates its spam suppressing mechanism to break the spam laundering. We implement a prototype of DBSpam based on libpcap, and validate its efficacy on spam detection and suppression through both theoretical analyses and trace-based experiments.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A study of spam filtering using support vector machines</title>
	<abstract>Electronic mail is a major revolution taking place over traditional communication systems due to its convenient, economical, fast, and easy to use nature. A major bottleneck in electronic communications is the enormous dissemination of unwanted, harmful emails known as spam emails. A major concern is the developing of suitable filters that can adequately capture those emails and achieve high performance rate. Machine learning (ML) researchers have developed many approaches in order to tackle this problem. Within the context of machine learning, support vector machines (SVM) have made a large contribution to the development of spam email filtering. Based on SVM, different schemes have been proposed through text classification approaches (TC). A crucial problem when using SVM is the choice of kernels as they directly affect the separation of emails in the feature space. This paper presents thorough investigation of several distance-based kernels and specify spam filtering behaviors using SVM. The majority of used kernels in recent studies concern continuous data and neglect the structure of the text. In contrast to classical kernels, we propose the use of various string kernels for spam filtering. We show how effectively string kernels suit spam filtering problem. On the other hand, data preprocessing is a vital part of text classification where the objective is to generate feature vectors usable by SVM kernels. We detail a feature mapping variants in TC that yield improved performance for the standard SVM in filtering task. Furthermore, to cope for realtime scenarios we propose an online active framework for spam filtering. We present empirical results from an extensive study of online, transductive, and online active methods for classifying spam emails in real time. We show that active online method using string kernels achieves higher precision and recall rates.</abstract>
	<search_task_number>12</search_task_number>
	<query>comparison spam filtering methods</query>
	<relevance>1</relevance>
  </item>



  <item>
    <title>Dynamic simulation model for planning physical distribution systems: Discussion of the computer model</title>
	<abstract>The general class of problem considered in this paper is that of long-range planning of physical distribution systems. The physical distribution activity includes design and administration of systems to control raw material and finished goods flow from manufacturing source to point of consumption.2 From an analytical viewpoint, a physical distribution system consists of several interactive activity centers or subsystems among which tradeoffs in cost and service exist. These subsystems are often referred to as the components of a physical distribution system. In this research the classification of components includes: facility network, inventory allocations, communications, transportation, and unitization. With the exception of unitization, these components and the relative range of system design alternatives are familiar to the reader. Unitization, in a broad sense, involves material handling, packaging, and containerization.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>The B-and-E model for adaptive wormhole routing</title>
	<abstract>In this paper we present a model named B-and-E (Basic-and-Extended) that can be conveniently used to design adaptable routing schemes for wormhole routing with a relatively low cost. The key idea is to divide channels into two separate groups: basic channels that are responsible for deadlock freedom, and extended channels that are in charge of adaptability. Applying the B-visibility:visible;  color: r-E Model to the well-known k-ary n-cube mesh topology, we construct a fully adaptable routing scheme with only two virtual channels sharing one physical channel. The simulation results demonstrate that, with respect to communication throughput and transfer latency, the new routing scheme indeed provides a superior performance. To explore the routing flexibility more efficiently, a heuristic policy called 2-Step Scoreboard is introduced, resulting in a further improvement.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>High Fidelity Haptic Rendering of Frictional Contact with Deformable Objects in Virtual Environments using Multi-rate Simulation</title>
	<abstract>Haptics is an increasingly common modality in human-computer interfacing. The focus of this paper is the problem arising from the difference between the high sampling rate requirements of haptic interfaces and the significantly lower update rates for physical models simulated in virtual environments. This is a critical problem, especially for applications involving haptic manipulation of deformable objects simulated in virtual environments, such as in surgical simulation. In this paper, a multi-rate simulation approach was developed to address this problem. The proposed method employs linear low-order approximations to model the inter-sample behavior of the high-order non-linear deformable object models. The basic method is also extended to achieve high-fidelity rendering of haptic manipulations involving sliding-type frictional contact. The proposed approach uses a local geometric model in addition to the local dynamic model, and performs collision detection and response as part of the high update rate haptic loop. Experimental results that validate the proposed methods are also presented.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Computational-Physical State Co-regulation in Cyber-Physical Systems</title>
	<abstract>From the perspective of physical system feedback control, the cyber or computer system's role has been to sample and compute control inputs sufficiently fast to maintain acceptable reference command tracking and disturbance rejection in the physical system. This strategy has been successful given the relatively low computational overhead for most control laws compared to computational resource availability. However, in many emerging applications this requirement may be insufficient, not because the computer is incapable of high-speed computations but instead because either more complex computations are required or because processor or network speed must be minimized to conserve energy. We propose the augmentation of traditional physical state models with a computational model to enable a cyber-physical system to co-regulate physical and computational actuation. Ultimately, our goal is to balance resources of the cyber system with quality of control of the physical system to provide a more energy-conscious CPS. As a first step, we propose a continuous-time representation of computational state and derive a continuous "dynamics" model approximation. Next, we propose the addition of a computational state into the closed-loop control law for the physical system states. Finally, we augment the derived cyber model with a second-order oscillator and demonstrate control via a LQR controller. In our simulation results, computational state and loop execution rate and oscillator "force" are regulated closed-loop at each control cycle based both on physical and computational state reference commands and errors. Results show that both physical and cyber state can be successfully regulated with the expected degradation in tracking performance as reference computational state (control loop rate) is slowed to values near the stability threshold.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Investigation and a practical compact network model of thermal stress in integrated circuits</title>
	<abstract>This paper first investigates the non-uniform spatial thermal distribution in sub-100nm integrated circuits. A practical network-based Compact Thermal Modeling (CTM) technique has been proposed to model this thermal stress. The proposed technique, which can be integrated with modern CAD tools, represents IC package as a network in which a set of boundary nodes serve as surrogates for physical regions of the chip. In this network a junction node represents the chip's power junction, and arcs permit thermal communication between adjacent nodes. Due to the complex nature of micro- and nano-systems, the discrete models (resulting from the finite element method (FEM) or other methods) are usually very large (of the order of 100,000). Even though modern computers are able to handle engineering problems of this size, the system-level simulation would become prohibitive if the detailed models were directly used. The proposed CTM is condensed using Static Matrix Condensation methodology where the system matrices obtained by the spatial discretization of heat transfer partial differential equation (PDE) are reduced by the elimination of heat sources, surface nodes and internal nodes, however maintaining the thermal communication among the nodes. The formal conversion of governing PDE systems to low-dimensional ordinary differential equation (ODE) systems is to reduce the complexity, and at the same time preserve the accuracy of the model. The reduced system can be used to either synthesize a resistive network or formulate a set of connection equations to be connected to higher simulation levels.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>The effects of the false vocal fold gaps in a model of the Larynx on pressures distributions and flows</title>
	<abstract>Human phonation does not merely depend upon the vibration of the vocal folds. The false vocal fold (FVF), as an important laryngeal constriction, has also been found by more and more research both in clinically and computer simulations that it plays an important role during phonation and contributes significantly to the aerodynamics and sound generation processes of human voice production. Among many parameters which are used to determine and describe the geometry of the false vocal folds, the false vocal fold gap (GFVF), which means the minimal distance between the two false vocal folds, is regarded as an important and dominant parameter. Therefore, this study explores the effects of the FVF gaps on the intralaryngeal pressure distributions, laryngeal resistance and flows using both three-dimensional Plexiglas model and commercially available computational fluid dynamics code.

Three glottal angles, divergent 40°, uniform 0°, and convergent -40°were used for this study to explore the effects of FVF gaps, as they represent the basic glottal shapes typically expected in phonation, the angle values also were typically expected for most phonation in modal Register. A wide variety of FVF gaps (GFVF) were parameterized with 12 different values: 0.02, 0.04, 0.06, 0.08, 0.09, 0.1, 0.11, 0.12, 0.16, 0.2, 0.4, 0.6 cm to represent important geometries often appearing within phonatory vibratory cycles. These gaps were used for each glottal angle. The specific design of the FVFs followed prior literature. The minimal glottal diameter (Dg) was constantly at 0.06 cm in this study for each FVF gaps, and the translaryngeal pressure were held constant at 8 cm H2O. A nonvibrating laryngeal airway Plexiglas model, which had linear dimensions 1.732 times of a normal male larynx, was used in this study. In order to measure pressures inside the Plexiglas model, twelve cylindrical ducts were made on the midline of the laryngeal wall of the model. The diameter of each duct was 0.07 cm human size (0.12 cm in the model), so that the connector of an Entran EPE-551 pressure transducer could fit snugly into the holes. The distance between the centers of each hole was 0.14 cm human size. FLUENT (Fluent, Inc., Lebanon, NH), a commercially available computational fluid dynamics code was also used to obtain estimates of the normal wall pressures along the laryngeal surfaces (including the surfaces of vocal folds, ventricles, and false vocal folds) as a function of the FVF gaps and the glottal angles. The code is based on the control-volume technique and was used to solve the Navier-Stokes equations for constant shapes (not for vibrating vocal folds), laminar, incompressible airflow physics occurring inside the symmetric laryngeal geometries. The flow field was assumed to be symmetric across the midline of the glottis in this study, and therefore only the half flow field was modeled.

The results suggest that (1) the intralaryngeal pressure was lowest and the flow was highest (least flow resistance) when the FVF gap was 1.5-2 times of Dg, the intralaryngeal pressures decreased and flows increased as smaller FVF gaps increased, and the intralaryngeal pressures increased and flows decreased as larger FVF gaps increased, indicating that the least pressure drop for any given flow (that is, the least flow resistance) was found to correspond to the 1.5-2 times of Dg for different glottal angle. Suggesting that the 1.5-2 times of Dg might be the optimal gap for pressure, and efficient phonation may involve laryngeal shaping of this condition. Therefore, the positioning and existing structure of the FVFs can aid in phonation by reducing energy losses and increasing airflow in the larynx when positioned appropriately; (2) both the pressure and flow were unaffected when the FVF gaps larger than 0.4 cm; (3) the divergent glottal angle gave lower pressure and greater flow than the convergent and uniform glottal angle as no FVF conditions; (4) the present of the FVF decreased the effects of the glottal angle on both the intralaryngeal pressure and flow to some extent, and the smaller the FVF gaps, the smaller this effect. Perhaps more important, (5) the present of the FVF also moving the separation points downstream, straitening the glottal jet for a longer distance, decreasing overall laryngeal resistance, and reducing the energy dissipation, suggesting that the FVF would be of importance to efficient voice production; (6) the empirical pressure distributions were supported by computational results. The results suggest that the intralaryngeal pressure distributions and the laryngeal flow resistance are highly affected by the presence of the FVFs, and the FVFs can aid in phonation when by reducing energy losses positioned appropriately. Therefore, the results might be helpful not only in maintaining healthy vocal habits, but also in exploring surgical and rehabilitative intervention of related voice problem. The results also suggest that they may be incorporated in the phonatory models (physical or computational) for better understanding of vocal mechanics.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Robust simulation of lamprey tracking</title>
	<abstract>Biologically realistic computer simulation of vertebrates is a challenging problem with exciting applications in computer graphics and robotics. Once the mechanics of locomotion are available it is interesting to mediate this locomotion with higher level behavior such as target tracking. One recent approach simulates a relatively simple vertebrate, the lamprey, using recurrent neural networks to model the central pattern generator of the spine and a physical model for the body. Target tracking behavior has also been implemented for such a model. However, previous approaches suffer from deficiencies where particular orientations of the body to the target cause the central pattern generator to shutdown. This paper describes an approach to making target tracking more robust.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>The UCLA AGCM in high performance computing environments</title>
	<abstract>General Circulation Models (GCMs) are at the top of the hierarchy of numerical models that are used to study the Earth's climate. To increase the significance of predictions using GCMs requires ensembles of integrations that in turn demand large amounts of computing resources. GCMs codes are particularly difficult to optimize in view of their heterogeneity. In this paper we focus on code optimization for GCMs of the atmosphere (AGCMs), one of the major components of the climate system.In this paper, we present our efforts in optimizing the parallel UCLA AGCM code. The UCLA AGCM is a state-of-the-art finite-difference model of the global atmosphere. Our optimization efforts include the implementation of load balancing schemes, new physical parameterizations of atmospheric processes, code restructuring and use of special mathematical functions. At the beginning of this work, the overall execution time of the code was 459 seconds per simulated day in 256 nodes of a CRAY T3D. At present, the same model configuration requires 51 seconds per simulated day in 256 nodes of a CRAY T3E-900, which is approximately 9 times faster. The peak model performance is about 40 GFLOPs on 512 T3E-900 nodes. We present results in support of our conclusion that major advances in our ability to carry out longer and more detailed climate simulations depend primarily upon development of more powerful supercomputers and that code optimization, for a particular computer architecture, and development of more efficient algorithms can be nearly as important.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Towards Configurable Real-Time Hybrid Structural Testing: A Cyber-Physical System Approach</title>
	<abstract>Real-time hybrid testing of civil structures represents agrand challenge in the emerging area of cyber-physical systems. Hybrid testing improves significantly on either purely numerical or purely empirical approaches by integrating physical structural components and computational models. Actuator dynamics, complex interactions among computers and physical components, and computation and communication delays all hamper the ability to conduct accurate tests. To address these challenges, this paper presents initial work towards a Cyber-physical Instrument for Real-time hybrid Structural Testing (CIRST). CIRST aims to provide two salient features: a highly configurable architecture for integrating computers and physical components; and system support for real-time operations in distributed hybrid testing. This paper presents the motivation of the CIRST architectureand preliminary test results from a proof-of-concept implementation that integrates a simple structural element and simulation model. CIRST will have broad impacts on thefields of both civil engineering and real-time computing.It will enable high-fidelity real-time hybrid testing of awide range of civil infrastructures, and will also providea high-impact cyber-physical application for the study andevaluation of real-time middleware.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>CartaBlanca— a pure-Java, component-based systems simulation tool for coupled non-linear physics on unstructured grids</title>
	<abstract>This paper describes a component-based non-linear physical system simulation prototyping package written entirely in Java using object-oriented design to provide scientists and engineers a “developer-friendly” software environment for large-scale computational method and physical model development. The software design centers on the Jacobian-Free Newton-Krylov solution method surrounding a finite-volume treatment of conservation equations. This enables a clean component-based implementation. We first provide motivation for the development of the software and then describe software structure. Discussion of software structure includes a description of the use of Java's built-in thread facility that enables data-parallel, shared-memory computations on a wide variety of unstructured grids with triangular, quadrilateral, tetrahedral and hexahedral elements. We also discuss the use of Java's inheritance mechanism in the construction of a hierarchy of physics-systems objects and linear and non-linear solver objects that simplify development and foster software re-use. As a compliment to the discussion of these object hierarchies, we provide a brief review of the Jacobian-Free Newton-Krylov nonlinear system solution method and discuss how it fits into our design. Following this, we show results from preliminary calculations and then discuss future plans including the extension of the software to distributed memory computer systems.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Real time physics: class notes</title>
	<abstract>Physically based simulation is a significant and active research field in computer graphics. It has emerged in the late eighties out of the need to make animations more physically plausible and to free the animator from explicitly specifying the motion of complex passively moving objects. In the early days, quite simple approaches were used to model physical behavior such as mass-spring networks or particle systems. Later, more and more sophisticated models borrowed from computational sciences were adopted. The computational sciences appeared decades before computer graphics with the goal of replacing real world experiments with simulations on computers. In contrast, the aim of using physical simulations in graphics was, and still is, the reproduction of the visual properties of physical processes for special effects in commercials and movies. Computer generated special effects have replaced earlier methods such as stop-motion frame-by-frame animation and offer almost unlimited possibilities.

Meanwhile, the rapid growth of the computational power of CPUs and GPUs in recent years has enabled real time simulation of physical effects. This possibility has opened the door to an entirely new world, a world in which the user can interact with the virtual physical environment. Real time physical simulations have become one of the main next-generation features of computer games. Allowing user interaction with physical simulations also poses new challenging research problems. In this class we address such problems and present basic as well as state-of-the-art methods for physically based simulation in real time.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Integrating Dynamic and Geometry Model Components through Ontology-Based Inference</title>
	<abstract>Modeling techniques tend to be found in isolated communities: geometry models in computer-aided design (CAD) and computer graphics, dynamic models in computer simulation, and information models in information technology. When models are included within the same digital environment, the ways of connecting them together seamlessly and visually are not well known even though elements from each model have many commonalities. A model is required to be able to connect models together in the interface; however, creating such models is time-consuming. This article addresses this deficiency by studying specific ways in which models can be interconnected within the same 3D space through effective ontology construction and human interaction techniques. The authors have developed a method for automatically constructing a human-computer interface model (i.e., which is termed an interaction model) from an ontology for an example physical environment. The work to date has resulted in an environment based on a 3D modeling and animation package, permitting users to explore dynamic model structure through interactions with geometric scene structure.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Cumulvs: Interacting with High-Performance Scientific Simulations, for Visualization, Steering and Fault Tolerance</title>
	<abstract>High-performance computer simulations are an increasingly popular alternative or complement to physical experiments or prototypes. However, as these simulations grow more massive and complex, it becomes challenging to monitor and control their execution. CUMULVS is a middleware infrastructure for visualizing and steering scientific simulations while they are running. Front-end "viewers" attach dynamically to simulation programs, to extract and collect intermediate data values, even if decomposed over many parallel tasks. These data can be graphically viewed or animated in a variety of commercial or custom visualization environments using a provided viewer library. In response to this visual feedback, scientists can "close the loop" and apply interactive control using computational steering of any user-defined algorithmic or model parameters. The data identification interfaces and gathering protocols can also be applied for parallel data exchange in support of coupled simulations, and for application-directed collection of key program data in checkpoints, for automated restart in response to software or hardware failures. CUMULVS was originally based on PVM, but interoperates well with simulations that use MPI or other parallel environments. Several alternate messaging systems are being integrated with CUMULVS to ease its applicability, e.g. to MPI. CUMULVS has recently been integrated with the Common Component Architecture (CCA) for visualization and parallel data redistribution (referred to as "MxN"), and also with Global Arrays. This paper serves as a comprehensive overview of the CUMULVS capabilities, their usage, and their development over several years.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>The Role of Modeling and Asynchronous Distributed Simulation in Analyzing Complex Systems of the Future</title>
	<abstract>The word simulate implies to imitate or to mimic while the word modeling refers to a small object, usually built to scale, that represents some existing object. Although the art of mimicking and modeling may be traced back to the beginning of civilization, with the emergence of computers, a few decades ago, the art of modeling and simulation experienced a remarkable transformation. The computational intelligence of the computer imparted the ability to encapsulate and simulate specific characteristics of not only living and inanimate objects but abstract concepts. While the human brain is equally capable of simulating abstract concepts, the precision and speed of the computers are unparalleled and they impart computer modeling and simulation a qualitative jump in its capability and fidelity. Also, while intimately connected to each other, modeling refers to the notion of representing the desired behavior of the target object or idea in the host computer and simulation refers to the execution of the model on a host computer. Today, towards the end of the twentieth century, the nature of modeling and simulation is undergoing another radical transformation. The emergence of economical and powerful computers coupled with the ability to network a large number of computers, promises the ability to model and simulate complex, real-world systems, that are rapidly becoming commonplace in the society, successfully and with a high degree of fidelity. Already, today's real-world systems including complex banking, credit-card transaction, transportation, ground-based communication, and space-based tele-communication systems defy the analytical modeling that had characterized the efforts over the past three decades. Virtually all analytical studies are severely restricted in the number of variables and the number of interacting units that may be modeled. Tomorrow's systems are expected to be far more complex, implying that modeling and large-scale simulation may be the most logical and, often, the only vehicle to study them objectively. This paper presents a fundamental analysis of the nature of complex physical and natural processes that will increasingly constitute the challenging problems of the future. It then develops a set of principles for modeling complex systems. Finally, it examines the role of asynchronous, distributed simulation in the study of a number of real-world systems. In general, modeling and simulation enables one to detect design errors, prior to developing a prototype, in a cost effective manner, identify potential problems during system operation, detect rare and elusive errors, and investigate hypothetical concepts that do not exist in nature. Upon execution, the simulation provides greater quantitative and qualitative insight into the behavior of the actual system. In addition, an asynchronous, distributed simulation, executing on a loosely-coupled parallel processor, closely resembles the actual, operational system, yielding results that potentially reflect reality, as close as possible. Furthermore, elements of the simulation code that emulate the system behavior may be transferred directly onto the operational system with minimal modifications. The knowledge encapsulated in this paper, has been derived from a number of actual case studies involving the modeling and simulation of a number of real-world problems.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Discrete Event Front-tracking Simulation of a Physical Fire-spread Model</title>
	<abstract>Simulation of moving interfaces such as a fire front usually requires resolution of a large-scale and detailed domain. Such computing involves the use of supercomputers to process the large amount of data and calculations. This limitation is mainly due to the fact that a large scale of space and time is usually split into nodes, cells, or matrices and the solving methods often require small time steps. In this paper we present a novel method that enables the simulation of large-scale/highresolution systems by focusing on the interface and its application to fire-spread simulation. Unlike the conventional explicit and implicit integration schemes, it is based on the discrete-event approach, which describes time advance in terms of increments of physical quantities rather than discrete time stepping. In addition, space is not split into discrete nodes or cells, but we use polygons with real coordinates. The system is described by the behavior of its interface and evolves by computing collision events of this interface in the simulation. As this simulation technique is suitable for a class of models that can explicitly provide the rate of spread, we developed a radiation-based propagation model of wild land fire. Simulations of a real large-scale fire performed by implementation of our method provide very interesting results in less than 30 s with a 3-m resolution with current personal computers.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>
  
  <item>
  <title>A simulation-based virtual environment to study cooperative robotic systems</title>
	<abstract>Simulation plays important roles in experimenting with, understanding, and evaluating the performance of cooperative robotic systems. Typically, simulation-based studies of robotic systems are conducted on the computer, without involving any real system components. This paper presents a new hybrid approach to simulation that allows real robots as well as robot models to work together in a simulation-based virtual environment. This capability of robot-in-the-loop simulation effectively bridges conventional simulation and real system execution, augmenting them to constitute an incremental study and measurement process. It is especially useful for large-scale cooperative robotic systems whose complexity and scalability severely limit experimentations in a physical environment using real robots. We present the architecture of this simulation-based virtual environment that, together with an incremental study process and associated experimental frames, can support systematic analysis of cooperative robotic systems. An example of robotic convoy is provided. Some measurement metrics are developed and simulation results are described. An experimental setup for robot-in-the-loop simulation is discussed.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Challenges of biological realism and validation in simulation-based medical education</title>
	<abstract>Overview: Simulation, both physical and computer-based, has a rich history in support of medical education. Essentially all these efforts have been aimed at instilling concrete measurable skills, akin to vocational training. They present learners with choices, facilitating a degree of learning by doing. The sets of learner choices are usually limited, with choices clearly classified into ''right'' and ''wrong''. But much of medicine is not much like a multiple-choice test. The realm of choices is broad and not always easily converted to a short list. The ''correct'' answer is not always known by the experienced physician beforehand, sometimes not even after the die is cast and the future unfolds. Computer simulation of human disease and its treatment can in principle be tremendously useful in the education of both basic and clinical scientists. This paper describes some challenges in the construction of simulation-based ''liberal arts'' biomedical education. Objectives: The educator attempting to develop a learning environment based on simulation of biology faces some special challenges. The challenges addressed in this paper are: face validity and deep validity; finding the right degree of realism; authoring biomedical models efficiently; managing randomness. To illustrate the issues, we trace the history of the Oncology Thinking Cap throughout several versions and expansions of educational objectives, and describe the detection and remediation of shortcomings related to these issues. Design: Dealing effectively with issues of validity and realism can be accomplished if the acquisition of information driving and justifying the model development choices is documented, preferably automatically, during the process. Efficiency in authoring is greatly enhanced by judicious modularity to encourage re-use, and by the use of templated statements rather than raw code or exotic graphical components to represent the instructions driving the model. Randomness can be used to familiarize learners with the true relative proportions of types of cases, or to enrich the encountered cases with rarer but more instructive cases. When a learner repeats an encounter with a scenario while changing a single option, proper management of randomness is essential to avoid artifacts of random number generators. Otherwise an outcome change caused by a shift in random number streams may masquerade as an outcome change due to the changed option. Conclusion: Effective use of computer simulation of human disease and its treatment for biomedical education faces daunting obstacles, but these problems can be solved.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>
  
  <item>
  <title>Simulating the dynamics of auroral phenomena</title>
	<abstract>Simulating natural phenomena has always been a focal point for computer graphics research. Its importance goes beyond the production of appealing presentations, since research in this area can contribute to the scientific understanding of complex natural processes. The natural phenomena, known as the Aurora Borealis and Aurora Australis, are geomagnetic phenomena of impressive visual characteristics and remarkable scientific interest. Aurorae present a complex behavior that arises from interactions between plasma (hot, ionized gases composed of ions, electrons, and neutral atoms) and Earth's electromagnetic fields. Previous work on the visual simulation of auroral phenomena have focused on static physical models of their shape, modeled from primitives, like sine waves. In this article, we focus on the dynamic behavior of the aurora, and we present a physically-based model to perform 3D visual simulations. The model takes into account the physical parameters and processes directly associated with plasma flow, and can be extended to simulate the dynamics of other plasma phenomena as well as astrophysical phenomena. The partial differential equations associated with these processes are solved using a complete multigrid implementation of the electromagnetic interactions, leading to a simulation of the shape and motion of the auroral displays. In order to illustrate the applicability of our model, we provide simulation sequences rendered using a distributed forward mapping approach.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>
  
  <item>
  <title>A mixed reality approach for interactively blending dynamic models with corresponding physical phenomena</title>
	<abstract>The design, visualization, manipulation, and implementation of models for computer simulation are key parts of the discipline. Models are constructed as a means to understand physical phenomena as state changes occur over time. One issue that arises is the need to correlate models and their components with the phenomena being modeled. For example, a part of an automotive engine needs to be placed into cognitive context with the diagrammatic icon that represents that part's function. A typical solution to this problem is to display a dynamic model of the engine in one window and the engine's CAD model in another. Users are expected to, on their own, mentally blend the dynamic model and the physical phenomenon into the same context. However, this contextualization is not trivial in many applications.

Our approach expands upon this form of user interaction by specifying two ways in which dynamic models and the corresponding physical phenomena may be viewed, and experimented with, within the same human interaction space. We present a methodology and implementation of contextualization for diagram-based dynamic models using an anesthesia machine, and then follow up with a human study of its effects on spatial cognition.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>
  
  <item>
  <title>Technical Section: Developmental modelling with SDS</title>
	<abstract>This paper describes modelling methods based on biological development for use in computer graphics applications, specifically the automated growth and development of complex organic shapes that are difficult to model directly. We examine previous approaches, including grammar-based methods, embedded systems and cellular models. Each system can be classified as endogenous (internally determined) or exogenous (externally determined), with some models exhibiting features of both. We then introduce a new model, the Simplicial Developmental System (SDS), which simulates individual cells embedded in a physical environment, with cell division, movement and growth controlled by morphogenetic chemical simulation. SDS uses a tetrahedral mesh as its base representation for geometric modelling and physical simulation. Cell growth, movement and division are determined by simulating chemical morphogens that are diffused between cells according to a set of user defined rules. We discuss the advantages and disadvantages of this model in terms of the competing goals of user control, developmental complexity and open-ended development (the ability to generate new component structures without explicit specification). Examples highlighting the strengths of the model are illustrated.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  
  <item>
  <title>Determining over- and under-constrained systems of equations using structural constraint delta</title>
	<abstract>Computer aided modeling and simulation of complex physical systems, using components from multiple application domains, such as electrical, mechanical, and hydraulic, have in recent years witnessed a significant growth of interest. In the last decade, equation-based object-oriented (EOO) modeling languages, (e.g. Modelica, gPROMS, and VHDL-AMS) based on acausal modeling using Differential Algebraic Equations (DAEs), have appeared. With such languages, it is possible to model physical systems at a high level of abstraction by using reusable components.A model in an EOO language needs to have the same number of equations as unknowns. A previously unsolved problem concerning this property is the efficient detection of over- or under-constrained models in the case of separately compiled models.This paper describes a novel technique to determine over- and under-constrained systems of equations in models, based on a concept called structural constraint delta. In many cases it is also possible to locate the source of the constraint-problem. Our approach makes use of static type checking and consists of a type inference algorithm. We have implemented it for a subset of the Modelica language, and successfully validated it on several examples.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Interactive virtual try-on clothing design systems</title>
	<abstract>Clothing computer design systems include three integrated parts: garment pattern design in 2D/3D, virtual try-on and realistic clothing simulation. Some important results have been obtained in pattern design and clothing simulation since the 1980s. However, in the area of virtual try-on, only limited methods have been proposed which are applicable to some defined garment styles or under restrictive sewing assumptions. This paper presents a series of new techniques from virtually sewing up complex garment patterns on human models to visualizing design effects through physical-based real-time simulation. We first employ an hierarchy of ellipsoids to approximate human models in which the bounding ellipsoids are optimized recursively. We also present a new scheme for including contact friction and resolving collisions. Four types of user interactive operation are introduced to manipulate cloth patterns for pre-positioning, virtual sewing and later obtaining cloth simulation. In the cloth simulation, we propose a simplified cloth dynamic model and an integration scheme to realize a high quality real-time cloth simulation. We demonstrate the robustness of our proposed systems by complex garment style virtual try-on and cloth simulation.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Physical simulation for animation and visual effects: parallelization and characterization for chip multiprocessors</title>
	<abstract>We explore the emerging application area of physics-based simulation for computer animation and visual special effects. In particular, we examine its parallelization potential and characterize its behavior on a chip multiprocessor (CMP). Applications in this domain model and simulate natural phenomena, and often direct visual components of motion pictures. We study a set of three workloads that exemplify the span and complexity of physical simulation applications used in a production environment: fluid dynamics, facial animation, and cloth simulation. They are computationally demanding, requiring from a few seconds to several minutes to simulate a single frame; therefore, they can benefit greatly from the acceleration possible with large scale CMPs.

Starting with serial versions of these applications, we parallelize code accounting for at least 96% of the serial execution time, targeting a large number of threads.We then study the most expensive modules using a simulated 64-core CMP.

For the code representing key modules, we achieve parallel scaling of 45x, 50x, and 30x for fluid, face, and cloth simulations, respectively. The modules have a spectrum of parallel task granularity and locking behavior, and all but one are dominated by loop-level parallelism. Many modules operate on streams of data. In some cases, modules iterate over their data, leading to significant temporal locality. This streaming behavior leads to very high on-die and main memory bandwidth requirements. Finally, most modules have little inter-thread communication since they are data-parallel, but a few require heavy communication between data-parallel operations.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Volumetric parameterization and trivariate B-spline fitting using harmonic functions</title>
	<abstract>We present a methodology based on discrete volumetric harmonic functions to parameterize a volumetric model in a way that it can be used to fit a single trivariate B-spline to data so that simulation attributes can also be modeled. The resulting model representation is suitable for isogeometric analysis [Hughes, T.J., Cottrell, J.A., B., Y., 2005. Isogeometric analysis: Cad, finite elements, nurbs, exact geometry, and mesh refinement. Computer Methods in Applied Mechanics and Engineering 194, 4135-4195]. Input data consists of both a closed triangle mesh representing the exterior geometric shape of the object and interior triangle meshes that can represent material attributes or other interior features. The trivariate B-spline geometric and attribute representations are generated from the resulting parameterization, creating trivariate B-spline material property representations over the same parameterization in a way that is related to [Martin, W., Cohen, E., 2001. Representation and extraction of volumetric attributes using trivariate splines. In: Symposium on Solid and Physical Modeling, pp. 234-240] but is suitable for application to a much larger family of shapes and attributes. The technique constructs a B-spline representation with guaranteed quality of approximation to the original data. Then we focus attention on a model of simulation interest, a femur, consisting of hard outer cortical bone and inner trabecular bone. The femur is a reasonably complex object to model with a single trivariate B-spline since the shape overhangs make it impossible to model by sweeping planar slices. The representation is used in an elastostatic isogeometric analysis, demonstrating its ability to suitably represent objects for isogeometric analysis.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>
  
  <item>
  <title>Overview of the Software Design of the Community Climate System Model</title>
	<abstract>The Community Climate System Model (CCSM) is a computer model for simulating the Earth's climate. The CCSM is built from four individual component models for the atmosphere, ocean, land surface, and sea ice. The notion of a physical/dynamical component of the climate system translates directly to the software component structure. Software design of the CCSM is focused on the goals of modularity, extensibility, and performance portability. These goals are met at both the component level and within the individual component models. Performance portability is the ability of a code to achieve good performance across a variety of computer architectures while maintaining a single source code. As a community model, the CCSM must run on a variety of machine architectures and must perform well on all these architectures for computationally intensive climate simulations.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>A CAD-oriented cloth simulation system with stable and efficient ODE solution</title>
	<abstract>Cloth modelling is a research field of increasing interest both for computer animation and computer-aided design purposes. Aiming at being closer to cloth manufacturers needs, this work proposes an integration of a particle-based modelling approach within a CAD oriented system for real design of apparel and complex-shaped cloth. The particle-based model, originally defined on single textile layers, is extended to take into account the physical effects of seams and darts, construction constraints, layered parts, and other manufacturing processes that contribute to the final shape of the garments. Starting from an initial 3D configuration of the particle grid consistent with mannequin and support geometries, the physical simulation process is then computed, based on constrained Newtonian dynamics laws. Stable and efficient algorithms for the time discretization of the Newtonian ODE problem are proposed, based on implicit and semi-implicit BDF(2) or hybrid Euler-BDF(2) techniques. As applications, several test models are considered, simulated on rigid supports or mannequins, for both numerical comparisons and global shape analysis, from simple cloth geometries up to complex-shaped models for real apparel manufacturing.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Controlling individual agents in high-density crowd simulation</title>
	<abstract>Simulating the motion of realistic, large, dense crowds of autonomous agents is still a challenge for the computer graphics community. Typical approaches either resemble particle simulations (where agents lack orientation controls) or are conservative in the range of human motion possible (agents lack psychological state and aren't allowed to 'push' each other). Our HiDAC system (for High-Density Autonomous Crowds) focuses on the problem of simulating the local motion and global wayfinding behaviors of crowds moving in a natural manner within dynamically changing virtual environments. By applying a combination of psychological and geometrical rules with a social and physical forces model, HiDAC exhibits a wide variety of emergent behaviors from agent line formation to pushing behavior and its consequences; relative to the current situation, personalities of the individuals and perceived social density.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Limits to Modeling: Balancing Ambition and Outcome in Astrophysics and Cosmology</title>
	<abstract>Computer simulations of complex physical objects and processes for which data are very sparse or inexistent have become a major tool of scientific investigation in astrophysics and cosmology. However, one must ask how these simulations acquire their epistemic credentials and whether their realistic ambition is legitimate. A close look at two model-building processes - one in galactic astrophysics, the other in cosmology - reveals heretofore underappreciated features of simulations, such as path dependency. This article argues that such features undermine our confidence in the outcomes of the simulation. Case studies presented here reveal a general tension in computer simulation between realistic ambitions and the possibility of empirical confirmation. The analysis will thus lead to a reassessment of the epistemic goals actually achieved by composite models of complex astrophysical and cosmological phenomena.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>
  
  <item>
  <title>Defining predictive maturity for validated numerical simulations</title>
	<abstract>The increasing reliance on computer simulations in decision-making motivates the need to formulate a commonly accepted definition for ''predictive maturity.'' The concept of predictive maturity involves quantitative metrics that could prove useful while allocating resources for physical testing and code development. Such metrics should be able to track progress (or lack thereof) as additional knowledge becomes available and is integrated into the simulations for example, through the addition of new experimental datasets during model calibration, and/or through the implementation of better physics models in the codes. This publication contributes to a discussion of attributes that a metric of predictive maturity should exhibit. It is contended that the assessment of predictive maturity must go beyond the goodness-of-fit of the model to the available test data. We firmly believe that predictive maturity must also consider the ''knobs,'' or ancillary variables, used to calibrate the model and the degree to which physical experiments cover the domain of applicability. The emphasis herein is placed on translating the proposed attributes into mathematical properties, such as the degree of regularity and asymptotic limits of the maturity function. Altogether these mathematical properties define a set of constraints that the predictive maturity function must satisfy. Based on these constraints, we propose a Predictive Maturity Index (PMI). Physical datasets are used to illustrate how the PMI quantifies the maturity of the non-linear, Preston-Tonks-Wallace model of plastic deformation applied to beryllium, a light-weight, high-strength metal. The question ''does collecting additional data improve predictive power?'' is answered by computing the PMI iteratively as additional experimental datasets become available. The results obtained reflect that coverage of the validation domain is as important to predictive maturity as goodness-of-fit. The example treated also indicates that the stabilization of predictive maturity can be observed, provided that enough physical experiments are available.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Physical modeling of biomolecular computers: Models, limitations, and experimental validation</title>
	<abstract>A principal challenge facing the development and scaling of biomolecular computers is the design of physically well-motivated, experimentally validated simulation tools. In particular, accurate simulations of computational behavior are needed to establish the feasibility of new architectures, and to guide process implementation, by aiding strand design. Key issues accompanying simulator development include model selection, determination of appropriate level of chemical detail, and experimental validation. In this work, each of these issues is discussed in detail, as presented at the workshop on simulation tools for biomolecular computers (SIMBMC), held at the 2003 Congress on Evolutionary Computation. The three major physical models commonly applied to model biomolecular processes, namely molecular mechanics, chemical kinetics, and statistical thermodynamics, are compared and contrasted, with a focus on the potential of each to simulate various aspects of biomolecular computers. The fundamental and practical limitations of each approach are considered, along with a discussion of appropriate chemical detail, at the biopolymer, process, and system levels. The relationship between system analysis and design is addressed, and formalized via the DNA Strand Design problem (DSD). Finally, the need for experimental validation of both underlying parameter sets and overall predictions is discussed, along with illustrative examples.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  
  <item>
  <title>Cyber attack modeling and simulation for network security analysis</title>
	<abstract>Cyber security methods are continually being developed. To test these methods many organizations utilize both virtual and physical networks which can be costly and time consuming. As an alternative, in this paper, we present a simulation modeling approach to represent computer networks and intrusion detection systems (IDS) to efficiently simulate cyber attack scenarios. The outcome of the simulation model is a set of IDS alerts that can be used to test and evaluate cyber security systems. In particular, the simulation methodology is designed to test information fusion systems for cyber security that are under development.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>
  
  <item>
  <title>Animating oscillatory motion with overlap: wiggly splines</title>
	<abstract>Oscillatory motion is ubiquitous in computer graphics, yet existing animation techniques are ill-suited to its authoring. We introduce a new type of spline for this purpose, known as a "Wiggly Spline." The spline generalizes traditional piecewise cubics when its resonance and damping are set to zero, but creates oscillatory animation when its resonance and damping are changed. The spline provides a combination of direct manipulation and physical realism. To create overlapped and propagating motion, we generate phase shifts of the Wiggly Spline, and use these to control appropriate degrees of freedom in a model. The phase shifts can be created directly by procedural techniques or through a paint-like interface. A further option is to derive the phase shifts statistically by analyzing a time-series of a simulation. In this case, the Wiggly Spline makes it possible to canonicalize a simulation, generalize it by providing frequency and damping controls and control it through direct manipulation.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>
  
  <item>
  <title>Solid and physical modeling with chain complexes</title>
	<abstract>In this paper we show that the (co)chain complex associated with a decomposition of the computational domain, commonly called a mesh in computational science and engineering, can be represented by a block-bidiagonal matrix that we call the Hasse matrix. Moreover, we show that topology-preserving mesh refinements, produced by the action of (the simplest) Euler operators, can be reduced to multi-linear transformations of the Hasse matrix representing the complex.

Our main result is a new representation of the (co)chain complex underlying field computations, a representation that provides new insights into the transformations induced by local mesh refinements. This paper is a further contribution towards bridging the subject of computer representations for solid and physical modeling---which flourished border-line between computer graphics, engineering mechanics and computer science with its own methods and data structures---under the general cover of linear algebra and algebraic topology. The main advantage of such an approach is that topology, geometry and physics may coexist in one and the same formalized framework, concurring together to define, represent and simulate the behavior of the model.

Our approach is based on first principles and is general in that it applies to most representational domains that can be characterized as cell complexes, without any restrictions on their type, dimension, codimension, orientability, manifoldness, connectedness. Contrary to what might appear at first sight, the theoretical complexity of the present approach is not greater than that of current methods, provided that sparse-matrix techniques with double element access (by rows and by columns) are employed. Last but not least, our tensorbased approach is a significant step forward in achieving close integration of geometrical representations and physics-based simulations, i.e., in the concurrent modeling of shape and behavior.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Density models for streamer discharges: Beyond cylindrical symmetry and homogeneous media</title>
	<abstract>Streamer electrical discharges are often investigated with computer simulations of density models (also called reaction-drift-diffusion models). We review these models, detailing their physical foundations, their range of validity and the most relevant numerical algorithms employed in solving them. We focus particularly on schemes of adaptive refinement, used to resolve the multiple length scales in a streamer discharge without a high computational cost. We then report recent results from these models, emphasizing developments that go beyond cylindrically symmetrical streamers propagating in homogeneous media. These include interacting streamers, branching streamers and sprite streamers in inhomogeneous media.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>HEMET: Mathematical model of biochemical pathways for simulation and prediction of HEpatocyte METabolism</title>
	<abstract>Many computer studies and models have been developed in order to simulate cell biochemical pathways. The difficulty of integrating all the biochemical reactions that occur in a cell in a single model is the main reason for the poor results in the prediction and simulation of cell behaviour under different chemical and physical stimuli. In this paper we have translated biochemical reactions into differential equations for the development of modular model of metabolism of a hepatocyte cultured in static and standard conditions (in a plastic multiwell placed in an incubator at 37^oC with 5% of CO"2). Using biochemical equations and energetic considerations a set of non-linear differential equations has been derived and implemented in Simulink^(R). This set of equations mimics some of the principal metabolic pathways of biomolecules present in the culture medium. The software platform developed is subdivided into separate modules, each one describing a different metabolic pathway; they constitute a library which can be used for developing new modules and models to project, predict and validate cell behaviour in vitro.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>A hybrid approach for multiresolution modeling of large-scale scientific data</title>
	<abstract>Simulations of complex scientific phenomena involve the execution of massively parallel computer programs. These simulation programs generate large-scale multidimensional data sets over the spatio-temporal region. Analyzing such massive data sets is an essential step in helping scientists glean new information. To this end, efficient and effective data models are needed. In this paper, we present a hybrid approach for constructing data models from large-scale multidimensional scientific data sets. Our models not only provide descriptive information about the data but also allow users to subsequently examine the data by querying the data models. Our approach combines a multiresolution-topological model of the data with a multivariate-physical model of the data to generate one hierarchical data model that efficiently captures both the spatio-temporal and the physical aspects of the data. In particular, this hybrid approach consists of three phases. In the first phase, we build a multiresolution model that encapsulates the data set's spatial information (i.e., topology and spatial connectivity). In the second phase, we build a multivariate model from the physical dimensions of the data set. Physical dimensions refer to those dimensions that are neither spatial (x, y, z) nor temporal (time). The exclusion of the spatial-temporal dimensions from the clustering phase is important since "similar" characteristics could be located (spatially) far from each other. Finally, in the third phase, we connect the multivariate-physical model to the multiresolution-topological model by utilizing ideas from information retrieval. The third phase is essential since the multivariate-physical model does not contain any topological information (without which the model does not have accurate spatial context information). Experimental evaluations on two large-scale multidimensional scientific data sets illustrate the value of our hybrid approach.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>
  
  <item>
  <title>Efficient Runtime Environment for Coupled Multi-physics Simulations: Dynamic Resource Allocation and Load-Balancing</title>
	<abstract>Coupled Multi-Physics simulations, such as hybrid CFD-MD simulations, represent an increasingly important class of scientific applications. Often the physical problems of interest demand the use of high-end computers, such as TeraGrid resources, which are often accessible only via batch-queues. Batch-queue systems are not developed to natively support the coordinated scheduling of jobs – which in turn is required to support the concurrent execution required by coupled multi-physics simulations. In this paper we develop and demonstrate a novel approach to overcome the lack of native support for coordinated job submission requirement associated with coupled runs. We establish the performance advantages arising from our solution, which is a generalization of the Pilot-Job concept – which in of itself is not new, but is being applied to coupled simulations for the first time. Our solution not only overcomes the initial co-scheduling problem, but also provides a dynamic resource allocation mechanism. Support for such dynamic resources is critical for a load balancing mechanism, which we develop and demonstrate to be effective at reducing the total time-to-solution of the problem. We establish that the performance advantage of using Big Jobs is invariant with the size of the machine as well as the size of the physical model under investigation. The Pilot-Job abstraction is developed using SAGA, which provides an infrastructure agnostic implementation, and which can seamlessly execute and utilize distributed resources.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Dealing with space in multi--agent systems: a model for situated MAS</title>
	<abstract>The paper introduces the Multilayered Multi--Agent Situated System (MMASS). The MMASS allows tha description of situated agents that is, agents sensitive to the spatial relationships that determine constraints and abilities for actions as well as privileged cooperation relationships. The main feature of the MMASS model is to give an explicit definition of the spatial structure of the environment in which the system of situated agents acts and interacts. Agent environment, possibly multilayered, can reproduce a physical space. It is nevertheless possible to define a `virtual space' that agents can roam and where they interact. Interactions take place when two or more agents are brought into dynamic spatial relationship through a set of reciprocal actions, and have an influence on the future behavior of the agents. There are several application domains that require the representation of the space in order to take advantage in the multi--agent approach. Typical examples of these domains are the Multi--Agent Based Simulation (MABS) and the Computer Supported Cooperative Work (CSCW). Two specific problems of these domains will be used in the paper to illustrate the MMASS model.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>
  
  <item>
  <title>Robot simulation physics validation</title>
	<abstract>Computer simulation of robot performance is an essential tool for the development of robot software. In order for simulation results to be valid for implementation on real hardware, the accuracy of the simulation model must be verified. If developers use a robot model that is not similar enough to the actual robot, then their results can be meaningless. To ensure the validity of the robot models, NIST proposes standardized test methods that can be easily replicated in both computer simulation and physical form. The actual robot can be tested, and the computer model can be finely tuned to replicate similar performances on equivalent tests. To illustrate this, we have accomplished this task with the Talon Robot on NIST standard test methods.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Optimizing the coupling in parallel air quality model systems</title>
	<abstract>Today, parallel computers facilitate complex simulations of physical and chemical processes. To obtain more accurate results and to include multiple aspects of environmental processes, model codes of different scientific areas are coupled. An often used coupling strategy is to run the individual codes concurrently on disjoint sets of processors, as this keeps the codes mostly independent. However, it is important to improve the workload balance between the codes to achieve a high efficiency on parallel computers. In this paper, the parallel air quality model system LM-MUSCAT is presented. It consists of the chemistry-transport model MUSCAT and the meteorological model LM. Since an adaptive time step control is applied in MUSCAT the overall load fluctuates during runtime, especially at applications with highly dynamical behavior of the simulated processes. This causes load imbalances between both models and, consequently, an inefficient usage of the parallel computer. Therefore, an alternative coupling method is investigated. In this approach, all processors calculate alternately both models, whereby the load is distributed equally. Performance tests show that this ''sequential'' approach is well suited to increase the efficiency of coupled systems that have workload fluctuations in one or more models. In general, load variations can occur in models which use adaptive grid techniques or an adaptive step size control. Systems using such techniques can take benefit from the described coupling approach.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  
  
  <item>
  <title>Quantum Computing and Information Extraction for Dynamical Quantum Systems</title>
	<abstract>We discuss the simulation of complex dynamical systems on a quantum computer. We show that a quantum computer can be used to efficiently extract relevant physical information. It is possible to simulate the dynamical localization of classical chaos and extract the localization length with quadratic speed up with respect to any known classical computation. We can also compute with algebraic speed up the diffusion coefficient and the diffusion exponent, both in the regimes of Brownian and anomalous diffusion. Finally, we show that it is possible to extract the fidelity of the quantum motion, which measures the stability of the system under perturbations, with exponential speed up. The so-called quantum sawtooth map model is used as a test bench to illustrate these results.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>
  
  <item>
  <title>A note on discreteness and virtuality in analog computing</title>
	<abstract>The need for physically motivated discreteness and finiteness conditions emerges in models of both analog and digital computing that are genuinely concerned with physically realizable computational processes. This is brought out by a critical examination of notional analog superTuring devices which involve physically untenable idealizations about the perfect functioning of analog apparatuses and infinite precision of physical measurements. The capability for virtual behaviour, that is, the capability of interpreting, storing, transforming, creating the code, and thereby mimicking the behaviour of (Turing) machines, is used here to introduce a new dimension in the discussion of the analog-digital watershed. In the light of recent results on the analog simulation of digital computing, we examine the role of virtuality as a discriminating factor between these two species of computing, and immerse this problem in the context of natural computing. Is virtuality instantiated in parts of the natural world other than computer technology? This broad issue is examined in connection with the computational modelling of brain and mental information processing.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>
  
  <item>
  <title>Simulation of dye solar cells: through and beyond one dimension</title>
	<abstract>In this work we present a Computer Aided Design (CAD) software, called TiberCAD, to simulate Dye Sensitized Solar Cells (DSC). DSCs are particularly interesting devices due to their high efficiency (more than 11% on small area and 8% on large area) and long stability. Since their first development, much progress has been made in terms of efficiency, stability, lifespan and engineering of the device. However, the field of DSCs still lacks a complete model able to simulate the entire device over a general domain including all its components. In our model a drift-diffusion set of equations for the different charge carriers coupled to Poisson equation has been implemented within finite element method. The model takes into account also trap assisted transport for electrons in the mesoporous titanium dioxide with a phenomenological model derived from multi-trapping model.

Three different applications of the code in 1, 2 and 3D are presented. The first 1D simulation is a study of correlation between physical parameters of the cell and energy conversion efficiency. A second application, 2D, discusses the effect on density and current distributions for different contacting of the cell and loss induced by the shadowing of metallic fingers. Finally, the third case, 3D, presents two different and innovative topologies for a DSC. A cell where contacts and illumination surface are completely decoupled and a DSC wrapped around an optical fiber.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>On the use of the resting potential and level set methods for identifying ischemic heart disease: An inverse problem</title>
	<abstract>The electrical activity in the heart is modeled by a complex, nonlinear, fully coupled system of differential equations. Several scientists have studied how this model, referred to as the bidomain model, can be modified to incorporate the effect of heart infarctions on simulated ECG (electrocardiogram) recordings. We are concerned with the associated inverse problem; how can we use ECG recordings and mathematical models to identify the position, size and shape of heart infarctions? Due to the extreme CPU efforts needed to solve the bidomain equations, this model, in its full complexity, is not well-suited for this kind of problems. In this paper we show how biological knowledge about the resting potential in the heart and level set techniques can be combined to derive a suitable stationary model, expressed in terms of an elliptic PDE, for such applications. This approach leads to a nonlinear ill-posed minimization problem, which we propose to regularize and solve with a simple iterative scheme. Finally, our theoretical findings are illuminated through a series of computer simulations for an experimental setup involving a realistic heart in torso geometry. More specifically, experiments with synthetic ECG recordings, produced by solving the bidomain model, indicate that our method manages to identify the physical characteristics of the ischemic region(s) in the heart. Furthermore, the ill-posed nature of this inverse problem is explored, i.e. several quantitative issues of our scheme are explored.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Geometric and haptic modelling of textile artefacts</title>
	<abstract>Geometric modelling and haptic rendering of textiles is an area of research in which interest has significantly increased over the last decade. A haptic representation is created by adding the physical properties of an object to its geometric configuration. While research has been conducted into geometric modelling of fabrics, current systems require textile data to be manually entered into the computer simulation by a technician. This study explores the possibility of automatic generation of geometric and haptic models of real world textile samples. The development of a scalable and generic methodology for geometric and haptic modelling of plain weave textiles made from wool yarn is reported. This system has been successfully implemented using a step-wise procedure. Initially, an image of the textile artefact is captured. Then the critical features of the image are extracted from the image and deployed in a finite element model. The geometric model is augmented by adding physical properties of the textile and developing the haptic model. Two different haptic rendering procedures are implemented based on Reachin Application Programming Interface 3.2 (API). The developed methodologies are described and results obtained are provided.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Methods for special applications: Cell-DEVS quantization techniques in a fire spreading application</title>
	<abstract>We present the use of the CD++ tool to model and simulate forest fire-spread. A semi-physical fire spread model is implemented using the Cell-DEVS formalism. The use of Cell-DEVS enables proving the correctness of the simulation engines and permits to model the problem even by a non-computer science specialist. The high level language of CD++ reduces the algorithmic complexity for the modeler while allowing complex cellular timing behaviors. Different Cell-DEVS quantization techniques are used and developed to decrease execution time. The study is realized regarding time improvement and trades-off between model evolution, simulation time and incurred error. Finally, based on experimentations, interesting perspectives are defined to develop new quantization techniques.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Development of a data assimilation algorithm</title>
	<abstract>It is important to incorporate all available observations when large-scale mathematical models arising in different fields of science and engineering are used to study various physical and chemical processes. Variational data assimilation techniques can be used in the attempts to utilize efficiently observations in a large-scale model (for example, in order to obtain more reliable initial values). Variational data assimilation techniques are based on a combination of three very important components *numerical methods for solving differential equations, *splitting procedures and *optimization algorithms. It is crucial to select an optimal (or, at least, a good) combination of these three components, because models which are very expensive computationally will become much more expensive (the computing time being often increased by a factor greater than 100) when a variational data assimilation technique is applied. Therefore, it is important to study the interplay between the three components of the variational data assimilation techniques as well as to apply powerful parallel computers in the computations. Some results obtained in the search for a good combination of numerical methods, splitting techniques and optimization algorithms will be reported. Parallel techniques described in [V.N. Alexandrov, W. Owczarz, P.G. Thomsen, Z. Zlatev, Parallel runs of a large air pollution model on a grid of Sun computers, Mathematics and Computers in Simulation, 65 (2004) 557-577] are used in the runs. Modules from a particular large-scale mathematical model, the Unified Danish Eulerian Model (UNI-DEM), are used in the experiments. The mathematical background of UNI-DEM is discussed in [V.N. Alexandrov, W. Owczarz, P.G. Thomsen, Z. Zlatev, Parallel runs of a large air pollution model on a grid of Sun computers, Mathematics and Computers in Simulation, 65 (2004) 557-577, Z. Zlatev, Computer Treatment of Large Air Pollution Models, Kluwer Academic Publishers, Dordrecht, Boston, London, 1995]. The ideas are rather general and can easily be applied in connection with other mathematical models.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Grid-Boxing for Spatial Simulation Performance Optimisation</title>
	<abstract>Computer simulations of complex systems such as physical aggregation processes or swarming and collective behaviour of life-forms, often require order N-squared computational complexity for N microscopic components. This is a significant handicap to simulating systems large enough to compare with real-world experimental data. We discuss space partitioning methods for two such simulation codes and demonstrate complexity improvements by taking advantage of information about locations and interaction distances of the microscopic model components. We present results for a diffusion limited cluster-cluster aggregation code and for an artificial life simulation code. We discuss the data structures necessary to support such algorithms and show how they can be implemented to obtain high performance and maximal simulation productivity for a given computational resource. There are some subtleties in whether such spatial partitioning algorithms should produce a computational complexity of N to some power between 1 and 2 or whether they should be order N log N. We discuss these effects in the context of our data.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>Anisotropic cloth modeling for material fabric</title>
	<abstract>Physically based cloth simulation has been challenging the graphics community for more than three decades. With the developing of virtual reality and clothing CAD, it has become the key technique of virtual garment and try-on system. Although it has received considerable attention in computer graphics, due to its flexible property and realistic feeling that the textile engineers pay much attention to, there is not a successful methodology to simulate cloth both in visual realism and physical accuracy. We present a new anisotropic textile modeling method based on physical mass-spring system, which models the warps and wefts separately according to the different material fabrics. The simulation process includes two main steps: firstly the rigid object simulation and secondly the flexible mass simulation near to be equilibrium. A multiresolution modeling is applied to enhance the tradeoff fruit of the realistic presentation and computation cost. Finally, some examples and the analysis results show the efficiency of the proposed method.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>1</relevance>
  </item>
  
  <item>
  <title>A survey of the modelling and rendering of the earth's atmosphere</title>
	<abstract>One of the extensively researched fields in todays computer graphics are techniques for simulation and visualisation of various natural phenomena. This state of the art report is a survey of the methods for modelling and rendering of the cloudless Earth's atmosphere and related light effects. A physically based lighting model describing the light propagation through the atmosphere is presented. The model takes into account absorption and scattering by particles suspended in the atmosphere and can be used for computing the colour of the sky. Combining the lighting model with the various atmospheric constituents (clear dry air, aerosols, ice crystals) and their properties one can reproduce lighting effects such as rainbow, halo or colour of the sky during the sunset. We also collect the relevant informations of the physical properties of the major atmospheric constituents.</abstract>
	<search_task_number>8</search_task_number>
	<query>computer and physical model simulation</query>
	<relevance>0</relevance>
  </item>



  <item>
    <title>Tradeoffs between branch mispredictions and comparisons for sorting algorithms</title>
	<abstract>Branch mispredictions is an important factor affecting the running time in practice. In this paper we consider tradeoffs between the number of branch mispredictions and the number of comparisons for sorting algorithms in the comparison model. We prove that a sorting algorithm using O(dn log n) comparisons performs Ω(n logdn) branch mispredictions. We show that Multiway MergeSort achieves this tradeoff by adopting a multiway merger with a low number of branch mispredictions. For adaptive sorting algorithms we similarly obtain that an algorithm performing O(dn(1 + log(1 + Inv/n))) comparisons must perform Ω(n logd(1 + Inv/n)) branch mispredictions, where Inv is the number of inversions in the input. This tradeoff can be achieved by GenericSort by Estivill-Castro and Wood by adopting a multiway division protocol and a multiway merging algorithm with a low number of branch mispredictions.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A Comparison of Parallel Sorting Algorithms on Different Architectures</title>
	<abstract>In this paper, we present a comparative performance evaluation of three different parallel sorting algorithms: bitonic sort, sample sort, and parallel radix sort. In order to study the interaction between the algorithms and the architecture, we implemented all the algorithms on three different architectures: a MasPar MP1202, a mesh-connected computer with 2048 processing elements; an nCUBE 2, a message-passing hypercube with 32 processors; and a Sequent Balance, a distributed shared-memory machine with 10 processors. For each machine, we found that the choice of algorithm depends upon the number of elements to be sorted. In addition, as expected, our results show that the relative performance of the algorithms differed on the various machines. It is our hope that our results can be extrapolated to help select appropriate candidates for implementation on machines with architectures similar to those that we have studied. As evidence for this, our findings on the nCUBE 2, a 32 node hypercube, are in accordance with the results obtained by Blelloch {\em et al.} \cite{blmpsz91} on the CM-2, a hypercube with 1024 processors. In addition, preliminary results we have obtained on the SGI Power Challenge, a distributed shared-memory machine, are in accordance with our findings on the Sequent Balance.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A practical performance comparison of parallel sorting algorithms on homogeneous network of workstations</title>
	<abstract>Three parallel sorting algorithms have been implemented and compared in terms of their overall execution time. The algorithms implemented are the odd-even transposition sort, parallel merge sort and parallel rank sort. A homogeneous cluster of workstations has been used to compare the algorithms implemented. The MPI library has been selected to establish the communication and synchronization between the processors. The time complexity for each parallel sorting algorithm will also be mentioned and analyzed.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Comparison of sorting algorithms for multi-fitness measurement of cooperative coevolution</title>
	<abstract>Implementing evaluation of individuals based on multi-fitness has received growing interest in evolution, especially in coevolution, in the past decade. Assigning multi-fitness to an individual was originally suggested in Multi-Objective Evolutionary Algorithms (MOEA). The primary purpose was to find solutions simultaneously optimizing all objectives for a given problem. Non-dominated sorting is an algorithm that has been widely used for multi-fitness measurement both on single-objective and multi-objective problems. In this work, we implement and compare three sorting strategies, greedy sorting, non-dominated sorting and even-distributed sorting, to measure multi-fitness in Cooperative Coevolutionary Genetic Algorithms (CCGA) on single-objective optimization problems, where even-distributed sorting is a new sorting algorithm we propose. We assign multi-fitness to individuals by using a new collaboration mechanism, called reference sharing collaboration. Our experimental results show that by using the novel evaluation model, the modified CCGA achieves better performance than standard CCGA even if the simplest sorting strategy is used. And the even-distributed sorting is able to perform more efficient multi-fitness measurement for cooperative coevolutionary algorithms on single-objective optimization problems.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A Comparison Based Parallel Sorting Algorithm</title>
	<abstract>We present a fast comparison based parallel sorting algorithm that can handle arbitrary key types. Data movement is the major portion of sorting time for most algorithms in the literature. Our algorithm is parameterized so that is can be tuned to control data movement time, especially for large data sets. Parallel histograms are used to partition the key set exactly. The algorithm is architecture independent, and has been implemented in the CHARM portable parallel programming system, allowing it to be efficiently run on virtually any MIMD computer. Performance results for sorting different data sets are presented.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The average complexity of deterministic and randomized parallel comparison sorting algorithms</title>
	<abstract>In practice, the average time of (deterministic or randomized) sorting algorithms seems to be more relevant than the worst case time of deterministic algorithms. Still, the many known complexity bounds for parallel comparison sorting include no nontrivial lower bounds for the average time required to sort by comparisons n elements with p processors (via deterministic or randomized algorithms). We show that for p ≥ n this time is Θ (log n/log(1 + p/n)), (it is easy to show that for p ≤ n the time is Θ (n log n/p) = Θ (log n/(p/n)). Therefore even the average case behaviour of randomized algorithms is not more efficient than the worst case behaviour of deterministic ones.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Optimal Sorting Algorithms for Parallel Computers</title>
	<abstract>The problem of sorting a sequence of n elements on a parallel computer with k processors is considered. The algorithms we present can all be run on a single instruction stream multiple data stream computer. For large n, each achieves an asymptotic speed-up ratio of k with respect to the best sequential algorithm, which is optimal in the number of processors used.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Algorithms for the Generalized Sorting Problem</title>
	<abstract>We study the generalized sorting problem where we are given a set of n elements to be sorted but only a subset of all possible pair wise element comparisons is allowed. The goal is to determine the sorted order using the smallest possible number of allowed comparisons. The generalized sorting problem may be equivalently viewed as follows. Given an undirected graph G(V, E) where V is the set of elements to be sorted and E defines the set of allowed comparisons, adaptively find the smallest subset E¡ä \subseteq E of edges to probe such that the directed graph induced by E¡ä contains a Hamiltonian path. When G is a complete graph, we get the standard sorting problem, and it is well-known that Theta(n log n) comparisons are necessary and sufficient. An extensively studied special case of the generalized sorting problem is the nuts and bolts problem where the allowed comparison graph is a complete bipartite graph between two equal-size sets. It is known that for this special case also, there is a deterministic algorithm that sorts using Theta(n log n) comparisons. However, when the allowed comparison graph is arbitrary, to our knowledge, no bound better than the trivial O(n^2) bound is known. Our main result is a randomized algorithm that sorts any allowed comparison graph using O(n^{3/2}) comparisons with high probability (provided the input is sortable). We also study the sorting problem in randomly generated allowed comparison graphs, and show that when the edge probability is p, O(min{ n/p^2, n^{3/2}\sqrt{p}) comparisons suffice on average to sort.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Genome rearrangements and sorting by reversals</title>
	<abstract>Sequence comparison in molecular biology is in the beginning of a major paradigm shift-a shift from gene comparison based on local mutations to chromosome comparison based on global rearrangements. In the simplest form the problem of gene rearrangements corresponds to sorting by reversals, i.e. sorting of an array using reversals of arbitrary fragments. Kececioglu and Sankoff gave the first approximation algorithm for sorting by reversals with guaranteed error bound and identified open problems related to chromosome rearrangements. One of these problems is Gollan's conjecture on the reversal diameter of the symmetric group. We prove this conjecture and further study the problem of expected reversal distance between two random permutations. We demonstrate that the expected reversal distance is very close to the reversal diameter thereby indicating that reversal distance provides a good separation between related and non-related sequences. The gene rearrangement problem forces us to consider reversals of signed permutations, as the genes in DNA are oriented. Our approximation algorithm for signed permutation provides a 'performance guarantee' of 3/2. Finally, we devise an approximation algorithm for sorting by reversals with a performance ratio of 7/4.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Parallel comparison algorithms for approximation problems</title>
	<abstract>The authors consider that they have n elements from a totally ordered domain and are allowed to perform p parallel comparisons in each time unit (round). They determine, up to a constant factor, the time complexity of several approximation problems in the common parallel comparison tree model of L.G. Valiant, for all admissible values of n, p, and epsilon , where epsilon is an accuracy parameter determining the quality of the required approximation. The problems considered include the approximate maximum problem, approximate sorting, and approximate merging. The results imply, as special cases, all the known results about the time complexity of parallel sorting, parallel merging, and parallel selection of the maximum (in the comparison model). They highlight one very special but representative result concerning the approximate maximum problem. They wish to find, among the given n elements, one which belongs to the biggest n/2, where in each round they are allowed to ask n binary comparisons. They show that log/sup */n+ Theta (1) rounds are both necessary and sufficient in the best algorithm for this problem. </abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Rainbow Sort: Sorting at the Speed of Light</title>
	<abstract>Rainbow Sort is an unconventional method for sorting, which is based on the physical concepts of refraction and dispersion. It is inspired by the observation that light that traverses a prism is sorted by wavelength. At first sight this "rainbow effect" that appears in nature has nothing to do with a computation in the classical sense, still it can be used to design a sorting method that has the potential of running in ¿ (n) with a space complexity of ¿ (n), where n denotes the number of elements that are sorted. In Section 1, some upper and lower bounds for sorting are presented in order to provide a basis for comparisons. In Section 2, the physical background is outlined, the setup and the algorithm are presented and a lower bound for Rainbow Sort of ¿ (n) is derived. In Section 3, we describe essential difficulties that arise when Rainbow Sort is implemented. Particularly, restrictions that apply due to the Heisenberg uncertainty principle have to be considered. Furthermore, we sketch a possible implementation that leads to a running time of O(n+m), where m is the maximum key value, i.e., we assume that there are integer keys between 0 and m. Section 4 concludes with a summary of the complexity and some remarks on open questions, particularly on the treatment of duplicates and the preservation of references from the keys to records that contain the actual data. In Appendix A, a simulator is introduced that can be used to visualise Rainbow Sort.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The Ford--Johnson algorithm still unbeaten for less than 47 elements</title>
	<abstract>Using exhaustive computer search we show that sorting 15 elements requires 42 comparisons, and that for n&lt;47 there is no algorithm of the following form: ''m and n-m elements are sorted using the Ford-Johnson algorithm first, then the sorted sequences are merged'', whose total number of used comparisons is smaller than the number of comparisons used by the Ford-Johnson algorithm to sort n elements directly.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An experimental study of sorting and branch prediction</title>
	<abstract>Sorting is one of the most important and well-studied problems in computer science. Many good algorithms are known which offer various trade-offs in efficiency, simplicity, memory use, and other factors. However, these algorithms do not take into account features of modern computer architectures that significantly influence performance. Caches and branch predictors are two such features and, while there has been a significant amount of research into the cache performance of general purpose sorting algorithms, there has been little research on their branch prediction properties. In this paper, we empirically examine the behavior of the branches in all the most common sorting algorithms. We also consider the interaction of cache optimization on the predictability of the branches in these algorithms. We find insertion sort to have the fewest branch mispredictions of any comparison-based sorting algorithm, that bubble and shaker sort operate in a fashion that makes their branches highly unpredictable, that the unpredictability of shellsort's branches improves its caching behavior, and that several cache optimizations have little effect on mergesort's branch mispredictions. We find also that optimizations to quicksort, for example the choice of pivot, have a strong influence on the predictability of its branches. We point out a simple way of removing branch instructions from a classic heapsort implementation and also show that unrolling a loop in a cache-optimized heapsort implementation improves the predicitability of its branches. Finally, we note that when sorting random data two-level adaptive branch predictors are usually no better than simpler bimodal predictors. This is despite the fact that two-level adaptive predictors are almost always superior to bimodal predictors, in general.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Sorting by merging or merging by sorting?</title>
	<abstract>In the comparison model the only operations allowed on input elements are comparisons and moves to empty cells of memory. We prove the existence of an algorithm that, for any set of s ≤n sorted sequences containing a total of n elements, computes the whole sorted sequence using O(nlogs) comparisons, O(n) data moves and O(1) auxiliary cells of memory besides the ones necessary for the n input elements. The best known algorithms with these same bounds are limited to the particular case s= O(1). From a more intuitive point of view, our result shows that it is possible to pass from merging to sorting in a seamless fashion, without losing the optimality with respect to any of the three main complexity measures of the comparison model. Our main statement has an implication in the field of adaptive sorting algorithms and improves [Franceschini and Geffert, Journal of the ACM, 52], showing that it is possible to exploit some form of pre-sortedness to lower the number of comparisons while still maintaining the optimality for space and data moves. More precisely, let us denote with OptM(X) the cost for sorting a sequence X with an algorithm that is optimal with respect to a pre-sortedness measure M. To the best of our knowledge, so far, for any pre-sortedness measure M, no full-optimal adaptive sorting algorithms were known (see [Estivill-Castro and Wood, ACM Comp. Surveys, 24], page 472). The best that could be obtained were algorithms sorting a sequence X using O(1) space, O(OptM(X)) comparisons and O(OptM(X)) moves. Hence, the move complexity seemed bound to be a function of M(X) (as for the comparison complexity). We prove that there exists a pre-sortedness measure for which that is false: the pre-sortedness measure Runs, defined as the number of ascending contiguous subsequences in a sequence. That follows directly from our main statement, since ${Opt}_{M}(X)=O(\left\vert{X}\right\vert \log Runs(X))$</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An in-place sorting with O(nlog n) comparisons and O(n) moves</title>
	<abstract>We present the first in-place algorithm for sorting an array of size n that performs, in the worst case, at most O(nlog n) element comparisons and O(n) element transports.This solves a long-standing open problem, stated explicitly, for example, in Munro and Raman [1992], of whether there exists a sorting algorithm that matches the asymptotic lower bounds on all computational resources simultaneously.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>In-place sorting</title>
	<abstract>We present an algorithm for asymptotically efficient sorting. Our algorithm sorts the given array A by the use of nċ lg n + O(nċlg lg n) comparisons and O(n) element moves. Moreover, this algorithm works in-place, using only a constant auxiliary workspace. This shrinks the gap between the known information-theoretic lower bound and the existing algorithms to O(nċlg lg n) comparisons, even if we require the algorithm to use only a constant auxiliary memory and a linear number of moves.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>On the adaptiveness of Quicksort</title>
	<abstract>Quicksort was first introduced in 1961 by Hoare. Many variants have been developed, the best of which are among the fastest generic-sorting algorithms available, as testified by the choice of Quicksort as the default sorting algorithm in most programming libraries. Some sorting algorithms are adaptive, i.e., they have a complexity analysis that is better for inputs, which are nearly sorted, according to some specified measure of presortedness. Quicksort is not among these, as it uses Ω(n log n) comparisons even for sorted inputs. However, in this paper, we demonstrate empirically that the actual running time of Quicksort is adaptive with respect to the presortedness measure Inv. Differences close to a factor of two are observed between instances with low and high Inv value. We then show that for the randomized version of Quicksort, the number of element swaps performed is provably adaptive with respect to the measure Inv. More precisely, we prove that randomized Quicksort performs expected O(n(1 + log(1 + Inv/n))) element swaps, where Inv denotes the number of inversions in the input sequence. This result provides a theoretical explanation for the observed behavior and gives new insights on the behavior of Quicksort. We also give some empirical results on the adaptive behavior of Heapsort and Mergesort.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An efficient non-dominated sorting method for evolutionary algorithms</title>
	<abstract>We present a new non-dominated sorting algorithm to generate the non-dominated fronts in multi-objective optimization with evolutionary algorithms, particularly the NSGA-II. The non-dominated sorting algorithm used by NSGA-II has a time complexity of O(MN2) in generating non-dominated fronts in one generation (iteration) for a population size N and M objective functions. Since generating non-dominated fronts takes the majority of total computational time (excluding the cost of fitness evaluations) of NSGA-II, making this algorithm faster will significantly improve the overall efficiency of NSGA-II and other genetic algorithms using non-dominated sorting. The new non-dominated sorting algorithm proposed in this study reduces the number of redundant comparisons existing in the algorithm of NSGA-II by recording the dominance information among solutions from their first comparisons. By utilizing a new data structure called the dominance tree and the divide-and-conquer mechanism, the new algorithm is faster than NSGA-II for different numbers of objective functions. Although the number of solution comparisons by the proposed algorithm is close to that of NSGA-II when the number of objectives becomes large, the total computational time shows that the proposed algorithm still has better efficiency because of the adoption of the dominance tree structure and the divide-and-conquer mechanism.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Generic discrimination: sorting and paritioning unshared data in linear time</title>
	<abstract>We introduce the notion of discrimination as a generalization of both sorting and partitioning and show that worst-case linear-time discrimination functions (discriminators) can be defined generically, by (co-)induction on an expressive language of order denotations. The generic definition yields discriminators that generalize both distributive sorting and multiset discrimination. The generic discriminator can be coded compactly using list comprehensions, with order denotations specified using Generalized Algebraic Data Types (GADTs). A GADT-free combinator formulation of discriminators is also given.

We give some examples of the uses of discriminators, including a new most-significant-digit lexicographic sorting algorithm.

Discriminators generalize binary comparison functions: They operate on n arguments at a time, but do not expose more information than the underlying equivalence, respectively ordering relation on the arguments. We argue that primitive types with equality (such as references in ML) and ordered types (such as the machine integer type), should expose their equality, respectively standard ordering relation, as discriminators: Having only a binary equality test on a type requires Θ(n2) time to find all the occurrences of an element in a list of length n, for each element in the list, even if the equality test takes only constant time. A discriminator accomplishes this in linear time. Likewise, having only a (constant-time) comparison function requires Θ(n log n) time to sort a list of n elements. A discriminator can do this in linear time.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Sorting and searching in the presence of memory faults (without redundancy)</title>
	<abstract>We investigate the design of algorithms resilient to memory faults, i. e., algorithms that, despite the corruption of some memory values during their execution, are able to produce a correct output on the set of uncorrupted values. In this framework, we consider two fundamental problems: sorting and searching. In particular, we prove that any O(nlog n) comparison-based sorting algorithm can tolerate at most O((nlog n)1/2) memory faults. Furthermore, we present one comparison-based sorting algorithm with optimal space and running time that is resilient to O((nlog n)1/3) faults. We also prove polylogarithmic lower and upper bounds on fault-tolerant searching.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Engineering a cache-oblivious sorting algorithm</title>
	<abstract>This paper is an algorithmic engineering study of cache-oblivious sorting. We investigate by empirical methods a number of implementation issues and parameter choices for the cache-oblivious sorting algorithm Lazy Funnelsort and compare the final algorithm with Quicksort, the established standard for comparison-based sorting, as well as with recent cache-aware proposals. The main result is a carefully implemented cache-oblivious sorting algorithm, which, our experiments show, can be faster than the best Quicksort implementation we are able to find for input sizes well within the limits of RAM. It is also at least as fast as the recent cache-aware implementations included in the test. On disk, the difference is even more pronounced regarding Quicksort and the cache-aware algorithms, whereas the algorithm is slower than a careful implementation of multiway Mergesort, such as TPIE.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Randomized minimum spanning tree algorithms using exponentially fewer random bits</title>
	<abstract>For many fundamental problems there exist randomized algorithms that are asymptotically optimal and are superior to the best-known deterministic algorithm. Among these are the minimum spanning tree (MST) problem, the MST sensitivity analysis problem, the parallel connected components and parallel minimum spanning tree problems, and the local sorting and set maxima problems. (For the first two problems there are provably optimal deterministic algorithms with unknown, and possibly superlinear, running times.) One downside of the randomized methods for solving these problems is that they use a number of random bits linear in the size of input. In this article we develop some general methods for reducing exponentially the consumption of random bits in comparison-based algorithms. In some cases we are able to reduce the number of random bits from linear to nearly constant, without affecting the expected running time.

Most of our results are obtained by adjusting or reorganizing existing randomized algorithms to work well with a pairwise or O(1)-wise independent sampler. The prominent exception, and the main focus of this article, is a linear-time randomized minimum spanning tree algorithm that is not derived from the well-known Karger-Klein-Tarjan algorithm. In many ways it resembles more closely the deterministic minimum spanning tree algorithms based on soft heaps. Further, using our algorithm as a guide, we present a unified view of the existing “nongreedy” minimum spanning tree algorithms. Concepts from the Karger-Klein-Tarjan algorithm, such as F-lightness, MST verification, and sampled graphs, are related to the concepts of edge corruption, subgraph contractibility, and soft heaps, which are the basis of the deterministic MST algorithms of Chazelle and Pettie-Ramachandran.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Deductive sort and climbing sort: New methods for non-dominated sorting</title>
	<abstract>In recent years an increasing number of real-world many-dimensional optimisation problems have been identified across the spectrum of research fields. Many popular evolutionary algorithms use non-dominance as a measure for selecting solutions for future generations. The process of sorting populations into non-dominated fronts is usually the controlling order of computational complexity and can be expensive for large populations or for a high number of objectives. This paper presents two novel methods for non-dominated sorting: deductive sort and climbing sort. The two new methods are compared to the fast non-dominated sort of NSGA-II and the non-dominated rank sort of the omni-optimizer. The results demonstrate the improved efficiencies of the deductive sort and the reductions in comparisons that can be made when applying inferred dominance relationships defined in this paper.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Sorting under partial information (without the ellipsoid algorithm)</title>
	<abstract>We revisit the well-known problem of sorting under partial information: sort a finite set given the outcomes of comparisons between some pairs of elements. The input is a partially ordered set $P$, and solving the problem amounts to discovering an unknown linear extension of P, using pairwise comparisons. The information-theoretic lower bound on the number of comparisons needed in the worst case is log e(P), the binary logarithm of the number of linear extensions of $P$. In a breakthrough paper, Jeff Kahn and Jeong Han Kim (STOC 1992) showed that there exists a polynomial-time algorithm for the problem achieving this bound up to a constant factor. Their algorithm invokes the ellipsoid algorithm at each iteration for determining the next comparison, making it impractical.

We develop efficient algorithms for sorting under partial information. Like Kahn and Kim, our approach relies on graph entropy. However, our algorithms differ in essential ways from theirs. Rather than resorting to convex programming for computing the entropy, we approximate the entropy, or make sure it is computed only once in a restricted class of graphs, permitting the use of a simpler algorithm. Specifically, we present: an O(n2) algorithm performing O(log n ⋅ log e(P)) comparisons; an O(n2.5) algorithm performing at most (1+ε) log e(P) + Oε(n) comparisons; an O(n2.5) algorithm performing O(log e(P)) comparisons. All our algorithms are simple to implement.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Implementing Sorting Networks with Spiking Neural P Systems</title>
	<abstract>Spiking neural P systems simulate the behavior of neurons sending signals through axons. Recently, some applications concerning Boolean circuits and sorting algorithms have been proposed. In this paper, we study the ability of such systems to simulate a well known parallel sorting model, sorting networks. First, we construct spiking neural P systems which act as comparators of two values, and then show how to assemble these building blocks according to the topology of a sorting network of N values. In the second part of the paper, we formalize a framework to transform any sorting network into a network composed of comparators which sort n values, 2 &lt; n &lt; N, having the same behaviour as the original sorting network, but using fewer neurons and synapses than the direct simulation. A comparison between the two models proposed here and the sorting model of Ionescu and Sburlan is also given.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Fast sort on CPUs and GPUs: a case for bandwidth oblivious SIMD sort</title>
	<abstract>Sort is a fundamental kernel used in many database operations. In-memory sorts are now feasible; sort performance is limited by compute flops and main memory bandwidth rather than I/O. In this paper, we present a competitive analysis of comparison and non-comparison based sorting algorithms on two modern architectures - the latest CPU and GPU architectures. We propose novel CPU radix sort and GPU merge sort implementations which are 2X faster than previously published results. We perform a fair comparison of the algorithms using these best performing implementations on both architectures. While radix sort is faster on current architectures, the gap narrows from CPU to GPU architectures. Merge sort performs better than radix sort for sorting keys of large sizes - such keys will be required to accommodate the increasing cardinality of future databases. We present analytical models for analyzing the performance of our implementations in terms of architectural features such as core count, SIMD and bandwidth. Our obtained performance results are successfully predicted by our models. Our analysis points to merge sort winning over radix sort on future architectures due to its efficient utilization of SIMD and low bandwidth utilization. We simulate a 64-core platform with varying SIMD widths under constant bandwidth per core constraints, and show that large data sizes of 240 (one trillion records), merge sort performance on large key sizes is up to 3X better than radix sort for large SIMD widths on future architectures. Therefore, merge sort should be the sorting method of choice for future databases.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Cache-aware and cache-oblivious adaptive sorting</title>
	<abstract>Two new adaptive sorting algorithms are introduced which perform an optimal number of comparisons with respect to the number of inversions in the input. The first algorithm is based on a new linear time reduction to (non-adaptive) sorting. The second algorithm is based on a new division protocol for the GenericSort algorithm by Estivill-Castro and Wood. From both algorithms we derive I/O-optimal cache-aware and cache-oblivious adaptive sorting algorithms. These are the first I/O-optimal adaptive sorting algorithms.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Sorting and selection with random costs</title>
	<abstract>There is a growing body of work on sorting and selection in models other than the unit-cost comparison model. This work treats a natural stochastic variant of the problem where the cost of comparing two elements is a random variable. Each cost is chosen independently and is known to the algorithm. In particular we consider the following three models: each cost is chosen uniformly in the range [0, 1], each cost is 0 with some probability p and 1 otherwise, or each cost is 1 with probability p and infinite otherwise. We present lower and upper bounds (optimal in most cases) for these problems. We obtain our upper bounds by carefully designing algorithms to ensure that the costs incurred at various stages are independent and using properties of random partial orders when appropriate.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Adapting Radix Sort to the Memory Hierarchy</title>
	<abstract>We demonstrate the importance of reducing misses in the translation-lookaside buffer (TLB) for obtaining good performance on modern computer architectures. We focus on least-significantbit first (LSB) radix sort, standard implementations of which make many TLB misses. We give three techniques which simultaneously reduce cache and TLB misses for LSB radix sort: reducing working set size, explicit block transfer and pre-sorting. We note that: • All the techniques above yield algorithms whose implementations outperform optimised cache-tuned implementations of LSB radix sort and comparison-based sorting algorithms. The fastest running times are obtained by the pre-sorting approach and these are over twice as fast as optimised cache-tuned implementations of LSB radix sort and quicksort. Even the simplest optimisation, using the TLB size to guide the choice of radix in standard implementations of LSB radix sort, gives good improvements over cache-tuned algorithms. • One of the pre-sorting algorithms and explicit block transfer make few cache and TLB misses in the worst case. This is not true of standard implementations of LSB radix sort.We also apply these techniques to the problem of permuting an array of integers, and obtain gains of over 30% relative to the naive algorithm by using explicit block transfer.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Quantum time-space tradeoffs for sorting</title>
	<abstract>We investigate the complexity of sorting in the model of sequential quantum circuits. While it is known that in general a quantum algorithm based on comparisons alone cannot outperform classical sorting algorithms by more than a constant factor in time complexity, this is wrong in a space bounded setting. We observe that for all storage bounds n/log ≥ S ≥ log3n, one can devise a quantum algorithm that sorts n numbers (using comparisons only) in time T=O(n3/2 log3/2 n/√S). We then show the following lower bound on the time-space tradeoff for sorting n numbers from a polynomial size range in a general sorting algorithm (not necessarily based on comparisons): TS=Ω(n3/2). Hence for small values of S the upper bound is almost tight. Classically the time-space tradeoff for sorting is TS=Θ(n2).</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>On the limits of cache-obliviousness</title>
	<abstract>In this paper, we present lower bounds for permuting and sorting in the cache-oblivious model. We prove that (1) I/O optimal cache-oblivious comparison based sorting is not possible without a tall cache assumption, and (2) there does not exist an I/O optimal cache-oblivious algorithm for permuting, not even in the presence of a tall cache assumption.Our results for sorting show the existence of an inherent trade-off in the cache-oblivious model between the strength of the tall cache assumption and the overhead for the case M » B, and show that Funnelsort and recursive binary mergesort are optimal algorithms in the sense that they attain this trade-off.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Data-specific analysis of string sorting</title>
	<abstract>We consider the complexity of sorting strings in the model that counts comparisons between symbols and not just comparisons between strings. We show that for any set of strings S the complexity of sorting S can naturally be expressed in terms of the trie induced by S. This holds not only for lower bounds but also for the running times of various algorithms. Thus this "data-specific" analysis allows a direct comparison of different algorithms running on the same data. We give such "data-specific" analyses for various versions of quicksort and versions of mergesort. As a corollary we arrive at a very simple analysis of quicksorting random strings, which so far required rather sophisticated mathematical tools. As part of this we provide insights in the analysis of tries of random strings which may be interesting in their own right.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A fast and progressive algorithm for skyline queries with totally- and partially-ordered domains</title>
	<abstract>We devise a skyline algorithm that can efficiently mitigate the enormous overhead of processing millions of tuples on totally- and partially-ordered domains (henceforth, TODs and PODs). With massive datasets, existing techniques spend a significant amount of time on a dominance comparison because of both a large number of skyline points and the unprogressive method of skyline computing with PODs. (If data has high dimensionality, the situation is undoubtedly aggravated.) The progressiveness property turns out to be the key feature for solving all remaining problems. This article presents a FAST-SKY algorithm that deals successfully with these two obstacles and improves skyline query processing time strikingly, even with high-dimensional data. Progressive skyline evaluation with PODs is guaranteed by new index structures and topological sorting order. A stratification technique is adopted to index data on PODs, and we propose two new index structures: stratified R-trees (SR-trees) for low-dimensional data and stratified MinMax treaps (SM-treaps) for high-dimensional data. A fast dominance comparison is achieved by using a reporting query instead of a dominance query, and a dimensionality reduction technique. Experimental results suggest that in general cases (anti-correlated and uniform distributions) FAST-SKY is orders of magnitude faster than existing algorithms.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>An Improvement on Tree Selection Sort</title>
	<abstract>The standard Tree Selection Sort is an efficient sorting algorithm but requires extra storage for n-1 pointers and n items. The goal of this paper is to not only reduce the extra storage of Tree Selection Sort to n bits, but also keep the number of comparisons at nlogn+O(n). The improved algorithm makes at most 3n data movements. The empirical results show that the improved algorithm is efficient. In some cases, say moving one item requires at least 3 assignment operations, the algorithm is the fastest on average among known fast algorithms.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Optimal in-place sorting of vectors and records</title>
	<abstract>We study the problem of determining the complexity of optimal comparison-based in-place sorting when the key length, k, is not a constant. We present the first algorithm for lexicographically sorting n keys in O(nk + n log n) time using O(1) auxiliary data locations, which is simultaneously optimal in time and space.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Inversion-sensitive sorting algorithms in practice</title>
	<abstract>We study the performance of the most practical inversion-sensitive internal sorting algorithms. Experimental results illustrate that adaptive AVL sort consumes the fewest number of comparisons unless the number of inversions is less than 1%; in such case Splaysort consumes the fewest number of comparisons. On the other hand, the running time of Quicksort is superior unless the number of inversions is less than 1.5%; in such case Splaysort has the shortest running time. Another interesting result is that although the number of cache misses for the cache-optimal Greedysort algorithm was the least, compared to other adaptive sorting algorithms under investigation, it was outperformed by Quicksort.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A variant of the Ford--Johnson algorithm that is more space efficient</title>
	<abstract>A variant of the Ford-Johnson or merge insertion sorting algorithm that we called four Ford-Johnson ("4FJ, for short) is presented and proved to execute exactly the same number of comparisons than the Ford-Johnson algorithm. The main advantage of our algorithm is that, instead of recursively working over lists of size the half of the input, as the Ford-Johnson algorithm does, "4FJ recursively works over lists of size the quarter of the input. This allows for implementations of data structures for coordinating the recursive calls of size only 33% of the ones needed for the Ford-Johnson algorithm. </abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Structural filtering: a paradigm for efficient and exact geometric programs</title>
	<abstract>We introduce a new and simple filtering technique that can be used in the implementation of geometric algorithms called ''structural filtering''. Using this filtering technique we gain about 20% when compared to predicate-filtered implementations. Of theoretical interest are some results regarding the robustness of sorting algorithms against erroneous comparisons. There is software support for the concept of structural filtering in LEDA (Library of Efficient Data Types and Algorithms, http://www.mpi-sb.mpg.de/LEDA/leda.html) and CGAL (Computational Geometry Algorithms Library, http://www.cgal.org). </abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Design patterns for sorting</title>
	<abstract>Drawing on Merritt's divide-and-conquer sorting taxonomy [1], we model comparison-based sorting as an abstract class with a template method to perform the sort by relegating the splitting and joining of arrays to its concrete subclasses. Comparison on objects is carried out via an abstract ordering strategy. This reduces code complexity and simplifies the analyses of the various concrete sorting algorithms. Performance measurements and visualizations can be added without modifying any code by utilizing the decorator design pattern. This object-oriented design not only provides the student a concrete way of unifying seemingly disparate sorting algorithms but also help him/her differentiate them at the proper level of abstraction.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>External selection</title>
	<abstract>Sequential selection has been solved in linear time by Blum et al. [M.B. Blum, R.W. Floyd, V.R. Pratt, R.L. Rivest, R.E. Tarjan, Time bounds for selection, J. Comput. System Sci. 7 (4) (1972) 448-461]. Running this algorithm on a problem of size N with N>M, the size of the main-memory, results in an algorithm that reads and writes O(N) elements, while the number of comparisons is also bounded by O(N). This is asymptotically optimal, but the constants are so large that in practice sorting is faster for most values of M and N. This paper provides the first detailed study of the external selection problem. A randomized algorithm of a conventional type is close to optimal in all respects. Our deterministic algorithm is more or less the same, but first the algorithm builds an index structure of all the elements. This effort is not wasted: the index structure allows the retrieval of elements so that we do not need a second scan through all the data. This index structure can also be used for repeated selections, and can be extended over time. For a problem of size N, the deterministic algorithm reads N+o(N) elements and writes only o(N) elements and is thereby optimal to within lower-order terms.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A Memetic Algorithm Guided by Quicksort for the Error-Correcting Graph Isomorphism Problem</title>
	<abstract>Sorting algorithms define paths in the search space of n! permutations based on the information provided by a comparison predicate. We guide a Memetic Algorithm with a new mutation operator. Our mutation operator performs local search following the path traced by the Quicksort mechanism. The comparison predicate and the evaluation function are made to correspond and guide the evolutionary search. Our approach improves previous results for a benchmark of experiments of the Error-Correcting Graph Isomorphism. For this case study, our new Memetic Algorithm achieves a better quality vs effort trade-off and remains highly effective even when the size of the problem grows.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Sorting stably, in-place, with O(n log n) comparisons and O(n) moves</title>
	<abstract>We settle a long-standing open question, namely whether it is possible to sort a sequence of n elements stably (i.e. preserving the original relative order of the equal elements), using O(1) auxiliary space and performing O(n log n) comparisons and O(n) data moves. Munro and Raman stated this problem in [J. Algorithms, 13, 1992] and gave an in-place but unstable sorting algorithm that performs O(n) data moves and O(n1+ε) comparisons. Subsequently [Algorithmica, 16, 1996] they presented a stable algorithm with these same bounds. Recently, Franceschini and Geffert [FOCS 2003] presented an unstable sorting algorithm that matches the asymptotic lower bounds on all computational resources.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Optimal suffix selection</title>
	<abstract>Given a string S[1·s n], the suffix selection problemis to find the kth lexicographically smallest amongst the n suffixes S[i·s n], for i=1,...,n. In particular, the fundamental question is if selection can be performed more efficiently than sorting all the suffixes.

If one considered n numbers, they can be sorted using Θ(n log n) comparisonsand the classical result from 70's is that selection can be done using O(n) comparisons. Thus selection is provably more efficient than sorting, for n numbers.

Suffix sorting can be done using Θ(n log n) comparisons, but does suffix selection need suffix sorting? We settle this fundamental problem by presenting an optimal, deterministic algorithm for suffix selection using O(n) comparisons.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Efficient unbalanced merge-sort</title>
	<abstract>Sorting algorithms based on successive merging of ordered subsequences are widely used, due to their efficiency and to their intrinsically parallelizable structure. Among them, the merge-sort algorithm emerges indisputably as the most prominent method. In this paper we present a variant of merge-sort that proceeds through arbitrary merges between pairs of quasi-ordered subsequences, no matter which their size may be. We provide a detailed analysis, showing that a set of n elements can be sorted by performing at most n@?logn@? key comparisons. Our method has the same optimal asymptotic time and space complexity as compared to previous known unbalanced merge-sort algorithms, but experimental results show that it behaves significantly better in practice.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An efficient algorithm for partial order production</title>
	<abstract>We consider the problem of partial order production: arrange the elements of an unknown totally ordered set T into a target partially ordered set S, by comparing a minimum number of pairs in T. Special cases of this problem include sorting by comparisons, selection, multiple selection, and heap construction.

We give an algorithm performing ITLB + o(ITLB) + O(n) comparisons in the worst case. Here, n denotes the size of the ground sets, and ITLB denotes a natural information-theoretic lower bound on the number of comparisons needed to produce the target poset. The overall complexity of our algorithm is polynomial. This answers a question of Yao (SICOMP, 1989).

Our strategy is to extend the poset S to a weak order W whose corresponding information-theoretic lower bound is provably not much larger than that for S. Taking W instead of S as a target poset, we then solve the problem by applying a multiple selection algorithm that performs not much more than ITLB comparisons.

We base our analysis on the entropy of the target poset S, a quantity that can be efficiently computed and provides a good estimate of ITLB.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Comparisons in Evolution and Engineering: The Collective Intelligence of Sorting</title>
	<abstract>Collaboration between biologists and roboticists can facilitate the creation of new behavioral algorithms by roboticists and help biologists by exposing the underlying mechanisms that allow the algorithms to function (for a review see Webb, 2000). This paper makes a direct comparison between robot annular puck sorting using real robots (Wilson, Melhuish, Sendova-Franks &amp; Scholes, 2004) and brood sorting in the ant Leptothorax albipennis (Franks &amp; Sendova-Franks, 1992). We compared the ants' and robots' structures in terms of radial displacement, shape, compactness, completeness and separation. This showed the effectiveness and limitations of using metrics developed for robots to quantify structures built by ants and helped relate common aspects of the structures to possible common aspects of the underlying algorithms. The ant behavioral rule set is still under investigation and a better understanding of the structure it creates has proved a very useful tool to examine aspects of the algorithm. We draw the conclusions that firstly, the size of the area available for sorting affects the robot algorithm and ant algorithm in opposite ways, secondly, the metrics have proved very useful in aiding discrimination between the robot and ant structures and thirdly, application of the metrics to the ant structure has offered biologists new ideas and a better understanding of the structure and how it is built.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Comparing integer data structures for 32- and 64-bit keys</title>
	<abstract>In this article, we experimentally compare a number of data structures operating over keys that are 32- and 64-bit integers. We examine traditional comparison-based search trees as well as data structures that take advantage of the fact that the keys are integers such as van Emde Boas trees and various trie-based data structures. We propose a variant of a burst trie that performs better in time than all the alternative data structures. In addition, even for small sets of keys, this burst trie variant occupies less space than comparison-based data structures such as red-black trees and B-trees. Burst tries have previously been shown to provide a very efficient base for implementing cache efficient string sorting algorithms. We find that with suitable engineering, they also perform excellently as a dynamic ordered data structure operating over integer keys. We provide experimental results when the data structures operate over uniform random data. We also present experimental results for other types of data, including datasets arising from Valgrind, a widely used suite of tools for the dynamic binary instrumentation of programs.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Oblivious vs. distribution-based sorting: an experimental evaluation</title>
	<abstract>We compare two algorithms for sorting out-of-core data on a distributed-memory cluster. One algorithm, Csort, is a 3-pass oblivious algorithm. The other, Dsort, makes two passes over the data and is based on the paradigm of distribution-based algorithms. In the context of out-of-core sorting, this study is the first comparison between the paradigms of distribution-based and oblivious algorithms. Dsort avoids two of the four steps of a typical distribution-based algorithm by making simplifying assumptions about the distribution of the input keys. Csort makes no assumptions about the keys. Despite the simplifying assumptions, the I/O and communication patterns of Dsort depend heavily on the exact sequence of input keys. Csort, on the other hand, takes advantage of predetermined I/O and communication patterns, governed entirely by the input size, in order to overlap computation, communication, and I/O . Experimental evidence shows that, even on inputs that followed Dsort’s simplifying assumptions, Csort fared well. The running time of Dsort showed great variation across five input cases, whereas Csort sorted all of them in approximately the same amount of time. In fact, Dsort ran significantly faster than Csort in just one out of the five input cases: the one that was the most unrealistically skewed in favor of Dsort. A more robust implementation of Dsort—one without the simplifying assumptions—would run even slower.</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Evaluation of amplitude-based sorting algorithm to reduce lung tumor blurring in PET images using 4D NCAT phantom</title>
	<abstract>Purpose: develop and validate a PET sorting algorithm based on the respiratory amplitude to correct for abnormal respiratory cycles. Method and materials: using the 4D NCAT phantom model, 3D PET images were simulated in lung and other structures at different times within a respiratory cycle and noise was added. To validate the amplitude binning algorithm, NCAT phantom was used to simulate one case of five different respiratory periods and another case of five respiratory periods alone with five respiratory amplitudes. Comparison was performed for gated and un-gated images and for the new amplitude binning algorithm with the time binning algorithm by calculating the mean number of counts in the ROI (region of interest). Results: an average of 8.87+/-5.10% improvement was reported for total 16 tumors with different tumor sizes and different T/B (tumor to background) ratios using the new sorting algorithm. As both the T/B ratio and tumor size decreases, image degradation due to respiration increases. The greater benefit for smaller diameter tumor and lower T/B ratio indicates a potential improvement in detecting more problematic tumors. </abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Deterministic Majority filters applied to stochastic sorting</title>
	<abstract>In this paper, we examine the problem of stochastic sorting, which is also known as sorting with errors, or sorting under a stochastic environment. We introduce a new concept of filtering the stochastic "signals" using deterministic filters, which, in turn, attenuate any errors which occur during the comparison of individual pairs of values. We show that these deterministic filters, which can be used by standard sorting algorithms to achieve stochastic sorting, significantly increase the probability that the lists will be sorted correctly. We introduce two such filters called the Majority filter, and its optimized variant, the Optimal Majority filter. They have been compared for accuracy and computational complexity. More detailed comparisons which involves these and other deterministic filters, and their stochastic versions are found in [15].</abstract>
	<search_task_number>7</search_task_number>
	<query>sorting algorithm comparison</query>
	<relevance>1</relevance>
  </item>



  <item>
    <title>The information cost of manipulation-resistance in recommender systems</title>
	<abstract>Attackers may seek to manipulate recommender systems in order to promote or suppress certain items. Existing defenses based on analysis of ratings also discard useful information from honest raters. In this paper, we show that this is unavoidable and provide a lower bound on how much information must be discarded. We use an information-theoretic framework to exhibit a fundamental tradeoff between manipulation-resistance and optimal use of genuine ratings in recommender systems. We define a recommender system to be (n, c)-robust if an attacker with n sybil identities cannot cause more than a limited amount c units of damage to predictions. We prove that any robust recommender system must also discard Ω(log (n/c)) units of useful information from each genuine rater.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Prototyping recommender systems in jcolibri</title>
	<abstract>Our goal is to support system developers in rapid prototyping recommender systems using Case-Based Reasoning (CBR) techniques. In this paper we describe how jCOLIBRI can serve to that goal. jCOLIBRI is an object-oriented framework in Java for building CBR systems that greatly benefits from the reuse of previously developed CBR systems.

jCOLIBRI includes a case base of templates for case-based recommender systems that can be easily adapted to prototype a great variety of alternatives. We describe the contents of the case base and show experimental results from our experience using the recommender templates case base with mid-size projects from undergraduate students.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The value of personalised recommender systems to e-business: a case study</title>
	<abstract>Recommender systems have recently grown in popularity both in e-commerce and in research. However, there is little, if any, direct evidence in the literature of the value of recommender systems to e-Businesses, especially relating to consumer packaged goods (CPG) sold in a supermarket setting. We have been working in collaboration with LeShop (www.LeShop.ch), to gather real evidence of the added business value of a personalised recommender system. In this paper, we present our initial evaluation of the performance of our model-based personalised recommender systems over the 21-month period from May 2006 to January 2008, with particular focus on the added-value to the business. Our analysis covers shopper penetration, as well as the direct and indirect extra revenue generated by our recommender systems. One of the key lessons we have learnt during this case study is that the effect of a recommender system extends far beyond the direct extra revenue generated from the purchase of recommended items. The importance of maintaining updated model files was also found to be key to maintaining the performance of such model-based systems.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>UTA-Rec: a recommender system based on multiple criteria analysis</title>
	<abstract>UTARec, a Recommender System that incorporates Multiple Criteria Analysis methodologies is presented. The system's performance and capability of addressing certain shortfalls of existing Recommender Systems is demonstrated in the case of movie recommendations. UTARec's accuracy is measured in terms of Kendall's tau and ROC curve analysis and is also compared to a Multiple Rating Collaborative Filtering (MRCF) approach. The results indicate that the proposed Multiple Criteria Analysis methodology can certainly improve the recommendation process by producing highly accurate results, from a user oriented perspective.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Who predicts better?: results from an online study comparing humans and an online recommender system</title>
	<abstract>Algorithmic recommender systems attempt to predict which items a target user will like based on information about the user's prior preferences and the preferences of a larger community. After more than a decade of widespread use, researchers and system users still debate whether such "impersonal" recommender systems actually perform as well as human recommenders. We compare the performance of MovieLens algorithmic predictions with the recommendations made, based on the same user profiles, by active MovieLens users. We found that algorithmic collaborative filtering outperformed humans on average, though some individuals outperformed the system substantially and humans on average outperformed the system on certain prediction tasks.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Implications of psychological phenomenons for recommender systems</title>
	<abstract>Internet users often face the challenge of identifying the most suitable product out of some product assortment available on a certain e-sales platform. Recommender systems can substantially alleviate this typically complex task. Since the rise of such systems a lot of effort has been done in developing different recommendation approaches and algorithms, which all of them have certain strengths and weaknesses. What has been widely ignored by the recommender community so far are the potentials and impacts of psychological and decision theoretical phenomenons, which already have been investigated and applied in the field of marketing. Such phenomenons promise big capability to support users in decision making when facing a comparison situation. This paper concentrates on two classes of phenomenons, which are decoy effects and serial position effects. Tightly coupled to these phenomenons is the problem of getting the utility function of a recommender right, as this function serves both as the basis of result set calculation as well as the fundament of exploitation of above mentioned phenomenons. Putting all these aspects together an extended architecture for recommender systems will be proposed in the end of the paper.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The influence limiter: provably manipulation-resistant recommender systems</title>
	<abstract>An attacker can draw attention to items that don't deserve that attention by manipulating recommender systems. We describe an influence-limiting algorithm that can turn existing recommender systems into manipulation-resistant systems. Honest reporting is the optimal strategy for raters who wish to maximize their influence. If an attacker can create only a bounded number of shills, the attacker can mislead only a small amount. However, the system eventually makes full use of information from honest, informative raters. We describe both the influence limits and the information loss incurred due to those limits in terms of information-theoretic concepts of loss functions and entropies.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Unsupervised retrieval of attack profiles in collaborative recommender systems</title>
	<abstract>Trust, reputation and recommendation are key components of successful e-commerce systems. However, e-commerce systems are also vulnerable in this respect because there are opportunities for sellers to gain advantage through manipulation of reputation and recommendation. One such vulnerability is the use of fraudulent user profiles to boost (or damage) the ratings of items in an online recommender system. In this paper we cast this problem as a problem of detecting anomalous structure in network analysis and propose a novel mechanism for detecting this anomalous structure. We present an evaluation that shows that this approach is effective at uncovering the types of recommender systems attack described in the literature.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Online-updating regularized kernel matrix factorization models for large-scale recommender systems</title>
	<abstract>Regularized matrix factorization models are known to generate high quality rating predictions for recommender systems. One of the major drawbacks of matrix factorization is that once computed, the model is static. For real-world applications dynamic updating a model is one of the most important tasks. Especially when ratings on new users or new items come in, updating the feature matrices is crucial.

In this paper, we generalize regularized matrix factorization (RMF) to regularized kernel matrix factorization (RKMF). Kernels provide a flexible method for deriving new matrix factorization methods. Furthermore with kernels nonlinear interactions between feature vectors are possible. We propose a generic method for learning RKMF models. From this method we derive an online-update algorithm for RKMF models that allows to solve the new-user/new-item problem. Our evaluation indicates that our proposed online-update methods are accurate in approximating a full retrain of a RKMF model while the runtime of online-updating is in the range of milliseconds even for huge datasets like Netflix.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Exploiting contextual information in recommender systems</title>
	<abstract>Recommender Systems help an on-line user to tame information overload and are being used now in complex domains where it could be beneficial to exploit context-awareness, e.g., in travel recommendation. Technically, in Recommender Systems we can interpret context as a set of constraints or preferences over the usage of items determined by the contextual conditions (e.g., today it is raining or the user is in a particular location). In fact, there is a lack of approaches to deal effectively with contextual data. This thesis investigates some approaches to exploit context in Recommender Systems. It provides a general architecture of context-aware Recommender Systems and analyzes separate components of this model. The main focus is to investigate new approaches that can bring a real added value to users. In this paper I also describe my initial results on item selection and item weighting for context-dependent Collaborative Filtering (CF). Moreover, I shall present my ongoing research on CF hybridization using context.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A recursive prediction algorithm for collaborative filtering recommender systems</title>
	<abstract>Collaborative filtering (CF) is a successful approach for building online recommender systems. The fundamental process of the CF approach is to predict how a user would like to rate a given item based on the ratings of some nearest-neighbor users (user-based CF) or nearest-neighbor items (item-based CF). In the user-based CF approach, for example, the conventional prediction procedure is to find some nearest-neighbor users of the active user who have rated the given item, and then aggregate their rating information to predict the rating for the given item. In reality, due to the data sparseness, we have observed that a large proportion of users are filtered out because they don't rate the given item, even though they are very close to the active user. In this paper we present a recursive prediction algorithm, which allows those nearest-neighbor users to join the prediction process even if they have not rated the given item. In our approach, if a required rating value is not provided explicitly by the user, we predict it recursively and then integrate it into the prediction process. We study various strategies of selecting nearest-neighbor users for this recursive process. Our experiments show that the recursive prediction algorithm is a promising technique for improving the prediction accuracy for collaborative filtering recommender systems.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Leveraging aggregate ratings for improving predictive performance of recommender systems</title>
	<abstract>One of the key problems in recommender systems is accurate estimation of unknown ratings of individual items for individual users in terms of the previously specified ratings and other characteristics of items and users. In this thesis, we investigate a way of improving estimations of individual ratings using externally provided properties of aggregate ratings for groups of items and users, such as an externally specified average rating of action movies provided by graduate students or externally specified standard deviation of ratings for comedy movies.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A recommender system to provide adaptive and inclusive standard-based support along the elearning life cycle</title>
	<abstract>Dynamic support in adaptive inclusive educational systems depends on properly managing the adaptation in the eLearning life cycle by combining design and runtime adaptations and making a pervasive usage of standards along the eLearning life cycle. My Ph.D research focuses on recommender systems for lifelong learning inclusive scenarios, which have particular differences in their need for personalized recommendations. The research presented here makes a proposal for addressing some of the existing challenges. It goes beyond issues that are usually considered when building recommender systems and focuses also on closing the cycle. In particular, I propose a graphical representation that will help to compare the recommenders' performance in eLearning scenarios.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Eigentaste 5.0: constant-time adaptability in a recommender system using item clustering</title>
	<abstract>Recommender systems strive to recommend items that users will appreciate and rate highly, often presenting items in order of highest predicted ratings first. In this working paper we present Eigentaste 5.0, a constant-time recommender system that dynamically adapts the order that items are recommended by integrating user clustering with item clustering and monitoring item portfolio effects. This extends our Eigentaste 2.0 algorithm, which uses principal component analysis to cluster users offline. In preliminary experiments we backtested Eigentaste 5.0 on data collected from Jester, our online joke recommender system. Results suggest that it will perform better than Eigentaste 2.0. The new algorithm also uses item clusters to address the cold-start problem for introducing new items.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A recommender system for on-line course enrolment: an initial study</title>
	<abstract>In this paper we report on our work to date concerning the development of a course recommender system for University College Dublin's on-line enrollment application. We outline the factors that influence student choices and propose solutions to address some of the key considerations that are identified. We empirically evaluate our approach using historical student enrolment data and show that promising performance is achieved with our initial design.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Social ranking: uncovering relevant content using tag-based recommender systems</title>
	<abstract>Social (or folksonomic) tagging has become a very popular way to describe, categorise, search, discover and navigate content within Web 2.0 websites. Unlike taxonomies, which overimpose a hierarchical categorisation of content, folksonomies empower end users by enabling them to freely create and choose the categories (in this case, tags) that best describe some content. However, as tags are informally defined, continually changing, and ungoverned, social tagging has often been criticised for lowering, rather than increasing, the efficiency of searching, due to the number of synonyms, homonyms, polysemy, as well as the heterogeneity of users and the noise they introduce. In this paper, we propose Social Ranking, a method that exploits recommender system techniques to increase the efficiency of searches within Web 2.0. We measure users' similarity based on their past tag activity. We infer tags' relationships based on their association to content. We then propose a mechanism to answer a user's query that ranks (recommends) content based on the inferred semantic distance of the query to the tags associated to such content, weighted by the similarity of the querying user to the users who created those tags. A thorough evaluation conducted on the CiteULike dataset demonstrates that Social Ranking neatly improves coverage, while not compromising on accuracy.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A user-centric evaluation framework for recommender systems</title>
	<abstract>This research was motivated by our interest in understanding the criteria for measuring the success of a recommender system from users' point view. Even though existing work has suggested a wide range of criteria, the consistency and validity of the combined criteria have not been tested. In this paper, we describe a unifying evaluation framework, called ResQue (Recommender systems' Quality of user experience), which aimed at measuring the qualities of the recommended items, the system's usability, usefulness, interface and interaction qualities, users' satisfaction with the systems, and the influence of these qualities on users' behavioral intentions, including their intention to purchase the products recommended to them and return to the system. We also show the results of applying psychometric methods to validate the combined criteria using data collected from a large user survey. The outcomes of the validation are able to 1) support the consistency, validity and reliability of the selected criteria; and 2) explain the quality of user experience and the key determinants motivating users to adopt the recommender technology. The final model consists of thirty two questions and fifteen constructs, defining the essential qualities of an effective and satisfying recommender system, as well as providing practitioners and scholars with a cost-effective way to evaluate the success of a recommender system and identify important areas in which to invest development resources.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Applying relevant set correlation clustering to multi-criteria recommender systems</title>
	<abstract>This thesis investigates application of clustering to multi-criteria ratings as a method of improving the precision of top-N recommendations. With the advent of ecommerce sites that allow multi-criteria rating of items, there is an opportunity for recommender systems to use the additional information to gain a better understanding of user preference. This thesis proposes the use of the relevant set correlation model for a clustering-based collaborative filtering system. It is anticipated this novel system will handle large numbers of users and items without sacrificing the relevance of recommended items.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Each to his own: how different users call for different interaction methods in recommender systems</title>
	<abstract>This paper compares five different ways of interacting with an attribute-based recommender system and shows that different types of users prefer different interaction methods. In an online experiment with an energy-saving recommender system the interaction methods are compared in terms of perceived control, understandability, trust in the system, user interface satisfaction, system effectiveness and choice satisfaction. The comparison takes into account several user characteristics, namely domain knowledge, trusting propensity and persistence. The results show that most users (and particularly domain experts) are most satisfied with a hybrid recommender that combines implicit and explicit preference elicitation, but that novices and maximizers seem to benefit more from a non-personalized recommender that just displays the most popular items.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Design and user issues in personality-based recommender systems</title>
	<abstract>Recommender systems have emerged as an intelligent information filtering tool to help users effectively identify information items of interest from an overwhelming set of choices and provide personalized services. Studies show that personality influences human decision making process and interests. However, little research has ventured in incorporating it into recommender systems. The utilization of personality characteristics into recommender systems and the exploration of user perception to such systems are the focuses of my thesis. The overall goal is to develop an efficient personality-based recommender system and to arrive at a series of design guidelines from the perspective of human computer interaction. In this paper, I present my up-to-date results on a proposed personality-based music recommender prototype, user perception investigations, and my ongoing research about addressing new user problem by utilizing personality characteristics. Finally, I shall present future works.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Anchoring effects of recommender systems</title>
	<abstract>We explore how consumer preferences at the time of consumption are impacted by predictions generated by recommender systems. We conducted three controlled laboratory experiments to explore the effects of system recommendations on preferences. Results provide strong evidence that the rating provided by a recommender system serves as an anchor for the consumer's constructed preference. Consumer's preferences appear malleable and can be significantly influenced by the recommendation received. Additionally, the effects of pure number-based anchoring can be separated from the effects of the perceived reliability of a recommender system. In particular, when the recommender system was described to the participants as being in testing phase, the anchoring effect was reduced. Finally, the effect of anchoring is roughly continuous, operating over a range of perturbations of the system.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Will recommenders kill search?: recommender systems - an industry perspective</title>
	<abstract>At the 2010 annual ACM Conference on Recommender Systems (RecSys 2010) a panel addressed emerging topics regarding recommender systems as a whole and specifically their role in industry. This report summarizes answers from a distinguished group of industry leaders representing different industries in which recommender systems are highly relevant. Panel members discuss questions regarding the role of recommender systems in their own industry area, killer applications, opportunities, and future directions.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Personalized pricing recommender system: multi-stage epsilon-greedy approach</title>
	<abstract>Many e-commerce sites use recommender systems, which suggest items that customers prefer. Though recommender systems have achieved great success, their potential is not yet fulfilled. One weakness of current systems is that the actions of the system toward customers are restricted to simply showing items. We propose a system that relaxes this restriction to offer price discounting as well as recommendations. The system can determine whether or not to offer price discounting for individual customers, and such a pricing scheme is called price personalization. We discuss how the introduction of price personalization improves the commercial viability of managing a recommender system, and thereby improving the customers' sense of the system's reliability. We then propose a method for adding price personalization to standard recommendation algorithms which utilize two types of customer data: preferential data and purchasing history. Based on the analysis of the experimental results, we reveal further issues in designing a personalized pricing recommender system.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Acceptance issues of personality-based recommender systems</title>
	<abstract>To understand users' acceptance of the emerging trend of personality-based recommenders (PBR), we evaluated an existing PBR using the technology acceptance model (TAM). We also compare it with a baseline rating-based recommender in a within-subject user study. Our results show that while the personality-based recommender is perceived to be only slightly more accurate than the rating-based one, it is much easier to use. The side-by-side comparison also reveals that users significantly favor the personality-based recommender and have a significantly higher intention to use such a system again. Therefore, we believe that if users accepted rating-based recommenders, they are most likely to accept personality-based recommenders and personality-based recommenders have a high likelihood to be widely adopted despite the fact that rating-based recommenders are now the industry norm. We further point out some preliminary guidelines on how to design personality-based recommender systems.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Trust-aware recommender systems</title>
	<abstract>Recommender Systems based on Collaborative Filtering suggest to users items they might like. However due to data sparsity of the input ratings matrix, the step of finding similar users often fails. We propose to replace this step with the use of a trust metric, an algorithm able to propagate trust over the trust network and to estimate a trust weight that can be used in place of the similarity weight. An empirical evaluation on Epinions.com dataset shows that Recommender Systems that make use of trust information are the most effective in term of accuracy while preserving a good coverage. This is especially evident on users who provided few ratings.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Understanding choice overload in recommender systems</title>
	<abstract>Even though people are attracted by large, high quality recommendation sets, psychological research on choice overload shows that choosing an item from recommendation sets containing many attractive items can be a very difficult task. A web-based user experiment using a matrix factorization algorithm applied to the MovieLens dataset was used to investigate the effect of recommendation set size (5 or 20 items) and set quality (low or high) on perceived variety, recommendation set attractiveness, choice difficulty and satisfaction with the chosen item. The results show that larger sets containing only good items do not necessarily result in higher choice satisfaction compared to smaller sets, as the increased recommendation set attractiveness is counteracted by the increased difficulty of choosing from these sets. These findings were supported by behavioral measurements revealing intensified information search and increased acquisition times for these large attractive sets. Important implications of these findings for the design of recommender system user interfaces will be discussed.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Identifying and utilizing contextual data in hybrid recommender systems</title>
	<abstract>Context-aware recommender systems are becoming a popular topic, still, there are many untouched aspects. In this paper, research involving context identification and the concepts related to hybrid and context-aware systems is presented. A conceptual architecture for a context-aware recommender system for movies and TV shows is furthermore introduced. The system consists of a number of processes for context identification and recommendation. Key contextual features are identified and used for the creation of several sets of recommendations, based on the predicted context. The main focus of the research presented here is the identification of context, which in turn is used for recommendation. The results will be evaluated and incorporated into the recommendation engine of movie and TV recommendation website Moviepilot.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>On the limitations of browsing top-N recommender systems</title>
	<abstract>To exploit the enormous potential of niche products, modern information systems must support users in exploring digital libraries and online catalogs. A straight-forward way of doing so is to support browsing the available items, which is in general realized by presenting a user the top-N recommendations for each item. However, recent research indicates that most of the niche products reside in the so-called Long Tail, and simple collaborative filtering-based recommender systems alone do not allow to explore these niche products. In this paper we show that it is not only a popularity problem related to the collaborative filtering approach that makes a portion of the elements of a digital library inaccessible via browsing, but also a consequence of the top N-recommendation approach itself.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Interface and interaction design for group and social recommender systems</title>
	<abstract>Group and social recommender systems aim to recommend items of interest to a group or a community of people. The user issues in such systems cannot be addressed by examining the satisfaction of their members as individuals. Rather, group satisfaction should be studied as a result of the interaction and interface methods that support group dynamics and interaction. In this paper, we survey the state-of-the-art in user experience design of group and social recommender systems. We further apply the techniques used in the current recommender systems to GroupFun, a music social group recommender system. After presenting the interface and interaction characteristics of GroupFun, we further analyze the design space and propose areas for future research in pursuit of an affective recommender.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A pragmatic procedure to support the user-centric evaluation of recommender systems</title>
	<abstract>As recommender systems are increasingly deployed in the real world, they are not merely tested offline for precision and coverage, but also "online" with test users to ensure good user experience. The user evaluation of recommenders is however complex and resource-consuming. We introduce a pragmatic procedure to evaluate recommender systems for experience products with test users, within industry constraints on time and budget. Researchers and practitioners can employ our approach to gain a comprehensive understanding of the user experience with their systems.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Beyond accuracy: evaluating recommender systems by coverage and serendipity</title>
	<abstract>When we evaluate the quality of recommender systems (RS), most approaches only focus on the predictive accuracy of these systems. Recent works suggest that beyond accuracy there is a variety of other metrics that should be considered when evaluating a RS. In this paper we focus on two crucial metrics in RS evaluation: coverage and serendipity. Based on a literature review, we first discuss both measurement methods as well as the trade-off between good coverage and serendipity. We then analyze the role of coverage and serendipity as indicators of recommendation quality, present novel ways of how they can be measured and discuss how to interpret the obtained measurements. Overall, we argue that our new ways of measuring these concepts reflect the quality impression perceived by the user in a better way than previous metrics thus leading to enhanced user satisfaction.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A belief propagation based recommender system for online services</title>
	<abstract>In this paper we report our progress in the first application of iterative probabilistic algorithms in the design and evaluation of recommender systems. The proposed iterative recommender system (referred to as BPRS) is based on the belief propagation, a powerful decoding algorithm for turbo codes and Low-Density Parity-Check (LDPC) codes. The belief propagation algorithm relies on a graph-based representation of an appropriately chosen factor graph for the recommender systems. The factor graph representation of the recommender systems turned out to be a bipartite graph, where the users and products are arranged as two sets of variable and factor nodes that are connected via some edges. Recommendations (predicted ratings) for each particular user can be computed by probabilistic message passing between nodes in the graph. We provide an evaluation of BPRS via computer simulations using the MovieLens dataset. We observed that BPRS iteratively reduces the error in the predicted ratings of the users until it converges. Further, our initial results indicate an improvement in the Mean Average Error (MAE) and Root Mean Square Error (RMSE) over the Item Averaging. Therefore, we are confident that the belief propagation is a new promising approach which will offer robustness and accuracy for the recommender systems.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Predicting performance in recommender systems</title>
	<abstract>Performance prediction has gained growing attention in the Information Retrieval field since the late nineties and has become an established research topic in the field. Our work restates the problem in the area of Recommender Systems, where it has barely been researched so far, despite being an appealing problem, as it enables an array of strategies for deciding when to deliver or hold back recommendations based on their foreseen accuracy. We investigate the adaptation and definition of different performance predictors based on the available user and item features. The properties of the predictor are empirically studied by checking the correlation of the predictor output with a performance measure. Then, we propose to introduce the performance predictor in a recommender system to produce a dynamic strategy. Depending on how the predictor is introduced we analyze two different problems: dynamic neighbor weighting in collaborative filtering and dynamic weighting of ensemble recommenders.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Rank and relevance in novelty and diversity metrics for recommender systems</title>
	<abstract>The Recommender Systems community is paying increasing attention to novelty and diversity as key qualities beyond accuracy in real recommendation scenarios. Despite the raise of interest and work on the topic in recent years, we find that a clear common methodological and conceptual ground for the evaluation of these dimensions is still to be consolidated. Different evaluation metrics have been reported in the literature but the precise relation, distinction or equivalence between them has not been explicitly studied. Furthermore, the metrics reported so far miss important properties such as taking into consideration the ranking of recommended items, or whether items are relevant or not, when assessing the novelty and diversity of recommendations.

We present a formal framework for the definition of novelty and diversity metrics that unifies and generalizes several state of the art metrics. We identify three essential ground concepts at the roots of novelty and diversity: choice, discovery and relevance, upon which the framework is built. Item rank and relevance are introduced through a probabilistic recommendation browsing model, building upon the same three basic concepts. Based on the combination of ground elements, and the assumptions of the browsing model, different metrics and variants unfold. We report experimental observations which validate and illustrate the properties of the proposed metrics.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Conceptual recommender system for CiteSeerX</title>
	<abstract>Short search engine queries do not provide contextual information, making it difficult for traditional search engines to understand what users are really requesting. One approach to this problem is to use recommender systems that identify user interests through various methods in order to provide information specific to the user's needs. However, many current recommender systems use a collaborative model based on a network of users to provide the recommendations, leading to problems in environments where network relationships are sparse or unknown. Content-based recommenders can avoid the sparsity problem but they may be inefficient for large document collections. In this paper, we propose a concept-based recommender system that recommends papers to general users of the CiteSeerx digital library of Computer Science research publications. We also represent a novel way of classifying documents and creating user profiles based on the ACM (Association for Computer Machinery) classification tree. Based on these user profiles which are built using past click histories, relevant papers in the domain are recommended to users. Experiments with a set of users on the CiteSeerX database show that our concept-based method provides accurate recommendations even with limited user profile histories.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A model for proactivity in mobile, context-aware recommender systems</title>
	<abstract>A proactive recommender system pushes recommendations to the user when the current situation seems appropriate, without explicit user request. This is conceivable in mobile scenarios such as restaurant or gas station recommendations. In this paper, we present a model for proactivity in mobile recommender systems. The model relies on domain-dependent context modeling in several categories. The recommendation process is divided into two phases to first analyze the current situation and then examine the suitability of particular items. We have implemented a prototype gas station recommender and conducted a survey for evaluation. Results showed good correlation of the output of our system with the assessment of users regarding the question when to generate recommendations.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Workshop on novelty and diversity in recommender systems - DiveRS 2011</title>
	<abstract>Novelty and diversity have been identified as key dimensions of recommendation utility in real scenarios, and a fundamental research direction to keep making progress in the field. Yet recommendation novelty and diversity remain a largely open area for research. The DiveRS workshop gathered researchers and practitioners interested in the role of these dimensions in recommender systems. The workshop seeks to advance towards a better understanding of what novelty and diversity are, how they can improve the effectiveness of recommendation methods and the utility of their outputs. The workshop pursued the identification of open problems, relevant research directions, and opportunities for innovation in the recommendation business.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Intelligent web usage clustering based recommender system</title>
	<abstract>Our work focuses on tackling the problem of efficiency and accuracy of web usage clustering for recommender systems. Accurate analysis and preprocessing of web usage data and efficient web usage clustering are the key factors that influence the development of clustering based implicit recommender system. We propose an analysis and preprocessing model to tackle the poor quality of web usage data. To address the problem of efficient web usage clustering, we propose a Particle Swarm Optimization (PSO) based clustering approach. Having shown our PSO based clustering performs well; we extend it for mining the usage behavior of web users. We select Java API (Application Programming Interface) documentation usage data as a case study for our recommender system.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Tutorial on using social trust for recommender systems</title>
	<abstract>As the Web has shifted to an interactive environment where vast amounts of content is created by users, the question of whom to trust and what information to trust has become both more important and more difficult to answer. At the same time, social networks have become very popular with over a billion accounts shared across hundreds of networks. Social trust relationships, derived from social networks, are uniquely suited to speak to the quality of online information; recommender systems are designed to personalize, sort, aggregate, and highlight information. Merging social networks, trust, and recommender systems can improve the accuracy of recommendations and improve the user's experience. In this tutorial, we will cover the use of social trust in recommender systems. Topics including the computation of trust in social networks, integration of trust into recommender systems, and a discussion of when trust offers benefits and the challenges it presents.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Effective diverse and obfuscated attacks on model-based recommender systems</title>
	<abstract>Robustness analysis research has shown that conventional memory-based recommender systems are very susceptible to malicious profile-injection attacks. A number of attack models have been proposed and studied and recent work has suggested that model-based collaborative filtering (CF) algorithms have greater robustness against these attacks. Moreover, to combat such attacks, several attack detection algorithms have been proposed. One that has shown high detection accuracy is based on using principal component analysis (PCA) to cluster attack profiles on the basis that such profiles are highly correlated. In this paper, we argue that the robustness observed in model-based algorithms is due to the fact that the proposed attacks have not targeted the specific vulnerabilities of these algorithms. We discuss how an effective attack targeting model-based algorithms that employ profile clustering can be designed. It transpires that the attack profiles employed in this attack, exhibit low rather than high pair-wise similarities and can easily be obfuscated to avoid PCA-based detection, while remaining effective.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The long tail of recommender systems and how to leverage it</title>
	<abstract>The paper studies the Long Tail problem of recommender systems when many items in the Long Tail have only few ratings, thus making it hard to use them in recommender systems. The approach presented in the paper splits the whole itemset into the head and the tail parts and clusters only the tail items. Then recommendations for the tail items are based on the ratings in these clusters and for the head items on the ratings of individual items. If such partition and clustering are done properly, we show that this reduces the recommendation error rates for the tail items, while maintaining reasonable computational performance.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>MARS: a MultilAnguage Recommender System</title>
	<abstract>The exponential growth of the Web is the most influential factor that contributes to the increasing importance of cross-lingual text retrieval and filtering systems. Indeed, relevant information exists in different languages, thus users need to find documents in languages different from the one the query is formulated in. In this context, an emerging requirement is to sift through the increasing flood of multilingual text: this poses a renewed challenge for designing effective multilingual Information Filtering systems.

In this paper, we propose a language-independent content-based recommender system, called MARS (MultilAnguage Recommender System), that builds cross-language user profiles, by shifting the traditional text representation based on keywords, to a more complex language-independent representation based on word meanings. As a consequence, the recommender system is able to suggest items represented in a language different from the one used in the content-based user profile.

Experiments conducted in a movie recommendation scenario show the effectiveness of the approach.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A recommender system for dynamically evolving online forums</title>
	<abstract>Recommender systems can be used in online forums to recommend discussion topics to users; however as these forums are characterized by a constant influx of new users and new posts, it is important to consider the performance of the recommender system under a scenario in which the internal composition of the items to be recommended, i.e., discussion threads, and the user preferences are constantly changing. In this paper we describe and evaluate a forum recommender designed to handle the challenges of dynamically evolving internet forums used to gather and discuss feature requests for various software products. In particular, we empirically show that two proposed enhancements to the representations of user profiles will result in improved recommendation effectiveness in dynamic environments.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Incorporating user control into recommender systems based on naive bayesian classification</title>
	<abstract>Recommender systems are increasingly being employed to personalize services, such as on the web, but also in electronics devices, such as personal video recorders. These recommenders learn a user profile, based on rating feedback from the user on, e.g., books, songs, or TV programs, and use machine learning techniques to infer the ratings of new items.

The techniques commonly used are collaborative filtering and naive Bayesian classification, and they are known to have several problems, in particular the cold-start problem and its slow adaptivity to changing user preferences. These problems can be mitigated by allowing the user to set up or manipulate his profile.

In this paper, we propose an extension to the naive Bayesian classifier that enhances user control. We do this by maintaining and flexibly integrating two profiles for a user, one learned by rating feedback, and one created by the user. We in particular show how the cold-start problem is mitigated.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Knowledge infusion into content-based recommender systems</title>
	<abstract>Content-based recommender systems try to recommend items similar to those a given user has liked in the past. The basic process consists of matching up the attributes of a user profile, in which preferences and interests are stored, with the attributes of a content object (item).

Common-sense and domain-specific knowledge may be useful to give some meaning to the content of items, thus helping to generate more informative features than "plain" attributes.

The process of learning user profiles could also benefit from the infusion of exogenous knowledge or open source knowledge, with respect to the classical use of endogenous knowledge (extracted from the items themselves).

The main contribution of this paper is a proposal for knowledge infusion into content-based recommender systems, which suggests a novel view of this type of systems, mostly oriented to content interpretation by way of the infused knowledge.

The idea is to provide the system with the "linguistic" and "cultural" background knowledge that hopefully allows a more accurate content analysis than classic approaches based on words. A set of knowledge sources is modeled to create a memory of linguistic competencies and of more specific world "facts", that can be exploited to reason about content as well as to support the user profiling and recommendation processes.

The modeled knowledge sources include a dictionary, Wikipedia, and content generated by users (i.e. tags provided on items), while the core of the reasoning component is a spreading activation algorithm.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Tutorial on evaluating recommender systems</title>
	<abstract>In this tutorial we discuss the evaluation of recommender systems. We discuss the main reason for evaluating recommender systems, i.e., the selection task. We overview some general guidelines for conducting evaluation tests. We then discuss the evaluation of the system accuracy given specific system tasks. We also overview many properties of recommender systems, and explain how these properties can be evaluated.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The keepup recommender system</title>
	<abstract>In this short paper, we describe our RSS recommender system, KeepUP. Too often recommender systems are seen as black box systems, resulting in general perplexity and dissatisfaction from users who are treated as passive, isolated consumers. Recent literature observes that recommendations rarely occur within such isolation and that there may be potential within more socially-orientated approaches. With KeepUP, we outline the design of a recommendation process that is based on an implicit social network where the relevancy and meaning of information can be negotiated not only with the recommender system but also with other users. Our overall goal is to support the formation and development of online communities of interest.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Iterative voting under uncertainty for group recommender systems</title>
	<abstract>Group Recommendation Systems (GRS) aim at recommending items that are relevant for the joint interest of a group of users. Voting mechanisms assume that users rate all items in order to identify an item that suits the preferences of all group members. This assumption is not feasible in sparse rating scenarios which are common in the recommender systems domain. In this paper we examine an application of voting theory to GRS. We propose a method to accurately determine the winning item while using a minimal set of the group members ratings, assuming that the recommender system has probabilistic knowledge about the distribution of users' ratings of items in the system. Since computing the optimal minimal set of ratings is computationally intractable, we propose two heuristic algorithms that proceed iteratively that aiming atto minimizing the number of required ratings, until identifying a "winning item". Experiments with the Netflix data show that the proposed algorithms reduce the required number of ratings for identifying the "winning item" by more than 50%.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Power to the people: exploring neighbourhood formations in social recommender system</title>
	<abstract>The explosive growth of online social networks in recent times has presented a powerful source of information to be utilised in personalised recommendations. Unsurprisingly there has already been a large body of work completed in the recommender system field to incorporate this social information into the recommendation process. In this paper we examine the practice of leveraging a user's social graph in order to generate recommendations. Using various neighbourhood selection strategies, we examine the user satisfaction and the level of perceived trust in the recommendations received.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>MoviExplain: a recommender system with explanations</title>
	<abstract>Providing justification to a recommendation gives credibility to a recommender system. Some recommender systems (Amazon.com etc.) try to explain their recommendations, in an effort to regain customer acceptance and trust. But their explanations are poor, because they are based solely on rating data, ignoring the content data. Our prototype system MoviExplain is a movie recommender system that provides both accurate and justifiable recommendations.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommender systems</query>
	<relevance>0</relevance>
  </item>


<item>
	<title>ATPC: adaptive transmission power control for wireless sensor networks</title>
	<abstract>Extensive empirical studies presented in this paper confirm that the quality of radio communication between low power sensor devices varies significantly with time and environment. This phenomenon indicates that the previous topology control solutions, which use static transmission power, transmission range, and link quality, might not be effective in the physical world. To address this issue, online transmission power control that adapts to external changes is necessary. This paper presents ATPC, a lightweight algorithm of Adaptive Transmission Power Control for wireless sensor networks. In ATPC, each node builds a model for each of its neighbors, describing the correlation between transmission power and link quality. With this model, we employ a feedback-based transmission power control algorithm to dynamically maintain individual link quality over time. The intellectual contribution of this work lies in a novel pairwise transmission power control, which is significantly different from existing node-level or network-level power control methods. Also different from most existing simulation work, the ATPC design is guided by extensive field experiments of link quality dynamics at various locations and over a long period of time. The results from the real-world experiments demonstrate that 1) with pairwise adjustment, ATPC achieves more energy savings with a finer tuning capability and 2) with online control, ATPC is robust even with environmental changes over time.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>CNS: a new energy efficient transmission scheme for wireless sensor networks</title>
	<abstract>We present in this paper a new energy-efficient communication scheme called CNS (Compression with Null Symbol) that combines the power of data compression and communication through silent symbol. The concept of communication through silent symbol is borrowed from the energy efficient schemes proposed in Sinha (Proceedings of 6th IEEE consumer communications and networking conference (CCNC), Las Vegas, pp. 1---5, 2009), Ghosh et al. (Proceedings of 27th IEEE international performance computing and communications conference (IPCCC), USA, pp. 85---92, 2008), and Sinha and Sinha (Proceedings of international conference on distributed computing and internet technologies (ICDCIT), LNCS, pp. 139---144, 2008). We show that the average theoretical energy saving at the transmitter by CNS is 62.5%, assuming an ideal channel and for equal likelihood of all possible binary strings of a given length. Next, we propose a transceiver design that uses a hybrid modulation scheme utilizing FSK and ASK so as to keep the cost/complexity of the radio devices low. Considering an additive white gaussian noise (AWGN) channel and a non-coherent detection based receiver, CNS shows a saving in transmitter energy by 30% when compared to binary FSK, for equal likelihood of all possible binary strings of a given length. Simultaneously, there is a saving of 50% at the receiver for all types of data modulation due to halving of the transmitted data duration, compared to binary encoding. In contrast, RBNSiZeComm proposed in Sinha (Proceedings of 6th IEEE consumer communications and networking conference (CCNC), Las Vegas, pp. 1---5, 2009), TSS proposed in Ghosh et al. (Proceedings of 27th IEEE international performance computing and communications conference (IPCCC), USA, pp. 85---92, 2008) and RZE proposed in Sinha and Sinha (Proceedings of international conference on distributed computing and internet technologies (ICDCIT), LNCS, pp. 139---144, 2008) generate average transmitter energy savings of about 41, 20, and 35.2%, respectively. Also, at the receiver side, while RBNSiZeComm does not generate any saving, TSS and RZE produce about 36.9 and 12.5% savings on an average, respectively. Considering certain data types that may occur in the context of some wireless sensor networks (WSN) based applications (e.g., remote healthcare, agricultural WSNs, etc.), our simulation results demonstrate that for AWGN noisy channels, the transmitter side savings vary from about 33---50% on an average, while for RBNSiZeComm, this saving is about 33---61% on the same data set (Sinha in Proceedings of 6th IEEE consumer communications and networking conference (CCNC), Las Vegas, pp. 1---5, 2009). Thus, taking into account the low cost/complexity of the proposed transceiver, these results clearly establish that CNS can be a suitable candidate for communication in low power wireless sensor networks, such as in remote healthcare applications, body area networks, home automation, WSNs for agriculture and many others.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Effect of overhearing transmissions on energy efficiency in dense sensor networks</title>
	<abstract>Energy efficiency is an important design criterion for the development of sensor networking protocols involving data dissemination and gathering. In-network processing of sensor data, aggregation, transmission power control in radios, and periodic cycling of node wake-up schedules are some techniques that have been roposed in the sensor networking literature for achieving energy efficiency. Owing to the broadcast nature of the wireless channel many nodes in the vicinity of a sender node may overhear its packet transmissions even if they are not the intended reci ients of these transmissions. Reception of these transmissions can result in unnecessary expenditure of battery energy of the recipients. In this paper,we investigate the impact of overhearing transmissions on total energy costs during data gathering and dissemination and attempt to minimize them systematically.We model the minimum energy data gathering problem as a directed minimum energy spanning tree problem where the energy cost of each edge in the wireless connectivity graph is augmented by the overhearing cost of the corresponding transmission. We observe that in dense sensor networks, overhearing costs constitute a significant fraction of the total energy cost and that computing the minimum spanning tree on the augmented cost metric results in energy savings, especially in networks with non-uniform s atial node distribution. We also study the impact of this new metric on the well known energy-efficient dissemination (also called broadcasting) algorithms for multihop wireless networks.We show via simulation that through this augmented cost metric, gains in energy efficiency of 10%or more are ossible without additional hardware and minimal additional complexity.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>1</relevance>
</item>
<item>
	<title>Ultra-low duty cycle MAC with scheduled channel polling</title>
	<abstract>Energy is a critical resource in sensor networks. MAC protocols such as S-MAC and T-MAC coordinate sleep schedules to reduce energy consumption. Recently, lowpower listening (LPL) approaches such as WiseMAC and B-MAC exploit very brief polling of channel activity combined with long preambles before each transmission, saving energy particularly during low network utilization. Synchronization cost, either explicitly in scheduling, or implicitly in long preambles, limits all these protocols to duty cycles of 1-2%. We demonstrate that ultra-low duty cycles of 0.1% and below are possible with a new MAC protocol called scheduled channel polling (SCP). This work prompts three new contributions: First, we establish optimal configurations for both LPL and SCP under fixed conditions, developing a lower bound of energy consumption. Under these conditions, SCP can extend lifetime of a network by a factor of 3-6 times over LPL. Second, SCP is designed to adapt well to variable traffic. LPL is optimized for known, periodic traffic, and long preambles become very costly when traffic varies. In one experiment, SCP reduces energy consumption by a factor of 10 under bursty traffic. We also show how SCP adapts to heavy traffic and streams data in multi-hop networks, reducing latency by 85% and energy by 95% at 9 hops. Finally, we show that SCP can operate effectively on recent hardware such as 802.15.4 radios. In fact, power consumption of SCP decreases with faster radios, but that of LPL increases.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Autonomic multi-agent management of power and performance in data centers</title>
	<abstract>The rapidly rising cost and environmental impact of energy consumption in data centers has become a multi-billion dollar concern globally. In response, the IT Industry is actively engaged in a first-to-market race to develop energy-conserving hardware and software solutions that do not sacrifice performance objectives. In this work we demonstrate a prototype of an integrated data center power management solution that employs server management tools, appropriate sensors and monitors, and an agent-based approach to achieve specified power and performance objectives. By intelligently turning off servers under low-load conditions, we can achieve over 25% power savings over the unmanaged case without incurring SLA penalties for typical daily and weekly periodic demands seen in webserver farms.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>1</relevance>
</item>
<item>
	<title>TiNA: a scheme for temporal coherency-aware in-network aggregation</title>
	<abstract>This paper presents TiNA, a scheme for minimizing energy consumption in sensor networks by exploiting end-user tolerance to temporal coherency. TiNA utilizes temporal coherency tolerances to both reduce the amount of information transmitted by individual nodes (communication cost dominates power usage in sensor networks), and to improve quality of data when not all sensor readings can be propagated up the network within a given time constraint. TiNA was evaluated against a traditional in-network aggregation scheme with respect to power savings as well as the quality of data for aggregate queries. Preliminary results show that TiNA can reduce power consumption by up to 50% without any loss in the quality of data.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Flexible power scheduling for sensor networks</title>
	<abstract>We propose a distributed on-demand power-management protocol for collecting data in sensor networks. The protocol aims to reduce power consumption while supporting fluctuating demand in the network and provide local routing information and synchronicity without global control. Energy savings are achieved by powering down nodes during idle times identified through dynamic scheduling. We present a real implementation on wireless sensor nodes based on a novel, two-level architecture. We evaluate our approach through measurements and simulation, and show how the protocol allows adaptive scheduling and enables a smooth trade-off between energy savings and latency. An example current measurement shows an energy savings of 83% on an intermediate node.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Efficient application integration in IP-based sensor networks</title>
	<abstract>Sensor networks are seen as an important part in emerging office and building energy management system, but the integration of sensor networks with future energy management systems is still an open problem. We present an IP-based sensor network system where nodes communicate their information using Web services, allowing direct integration in modern IT systems. Our system uses two mechanisms to provide a good performance and low-power operation: a session-aware power-saving radio protocol and the use of the HTTP Conditional GET mechanism. We perform an extensive evaluation of our system and show that Web services are a viable mechanism for use in low-power sensor networks. Our results show that Web service requests can be completed well below one second and with a low power consumption, even in a multi-hop setting.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>1</relevance>
</item>
<item>
	<title>Increasing ZigBee network lifetime with X-MAC</title>
	<abstract>The ZigBee standard builds on the assumption that infrastructure nodes have a constant power supply. ZigBee therefore does not provide any power-saving mechanisms for routing nodes, which limits the lifetime of battery-powered ZigBee networks to a few days. This short lifetime drastically restricts the possible application scenarios for ZigBee. To expand the usefulness of ZigBee, we present a ZigBee implementation where we replace the default ZigBee MAC protocol with the power-saving MAC protocol X-MAC. Our results show that X-MAC reduces the power consumption for ZigBee routing nodes with up to 90%, leading to a ten-fold increase in network lifetime at the price of a slight increase in network latency. Furthermore, we are the first to experimentally quantify the energy-efficiency of X-MAC in a multihop scenario. Our results indicate that X-MAC reduces the power consumption of idle nodes that are within communication range of two communicating nodes, suggesting that X-MAC may be able to mitigate the hot-spot problem in sensor networks.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Study of an energy efficient multi rate scheme for wireless sensor network MAC protocol</title>
	<abstract>Real-time Digital Signal Processing (DSP) applications in wireless sensor network are often multi-rate in nature. These multi rate attributes provide additional significant opportunities in network design for achieving higher energy efficiency, besides traditional approaches of saving energy. In this paper, we propose a novel multi rate sensor network scheme to take advantage of power scaling for saving energy consumption. It can be easily synthesized into existing wireless sensor network MAC protocols, providing multi rate service for upper layer, and achieving communication energy efficiency. By monitoring a desirable BER threshold as the critical system parameter, the quality of received multi-rate data is assured. Based on this BER and the corresponding modulation scheme, the SNR at receiver end can be derived. The transmission power can be supplied optimally according to this SNR and the channel attenuation between sender and receiver. This dynamic power scaling scheme can lead to optimal energy efficiency since it demands that only a necessary amount of communication energy be consumed for multi-rate DSP applications. Our simulation shows that this multi data rate scheme with dynamic power scaling achieves energy saving significantly</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>1</relevance>
</item>
<item>
	<title>Power-Aware processors for wireless sensor networks</title>
	<abstract>Today, wireless sensor networks (WSNs) enable us to run a new range of applications from habitat monitoring, to military and medical applications. A typical WSN node is composed of several sensors, a radio communication interface, a microprocessor, and a limited power supply. In many WSN applications, such as forest fire monitoring or intruder detection, user intervention and battery replenishment is not possible. Since the battery lifetime is directly related to the amount of processing and communication involved in these nodes, optimal resource usage becomes a major issue. A typical WSN application may sense and process very close or constant data values for long durations, when the environmental conditions are stable. This is a common behavior that can be exploited to reduce the power consumption of WSN nodes. This study combines two orthogonal techniques to reduce the energy dissipation of the processor component of the sensor nodes. First, we briefly discuss silent-store filtering MoteCache. Second, we utilize Content-Aware Data MAnagement (CADMA) on top of MoteCache architecture to achieve further energy savings and performance improvements. The complexity increase introduced by CADMA is also compensated by further complexity reduction in MoteCache. Our optimal configuration reduces the total node energy, and hence increases the node lifetime, by 19.4% on the average across a wide variety of simulated sensor benchmarks. Our complexity-aware configuration with a minimum MoteCache size achieves not only energy savings up to 16.2% but also performance improvements up to 4.3%, on the average.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Towards in-network data prediction in wireless sensor networks</title>
	<abstract>Researches in Wireless Sensor Networks (WSN) have been focused on saving power in sensor nodes. An efficient strategy to achieve this goal is to reduce the amount of data sent through the network, In this work, we propose an efficient strategy that predicts data in WSNs aiming at reducing the data traffic in WSNs and thus maximizing the network lifetime. The proposed prediction strategy, denoted ADAGA-P, is based on a linear regression model, using data acquired from one or several sensors. ADAGA-P is executed in an in-network fashion by several sensors geographically distributed in a WSN. Furthermore, in ADAGA-P the regression model is adjusted dynamically, if any sensor in the WSN senses an "outlier". Experimental results are presented using real data. These results show that the proposed strategy reduces energy consumption in WSNs.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Energy efficient sensor data logging with amnesic flash storage</title>
	<abstract>We present FlashLogger, an energy-efficient sensor data logging system that uses lazy amnesic compression in a flash-efficient manner. FlashLogger incorporates a suite of compression algorithms suitable for progressively compressing time series scalar, audio, and image data. It uses a novel data structure for efficiently organizing and querying compressed data on flash memory. All our methods are designed for the limited memory and processing capabilities typical of low power sensor nodes, and are prototyped on Tmote Sky platform running TinyOS. Evaluation of FlashLogger with several real world data sets shows orders of magnitude energy savings for both logging data and retrieving data within a time range.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Implementation of hierarchical GAF, a cooperative power saving scheme for sensor networks</title>
	<abstract>The sensor nodes of wireless sensor networks are placed in observation areas and transmit data to the observer by using multi-hop communication between nodes. Because these nodes are small and have a limited power supply, they must save power in order to prolong the networks lifetime. In our previous work, we proposed a power saving technique called HGAF (Hierarchical Geographic Adaptive Fidelity) that gives a layered structure to GAF (Geographic Adaptive Fidelity) and takes into account the positions of nodes. We implemented HGAF on MICAz-Motes and evaluated its energy consumption in comparison with that of GAF in an actual environment. The experimental results shows that HGAF can prolong the network lifetime.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>A power efficient routing protocol in wireless sensor networks</title>
	<abstract>Saving energy consumption for a long network lifetime is one of the most important issues in design of wireless sensor networks because they usually consist of a large number of nodes with limited battery power. In this paper, we propose a balanced tree construction scheme called BATR (Balanced Aggregation Tree Routing), which uses near optimal minimal spanning tree for balancing the power consumptions over all nodes. Our main idea is that if energy consumed for transmission and reception can be nicely balanced over all nodes, an optimal data aggregation can be achieved in terms of network lifetime. Simulation results assure that the BATR can lengthen system lifetime as compared with the conventional data aggregation protocols.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Saving is fun: designing a persuasive game for power conservation</title>
	<abstract>EnergyLife is a mobile game application that aims at increasing energy awareness and saving in the household; it centers around a feedback system with detailed, historical and real time information that is based on wireless power sensors data. The challenge is to provide through feedback knowledge and motivation for sustainable saving. A three-month field test in eight households was organized for EnergyLife. The test involved the automatic collection of access data to the application, and the administration of satisfaction questionnaires, interviews, and usability tasks in the tested families. The paper describes the results of the test and the ensuing re-design strategy, centered on better tailoring the application to the players' actions. The lessons learned can be useful to other persuasive games, since a good fit to the actions of the user is a precondition of effectiveness of any persuasive application.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>1</relevance>
</item>
<item>
	<title>Integrated distributed energy awareness for wireless sensor networks</title>
	<abstract>Energy in sensor networks is distributed and non-transferable. Over time, differences in energy availability across the network are likely to arise. Protocols such as routing engines can concentrate energy load at certain nodes. Variations in incident sunlight can produce different solar charging rates at different nodes. Because many sensor network applications require nodes collaborate --- to ensure complete sensor coverage or route data to the network's edge --- a small set of nodes threatened by low energy availability can have a disproportionate impact on the entire network. For example, the loss of a single sink node may render the application unable to communicate with all the other nodes.

However, network density provides redundancy that can be exploited to control the distribution of energy load. Multiple possible routing paths may link a node and the sink, or several sinks may exist. Adjusting MAC-level parameters may allow a node to conserve energy by forcing additional load on its neighbors. Inputs from multiple sensors may prove redundant to the application, allowing some sensors to be disabled or operated at reduced fidelity, saving power at those nodes. These choices imply that energy load can be tuned to match availability, and this tuning can extend the useful lifetime of the network.

Effective distributed energy management requires network-wide awareness of energy availability and load integrated with algorithms guiding protocols toward states producing longer lifetimes or higher node duty-cycles. Intelligent Distributed Energy Awareness (IDEA) is a sensor network service that can be used by both protocols and applications. Given the current energy availability and a set of protocol states, each with different implications for network-wide energy consumption, IDEA projects future energy availability in order to make the best choice. By simplifying decisions impacting distributed energy availability, it facilitates the implementation of energy-aware services.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>The Energy Endoscope: Real-Time Detailed Energy Accounting for Wireless Sensor Nodes</title>
	<abstract>This paper describes a new embedded networked sensor platform architecture that combines hardware and software tools providing detailed, fine-grained real-time energy usage information. We introduce the LEAP2 platform, a qualitative step forward over the previously developed LEAP and other similar platforms. LEAP2 is based on a new low power ASIC system and generally applicable supporting architecture that provides unprecedented capabilities for directly observing energy usage of multiple subsystems in real-time. Real-time observation with microsecond-scale time resolution enables direct accounting of energy dissipation for each computing task as well as for each hardware subsystem. The new hardware architecture is exploited with our new software tools, etop and endoscope. A series of experimental investigations provide high-resolution power information in networking, storage, memory and processing for primary embedded networked sensing applications. Using results obtained in real-time we show that for a large class of wireless sensor network nodes, there exist several interdependencies in energy consumptionbetween different subsystems. Through the use of our measurement tools we demonstrate that by carefully selecting the system operating points, energy savings of over 60% can be achieved while retaining system performance.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>System-Level Synthesis for Wireless Sensor Node Controllers: A Complete Design Flow</title>
	<abstract>Wireless sensor networks (WSN) is a new and very challenging research field for embedded system design automation. Engineering a WSN node hardware platform is known to be a tough challenge, as the design must enforce many severe constraints, among which energy dissipation is by far the most important one. WSN node devices have until now been designed using off-the-shelf low-power microcontroller units (MCUs), even if their power dissipation is still an issue and hinders the widespread use of this new technology. In this work, we propose a complete system-level flow for an alternative approach based on the concept of hardware microtasks, which relies on hardware specialization and power gating to drastically improve the energy efficiency of the computational/control part of the node. Our case study shows that power savings between one to two orders of magnitude are possible w.r.t. MCU-based implementations.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Experimental evaluation of synchronization and topology control for in-building sensor network applications</title>
	<abstract>While multi-hop networks consisting of 100s or 1000s of inexpensive embedded sensors are emerging as a means of mining data from the environment, inadequate network lifetime remains a major impediment to real-world deployment. This paper describes several applications deployed throughout our building that monitor conference room occupancy and environmental statistics and provide access to room reservation status. Because it is often infeasible to locate sensors and display devices near power outlets, we designed two protocols that allow energy conservation in a large class of sensor network applications. The first protocol, Relay Organization (ReOrg), is a topology control protocol which systematically shifts the network's routing burden to energy-rich nodes, exploiting heterogeneity. The second protocol, Relay Synchronization (ReSync), is a MAC protocol that extends network lifetime by allowing nodes to sleep most of the time, yet wake to receive packets. When combined, ReOrg and ReSync lower the duty cycle of the nodes, extending network lifetime. To our knowledge, this paper presents the first experimental testbed evaluation of energy-aware topology control integrated with energy-saving synchronization. Using a 54-node testbed, we demonstrate an 82-92% reduction in energy consumption, depending on traffic load. By rotating the burden of routing, our protocols can extend network lifetime by 5-10 times. Finally, we demonstrate that a small number of wall-powered nodes can significantly improve the lifetime of a battery-powered network.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Experimental evaluation of topology control and synchronization for in-building sensor network applications</title>
	<abstract>While multi-hop networks consisting of 100s or 1000s of inexpensive embedded sensors are emerging as a means of mining data from the environment, inadequate network lifetime remains a major impediment to real-world deployment. This paper describes several applications deployed throughout our building that monitor conference room occupancy and environmental statistics and provide access to room reservation status. Because it is often infeasible to locate sensors and display devices near power outlets, we designed two protocols that allow energy conservation in a large class of sensor network applications. The first protocol, Relay Organization (ReOrg), is a topology control protocol which systematically shifts the network's routing burden to energy-rich nodes, exploiting heterogeneity. The second protocol, Relay Synchronization (ReSync), is a MAC protocol that extends network lifetime by allowing nodes to sleep most of the time, yet wake to receive packets. When combined, ReOrg and ReSync lower the duty cycle of the nodes, extending network lifetime. To our knowledge, this research provides the first experimental testbed evaluation of energy-aware topology control integrated with energy-saving synchronization. Using a 54-node testbed, we demonstrate an 82-92% reduction in energy consumption, depending on traffic load. By rotating the burden of routing, our protocols can extend network lifetime by 5-10 times. Finally, we demonstrate that a small number of wall-powered nodes can significantly improve the lifetime of a battery-powered network.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Energy Conservation in Sensor Networks through Selective Node Activation</title>
	<abstract>This work presents a novel algorithm to improve energy conservation in sensor networks. The algorithm is based upon Selective Activation of sensor nodes using Thresholding (SAT). The sensor continuously monitors the received signal and makes a binary decision to join the network if the average received signal power falls within the specified minimum and maximum threshold range. Thus, SAT divides all the receivers within the coverage range of the transmitter into sets of active and inactive nodes realizing a saving in power consumption proportional to the number of inactive nodes. The sensor life-time is enhanced by allowing the node to transmit at a power that is a constant fraction of the total residual energy. For two cases of linear and hexagonal networks considered in this work, it is shown that for each transmission, the fraction of inactive nodes may exceed by over 30% of the total number of nodes present within maximum one-hop distance of the transmitter. The cost associated with energy conservation through SAT is (a) Increased sensor node density or (b) Increased transmission power requirement.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Scheduling sleeping nodes in high density cluster-based sensor networks</title>
	<abstract>In order to conserve battery power in very dense sensor networks, some sensor nodes may be put into the sleep state while other sensor nodes remain active for the sensing and communication tasks. In this paper, we study the node sleep scheduling problem in the context of clustered sensor networks. We propose and analyze the Linear Distance-based Scheduling (LDS) technique for sleeping in each cluster. The LDS scheme selects a sensor node to sleep with higher probability when it is farther away from the cluster head. We analyze the energy consumption, the sensing coverage property, and the network lifetime of the proposed LDS scheme. The performance of the LDS scheme is compared with that of the conventional Randomized Scheduling (RS) scheme. It is shown that the LDS scheme yields more energy savings while maintaining a similar sensing coverage as the RS scheme for sensor clusters. Therefore, the LDS scheme results in a longer network lifetime than the RS scheme.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Trade-off between energy savings and source-to-sink delay in data dissemination for wireless sensor networks</title>
	<abstract>Wireless sensor networks (WSNs) consist of large numbers of unattended sensors with limited storage, energy (battery power) and computational and communication capabilities. Because battery power is the most crucial resource for sensor nodes and delay time is a critical metric for certain WSN applications that require fast response time, data dissemination between source sensors and sinks, which is an essential activity in WSNs, should be done in an energy efficient and timely manner. In this paper, we characterize the trade-off between energy savings and source-to-sink delay in order to extend the operation of individual sensors and hence increase the lifetime of the WSN, and enable sinks to receive sensed data in a timely fashion and make appropriate decisions quickly. To this end, the proposed data dissemination protocol decomposes the transmission range of sensors into a certain number of concentric circular bands (CCBs) based on a minimal distance between consecutive forwarding sensors. Then, it provides a classification of these CCBs based on their exterior radii which will help a source sensor express its degree of interest (DoI) in minimizing two metrics, namely energy consumption and source-to-sink delay. We prove that the use of sensors nodes, which lie on or closely to the shortest path between a source and the sink, as proxy forwarders, helps minimize these two metrics. Our numerical results show that the second CCB minimizes energy consumption; the last CCB minimizes source-to-sink delay; and the middle CCBs trade off between the two metrics in disseminating the monitored data towards the sink.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>An address-light, integrated MAC and routing protocol for wireless sensor networks</title>
	<abstract>We propose an address-light, integrated MAC and routing protocol (abbreviated AIMRP) for wireless sensor networks (WSNs). Due to the broad spectrum of WSN applications, there is a need for protocol solutions optimized for specific application classes. AIMRP is proposed for WSNs deployed for detecting rare events which require prompt detection and response. AIMRP organizes the network into concentric tiers around the sink(s), and routes event reports by forwarding them from one tier to another, in the direction of (one of) the sink(s). AIMRP is address-light in that it does not employ unique per-node addressing, and integrated since the MAC control packets are also responsible for finding the next-hop node to relay the data, via an anycast query. For reducing the energy expenditure due to idle-listening, AIMRP provides a power-saving algorithm which requires absolutely no synchronization or information exchange. We evaluate AIMRP through analysis and simulations, and compare it with another MAC protocol proposed for WSNs, S-MAC. AIMRP outperforms S-MAC for event-detection applications, in terms of total average power consumption, while satisfying identical sensor-to-sink latency constraints.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Error control in wireless sensor networks: a cross layer analysis</title>
	<abstract>Error control is of significant importance for Wireless Sensor Networks (WSNs) because of their severe energy constraints and the low power communication requirements. In this paper, a cross-layer methodology for the analysis of error control schemes in WSNs is presented such that the effects of multi-hop routing and the broadcast nature of the wireless channel are investigated. More specifically, the cross-layer effects of routing, medium access, and physical layers are considered. This analysis enables a comprehensive comparison of forward error correction (FEC) codes, automatic repeat request (ARQ), and hybrid ARQ schemes in WSNs. The validation results show that the developed framework closely follows simulation results.

Hybrid ARQ and FEC schemes improve the error resiliency of communication compared to ARQ. In a multi-hop network, this improvement can be exploited by constructing longer hops (hop length extension), which can be achieved through channel-aware routing protocols, or by reducing the transmit power (transmit power control). The results of our analysis reveal that for hybrid ARQ schemes and certain FEC codes, the hop length extension decreases both the energy consumption and the end-to-end latency subject to a target packet error rate (PER) compared to ARQ. This decrease in end-to-end latency is crucial for delay sensitive, real-time applications, where both hybrid ARQ and FEC codes are strong candidates. We also show that the advantages of FEC codes are even more pronounced as the network density increases. On the other hand, transmit power control results in significant savings in energy consumption at the cost of increased latency for certain FEC codes. The results of our analysis also indicate the cases where ARQ outperforms FEC codes for various end-to-end distance and target PER values.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Adaptive sink mobility in event-driven multi-hop wireless sensor networks</title>
	<abstract>Optimizing energy consumption in wireless sensor networks is of paramount importance. There is a recent trend to deal with this problem by introducing mobile elements (sensors or sink nodes) in the network. The majority of these approaches assume time-driven scenarios and/or single-hop communication between participating nodes. However, there are several real-life applications for which an event-based and multi-hop operation is more appropriate. In this paper we propose to adaptively move the sink node inside the covered region, according to the evolution of current events, so as to minimize the energy consumption incurred by the multi-hop transmission of the event-related data. Both analytical and simulation results are given for two optimization strategies: minimizing the overall energy consumption, and minimizing the maximum load on a specific sensor respectively. We show that by adaptively moving the sink, significant power saving can be achieved, prolonging the lifetime of the network.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Exploiting mobility for energy efficient data collection in wireless sensor networks</title>
	<abstract>We analyze an architecture based on mobility to address the problem of energy efficient data collection in a sensor network. Our approach exploits mobile nodes present in the sensor field as forwarding agents. As a mobile node moves in close proximity to sensors, data is transferred to the mobile node for later depositing at the destination. We present an analytical model to understand the key performance metrics such as data transfer, latency to the destination, and power. Parameters for our model include: sensor buffer size, data generation rate, radio characteristics, and mobility patterns of mobile nodes. Through simulation we verify our model and show that our approach can provide substantial savings in energy as compared to the traditional ad-hoc network approach.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>A distributed energy aware routing protocol for wireless sensor networks</title>
	<abstract>Future sensor networks will be composed of a large number of densely deployed sensors. A key feature of such networks is that their nodes have a limited battery power and they are unattended. Consequently, energy efficiency is an important design consideration for these networks. In this paper, we propose a Distributed Energy-efficient Clustering Hierarchy Protocol (DECHP), which distributes the energy dissipation evenly among all sensor nodes to improve network lifetime and average energy savings. The performance of DECHP is then compared to clustering-based schemes such as Low-Energy Adaptive Clustering Hierarchy (LEACH), LEACH-Centralized (LEACH-C), and Power-Efficient Gathering in Sensor Information Systems (PEGASIS). Simulation results show that DECHP reduces overall energy consumption and improves network lifetime over its comparatives.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Network-adaptive management of computation energy in wireless sensor networks</title>
	<abstract>Today's sensor nodes can be equipped with powerful microcontrollers to address the increasing need of real-time processing of sensed data. For instance, body sensor networks for gesture recognition require filtering of acceleration values at line rate. This requirement imposes a paradigm shift with regard to more traditional sensor networks characterized by low activity duty cycles. Therefore, energy conservation strategies applied to wireless sensor nodes to increase their lifetime must take into account computation power rather than focusing only on communication power. In this paper we present a novel approach which aims at exploiting the knowledge of network status to optimize the power consumption of the node microcontroller. The proposed approach is tested in various network conditions, both synthetic and realistic, in the context of IEEE 802.15.4 standard. Experimental results demonstrate that the proposed approach allows to achieve power savings of up to 70% with minimum performance penalty.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Analysis of the contention access period of IEEE 802.15.4 MAC</title>
	<abstract>The recent ratification of IEEE 802.15.4 PHY-MAC specifications for low-rate wireless personal area networks represents a significant milestone in promoting deployment of wireless sensor networks (WSNs) for a variety of commercial uses. The 15.4 specifications specifically target wireless networking among low-rate, low-power and low-cost devices that is expected to be a key market segment for a large number of WSN applications. In this article, we first analyze the performance of the contention access period specified in the IEEE 802.15.4 standard in terms of throughput and energy consumption. This analysis is facilitated by a modeling of the contention access period as nonpersistent CSMA with backoff. We show that, in certain applications in which having an inactive period in the superframe may not be desirable due to delay constraints, shutting down the radio between transmissions provides significant savings in power without significantly compromising the throughput. We also propose and analyze the performance of a modification to the specification which could be used for applications in which MAC-level acknowledgements are not used. Extensive ns-2 simulations are used to verify the analysis.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Link layer support for unified radio power management in wireless sensor networks</title>
	<abstract>Radio power management is of paramount concern in wireless sensor networks that must achieve long lifetimes on scarce amounts of energy. While a multitude of power management protocols have been proposed in the past, the lack of system support for flexibly integrating them with a diverse set of applications and network platforms has made them diffcult to use. Instead of proposing yet another power management protocol, this paper focuses on providing link layer support towards realizing a Unified Power Management Architecture (UPMA) for flexible radio power management in wireless sensor networks. In contrast to the monolithic approaches adopted by existing power management solutions, we provide (1) a set of standard interfaces that allow different power management protocols existing at the link layer to be easily implemented on top of common MAC level functionality, (2) an architectural framework for enabling these protocols to be easily swapped in and out depending on the needs of the applications that require them, and (3) a mechanism for coordinating the existence of multiple applications, each of which may have different requirements for the same underlying power management protocol. We have implemented these features on the Mica2 and Telosb radio stacks in TinyOS-2.0. Microbenchmark results demonstrate that the separation of power management from MAC level functionality incurs a negligible decrease in performance when compared to existing monolithic implementations. Two case studies show that the power management requirements of multiple applications can be easily coordinated, sometimes even resulting in better power savings than any one of them can achieve individually.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Simulation of power-aware wireless sensor network architectures</title>
	<abstract>Power consumption is recognized to be one of the key design parameters towards the deployment of effective and fully operational wireless sensor networks. Many studies and efforts are currently under way for developing better power management strategies. Towards this end, various simulation tools are being developed for enabling the analysis of the impact of various system parameters over the energy required by the various system elements of a wireless sensor network. In this paper, we focus on the simulation of power consumption management based on the principles of cross-layer protocol engineering. We start by reviewing the most relevant work on the area of simulators of wireless sensor networks focusing mainly on those tools offering capabilities for evaluating the power consumption. Thereafter, based on the principles of cross-layer protocol engineering, we study the interaction between the various layers by focusing on the study of power-aware network architectures. Throughout an exhaustive campaign of simulations, we compare various protocol architecture configurations in terms of their power saving capabilities</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>SOM-based dynamic power management (SDPM) framework for wireless sensor networks</title>
	<abstract>Energy is one of the most precious resources for wireless sensor networks. Dynamic power management (DPM) is an efficient technique to reduce the energy consumption and increase the lifetime of wireless sensor networks. DPM for the event-driven applications in wireless sensor networks has been studied extensively, but power saving for the time-driven applications has not been studied deeply. This paper proposes two novel DPM schemes for the time-driven applications in wireless sensor networks. In the proposed two schemes, sensor nodes reduce redundant data reporting and hence save energy based on neighbor distance and data mining information, respectively. The power saving performance and effectiveness of the proposed schemes are evaluated by confirmed numerical analysis and simulation.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Design approaches for optimizing power consumption of sensor node with N-policy M/G/1 queuing model</title>
	<abstract>Energy saving is an important issue in wireless sensor networks for majority of sensor nodes equipped with non-rechargeable batteries. For increasing the lifetime of sensor networks, each node must conserve energy as much as possible. To prolong the lifetime of sensor nodes, most research works have focused on how to optimally increase the probability of sleeping states using multifarious wake-up strategies. Making things different, in this article, we propose a novel optimization framework for power consumption of sensor node with the N-policy M/G/1 queuing approach. There is a heavy overhead for packet collisions and channel contention resulting from restarting the process of medium contention. Based on the theory of N-policy M/G/1 queuing system with general startup time, our framework can be incorporated with most of existing MAC protocols to improve power consumption by alleviating total number of medium-contention throughout the lifetime of a generic sensor node. To meet the mission requirements in sensor networks, the proposed approach can also provide a design guideline for the sensor administrator to optimize relevant system parameters including power consumption and latency delay.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Bit-per-joule performance of power saving ad hoc networks with a mobile backbone</title>
	<abstract>Energy efficient MAC protocols have been developed for wireless sensor and mobile ad hoc networks so that inactive nodes can transition into sleep state to conserve energy. It has been recognized that maintaining a continuously awake connected dominating set (CDS) serves to reduce the route setup latency. Under the mobile backbone network (MBN) architecture introduced by Rubin et al., a mobile backbone (Bnet) is dynamically constructed to provide a topological covering of the network. The MBN employs a hybrid routing algorithm under which flows that travel a distance longer than a threshold are directed along routes across the Bnet. In turn, a limited span network-wide global route discovery process is applied for routing shorter distance flows. In this paper, we introduce and analyze an MBN based power saving protocol (MBN-PS) that employs this hybrid routing scheme. Under the MBN-PS scheme, dynamically elected backbone nodes are kept awake, while inactive non-backbone nodes can reside in sleep state. We analytically show that, when the number of network flows is above a minimal level, the throughput per watt efficiency attained in an ad hoc network under complete backbone coverage is better than that achieved by a corresponding network that does not form a backbone. We present a model for the calculation of the bit-per-joule performance of the network as a function of the distance threshold. We confirm the validity of our analytical approach through simulations. Using our method, a network designer is able to choose the optimal distance threshold to be used by this scheme, based on traffic loading conditions.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>A novel cluster-chain channel adaptive routing protocol in wireless sensor networks</title>
	<abstract>The energy constraint in wireless sensor networks is a crucial issue affecting the network lifetime and connectivity. To realize true energy saving in a wireless environment and ensure reliable communications, the noise condition of the wireless channel should be taken into account. In this paper, we propose a cluster-chain routing protocol (CCRP). Besides, we devise an adaptive power adjustment strategy which can dynamically adjust transmission power according to the receiver noise condition and the distance between the transmitter and receiver to ensure the required Packet Reception Rate (PRR). The adaptive power adjustment strategy is incorporated into CCRP to form a novel protocol---cluster-chain channel adaptive routing protocol (CCARP). Simulation results indicate that CCRP and CCARP outperform LEACH by at least 300% for a 200mx200m network when the scenario of 1% dead nodes is considered.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Multi-hop scheduling and local data link aggregation dependant Qos in modeling and simulation of power-aware wireless sensor networks</title>
	<abstract>In this study of wireless sensor networks (WSN) protocols, the application Qos, system, and protocol performance metrics are measured for a large scalable wireless deployment using a typical wireless radio and an energy model. As there are many different types of WSN algorithms, we have categorized it into pro-active, re-active, and query driven information processing. A typical Qos is based on the useful lifetime of sensor nodes, after which reliability of the sensor data cannot be guaranteed and typically, a threshold such as a percentage of the sensor drains out of energy or a minimum through-put of real-time data from the sensor network is expected, which is used to compare the Qos of the routing algorithm. The results from lifetime based Qos, measured in simulation seconds, for the implemented protocols show that with varying sampled data sources for a BE Qos multi-hop deployment and varying percentage of cluster heads in a time- synchronized deployment, the lifetime is based on network size and protocol invariant. However, low sensing ranges result in dense networks, and therefore, it becomes necessary to achieve an efficient medium-access protocol subjected to power constraints. Scalability of sensor network applications are based on energy energy-harvesting techniques in which the various layers of the network inter-operate and extend the system network lifetime, the battery residual power per node, and the application reliability in terms of cross-layer energy savings. In this study, we have extended the lifetime metrics from a constant metrics into a break down of how much percentage of time is spent for Tx, Rx, and Idle tasks, respectively. This helps one to highlight the cross-layer energy dissipation per node and how the performance of an algorithm differs in terms of duty-cycling. Furthermore, we have shown that the energy savings due owing to the distributed algorithms in a large sensor network will not be practical without a complimentary lower-layer MAC. We show have demonstrated that the Qos is very much related to the ambient conditions, namely, are the Rx and Idle modes. From these preliminary results, we have added a new category of WSN protocols which are based ion the renewable energy resources, namely, the Fusion Ambient Renewable Measuring Sensors (FARMS). The study of sensor FARMS -harvesting applications allows one to measure the impact on Idle, Sleep, and renewable energy cycles as well as their unique deployment (density) needs, as all the sensor are not active(Rx) at all times. We have also shown that the efficiency of cross-layer Qos performance of routing algorithms with MAC losses has a long tail which is similarly observed in Power Law. In this sensor network model we like to show the complexity of clustering, messaging and data rate in terms of O(?(N) log N), O(N) and O(log2 N) where N is the number of nodes.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>A coverage-preserving node scheduling scheme for large wireless sensor networks</title>
	<abstract>In wireless sensor networks that consist of a large number of low-power, short-lived, unreliable sensors, one of the main design challenges is to obtain long system lifetime, as well as maintain sufficient sensing coverage and reliability. In this paper, we propose a node-scheduling scheme, which can reduce system overall energy consumption, therefore increasing system lifetime, by turning off some redundant nodes. Our coverage-based off-duty eligibility rule and backoff-based node-scheduling scheme guarantees that the original sensing coverage is maintained after turning off redundant nodes. We implement our proposed scheme in NS-2 as an extension of the LEACH protocol. We compare the energy consumption of LEACH with and without the extension and analyze the effectiveness of our scheme in terms of energy saving. Simulation results show that our scheme can preserve the system coverage to the maximum extent. In addition, after the node-scheduling scheme turns off some nodes, certain redundancy is still guaranteed, which we believe can provide enough sensing reliability in many applications.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Energy-conserving data cache placement in sensor networks</title>
	<abstract>Wireless sensor networks hold a very promising future. The nodes of wireless sensor networks (WSN) have a small energy supply and limited bandwidth available. Since radio communication is expensive in terms of energy consumption, the nodes typically spend most of their energy reserve on wireless communication (rather than on CPU processing) for data dissemination and retrieval. Therefore, the role of energy conserving data communication protocols and services in WSN can not be overemphasized. Caching data at locations that minimize packet transmissions in the network reduces the power consumption in the network, and hence extends its lifetime. Finding locations of the nodes for caching data to minimize communication cost corresponds to finding the nodes of a weighted Minimum Steiner tree whose edge weights depend on the edge's Euclidean length and its data traffic rate. We call this tree a Steiner Data Caching Tree (SDCT). We prove that an optimal SDCT is binary, and that at-least two of the three internal angles formed at the Steiner points are equal. We derive expressions that determine the exact location of a Steiner point for a set of three nodes based on their location and their data refresh rate requirements. Based on these (optimality) results, we present a dynamic distributed energy-conserving application-layer service for data caching and asynchronous multicast. We present the results of simulation of our service that verifies its power saving properties.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>A new energy efficient target detection scheme for pervasive computing</title>
	<abstract>Energy is one of the critical constraints for the applications of sensor network. In the earlier target detection and tracking algorithm power saving is achieved by letting most of the non-border sensor nodes in the cluster stay in hibernation state. However, the border sensor nodes consume a significant amount of energy since they are supposed to be on all the time for target detection. In this paper we propose a new target detection scheme which lets the border sensor nodes be on shortly one after another in a circular fashion to minimize the energy consumption. Computer simulation shows that the proposed scheme can significantly reduce the energy consumption in target detection and tracking compared to the earlier scheme.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>FLIP: a flexible interconnection protocol for heterogeneous internetworking</title>
	<abstract>This paper describes the Flexible Interconnection Protocol, or FLIP, whose main goal is to allow interconnection of heterogeneous devices with varying power, processing, and communication capabilities, ranging from simple sensors to more powerful computing devices such as laptops and desktops. The vision is that FLIP will be used to interconnect such devices forming clouds in the farthest branches/leaves of the Internet, while still providing connectivity with the existing IP-based Internet infrastructure. Through its flexible, customizable headers FLIP integrates just the functions required by a given application and that can be handled by the underlying device. Simple devices like sensors will benefit from incurring close to optimal overhead saving not only bandwidth, but, more importantly, energy. More sophisticated devices in the cloud can be responsible for implementing more complex functions like reliable/ordered data delivery, communication with other device clouds and with the IP infrastructure.FLIP is designed to provide a basic substrate on which to build network-and transport-level functionality. In heterogeneous environments, FLIP allows devices with varying capabilities to coexist and interoperate under the same network infrastructure. We present the basic design of FLIP and describe its implementation under Linux. We also report on FLIP's performance when providing IPv4 and IPv6 as well as transport-layer functionality a la TCP and UDP. We show FLIP's energy efficiency in different sensor network scenarios. For example, we use FLIP to implement the directed diffusion communication paradigm and obtain an improvement of 50% in energy savings over an existing directed diffusion implementation. Finally, we showcase FLIP's flexibility by demonstrating its ability to incorporate new protocol functions seamlessly. In particular, we add data aggregation functionality onto FLIP and show that it significantly increases the system's energy efficiency.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Routing for wireless sensor networks lifetime maximisation under energy constraints</title>
	<abstract>Power limitation is one of the main constraints in sensor networks. Different power-saving strategies are proposed in the literature, but most of them are mainly focusing on power saving rather than maximizing the network lifetime. Moreover, Energy consumption related to reception operations is generally not considered. In this paper, we propose a new power based routing strategy that maximizes the network lifetime. The main idea is to dynamically select routes according to the remaining available power on sensors and the required power for both the transmission and reception operations. The objective is to not focus on minimizing energy consumption, but to fairly distribute power consumption so that the lifetime of the whole sensor network is maximized. The scheme is detailed and modeled. It is a non-linear programming problem that we analyzed by simulations. Numerical results show that the proposed scheme converges to an optimal routing that maximizes the network lifetime. They also demonstrate that energy consumption linked to reception operations has an important effect on the network lifetime.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Multi-level clustering architecture and protocol designs for wireless sensor networks</title>
	<abstract>Wireless sensor network (WSN) consists of sensors for measuring and gathering data in a variety of environments. These sensors, with the size and battery constraints, usually have limited transmission ranges due to the low-power wireless radio transceivers. In a sensor network, sensed data should be collected at a centralized location, called sink, for processing and analysis. With limited transmssion distances, sensed data may require multiple relays to reach the sink. In this paper, a novel multi-level clustering (MLC) wireless sensor network design and its associated operating protocol will be presented. Energy optimization is always a critical factor in the designs and deployments of wireless sensor networks. The goal is to create an energy-efficient and effective routing protocol for the networks. Cluster creation in this paper is different from the well-known Low-Energy Adaptive Clustering Hierarchy (LEACH) design. Cluster-heads in our proposed design form a tree with a goal to reach all sensor nodes in a network. Subsequently, all sensed data in the tree can be delivered to the sink while LEACH can not offer this guarantee. Energy savings may be improved with different numbers of levels in the hierarchical clustering architecture. To validate the proposed design, thorough simulations have been carried out. Upon comparing to a multi-hop LEACH protocol, the proposed design offers consistent wider coverage area and longer life span of a wireless sensor network.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Energy-efficient, collision-free medium access control for wireless sensor networks</title>
	<abstract>The traffic-adaptive medium access protocol (TRAMA) is introduced for energy-efficient collision-free channel access in wireless sensor networks. TRAMA reduces energy consumption by ensuring that unicast and broadcast transmissions incur no collisions, and by allowing nodes to assume a low-power, idle state whenever they are not transmitting or receiving. TRAMA assumes that time is slotted and uses a distributed election scheme based on information about traffic at each node to determine which node can transmit at a particular time slot. Using traffic information, TRAMA avoids assigning time slots to nodes with no traffic to send, and also allows nodes to determine when they can switch off to idle mode and not listen to the channel. TRAMA is shown to be fair and correct, in that no idle node is an intended receiver and no receiver suffers collisions. An analytical model to quantify the performance of TRAMA is presented and the results are verified by simulation. The performance of TRAMA is evaluated through extensive simulations using both synthetic-as well as sensor-network scenarios. The results indicate that TRAMA outperforms contention-based protocols (CSMA, 802.11 and S-MAC) and also static scheduled-access protocols (NAMA) with significant energy savings.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>From vertical to horizontal architecture: a cross-layer implementation in a sensor network node</title>
	<abstract>Some of the main challenges related to wireless sensor networks implementation are low-quality communication, energy conservation, resource-constrained computation, distributed network management, data processing and the scalability of the protocols. This combination makes the implementation of software a demanding task and encourages to new approaches when thinking of software architecture.In this paper an architecture combining a low protocol stack with a cross-layer management entity is presented. One of the main ideas behind the architecture presented is to make application programming easier and to simplify the protocol stack in such a way that it would suit better for the limited resources available. The role of the cross-layer management entity is to offer a shared data structure and to take care of some sensor network specific functions, like topology management and power saving. It also provides certain services that applications and the layers in the protocol stack can use.This architecture has been created specially for needs of wireless sensor networks implementation and the special attention has been paid to modularity and testability of implementation. An implementation based on this cross-layer architecture, CiNet, is also presented in this paper. The functionality of the architecture and the CiNet network was verified by using two different protocol stacks. Wireless communication of the network is based on the 802.15.4 technology.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Model-based validation of QoS properties of biomedical sensor networks</title>
	<abstract>A Biomedical Sensor Network (BSN) is a small-size sensor network for medical applications, that may contain tens of sensor nodes. In this paper, we present a formal model for BSNs using timed automata, where the sensor nodes communicate using the Chipcon CC2420 transceiver (developed by Texas Instruments) according to the IEEE 802.15.4 standard. Based on the model, we have used UPPAAL to validate and tune the temporal configuration parameters of a BSN in order to meet desired QoS requirements on network connectivity, packet delivery ratio and end-to-end delay. The network studied allows dynamic reconfigurations of the network topology due to the temporally switching of sensor nodes to power-down mode for energy-saving or their physical movements. Both the simulator and model-checker of UPPAAL are used to analyze the average-case and worst-case behaviors. To enhance the scalability of the tool, we have implemented a (new text-based) version of the UPPAAL simulator optimized for exploring symbolic traces of automata containing large data structures such as matrices. Our experiments show that even though the main feature of the tool is model checking, it is also a promising and competitive tool for efficient simulation and parameter tuning. The simulator scales well; it can easily handle up to 50 nodes in our experiments. The model checker installed on a notebook can also deal with networks with 5 up to 16 nodes within minutes depending on the properties checked; these are BSNs of reasonable size for medical applications. Finally, to study the accuracy of our model and analysis results, we compare simulation results by UPPAAL for two medical scenarios with traditional simulation techniques using OMNeT++, one of the most used simulation tools for wireless sensor networks. The comparison shows that our analysis results coincide with the simulation results by OMNeT++ in most cases although there are some differences caused the simplified wireless channel model in UPPAAL.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>An application adaptive energy model for wireless sensor nodes</title>
	<abstract>The energy efficient design of wireless sensor networks necessitates optimizations at both the system and individual node level. Node-level energy conservation and power aware decisions can lead to better overall performance and may complement any power saving techniques employed in the network infrastructure. A computationally efficient, polynomial shift algorithm that enables energy aware decisions under dynamic load conditions for wireless sensor nodes is presented. The corresponding energy model accurately approximates the energy decay characteristics for applications exhibiting different energy usage profiles. It adapts to the prevailing load conditions, and can be used by applications supporting self-learnt or over-the-air reconfiguration of nodes within the wireless sensor network. The model's computational complexity is such that it can be computed in runtime configurations on the Mica family of motes.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>A new clustering scheme for wireless sensor networks</title>
	<abstract>Energy consumption has always been the most serious issue to consider while deploying Wireless Sensor Networks (WSNs). Sensor nodes are limited in power, computational capacities and memory so reporting the occurring of specific events, such as fire or flooding, as quickly as possible using minimal energy resources is definitely a challenging issue. In this work, we propose a novel, reactive and energy-efficient scheme in order to report events. In our algorithm, nodes that detect a certain event will organize themselves into a cluster, elect a clusterhead that will collect data from the cluster members, aggregate it and forward it to the mobile sink. Our approach considers the signal strength received from the mobile sink as well as the residual energy of sensor nodes. It is worth to mention that our scheme is completely reactive, so no pre-configuration phase is needed, which will lead to a high energy saving. In order to implement and evaluate our algorithm, we used Opnet simulator. A new Opnet sensor node model, where the network layer is implemented from scratch, is also proposed in this paper; our scheme is implemented using Opnet Process Modeling Methodology (PMM).</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Energy-efficient collision-free medium access control for wireless sensor networks</title>
	<abstract>The traffic-adaptive medium access protocol (TRAMA) is introduced for energy-efficient collision-free channel access in wireless sensor networks. TRAMA reduces energy consumption by ensuring that unicast, multicast, and broadcast transmissions have no collisions, and by allowing nodes to switch to a low-power, idle state whenever they are not transmitting or receiving. TRAMA assumes that time is slotted and uses a distributed election scheme based on information about the traffic at each node to determine which node can transmit at a particular time slot. TRAMA avoids the assignment of time slots to nodes with no traffic to send, and also allows nodes to determine when they can become idle and not listen to the channel using traffic information. TRAMA is shown to be fair and correct, in that no idle node is an intended receiver and no receiver suffers collisions. The performance of TRAMA is evaluated through extensive simulations using both synthetic- as well as sensor-network scenarios. The results indicate that TRAMA outperforms contention-based protocols (e.g., CSMA, 802.11 and S-MAC) as well as scheduling-based protocols (e.g., NAMA) with significant energy savings.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>
<item>
	<title>Power-efficient data dissemination in wireless sensor networks</title>
	<abstract>This paper presents a new event-based communication model for wireless multi-hop networks of energy-constrained devices such as sensor networks. The network is arranged as an event dissemination tree, with nodes subscribing to the event types they are interested in. An event scheduler dynamically allocates and multiplexes upstream and downstream time slots for each event type. Power consumption among wireless nodes is reduced by allowing each node to power down its radio during the portions of the schedule that do not match its particular event subscription. The event dissemination schedule can be determined in both a centralized and distributed fashion, and is highly dynamic to suit the changing rates at which events are generated and distributed through the network. The paper also presents preliminary performance results that demonstrate the power savings achieved by the proposed protocols.</abstract>
	<search_task_number>19</search_task_number>
	<query>power sensor saving energy</query>
	<relevance>0</relevance>
</item>



	<item>
		<title>Infinitesimal reasoning in information retrieval and trust-based recommendation systems</title>
		<abstract>We propose preferential and trust-based frameworks for Information Retrieval and Recommender Systems, which utilize the power of Hyperreal Numbers. In the first part of our research, we propose a preferential framework for Information Retrieval which enables expressing preference annotations on search keywords and document elements, respectively. Our framework is flexible and allows expressing preferences such as “A is infinitely more preferred than B,” which we capture by using hyperreal numbers. Due to widespread use of XML as a standard for representing documents, we consider XML documents in this research and propose a consistent preferential weighting scheme for nested document elements. We show how to naturally incorporate preferences on search keywords and document elements into an IR ranking process using the well-known TF-IDF (Term Frequency - Inverse Document Frequency) ranking measure. In the second part of our research we propose a novel recommender system which enhances user-based collaborative filtering by using a trust-based social network. Again, we use hyperreal numbers and polynomials for capturing natural preferences in aggregating opinions of trusted users. We use these opinions to “help” users who are similar to an active user to come up with recommendations for items for which they might not have an opinion themselves. We argue that the method we propose reflects better the real life behaviour of the people. Our method is justified by the experimental results; we are the first to break a stated “barrier” of 0.73 for the mean absolute error (MAE) of the predicted ratings. Our results are based on a large, real life dataset from Epinions.com, for which, we also achieve a prediction coverage that is significantly better than that of the state-of-the-art methods.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>A Study on Information Recommendation System that Provides Topical Information Related to User's Inquiry for Information Retrieval</title>
		<abstract>Since the Internet has been widely common in our everyday lives, quite a few benefit has been provided to our lives, whereas we also face a big problem to find necessary information from enormous information sources on the Web. Therefore, information recommendation systems have been required and studied hard in these days. Many information recommendation systems have been proposed, and most systems have to possess and manage quite a few amounts of data such as users' profile information, history of operation behaviors, etc. In this sense, it costs much to develop a system, though we can receive useful services from the system. In this paper, we do not aim to develop such an expensive information recommendation system, but to develop a system, working beside normal search engine, that recommends information related to user's interest, i.e., user's inquiry to search engine.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>The social bookmark and publication management system bibsonomy</title>
		<abstract>Social resource sharing systems are central elements of the Web 2.0 and use the same kind of lightweight knowledge representation, called folksonomy. Their large user communities and ever-growing networks of user-generated content have made them an attractive object of investigation for researchers from different disciplines like Social Network Analysis, Data Mining, Information Retrieval or Knowledge Discovery. In this paper, we summarize and extend our work on different aspects of this branch of Web 2.0 research, demonstrated and evaluated within our own social bookmark and publication sharing system BibSonomy, which is currently among the three most popular systems of its kind. We structure this presentation along the different interaction phases of a user with our system, coupling the relevant research questions of each phase with the corresponding implementation issues. This approach reveals in a systematic fashion important aspects and results of the broad bandwidth of folksonomy research like capturing of emergent semantics, spam detection, ranking algorithms, analogies to search engine log data, personalized tag recommendations and information extraction techniques. We conclude that when integrating a real-life application like BibSonomy into research, certain constraints have to be considered; but in general, the tight interplay between our scientific work and the running system has made BibSonomy a valuable platform for demonstrating and evaluating Web 2.0 research.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>An eye-tracking-based approach to facilitate interactive video search</title>
		<abstract>This paper investigates the role of gaze movements as implicit user feedback during interactive video retrieval tasks. In this context, we use a content-based video search engine to perform an interactive video retrieval experiment, during which, we record the user gaze movements with the aid of an eye-tracking device and generate features for each video shot based on aggregated past user eye fixation and pupil dilation data. Then, we employ support vector machines, in order to train a classifier that could identify shots marked as relevant to a new query topic submitted by new users. The positive results provided by the classifier are used as recommendations for future users, who search for similar topics. The evaluation shows that important information can be extracted from aggregated gaze movements during video retrieval tasks, while the involvement of pupil dilation data improves the performance of the system and facilitates interactive video search.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>An Expert Recommendation System using Concept-based Relevance Discernment</title>
		<abstract>An expert recommendation system using concept-based relevance discernment is proposed. This system processes the description of a technical topic as input and then finds engineers who have a high level of expertise in that area. The technique employed is an extended vector space model that locates both technical topics and engineers in the same multi-dimensional space, and then calculates their relevance. This system can also retrieve engineers or documents that are related to a field matching a given engineer's technical interests. Such a system can be expected to play the role of a person's professional network, and be a valuable tool for knowledge management among several organizations.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Mining Music from Large-Scale, Peer-to-Peer Networks</title>
		<abstract>Using song files shared by users in the Gnutella network, the authors present a method for constructing a song-similarity graph to create scalable and efficient applications.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Inconsistent Requirements: an Argumentation View</title>
		<abstract>In this article, we present a logical framework for reasoning about inconsistent requirements in the context of multi-viewpoint requirements engineering process. In order to analyse the sources of inconsistencies and to reason with inconsistent requirements, we present an argumentation view of the requirements. Intuitively, argumentation is a tool for reasoning with inconsistent knowledge: requirements are defined in terms of arguments (a conclusion with its support); then, a class of acceptable arguments is built (arguments with no counterarguments). We propose to characterize different classes of requirements which are ordered: from weakly confident to strongly confident (i.e. consistent). In the paper, we present inference rules to build intra and inter-viewpoint reasoning. Inference rules are issued from the classes of requirements. We show how this work is useful for the requirements engineers to analyse inconsistent fragments of requirements.A Multi-Agent System to the Common Management of a Renewable Resource: Application to Water Sharing M. Le Bars a,b and J.M. Attonaty b a LAMSADE laboratory Université Paris-Dauphine 75775 Paris Cedex 16. France. bINRA Station d'économie rurale, BP 01 78850 Grignon France. {lebarsm@aol.com;attonaty@grigon.inra.fr} Abstract. Water sharing has become an important problem in France. A lot of negotiations are taking place at a local level between farmers, water suppliers, public services and environmentalists to allocate water resources between users. The problem is to share water with respect of different criteria like economic (global output) ethical (disparities between actors) environmental (water savings). Different approaches have been already taken using linear programming or game theory, but they are always based on the hypothesis that decision-makers are completely rational, take into account few players and are often monoperiodic. We suggest that an Agent-Based Modelling (ABM) built with a Multi-Agent approach could help negotiations between different players by showing the consequences of water allocation rules and taking in consideration the players 'respective attitudes and their ability to change their behaviour. In this paper we will first present the model structure with the different types of agents modelled, how the model runs over a number of years and the first results of simulations. Keywords. Distributed Artificial Intelligence; Multi-Agent Systems Title: A new hybrid method for solving constraint optimization problems in anytime contexts Authors: Samir Loudni and Patrice Boizumault Affiliation: Ecole Des Mines de Nantes Abstract: In this paper, we present a new hybrid method for solving constraint optimization problems in anytime contexts. We use the Valued Constraint Satisfaction Problem (VCSP) framework to model numerous discrete optimization problems. Our method (VNS/LDS+CP) combines a Variable Neighborhood Search (VNS) scheme with Limited Discrepancy Search (LDS) using Constraint Propagation (CP) to evaluate cost and legality of moves made by VNS. Our experimental results on real-word problem instances demonstrate that our method clearly outperforms both LNS/CP/GR (another hybrid method which also relies on the VCSP framework) and other standard local search methods as Simulated-Annealing. This confirm the benefit of the use, in a local search, of the LDS partial search with constraint propagation. keywords: Anytime Problems, Constraint-Satisfaction, Constraint-Optimization, Local Search Methods, Hybrid Methods. Using Software Agents to avoid Collisions among Multiple Robots Markus Jäger Corporate Technology, Information and Communications Siemens AG 81739 Munich, Germany markus.jaeger@mchp.siemens.de AI Algorithms Collaborative Software Agents Cooperating Robots This paper describes a method where collaborative software agents are used to coordinate the independently planned trajectories of multiple mobile robots to avoid collisions and deadlocks among them. Whenever the distance between two robots drops below a certain value, the agents exchange information about the planned trajectories of the robots and determine whether they are in danger of a collision. If a possible collision is detected, the agents monitor the robots movements and, if necessary, insert idle times between certain segments of the trajectories in order to avoid the collision. Deadlocks among two or more robots occur if a number of robots block each other in a way such that none of them is able to continue along its trajectory without causing a collision. These deadlocks are reliably detected by the agents. After a deadlock is detected, alternative trajectories for each of the involved robots are successively planned until the deadlock is resolved. The agents use a combination of three fully distributed algorithms to reliably solve the task. They do not use any global synchronization. ----------- Authors Affiliations: - Cooperating Software Agents - Cooperating Robots - Collision Avoidance among multiple Robots - Area Partitioning - Area Coverage The title : Successive Search Method for Valued Constraint Satisfaction and Optimization Problems. Authors Names : Mohamed TOUNSI and Philippe DAVID Email Adresses : mohamed.tounsi@emn.fr and philippe.david@emn.fr Affiliation : Computer Science Department, Ecole des Mines de Nantes , 4 Rue Alfred Kastler 44307 Nantes, FRANCE. Abstract : In this paper we introduce a new method based on Russian Doll Search (RDS) for solving optimization problems expressed as Valued Constraint Satisfaction Problems (VCSPs). The RDS method solves problems of size n (where n is the number of variables) by replacing one search by n successive searches on nested subproblems using the results of each search to produce a better lower bound. The main idea of our method is to introduce the variables through the successive searches not one by one but by sets of k variables. We present two variants of our method: the first one where the number k is fixed, noted kfRDS; the second one, kvRDS, where k can be variable. Finally, we show that our method improves RDS on daily management of an earth observation satellite. Keywords : Constraint Satisfaction, VCSP, Optimization Problems. B-Course: A Web Service for Bayesian Data Analysis Petri Myllymaki, Tomi Silander, Henry Tirri, Pekka Uronen Complex Systems Computation Group (CoSCo) P.O.Box 26, Department of Computer Science FIN-00014 University of Helsinki, Finland URL: http://www.cs.Helsinki.FI/research/cosco/ B-Course (http://b-course.cs.helsinki.fi) is a free web-based online data analysis tool, which allows the users to analyze their data for multivariate probabilistic dependencies. These dependencies are represented as Bayesian network models. In addition to this, B-Course also offers facilities for inferring certain type of causal dependencies from the data. The software uses a novel "tutorial style" user-friendly interface which intertwines the steps in the data analysis with support material that gives an informal introduction to the Bayesian approach adopted. Although the analysis methods, modeling assumptions and restrictions are totally transparent to the user, this transparency is not achieved at the expense of analysis power: with the restrictions stated in the support material, B-Course is a powerful analysis tool exploiting several theoretically elaborate results developed recently in the fields of Bayesian and causal modeling. B-Course can be used with most web-browsers (even Lynx), and the facilities include features such as automatic missing data handling and discretization, a flexible graphical interface for probabilistic inference on the constructed Bayesian network models (for Java enabled browsers), automatic pretty-printed layout for the networks, exportation of the models, and analysis of the importance of the derived dependencies. In this paper we discuss both the theoretical design principles underlying the B-Course tool, and the pragmatic methods adopted in the implementation of the software. Artificial Neural Networks in Hydrological Watershed Modeling: Surface Flow Contribution from the Ungaged Parts of a Catchment Richard Chibanga1, Jean Berlamont2 and Joos Vandewalle3 1PhD student in the Civil Eng. Dept. and 2Prof. Civil Eng. Dept., and Head of Hydraulics Laboratory, 3 Prof. Head of Electrical Engineering Dept - SISTA/COSIC, Katholieke Universiteit, Kasteelpark Arenberg 40, 3001 Heverlee (Leuven), Belgium Abstract Watershed modeling is often faced with the difficulty of determining the flow contribution from the ungaged sections of the catchment. Where the main concern is making accurate streamflow forecasts at specific watershed locations, it is cost-effective and efficient to implement a simple system theoretic model. In this paper Artificial Neural Networks (ANNs) are used as system theoretic models to model the ungaged flows. Using data from the Kafue River sub-catchment in Zambia and a simple reservoir routing model, an estimate of the flow contribution from the ungaged sections is derived. Inputs: rainfall, evaporation, previous-time-step flow are fed to a series of Feedforward-Backpropagation ANNs with target-output the current derived flow. Selected best- performing ANNs are compared with Autoregressive Moving Average models with exogenous inputs (ARMAX) and they give accurate and more robust forecasts over long term than the best performing ARMAXs thereby making ANNs a viable alternative in time-series forecasting. Keywords: semi conceptual-system theoretic; Artificial neural networks; subsystem; tributary-runoff; forecasting; mapping. Title: Generation of Propagation Rules for Intentionally Defined Constraints Authors: Slim Abdennadher Computer Science Department, University of Munich Oettingenstr. 67, 80538 Munich, Germany Slim.Abdennadher@informatik.uni-muenchen.de Christophe Rigotti Laboratoire d'Ingenierie des Systemes d'Information Batiment 501, INSA Lyon, 69621 Villeurbanne Cedex, France Christophe.Rigotti@insa-lyon.fr Abstract: A general approach to implement propagation and simplification of constraints consists of applying rules over these constraints. However, a difficulty that arises frequently when writing a constraint solver is to determine the constraint propagation algorithm. In previous work, different methods for automatic generation of propagation rules for constraints defined over finite domains have been proposed. In this paper, we present a method for generating propagation rules for constraint predicates defined by means of a constraint logic program. Keywords: Constraint solving, Machine learning, Rule-based programming Title : "Data Flow Coherence Criteria in ILP Tools" Authors : Smaranda Muresan Department of Computer Science, Columbia University, New York, USA smara@cs.columbia.edu Tudor Muresan Department of Computer Science, Technical Univ. of Cluj-Napoca, Cluj-Napoca, Romania tmuresan@cs.utcluj.ro Rodica Potolea Department of Computer Science, Technical Univ. of Cluj-Napoca, Cluj-Napoca, Romania potolea@cs.utcluj.ro Keywords : Inductive Logic Programming, Automatic Program Generation, Data Flow Coherence Criteria, Pruning the Search Space Abstract: In this paper we present a new method that uses data-flow coherence criteria in definite logic program generation. We outline three main advantages of these criteria supported by our results: i) drastically pruning the search space (around 90%), ii) reducing the set of positive examples and reducing or even removing the need for the set of negative examples, and iii) allowing the induction of predicates that are difficult or even impossible to generate by other methods. Besides these criteria, the approach takes into consideration the program termination condition for recursive predicates. The paper outlines some theoretical issues and implementation aspects of our system for automatic logic program induction. Title: An Expert Recommendation System using Concept-based Relevance Discernment Authors: Takashi Yukawa NTT Corporation NTT Communication Science Laboratories 2-4 Hikaridai, Seika-cho, Kyoto, Japan yukawa@cslab.kecl.ntt.co.jp Kaname Kasahara NTT Corporation NTT Communication Science Laboratories 2-4 Hikaridai, Seika-cho, Kyoto, Japan kaname@cslab.kecl.ntt.co.jp Tsuneaki Kato The University of Tokyo Graduate School of Arts and Science Dept. of Language and Information Science 3-1-8 Komaba, Meguro-ku, Tokyo, Japan kato@boz.c.u-tokyo.ac.jp Toshiro Kita NTT Communications Corporation Solution Business Division Yamato Seimei Bldg. 1-1-7, Uchisaiwai-cho Chiyoda-ku, Tokyo, Japan toshiro.kita@ntt.com Keywords: information retrieval, recommendation system, vector space model, concept base, knowledge management Abstract: An expert recommendation system using concept-based relevance discernment is proposed. This system processes the description of a technical topic as input and then finds engineers who have a high level of expertise in that area. The technique employed is an extended vector space model that locates both technical topics and engineers in the same multi-dimensional space, and then calculates their relevance. This system can also retrieve engineers or documents that are related to a field matching a given engineer's technical interests. Such a system can be expected to play the role of a person's professional network, and be a valuable tool for knowledge management among several organizations. Paper Title: Combinatorial Optimization through Statistical Instance-Based Learning Keywords: constructive search, heuristics, optimization, instance-based learning Authors: Orestis Telelis, Panagiotis Stamatopoulos Department of Informatics and Telecommunications University of Athens 157 84 Athens, Greece {telelis,takis}@di.uoa.gr Abstract: Different successful heuristic approaches have been proposed for solving combinatorial optimization problems. Commonly, each of them is specialized to serve a different purpose or address specific difficulties. However, most combinatorial problems that model real world applications have a priori well known measurable properties. Embedded machine learning methods may aid towards the recognition and utilization of these properties for the achievement of satisfactory solutions. In this paper, we present a heuristic methodology which employs the instance-based machine learning paradigm. This methodology can be adequately configured for several types of optimization problems which are known to have certain properties. Experimental results are discussed concerning two well known problems, namely the knapsack problem and the set partitioning problem. These results show that the proposed approach is able to find significantly better solutions compared to intuitive search methods based on heuristics which are usually applied to the specific problems. TITLE: Interleaved Backtracking in Distributed Constraint Networks  AUTHOR: Youssef Hamadi  AFFILIATION: Hewlett Packards Labs Filton road, Stoke Gifford,  Bristol BS34 8QZ, United Kingdom  EMAIL: yh@hplb.hpl.hp.com  Abstract The adaptation of software technology to distributed  environments is an important challenge today. In this  work we combine parallel and distributed search. By  this way we add the potential speed up of a parallel  exploration in the processing of distributed problems.  This paper extends DIBT, a distributed search proce  dure operating in distributed constraint networks [6].  The extension is twofold. First the procedure is up  dated to face delayed information problems upcoming  in heterogeneous systems. Second, the search is ex  tended to simultaneously explore independent parts of  a distributed search tree. By this way we introduce  parallelism into distributed search, which brings to In  terleaved Distributed Intelligent BackTracking (IDIBT).  Our results show that 1) insoluble problems do not  greatly degrade performance over DIBT and 2) super  linear speed up can be achieved when the distribution  of solution is nonuniform.   Keywords: Distributed Constraint Satisfaction, Distributed AI,  Collaborative Software Agents, Search</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Text Mining</title>
		<abstract>Most data mining methods assume that the data to be mined is represented in a structured relational database. However, in many applications, available electronic information is in the form of unstructured natural-language documents rather than structured databases. This tutorial will review machine learning methods for text mining. First, we will review standard classification and clustering methods for text which assume a vector-space or "bag of words" representation of documents that ignores the order of words in text. We will discuss naive Bayes, Rocchio, nearest neighbor, and SVMs for classifying texts and hierarchical agglomerative, spherical k-means and Expectation Maximization (EM) methods for clustering texts. Next we will review information extraction (IE) methods that use sequence information to identify entities and relations in documents. We will discuss hidden Markov models (HMMs) and conditional random fields (CRFs) for sequence labeling and IE. We will motivate the methods discussed with applications in spam filtering, information retrieval, recommendation systems, and bioinformatics.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Task-specific information retrieval systems for software engineers</title>
		<abstract>This paper discusses the development of task-specific information retrieval systems for software engineers. We discuss how software engineers interact with information and information retrieval systems and investigate to what extent a domain-specific search and recommendation system can be developed in order to support their work related activities. We have conducted a user study which is based on the ''Cognitive Research Framework'' to identify the relation between the information objects used during the code development (code snippets and search queries), the tasks users engage in and the associated use of search interfaces. Based on our user studies, a questionnaire and an automated observation of user interactions with the browser and software development environment, we identify that software engineers engage in a finite number of work related tasks and they also develop a finite number of ''work practices''/''archetypes of behaviour''. Secondly we identify a group of domain specific behaviours that can successfully be used as a source of strong implicit relevance feedback. Based on our results, we design a snippet recommendation interface, and a code related recommendation interface which are embedded within the standard search engine.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Software engineers' information behaviour and implicit relevance indicators</title>
		<abstract>Software engineering is a cognitively challenging process that requires continuous access to multiple sources of information. As a consequence Software Engineers spend a significant proportion of time (20-30%) searching for information and engaging in opportunistic programming practices, reusing the existing software elements. In this paper, we summarise the findings, discussing how software developers interact with information and information retrieval systems. Importantly we investigate to what extent a domain-specific search and recommendation system can be delivered in order to support their daily activities. Based on two user studies, a questionnaire and an automated observation of user interactions with the browser, we identify that software engineers engage in a finite number of work-related tasks and also develop a finite number of 'work practices'/'archetypes of behaviour'. Secondly we identify a group of domain-specific behaviours that can successfully be used for relevance feedback of a domain-specific and semi-collaborative information recommendation system that can support software engineers in performing their daily work-related tasks more effectively.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Effects of Usage-Based Feedback on Video Retrieval: A Simulation-Based Study</title>
		<abstract>We present a model for exploiting community-based usage information for video retrieval, where implicit usage information from past users is exploited in order to provide enhanced assistance in video retrieval tasks, and alleviate the effects of the semantic gap problem. We propose a graph-based model for all types of implicit and explicit feedback, in which the relevant usage information is represented. Our model is designed to capture the complex interactions of a user with an interactive video retrieval system, including the representation of sequences of user-system interaction during a search session. Building upon this model, four recommendation strategies are defined and evaluated. An evaluation strategy is proposed based on simulated user actions, which enables the evaluation of our recommendation strategies over a usage information pool obtained from 24 users performing four different TRECVid tasks. Furthermore, the proposed simulation approach is used to simulate usage information pools with different characteristics, with which the recommendation approaches are further evaluated on a larger set of tasks, and their performance is studied with respect to the scalability and quality of the available implicit information.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Recommender Systems Research: A Connection-Centric Survey</title>
		<abstract>Recommender systems attempt to reduce information overload and retain customers by selecting a subset of items from a universal set based on user preferences. While research in recommender systems grew out of information retrieval and filtering, the topic has steadily advanced into a legitimate and challenging research area of its own. Recommender systems have traditionally been studied from a content-based filtering vs. collaborative design perspective. Recommendations, however, are not delivered within a vacuum, but rather cast within an informal community of users and social context. Therefore, ultimately all recommender systems make connections among people and thus should be surveyed from such a perspective. This viewpoint is under-emphasized in the recommender systems literature. We therefore take a connection-oriented perspective toward recommender systems research. We posit that recommendation has an inherently social element and is ultimately intended to connect people either directly as a result of explicit user modeling or indirectly through the discovery of relationships implicit in extant data. Thus, recommender systems are characterized by how they model users to bring people together: explicitly or implicitly. Finally, user modeling and the connection-centric viewpoint raise broadening and social issues—such as evaluation, targeting, and privacy and trust—which we also briefly address.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>The task-dependent effect of tags and ratings on social media access</title>
		<abstract>Recently, online social networks have emerged that allow people to share their multimedia files, retrieve interesting content, and discover like-minded people. These systems often provide the possibility to annotate the content with tags and ratings. Using a random walk through the social annotation graph, we have combined these annotations into a retrieval model that effectively balances the personal preferences and opinions of like-minded users into a single relevance ranking for either content, tags, or people. We use this model to identify the influence of different annotation methods and system design aspects on common ranking tasks in social content systems. Our results show that a combination of rating and tagging information can improve tasks like search and recommendation. The optimal influence of both sources on the ranking is highly dependent on the retrieval task and system design. Results on content search and tag suggestion indicate that the profile created by a user's annotations can be used effectively to adapt the ranking to personal preferences. The random walk reduces sparsity problems by smoothly integrating indirectly related concepts in the relevance ranking, which is especially valuable for cold-start users or individual tagging systems like YouTube and Flickr.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Using the structure of overlap between search results to rank retrieval systems without relevance judgments</title>
		<abstract>This paper addresses the problem of how to rank retrieval systems without the need for human relevance judgments, which are very resource intensive to obtain. Using TREC 3, 6, 7 and 8 data, it is shown how the overlap structure between the search results of multiple systems can be used to infer relative performance differences. In particular, the overlap structures for random groupings of five systems are computed, so that each system is selected an equal number of times. It is shown that the average percentage of a system's documents that are only found by it and no other systems is strongly and negatively correlated with its retrieval performance effectiveness, such as its mean average precision or precision at 1000. The presented method uses the degree of consensus or agreement a retrieval system can generate to infer its quality. This paper also addresses the question of how many documents in a ranked list need to be examined to be able to rank the systems. It is shown that the overlap structure of the top 50 documents can be used to rank the systems, often producing the best results. The presented method significantly improves upon previous attempts to rank retrieval systems without the need for human relevance judgments. This ''structure of overlap'' method can be of value to communities that need to identify the best experts or rank them, but do not have the resources to evaluate the experts' recommendations, since it does not require knowledge about the domain being searched or the information being requested.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Applying associative retrieval techniques to alleviate the sparsity problem in collaborative filtering</title>
		<abstract>Recommender systems are being widely applied in many application settings to suggest products, services, and information items to potential consumers. Collaborative filtering, the most successful recommendation approach, makes recommendations based on past transactions and feedback from consumers sharing similar interests. A major problem limiting the usefulness of collaborative filtering is the sparsity problem, which refers to a situation in which transactional or feedback data is sparse and insufficient to identify similarities in consumer interests. In this article, we propose to deal with this sparsity problem by applying an associative retrieval framework and related spreading activation algorithms to explore transitive associations among consumers through their past transactions and feedback. Such transitive associations are a valuable source of information to help infer consumer interests and can be explored to deal with the sparsity problem. To evaluate the effectiveness of our approach, we have conducted an experimental study using a data set from an online bookstore. We experimented with three spreading activation algorithms including a constrained Leaky Capacitor algorithm, a branch-and-bound serial symbolic search algorithm, and a Hopfield net parallel relaxation search algorithm. These algorithms were compared with several collaborative filtering approaches that do not consider the transitive associations: a simple graph search approach, two variations of the user-based approach, and an item-based approach. Our experimental results indicate that spreading activation-based approaches significantly outperformed the other collaborative filtering methods as measured by recommendation precision, recall, the F-measure, and the rank score. We also observed the over-activation effect of the spreading activation approach, that is, incorporating transitive associations with past transactional data that is not sparse may "dilute" the data used to infer user preferences and lead to degradation in recommendation performance.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Enhancement of information seeking using an information needs radar model</title>
		<abstract>Information seeking is the act of obtaining information from existing resources in both human and technological contexts, and past studies have applied the behavior of users to determine the user needs. Search engines, information retrieval, and recommendation systems are the major solutions of information seeking. However, these techniques lack a description method for overall information needs and other limitations. Information seeking behavior is related to the content and concepts in content, and this study proposes an information needs radar model, which consists of users, content and concepts to describe information needs. The information seeking architecture based on this model is used to evaluate and obtain information about users' needs. The experimental results indicated that our proposed architecture has stable and better performance irrespective of data size, which demonstrates the applicability and effectiveness of the architecture. Furthermore, the information needs the radar model to be able to satisfy customer demands; it is not only helpful in the development of information filtering, recommendation systems, and knowledge-based systems, but also enhances the reliance and loyalty of users towards the system.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Web-Based Recommender Systems and User Needs --the Comprehensive View</title>
		<abstract>Recommender systems became an important element of many web-based systems. The general process about how recommender systems fulfil various user needs based on the analysis of recent literature in presented in the survey paper. Recommender systems are compared with other method like Information Retrieval and browsing. The paper also includes the comprehensive view over the typical components and structure of recommender systems. The state-of-the-art of recommendation methods is described as well.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Unified relevance models for rating prediction in collaborative filtering</title>
		<abstract>Collaborative filtering aims at predicting a user's interest for a given item based on a collection of user profiles. This article views collaborative filtering as a problem highly related to information retrieval, drawing an analogy between the concepts of users and items in recommender systems and queries and documents in text retrieval. We present a probabilistic user-to-item relevance framework that introduces the concept of relevance into the related problem of collaborative filtering. Three different models are derived, namely, a user-based, an item-based, and a unified relevance model, and we estimate their rating predictions from three sources: the user's own ratings for different items, other users' ratings for the same item, and ratings from different but similar users for other but similar items.To reduce the data sparsity encountered when estimating the probability density function of the relevance variable, we apply the nonparametric (data-driven) density estimation technique known as the Parzen-window method (or kernel-based density estimation). Using a Gaussian window function, the similarity between users and/or items would, however, be based on Euclidean distance. Because the collaborative filtering literature has reported improved prediction accuracy when using cosine similarity, we generalize the Parzen-window method by introducing a projection kernel.Existing user-based and item-based approaches correspond to two simplified instantiations of our framework. User-based and item-based collaborative filterings represent only a partial view of the prediction problem, where the unified relevance model brings these partial views together under the same umbrella. Experimental results complement the theoretical insights with improved recommendation accuracy. The unified model is more robust to data sparsity because the different types of ratings are used in concert.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Designing consumer health information systems: what do user-generated questions tell us?</title>
		<abstract>Searching for health information has become a prevalent activity on the web. The information found online has a significant impact on people's decisions on whether to seek medical care and what treatments to undergo. However, existing studies consistently suggest that general consumers have various difficulties in formulating search queries using existing search engines and the queries were often not effective in retrieving personal- and situationalrelevant information. Understanding users' information needs is a gateway to designing effective information retrieval (IR) systems. In this study, we examined the types of information requested by users, the characteristics of consumers' expressions of their information needs, and their expectations for results by analyzing the questions that general users posted on Yahoo! Answers, a popular social Q et A site. Based on the results, we proposed design recommendations for facilitating users' ability to articulate their health information needs and recommendations for the presentation of information in health-related IR systems.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Novelty and Diversity in Top-N Recommendation -- Analysis and Evaluation</title>
		<abstract>For recommender systems that base their product rankings primarily on a measure of similarity between items and the user query, it can often happen that products on the recommendation list are highly similar to each other and lack diversity. In this article we argue that the motivation of diversity research is to increase the probability of retrieving unusual or novel items which are relevant to the user and introduce a methodology to evaluate their performance in terms of novel item retrieval. Moreover, noting that the retrieval of a set of items matching a user query is a common problem across many applications of information retrieval, we formulate the trade-off between diversity and matching quality as a binary optimization problem, with an input control parameter allowing explicit tuning of this trade-off. We study solution strategies to the optimization problem and demonstrate the importance of the control parameter in obtaining desired system performance. The methods are evaluated for collaborative recommendation using two datasets and case-based recommendation using a synthetic dataset constructed from the public-domain Travel dataset.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Integrating collaborate and content-based filtering for personalized information recommendation</title>
		<abstract>To achieve high quality of push-based information service, in this paper, collaborative filtering and content-based adaptability approaches are surveyed for user-centered personalized information, then based on the above method, we proposed a mixed two-phased recommendation algorithm for high-quality information recommendation, upon which performance evaluations showed that the mixed algorithm is more efficient than pure content-based or collaborative filtering methods, for pure of either approaches is not so efficient for the lack of enough information need information. And moreover we found with large amount registered users, it is necessary and important for the system to serve users in a group mode, which involved merged retrieval issues.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Knowledge reuse for software reuse</title>
		<abstract>Software reuse can provide significant improvements in software productivity and quality whilst reducing development costs. Expressing software reuse intentions can be difficult though. A developer may aspire to reuse a software component but experience difficulty expressing their reuse intentions in a manner that is compatible with, or understood by, the component retrieval system. Various intelligent retrieval techniques have been developed that assist a developer in locating or discovering components in an efficient manner. These solutions share a common shortcoming: the developer must be capable of anticipating all reuse opportunities and initiating the retrieval process. There is a need for a comprehensive technique that not only assists with retrievals but that can also identify reuse opportunities. This paper advocates that component-based reuse can be supported through knowledge collaboration. Often programming tasks and solutions are replicated; this characteristic of software can be exploited for the benefit of future developments. Through the mining of existing source code solutions, knowledge, relating to how components are used by developers, can be extracted. Based on a developer's current programming task, this knowledge can subsequently be filtered and used to recommend a candidate set of reusable components. This novel recommendation approach applies and extends commonly used Information Retrieval and Information Filtering techniques such as Collaborative Filtering, Content-Based Filtering, and Bayesian Clustering Models, to the software reuse domain. This recommendation technology is applied to several thousand open-source Java classes. The most effective recommendation algorithm produces recommendations of a high quality at a low cost.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Search trails using user feedback to improve video search</title>
		<abstract>In this paper we present an innovative approach for aiding users in the difficult task of video search. We use community based feedback mined from the interactions of previous users of our video search system to aid users in their search tasks. This feedback is the basis for providing recommendations to users of our video retrieval system. The ultimate goal of this system is to improve the quality of the results that users find, and in doing so, help users to explore a large and difficult information space and help them consider search options that they may not have considered otherwise. In particular we wish to make the difficult task of search for video much easier for users. The results of a user evaluation indicate that we achieved our goals, the performance of the users in retrieving relevant videos improved, and users were able to explore the collection to a greater extent.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>A Web-Based Framework for User-Centred Evaluation of End-User Experience in Adaptive and Personalized E-Learning Systems</title>
		<abstract>The evaluation of interactive adaptive and personalised systems has long been acknowledged as a difficult, complicated and very demanding Endeavour due to the complex nature of these systems. This paper describes a web-based framework for the evaluation of end-user experience in adaptive and personalised e-Learning systems. The benefits of the framework include: i) the provision of an interactive reference and recommendation tool to encourage the evaluation of systems that fulfil certain methodological requirements, ii) the collaborative nature of the framework facilitates the sharing of information among researchers from the information technology, adaptive hypermedia, information retrieval and e-Learning communities, iii) the identification of pitfalls in the evaluation planning process as well as in data analysis, and iv) the translation of presented information into users language of choice. This paper also presents a review of User-Centred Evaluation approaches, methodologies and techniques adopted by current systems and frameworks. The results of this review are analysed. From these results, an architectural design for the framework was specified.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Personalized topic-based tag recommendation</title>
		<abstract>More and more content on the Web is generated by users. To organize this information and make it accessible via current search technology, tagging systems have gained tremendous popularity. Especially for multimedia content they allow to annotate resources with keywords (tags) which opens the door for classic text-based information retrieval. To support the user in choosing the right keywords, tag recommendation algorithms have emerged. In this setting, not only the content is decisive for recommending relevant tags but also the user's preferences. In this paper we introduce an approach to personalized tag recommendation that combines a probabilistic model of tags from the resource with tags from the user. As models we investigate simple language models as well as Latent Dirichlet Allocation. Extensive experiments on a real world dataset crawled from a big tagging system show that personalization improves tag recommendation, and our approach significantly outperforms state-of-the-art approaches.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Recommendation in the end-to-end encrypted domain</title>
		<abstract>In recommendation systems, a central host typically requires access to user profiles in order to generate useful recommendations. This access, however, undermines user privacy; the more information is revealed to the host, the more the user's privacy is compromised. In this paper, we propose a novel end-to-end encrypted recommendation mechanism which encrypts sensitive private data at the user end, without ever exposing plaintext private data to the host server. Unlike previously proposed privacy-preserving recommendation mechanisms, the data in this proposed system are lossless - a pivotal feature to many applications, e.g., in health informatics, business analytics, cyber security, etc. We achieve this goal by developing encrypted-domain polynomial ring homomorphism cryptographic algorithms to compute similarity of encrypted scores on the server, so that collaborative recommendations can be computed in the encryption domain and only an authorized person can decrypt the exact results. We also propose a novel key management system to make sure private information retrieval and recommendation computations can be executed in the encrypted domain in practice. Our experiments show that the proposed scheme offers robust security and lossless accurate recommendation, as well as high efficiency. Our preliminary results show the recommendation accuracy is 21% better than the existing statistical lossy privacy-preserving mechanisms based on random perturbation and user profile distribution. This new approach can potentially be applied to various data mining and cloud computing environments and significantly alleviates the privacy concerns of users.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Applying data mining techniques to address disaster information management challenges on mobile devices</title>
		<abstract>The improvement of Crisis Management and Disaster Recovery techniques are national priorities in the wake of man-made and nature inflicted calamities of the last decade. Our prior work has demonstrated that the efficiency of sharing and managing information plays an important role in business recovery efforts after disaster event. With the proliferation of smart phones and wireless tablets, professionals who have an operational responsibility in disaster situations are relying on such devices to maintain communication. Further, with the rise of social media, technology savvy consumers are also using these devices extensively for situational updates. In this paper, we address several critical tasks which can facilitate information sharing and collaboration between both private and public sector participants for major disaster recovery planning and management. We design and implement an All-Hazard Disaster Situation Browser (ADSB) system that runs on Apple's mobile operating system (iOS) and iPhone and iPad mobile devices. Our proposed techniques create a collaborative solution on a mobile platform using advanced data mining and information retrieval techniques for disaster preparedness and recovery that helps impacted communities better understand the current disaster situation and how the community is recovering. Specifically, hierarchical summarization techniques are used to generate brief reviews from a large collection of reports at different granularities; probabilistic models are proposed to dynamically generate query forms based on user's feedback; and recommendation techniques are adapted to help users identify potential contacts for report sharing and community organization. Furthermore, the developed techniques are designed to be all-hazard capable so that they can be used in earthquake, terrorism, or other unanticipated disaster situations.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Mining the real-time web: A novel approach to product recommendation</title>
		<abstract>Real-time web (RTW) services such as Twitter allow users to express their opinions and interests, often expressed in the form of short text messages providing abbreviated and highly personalized commentary in real-time. Although this RTW data is far from the structured data (movie ratings, product features, etc.) that is familiar to recommender systems research, it can contain useful consumer reviews on products, services and brands. This paper describes how Twitter-like short-form messages can be leveraged as a source of indexing and retrieval information for product recommendation. In particular, we describe how users and products can be represented from the terms used in their associated reviews. An evaluation performed on four different product datasets from the Blippr service shows the potential of this type of recommendation knowledge, and the experiments show that our proposed approach outperforms a more traditional collaborative-filtering based approach.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Vizier: a generic and multidimensional agent-based recommendation framework</title>
		<abstract>Recommender Systems have emerged to help support, augment and systematize the everyday natural social process of creating and sharing recommendations by developing tools that can be used to quickly identify interesting products, and therefore, reduce a search space of alternatives. This paper aims to present a framework, constructed under a generic approach, which provides services to Information Retrieval applications so these may offer product recommendations that consider several Adaptation/Personalization dimensions (e.g., user dimension, context, among others). With this purpose, the Multi-Agent Vizier Recommendation Framework (Vizier) is proposed; on the one hand, to assist those entities that currently develop Information Retrieval applications and wish to add recommendations to their services (e.g., E-Commerce applications); on the other hand, in order to offer a solution that hopefully provides better adapted/personalized results than current solutions by considering the multidimensionality of users, items and context. In order to validate Vizier, ZoundBeat was implemented. ZoundBeat is a functional application of a music player that is capable of invoking the proposed framework to offer its users song recommendations.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>A framework for the evaluation of adaptive IR systems through implicit recommendation</title>
		<abstract>Personalised Information Retrieval (PIR) has gained considerable attention in recent literature. In PIR different stages of the retrieval process are adapted to the user, such as adapting the user's query or the results. Personalised recommender frameworks are endowed with intelligent mechanisms to search for products, goods and services that users are interested in. The objective of such tools is to evaluate and filter the huge amount of information available within a specific scope to assist users in their information access processes. This paper presents a web-based adaptive framework for evaluating personalised information retrieval systems. The framework uses implicit recommendation to guide users in deciding which evaluation techniques, metrics and criteria to use. A task-based experiment was conducted to test the functionality and performance of the framework. A Review of evaluation techniques for personalised IR systems was conducted and the results of the analysed survey are presented.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Modeling information sources as integrals for effective and efficient source selection</title>
		<abstract>In this paper, a new source selection algorithm for uncooperative distributed information retrieval environments is presented. The algorithm functions by modeling each information source as an integral, using the relevance score and the intra-collection position of its sampled documents in reference to a centralized sample index and selects the collections that cover the largest area in the rank-relevance space. Based on the above novel metric, the algorithm explicitly focuses on addressing the two goals of source selection; high-recall, which is important for source recommendation applications and high-precision which is important for distributed information retrieval, aiming to produce a high-precision final merged list. For the latter goal in particular, the new approach steps away from the usual practice of DIR systems of explicitly declaring the number of collections that must be queried and instead focuses solely on the number of retrieved documents in the final merged list, dynamically calculating the number of collections that are selected and the number of documents requested from each. The algorithm is tested in a wide range of testbeds in both recall and precision-oriented settings and its effectiveness is found to be equal or better than other state-of-the-art algorithms.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Multiagent system for learning objects retrieval with context attributes</title>
		<abstract>Educational standards that include content metadata description for materials have great potential for managing e-learning information and content units and facilitate their interoperability and reutilisation. These open semantic and distance learning aspects contribute new and important possibilities for online education systems. The Learning Objects (LOs) paradigm focuses this new gap on the management and exchange of educational materials. This paper presents an application approach to educational content recommendation based on Learning Objects. It describes an architecture based on a multi-agent system with a bio-inspired algorithm rooted in self-organisation theory. It supports the retrieval, search, selection and composition of these LOs.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>On the role of trust in collaborative Web search</title>
		<abstract>Recommender systems combine ideas from information retrieval, user modelling, and artificial intelligence to focus on the provision of more intelligent and proactive information services. As such, recommender systems play an important role when it comes to assisting the user during both routine and specialised information retrieval tasks. Like any good assistant it is important that users can trust in the ability of a recommender system to respond with timely and relevant suggestions. In this paper, we will look at a collaborative recommendation system operating in the domain of Web search. We will show how explicit models of trust can help to inform more reliable recommendations that translate into more relevant search results. Moreover, we demonstrate how the availability of this trust-model facilitates important interface enhancements that provide a means to declare the provenance of result recommendations in a way that will allow searchers to evaluate their likely relevance based on the reputation and trustworthiness of the recommendation partners behind these suggestions.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>An intelligent system integrated with fuzzy ontology for product recommendation and retrieval</title>
		<abstract>Systems that are integrated with recommendation features to support decision processes can provide a more personalized and proactive retrieval experience for users to choose among retrieval alternatives and subsequent refinement of choices. Many real-world systems call for autonomous intelligent agents acting in the face of uncertain knowledge and limited computational resources. This paper explores the development of an intelligence multi-agent based system for book recommendation and retrieval. It enlists the features of our iJADE (intelligent Java Agent-based Development Environment platform) to provide convenient Java tools and libraries for the agent components. The system possesses fuzzy reasoning features suitable for comparing, evaluating and classifying the webagents behavior. The fuzzy decision-making is based on user preferences, product feature selection and system recommendation. We can make use of the platform to build a web system integrated with the mobile multi-agent system and fuzzy ontology. The use of mobile agent technology provides a specific solution for overcoming the problem of being overloaded with too much information. User can simply enter preference information through the web browser and the backend iJADE server gets the request and sends back the relevant recommendations to the client. The simulation testing shows promising results.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>A mobile location-based information recommendation system based on GPS and WEB2.0 services</title>
		<abstract>Combining the GPS location-based services and the latest Web2.0 technologies, this paper builds a scalable personalized mobile information pushing platform, which can provide user-friendly and flexible location-based service. We first propose a Location-based Data and Service Middleware based on Service-Oriented Architecture in order to implement Mobile Information Pushing System involved in a variety of formats of data integration and conversion, as well as a combination of a wide range of services. Then, we propose a novel 3-D Tag-Cloud module, so that it can visualize useful retrieval information even in the limited mobile screen. Especially, we design a multi-dimensional collaborative filtering algorithms, in order to achieve dynamic personalized recommendation and mobile information sharing. Cooperating with some restuarants, we also develop a dynamic restaurant mobile location-based recommendation and discount counpons pushing system. The successful application of the application system do show the efficiency of our ideas.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Awareness, Representation and Interpretation</title>
		<abstract>This paper discusses how representation and interpretation affect the degree and character of awareness afforded by computer systems: awareness of people and of information artifacts. Our discussion ranges from system design to theoretical concepts, and we focus on consistencies across this spectrum. We begin by briefly describing a prototype collaborative filtering system, Recer. This system tracks ongoing activity in the web browsers and text editors of a group of people, and offers recommendations of URLs and local program files that are specific to and adaptive with that activity, and that reflect patterns of earlier activity within the community of use. We then take a more general look at collaborative filtering, and compare it with two other approaches to engendering awareness of useful artifacts: information retrieval and software patterns. We discuss how each implicitly or explicitly involves collaboration, formalisation and subjectivity in its core representations. We then explore the artifact-centred approach to awareness that Recer represents, and relate it to the activity-centred approach more familiar within CSCW. We use this comparison in discussing, in more theoretical terms, how representation and formalisation affects awareness, interpretation and use. Our intention is to explore and understand the choices that designers have for the core representations of information systems, and the consequences for awareness that follow for users. We wish to relate such practical design issues to the more theoretical discussion in CSCW around concepts such as common information spaces, the space-place distinction, and the status of formal constructs.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Categorising social tags to improve folksonomy-based recommendations</title>
		<abstract>In social tagging systems, users have different purposes when they annotate items. Tags not only depict the content of the annotated items, for example by listing the objects that appear in a photo, or express contextual information about the items, for example by providing the location or the time in which a photo was taken, but also describe subjective qualities and opinions about the items, or can be related to organisational aspects, such as self-references and personal tasks. Current folksonomy-based search and recommendation models exploit the social tag space as a whole to retrieve those items relevant to a tag-based query or user profile, and do not take into consideration the purposes of tags. We hypothesise that a significant percentage of tags are noisy for content retrieval, and believe that the distinction of the personal intentions underlying the tags may be beneficial to improve the accuracy of search and recommendation processes. We present a mechanism to automatically filter and classify raw tags in a set of purpose-oriented categories. Our approach finds the underlying meanings (concepts) of the tags, mapping them to semantic entities belonging to external knowledge bases, namely WordNet and Wikipedia, through the exploitation of ontologies created within the W3C Linking Open Data initiative. The obtained concepts are then transformed into semantic classes that can be uniquely assigned to content- and context-based categories. The identification of subjective and organisational tags is based on natural language processing heuristics. We collected a representative dataset from Flickr social tagging system, and conducted an empirical study to categorise real tagging data, and evaluate whether the resultant tags categories really benefit a recommendation model using the Random Walk with Restarts method. The results show that content- and context-based tags are considered superior to subjective and organisational tags, achieving equivalent performance to using the whole tag space.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>3PRS: a personalized popular program recommendation system for digital TV for P2P social networks</title>
		<abstract>Digital TV channels require users to spend more time to choose their favorite TV programs. Electronic Program Guides (EPG) cannot be used to find popular TV programs. Hence, this paper proposes a personalized Digital Video Broadcasting -- Terrestrial(DVB-T) Digital TV program recommendation system for P2P social networks. From the DVB-T signal, we obtain EPG of TV programs. The frequency and duration of the programs that users have watched are used to extract programs that users are interested in. The information is collected and weighted by Information Retrieval (IR). The program information is then clustered by k-means. Clusters of users are also grouped by k-means to find cluster relationships. In each group, we decide the most popular program in the group according to the program weight of the channel. When a new user begins to watch the TV program, the K-Nearest Neighbor (kNN) classification method is used to determine the user's predicted cluster label. Then, our system recommends popular programs in the predicted cluster and similar clusters.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Establishing the utility of non-text search for news video retrieval with real world users</title>
		<abstract>TRECVID participants have enjoyed consistent success using storyboard interfaces for shot-based retrieval, as measured by TRECVID interactive search mean average precision (MAP). However, much is lost by only looking at MAP, and especially by neglecting to bring in representatives of the target user communities to conduct such tasks. This paper reports on the use of within-subjects experiments to reduce subject variability and emphasize the examination of specific video search interface features for their effectiveness in interactive retrieval and user satisfaction. A series of experiments is surveyed to illustrate the gradual realization of getting non-experts to utilize non-textual query features through interface adjustments. Notably, the paper explores the use of the search system by government intelligence analysts, concluding that a variety of search methods are useful for news video retrieval and lead to improved satisfaction. This community, dominated by text search system expertise but still new to video and image search, performed better with and favored a system with image and concept query capabilities over an exclusive text-search system. The user study also found that sports topics mean nothing for this user community and tens of relevant shots collected into the answer set are considered enough to satisfy the information need. Lessons learned from these user interactions are reported, with recommendations on both interface improvements for video retrieval systems and enhancing the ecological validity of video retrieval interface evaluations.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Context-aware recommendations on mobile services: the m:Ciudad approach</title>
		<abstract>The European FP7 research project m:Ciudad - a metropolis of ubiquitous services - aims at the empowerment of users to create services on mobile terminals. The project demonstrates various scenarios in which users either act as creator of services or interact with the system to search for services or service construction components. The search and recommendation process in the system facilitates retrieval of related entities and shows the results to the users according to their preference and contextual information. The paper demonstrates an approach to integrate contextual information with other search attributes to enable efficient service retrieval and recommendation in mobile user and application scenarios. The specifics of a mobile environment are taken into consideration and are reflected in the design of the m:Ciudad Search and Recommendation Engine. We discuss how context-awareness and proactivity can be implemented and utilised for mobile services.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Comparison-Based Recommendation</title>
		<abstract>Recommender systems combine user profiling and filtering techniques to provide more pro-active and personal information retrieval systems, and have been gaining in popularity as a way of overcoming the ubiquitous information overload problem. Many recommender systems operate as interactive systems that seek feedback from the end-user as part of the recommendation process to revise the user's query. In this paper we examine different forms of feedback that have been used in the past and focus on a low-cost preference-based feedback model, which to date has been very much under utilised. In particular we describe and evaluate a novel comparison-based recommendation framework which is designed to utilise preference-based feedback. Specifically, we present results that highlight the benefits of a number of new query revision strategies and evidence to suggest that the popular more-like-this strategy may be flawed.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Personalized peer filtering for a dynamic information push</title>
		<abstract>Due to the anonymity of the user during Web searching, no support for long-term information needs exists. First attempts for personalized Web retrieval are made, however these approaches are limited to static objects and no individual recommendations from a dynamic data set can be determined. Peer-to-peer architectures build a promising platform for a personalized information filtering system, where all steps during information exchange are transparent to the user. Our approach assists active requests in the form of an information pull as well as a system initiated information push. In a cooperative manner all peers have the function of information providers and consumers. The ranking of recommendations is established by a community-based filtering approach.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Let's stop pushing the envelope and start addressing it: a reference task agenda for HCI</title>
		<abstract>We identify a problem with the process of research in the human-computer interaction (HCI) community-an overemphasis on "radical invention" at the price of achieving a common research focus. Without such a focus, it is difficult to build on previous work, to compare different interaction techniques objectively, and to make progress in developing theory. These problems at the research level have implications for practice, too; as researchers we often are unable to give principled design advice to builders of new systems. We propose that the HCI community try to achieve a common focus around the notion of reference tasks. We offer arguments for the advantages of this approach as well as consider potential difficulties. We explain how reference tasks have been highly effective in focusing research into information retrieval and speech recognition. We discuss what factors have to be considered in selecting HCI reference tasks and present an example reference task (for searching speech archives). This example illustrates the nature of reference tasks and points to the issues and problems involved in constructing and using them. We conclude with recommendations about what steps need to be taken to execute the reference task research agenda. This involves recommendations about both the technical research that needs to be done and changes in the way that the HCI research community operates. The technical research involves identification of important user tasks by systematic requirements gathering, definition and operationalization of reference tasks and evaluation metrics, and execution of task-based evaluation, along with judicious use of field trials. Perhaps more important, we have also suggested changes in community practice that HCI must adopt to make the reference tasks idea work. We must create forums for discussion of common tasks and methods by which people can compare systems and techniques. Only by doing this can the notion of reference tasks be integrated into the process of research and development, enabling the field to achieve the focus it desperately needs.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>MARS: a MultilAnguage Recommender System</title>
		<abstract>The exponential growth of the Web is the most influential factor that contributes to the increasing importance of cross-lingual text retrieval and filtering systems. Indeed, relevant information exists in different languages, thus users need to find documents in languages different from the one the query is formulated in. In this context, an emerging requirement is to sift through the increasing flood of multilingual text: this poses a renewed challenge for designing effective multilingual Information Filtering systems. In this paper, we propose a language-independent content-based recommender system, called MARS (MultilAnguage Recommender System), that builds cross-language user profiles, by shifting the traditional text representation based on keywords, to a more complex language-independent representation based on word meanings. As a consequence, the recommender system is able to suggest items represented in a language different from the one used in the content-based user profile. Experiments conducted in a movie recommendation scenario show the effectiveness of the approach.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Mobile location-based information pushing system</title>
		<abstract>This paper builds a scalable personalized mobile information pushing platform, which can provide user-friendly and flexible location-based service. We first propose a Location-based Data and Service Middleware based on Service-Oriented Architecture in order to implement Mobile Information Pushing System involved in a variety of formats of data integration and conversion, as well as a combination of a wide range of services. Then, we propose a novel 3-D Tag-Cloud module, so that it can visualize useful retrieval information even in the limited mobile screen. Especially, we design a multi-dimensional collaborative filtering algorithms, in order to achieve dynamic personalized recommendation and mobile information sharing.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<abstract>Welcome to the 34th ACM SIGIR International Conference on Research and Development in Information Retrieval. The record number of papers in this year's conference represents both the breadth and depth of the research being done in this vibrant field, both in academia and industry. We have done our best to ensure that these papers meet high standards of quality in terms of presentation, citations, and experimental methodology. At the same time, we have tried to be flexible in the application of these criteria in order to accept papers describing novel and innovative work that may be somewhat unconventional. The conference received 543 full paper submissions this year, with 240 (44%) coming from Asia and Pacific region, 185 (34%) from the Americas, and 112 (21%) from Europe (the rest were "unknown"). Of these papers, 108 (19.9%) were accepted, up from the acceptance rate of 16.7% in last year's conference. The top five countries in terms of accepted papers were the U.S.A. (52), China (18), Germany (7), and then the U.K. and Spain (both 5). In addition, 274 short papers were submitted to the poster track, of which 89 (32.5%) were accepted. In the other categories, there were 15 (42.8%) demonstrations, 8 workshops, and 11 half-day tutorials accepted. In terms of the technical areas that the accepted papers cover, using the primary keyword assigned by the authors, the top five areas are document representation and content analysis (20%), retrieval models and ranking (17%), users and interactive IR (13%), queries and query analysis (11%), and filtering and recommendation (11%). Perhaps the only surprise there is the increase in the number of papers in filtering and recommendation. We believe that the papers at this year's conference provide an excellent cross-section of what is going on in our field. We hope that you find that reading them and listening to the presenters to be a rewarding experience. SIGIR uses a two-tier double blind review system. For the full papers, the first step is that at least three first-tier reviewers read every paper and provide ratings and comments. Then two additional reviewers, referred to as the primary or secondary area chairs, study those reviews, and introduce their own opinions and summaries where appropriate by making additional comments. In some cases, the area chairs initiate the discussion among the first-tier reviewers to work out any controversial issues or significant differences of opinion. A new step introduced this year was to request author feedback for specific issues in some papers. Another change this year was that final decisions for nearly all papers were made by the two area chairs together with the reviewers. At the program committee meeting in Barcelona, the program chairs and some area chairs went over the reviews, obtained additional input, and made decisions in the few cases where the area chairs had requested more discussion.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Proceedings of the 16th international conference on Intelligent user interfaces</title>
		<abstract>It is our great pleasure to welcome you to the 2011 ACM International Conference on Intelligent User Interfaces -- IUI'11. Intelligent User Interfaces (IUI) is the premier conference for reporting on the study of user interfaces with intelligent devices. This topic is of increasing importance as the consumer is interfacing with a wide variety of devices with embedded computation and connectivity and the computer is fading into the background. IUI is where the community of people interested in Human-Computer Interaction (HCI) meets the Artificial Intelligence (AI) community. We have retained the successful format of the conference with Long Papers, Short Papers and Demonstrations. The accepted submissions cover a wide range of topics, including handheld devices, multimodal interfaces, social computing and navigation, intelligent help agents, input technologies, user modeling and personalization, intelligent authoring and information presentation, and pen-based interfaces. Geographically, the accepted work also represents researchers and institutions in many countries across four continents, including Argentina, Australia, Belgium, Canada, China, Denmark, Finland, France, Germany, Ireland, Israel, Italy, Japan, Korea, Netherlands, Pakistan, Spain, Switzerland, Thailand, United Kingdom, and the United States of America. As always, the selection process has been the object of careful consideration and multiple discussions. In order to ensure that all topics were adequately covered more than 150 experts have contributed to the selection of this year's program, and we trust that this had a very positive impact on the relevance and quality of individual reviews. We are grateful to the reviewers, the program committee and the senior program committee, who worked very hard in reviewing papers and providing feedback for authors. Following a trend adopted by several high-quality conferences, we had a rebuttal phase for Long Papers. We have asked the Senior PC moderators and the Demo Chair to formulate recommendations for acceptance and in the vast majority of cases it has been straightforward for us to endorse their choice. It is always down to the Program Chairs to make final decisions, sometimes difficult ones, on the total number of contributions to be accepted. Out of 180 submissions, we selected 28 long papers, 36 short papers and 15 demo papers. We have adopted a continuity policy from previous editions (around 30% for Long Papers); this year's overall acceptance rate achieves the right balance between selectivity and openness to innovative papers. The conference program highlights three invited talks: Andrei Broder from Yahoo! Research, Eric Horvitz from Microsoft Research and Ken Perlin from New York University. We are also glad that we got Anthony Jameson from DFKI GmbH and Joseph Konstan from the University of Minnesota as tutorial speakers.This year's Conference also features ten workshops, covering several hot topics in the area of IUI, with strong emphasis on multimodality of interfaces, smart interaction and personalization including: Sketch recognition, Semantic Models for Adaptive Interactive Systems, Intelligent User Interfaces for Developing Regions, Context-Awareness in Retrieval and Recommendation, Multimodal Interfaces for Automotive Applications, Visual Interfaces to the Social and Semantic Web, Location-Awareness for Mixed and Dual Reality, Eye Gaze in Intelligent Human Machine Interaction, Interacting with Smart Objects and Personalized Access to Cultural Heritage.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Large scale rich media information search</title>
		<abstract>In recent years, the emergence of online social network and other web applications is dramatically changing how rich media information is accessed and presented, how knowledge is processed and extracted, and how business can be conducted. Characteristics of the related data, users and computing environment influence the development, implementation and evaluation of media (e.g., text, audio, image and video) retrieval systems significantly. Designing effective search algorithms plays a critical role to satisfy user's information needs. In this article, we try to identify several potential research directions and discuss critical technical challenges on how to efficiently develop rich/multimedia retrieval and data management techniques from different respects. They include query formulation, recommendation systems, and index structure for query speed and scalability improvement.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Exploring folksonomy and cooking procedures to boost cooking recipe recommendation</title>
		<abstract>Recommender systems have gained great popularity in Internet applications in recent years, due to that they facilitate users greatly in information retrieval despite the explosive data growth. Similar to other popular domains such as the movie-, music-, and book- recommendations, cooking recipe selection is also a daily activity in which user experiences can be greatly improved by adopting appropriate recommendation strategies. Based on content-based and collaborative filtering approaches, we present in this paper a comprehensive recipe recommendation framework encompassing the modeling of the recipe cooking procedures and adoption of folksonomy to boost the recommendations. Empirical studies are conducted on a real data set to show that our method outperforms baselines in the recipe domain.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>A contextual user model for web personalization</title>
		<abstract>Over the past years, information personalization has provided several valuable achievements on the improvement and optimization of Web searching and recommendation taking into account user's interests, preferences and contextual information. The main objective of a personalization system is to perform an information retrieval process taking into account the perception and the interest of the end-users. This paper focuses on how to model the user and his context in an extensible way that can be interpreted and used for personalization. We describe the architecture that provides personalization facilities based on the contextual user model for tourism usage.</abstract>
		<search_task_number>2</search_task_number>
		<query>information retrieval recommendation systems</query>
		<relevance>1</relevance>
	</item>



  <item>
    <title>The Implementation of Real-Time On-line Vehicle Diagnostics and Early Fault Estimation System</title>
	  <abstract>This paper developed an intelligent technology for real-time vehicle diagnostics and early fault estimation. The proposed system consists of vehicle on board unit (OBU) and vehicle diagnostics server (VDS). The vehicle OBU and VDS integrate with several subsystems, including vehicle wireless network, global positioning system (GPS), CAN bus, on-board diagnostics (OBD), and online expert system. The vehicle OBU could obtain real-time vehicle operation data, such as speed, engine RPM (Revolution(s) Per Minute), throttle, break, coolant temperature, battery voltage, O2 sensor, fuel trim, instant fuel consumption, and etc., from CAN bus and OBD. These vehicle operation data will be uploaded to the VDS via 3.5G or WiMAX wireless network. Then, the expert system built-in the VDS will analyze these vehicle operation data and perform real-time vehicle diagnostics or fault early warning. Once abnormal conditions have been detected, the VDS will inform deriver or qualified factory of the requirement of vehicle maintenance or repair. Using this system could increase the driving safety as well as decrease the air pollution and unnecessary fuel consumption caused by vehicle faults. This will also benefit our living environment, energy saving, and carbon reduction.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>  
  <item>
    <title>Measuring of exhaust gas emissions using absorption spectroscopy</title>
	  <abstract>This paper describes an optical fibre sensor for the detection of NOx (NO2 and NO) and CO2 in the exhaust system of a road vehicle. The measurement is based on a free path interaction zone which is interrogated using UV and NIR light guiding optical fibres and collimated lenses. Results are presented in the absorption spectra of the gases in the UV region for the NOx gases and NIR region for CO2. These demonstrate that using this method it is feasible to identify the individual CO2, NO and NO2 species as well as other gases in the exhaust system. Measurement of concentrations to the level of ten's parts per million (ppm) have been demonstrated for the NOxx gases.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item> 
  <item>
    <title>Performance optimization of gas turbine engine</title>
	  <abstract>Performance optimization of a gas turbine engine can be expressed in terms of minimizing fuel consumption while maintaining nominal thrust output, maximizing thrust for the same fuel consumption and minimizing turbine blade temperature. Additional control layers are used to improve engine performance. This paper presents an evolutionary approach called the StudGA as the optimization framework to design for optimal performance in terms of the three criteria above. This approach converges fast and can potentially save on computing cost. Model-based experimental results are used to illustrate this approach.
</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>
  <item>
    <title>Research Opportunities in Advanced Aerospace Concepts</title>
	  <abstract>This report is a review of a team effort that focuses on advanced aerospace concepts of the 21st Century. The paper emphasis advanced technologies, rather than cataloging every unusual aircraft that has ever been attempted. To dispel the myth that "aerodynamics is a mature science" an extensive list of "What we cannot do, or do not know" was enumerated. A zeit geist, a feeling for the spirit of the times, was developed, based on existing research goals. Technological drivers and the constraints that might influence these technological developments in a future society were also examined. The present status of aeronautics, space exploration, and non-aerospace applications, both military and commercial, including enabling technologies are discussed. A discussion of non-technological issues affecting advanced concepts research is presented. The benefit of using the study of advanced vehicles as a tool to uncover new directions for technology development is often necessary. An appendix is provided containing examples of advanced vehicle configurations currently of interest.
</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Cooperative safety: a combination of multiple technologies</title>
	  <abstract>Governmental Transportation Authorities' interest in Car to Car and Car to Infrastructure has grown dramatically over the last few years in order to increase the road safety and reduce traffic emission.

The achievement of these objectives is subject to development of three aspects: Transmission, Localization and Sensor Networks.

New wireless technique evolved form current WiFi technology shall be able to curb down the timing latency to achieve timely and efficient communication among vehicles.

Relative positioning is essential to predict whether two cars are on route of collision. Experts estimate that positioning accuracy must be below one meter in order to provide the necessary reaction time. Many technical issues exist in this field as current GPS solutions do not provide this level of accuracy.

There are multiple standalone approaches existing for sensing networks including imaging, radar and lidar. In order to create fault tolerant SIL3 compliant systems, data fusion is obligatory. The amalgamation of these different data streams requires powerful multicore processing to recognize and react to multiple concurrent scenarios.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>
  <item>
    <title>Formal Specification and Verification of a Fault-Masking and Transient-Recovery Model for Digital Flight-Control Systems</title>
	  <abstract>The formal specification and mechanically checked verification for a model of fault-masking and transient-recovery among the replicated computers of digital flight-control systems are presented. The verification establishes, subject to certain carefully stated assumptions, that faults among the component computers are masked so that commands sent to the actuators are the same as those that would be sent by a single computer that suffers no failures.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Health Monitoring System Technology Assessments---Cost Benefits Analysis</title>
	  <abstract>The subject of sensor-based structural health monitoring is very diverse and encompasses a wide range of activities including initiatives and innovations involving the development of advanced sensor, signal processing, data analysis, and actuation and control technologies. In addition, it embraces the consideration of the availability of low-cost, high-quality contributing technologies, computational utilities, and hardware and software resources that enable the operational realization of robust health monitoring technologies. This report presents a detailed analysis of the cost benefit and other logistics and operational considerations associated with the implementation and utilization of sensor-based technologies for use in aerospace structure health monitoring. The scope of this volume is to assess the economic impact, from an end-user perspective, implementation health monitoring technologies on three structures. It specifically focuses on evaluating the impact on maintaining and supporting these structures with and without health monitoring capability.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Aviation Weather Information Requirements Study</title>
	  <abstract>The Aviation Safety Program (AvSP) has as its goal an improvement in aviation safety by a factor of 5 over the next 10 years and a factor of 10 over the next 20 years. Since weather has a big impact on aviation safety and is associated with 30 percent of all aviation accidents, Weather Accident Prevention (WxAP) is a major element under this program. The Aviation Weather Information (AWIN) Distribution and Presentation project is one of three projects under this element. This report contains the findings of a study conducted by the Georgia Tech Research Institute (GTRI) under the Enhanced Weather Products effort, which is a task under AWIN. The study examines current aviation weather products and there application. The study goes on to identify deficiencies in the current system and to define requirements for aviation weather products that would lead to an increase in safety. The study also provides an overview the current set of sensors applied to the collection of aviation weather information. New, modified, or fused sensor systems are identified which could be applied in improving the current set of weather products and in addressing the deficiencies defined in the report. In addition, the study addresses and recommends possible sensors for inclusion in an electronic pilot reporting (EPIREP) system.
</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Scalable Data Acquisition for Densely Instrumented Cyber-Physical Systems</title>
	  <abstract>Consider the problem of designing an algorithm for acquiring sensor readings. Consider specifically the problem of obtaining an approximate representation of sensor readings where (i) sensor readings originate from different sensor nodes, (ii) the number of sensor nodes is very large, (iii) all sensor nodes are deployed in a small area (dense network) and (iv) all sensor nodes communicate over a communication medium where at most one node can transmit at a time (a single broadcast domain). We present an efficient algorithm for this problem, and our novel algorithm has two desired properties: (i) it obtains an interpolation based on all sensor readings and (ii) it is scalable, that is, its time-complexity is independent of the number of sensor nodes. Achieving these two properties is possible thanks to the close interlinking of the information processing algorithm, the communication system and a model of the physical world.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>
  <item>
    <title>Review: Sensor systems for measuring soil compaction: Review and analysis</title>
	  <abstract>Spatially variable soil compaction often causes inconsistent growing conditions in many fields. Various soil compaction sensor systems have been deployed to obtain georeferenced maps of certain state and behavioral properties (e.g., soil strength, water content, air permeability) related to compaction. This publication classifies different prototype sensors, and reviews alternative measurement concepts from the viewpoint of soil mechanics. The majority of discussion is dedicated to a diversified family of soil strength sensors developed around the world. Through the follow-up analysis, a concept of sensor fusion has been emphasized as an option capable of improving future applicability of soil compaction sensor systems, while implementing site-specific control of localized compaction occurrences.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Compacting SQL queries for saving resources in mobile ad hoc and sensor networks</title>
	  <abstract>This paper presents an SQL query compaction scheme that makes accessing data in mobile ad hoc networks and in wireless sensor networks more efficient. The proposed scheme exploits the SQL syntax and generates a light-weight binary representation that significantly shrinks the size of the queries. Its performance was evaluated after being implemented using the Java 2 Micro Edition (J2ME) platform and deployed on a Sony Ericsson P800 mobile phone. The considered test queries were a representative sample applicable to the SQL syntax of the TinyDB sensor network database. Results show an achieved compaction ratio that is less than 1.6 bits per byte or equivalently greater than 80&#37;, which greatly outperforms the Huffman coding algorithm.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>On-chip hybrid power supply system for wireless sensor nodes</title>
	  <abstract>With the miniaturization of electronic devices, small size but high capacity power supply system appears to be more and more important. A hybrid power source, which consists of a fuel cell (FC) and a rechargeable battery, has the advantages of long lifetime and good load following capabilities. In this paper, we propose the schematic of a hybrid power supply system, that can be integrated on a chip compatible with present CMOS process. Besides, considering the problem of maximizing the on-chip fuel cell's lifetime, we propose a modified dynamic power management (DPM) algorithm for on-chip fuel cell based hybrid power system in wireless sensor node design. Taking the wireless sensor node powered by this hybrid power system as an example, we analyze the improvement of the FC-Bat hybrid power system. The simulation results demonstrate that the on-chip FC-Bat hybrid power system can be used for wireless sensor node under different usage scenarios. Meanwhile, for an on-chip power system with 1cm2 area consumption, the wafer-level battery can power a typical sensor node for only about 5 months, while our on-chip hybrid power system will supply the same sensor node for 2 years steadily.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>
  <item>
    <title>Cooperative control of modular space robots</title>
	  <abstract>Modular self-assembling on-orbit robots have the potential to reduce mission costs, increase reliability, and permit on-orbit repair and refueling. Modules with a variety of specialized capabilities would self-assemble from orbiting inventories. The assembled modules would then share resources such as power and sensors. As each free-flying module carries its own attitude control actuators, the assembled system has substantial sensor and actuator redundancy. Sensor redundancy enables sensor fusion that reduces measurement error. Actuator redundancy gives a system greater flexibility in managing its fuel usage. In this paper, the control of self-assembling space robots is explored in simulations and experiments. Control and sensor algorithms are presented that exploit the sensor and actuator redundancy. The algorithms address the control challenges introduced by the dynamic interactions between modules, the distribution of fuel resources among modules, and plume impingement.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>
  <item>
    <title>Developing Virtual Reality Applications: Foundations of Effective Design</title>
	  <abstract>Virtual Reality systems enable organizations to cut costs and time, maintain financial and organizational control over the development process, digitally evaluate products before having them created, and allow for greater creative exploration. In this book, VR developers Alan Craig, William Sherman, and Jeffrey Will examine a comprehensive collection of current,unique, and foundational VR applications in a multitude of fields, such as business, science, medicine, art, entertainment, and public safety among others.

An insider's view of what works, what doesn't work, and why, Developing Virtual Reality Applications explores core technical information and background theory as well as the evolution of key applications from their genesis to their most current form. Developmental techniques are cross-referenced between different applications linking information to describe overall VR trends and fundamental best practices. This synergy, coupled with the most up to date research being conducted, provides a hands-on guide for building applications, and an enhanced, panoramic view of VR development. Developing Virtual Reality Applications is an indispensable one-stop reference for anyone working in this burgeoning field.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Rough Terrain Autonomous Mobility—Part 2: An Active Vision, Predictive Control Approach</title>
	  <abstract>Off-road autonomous navigation is one of the most difficult automation challenges from the point of view of constraints on mobility, speed of motion, lack of environmental structure, density of hazards, and typical lack of prior information. This paper describes an autonomous navigation software system for outdoor vehicles which includes perception, mapping, obstacle detection and avoidance, and goal seeking. It has been used on several vehicle testbeds including autonomous HMMWV‘s and planetary rover prototypes. To date, it has achieved speeds of 15 km/hr and excursions of 15 km.

We introduce algorithms for optimal processing and computational stabilization of range imagery for terrain mapping purposes. We formulate the problem of trajectory generation as one of predictive control searching trajectories expressed in command space. We also formulate the problem of goal arbitration in local autonomous mobility as an optimal control problem. We emphasize the modeling of vehicles in state space form. The resulting high fidelity models stabilize coordinated control of a high speed vehicle for both obstacle avoidance and goal seeking purposes. An intermediate predictive control layer is introduced between the typical high-level strategic or artificial intelligence layer and the typical low-level servo control layer. This layer incorporates some deliberation, and some environmental mapping as do deliberative AI planners, yet it also emphasizes the real-time aspects of the problem as do minimalist reactive architectures.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Methods and procedures for automatic collection and management of data acquired from on-the-go sensors with application to on-the-go soil sensors</title>
	  <abstract>Sensors for on-the-go collection of data on soil and crop have become essential for successful implementation of precision agriculture. This paper analyses the potentials and develops general procedures for on-the-go data acquisition of soil sensors. The methods and procedures used to manage data with respect to a farm management information system (FMIS) are described. The current data communication standard for tractors and machinery in agriculture is ISO 11783, which is rather well established and has gained market acceptance. However, there are a significant number of non-ISO 11783 compliant sensors in practice. Thus, two concepts are proposed. The first concept is on-the-go data collection based on ISO 11783, which mostly covers data on parameters related to tractor and machine performance, e.g. speed, draught, fuel consumption, etc. Process data from sensors with Control Area Network (CAN) interfaces is converted into ISO 11783 XML and then imported into relational database at FMIS using RelaXML tool. There is also the export function from database to task controller (TC) to provide task management, as described in ISO 11783:10. The second concept is on-the-go data collection with non-ISO 11783 sensors. This data is likely to be recorded in many formats, which require an import service. An import service is based on local or public sharing or semantic mapping outputting a common format for FMIS (e.g. AgroXML). Import is best performed as close to the generation of sensor data as possible to maximise the availability of metadata. A case study of sensor based variable rate fertilisation (VRF) has been undertaken focussing on German fertilisation rules.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Complexity reduction for the design of interacting controllers
</title>
	  <abstract>The complexity of embedded controllers in important industrial domains such as automotive and avionics is growing constantly making error-free system integration almost impossible. We address the complexity issues posed by the analysis and design of interacting controllers introducing approximation techniques that are shown to be effective on a substantial industrial test case: the control system for common-rail fuel-injection developed by Magneti Marelli Powertrain.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>
  <item>
    <title>Monitoring of the pressure inside the cylinder for an internal-combustion engine</title>
	  <abstract>This work presents the achievement of a monitoring, recording, processing and interpretation concept, at laboratory scale, of the pressure inside the combustion chamber for a spark-ignition engine, based on electronic equipment, which measures the values of 17 parameters that describe the operation of an internal-combustion engine. The measured pressure depends on the crankshaft's position. Is used a pressure transducer with strain-gauge socket, and measurement of the crankshaft's position with a speed transducer with variable reluctance. From experimental data, on one hand, can be analyzed the pressure modification in the combustion chamber depending on the crankshaft's position, and on the other hand, the maximum pressure modification in the cylinder depending on the speed.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>
  <item>
    <title>International Space Station Evolution Data Book - Volume 1. Baseline Design - Revision A</title>
	  <abstract>This report provides a focused and in-depth look at the opportunities and drivers for the enhancement and evolution of the International Space Station (ISS) during assembly and beyond the assembly complete stage. These enhancements would expand and improve the current baseline capabilities of the ISS and help to facilitate the commercialization of the ISS by the private sector. Volume 1 provides the consolidated overview of the ISS baseline systems; information on the current facilities available for pressurized and unpressurized payloads; and information on current plans for crew availability and utilization, resource timelines and margin summaries including power, thermal, and storage volumes; and an overview of the vehicle traffic model. Volume 2 includes discussions of advanced technologies being investigated for use on the ISS and potential commercial utilization activities being examined including proposed design reference missions (DRM''s) and the technologies being assessed by the Preplanned Program Improvement (P3I) Working Group. This information is very high level and does not provide the relevant information necessary for detailed design efforts. This document is meant to educate readers on the ISS and to stimulate the generation of ideas for enhancement and utilization of the ISS, either by or for the government, academia, and commercial industry.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Rendezvous design algorithms for wireless sensor networks with a mobile base station</title>
	  <abstract>Recent research shows that significant energy saving can be achieved in wireless sensor networks with a mobile base station that collects data from sensor nodes via short-range communications. However, a major performance bottleneck of such WSNs is the significantly increased latency in data collection due to the low movement speed of mobile base stations. To address this issue, we propose a rendezvous-based data collection approach in which a subset of nodes serve as the rendezvous points that buffer and aggregate data originated from sources and transfer to the base station when it arrives. This approach combines the advantages of controlled mobility and in-network data caching and can achieve a desirable balance between network energy saving and data collection delay. We propose two efficient rendezvous design algorithms with provable performance bounds for mobile base stations with variable and fixed tracks, respectively. The effectiveness of our approach is validated through both theoretical analysis and extensive simulations.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>
  <item>
    <title>The smart tachograph – individual accounting of traffic costs and its implications</title>
	  <abstract>Today, several costs caused by road traffic may either be only roughly approximated, or cannot be clearly assigned to the drivers causing them, or both. They are typically distributed evenly among a large fraction of drivers, which is both unfair and economically inefficient. We have built a prototypical platform, called the “Smart Tachograph”, that allows us to measure traffic-related costs on an individual basis, thus supporting a more fine-granular charging of the responsible parties. Sensors observe the manner and circumstances in which a vehicle is driven, while several accounting authorities can evaluate this information and charge motorists on a pay-per-use basis. The Smart Tachograph offers valuable insights for the deployment of future ubiquitous computing services in general: its implementation has obvious requirements in terms of security and privacy; its deployment model is realistic through the strong economic incentives it offers; and its usage directly affects core societal values such as fairness and trust. This paper summarizes our design considerations and discusses the feasibility and wider economic and societal implications of fielding such a system.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Research and development of hydrogen direct-injection internal combustion engine system</title>
	  <abstract>The research and development of hydrogen-internal combustion engine (ICE) system for heavy-duty trucks, with the goal of allowing carbon dioxide (CO2)-free operation in transportation department, has been carried out. The high-pressure hydrogen gas direct-injector was adapted to the single-cylinder ICE. The high-pressure direct-injection combustion system shows that high thermal efficiency, high specific output and low NOx emission were confirmed. From these results, it is concluded that the possibility of the application from those results to a multi-cylinder hydrogen-ICE is high. The NOx storage reduction (NSR) catalyst system was applied as the after-treatment device of the exhaust NOx emission from hydrogen-ICE and it was confirmed that very low effect to fuel consumption and high reduction effect of NOx emission were feasible experimentally.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>
  <item>
    <title>Pattern recognition in multivariate time series: dissertation proposal</title>
	  <abstract>Nowadays computer scientists are faced with fast growing and permanently evolving data, which are represented as observations made sequentially in time. A common problem in the data mining community is the recognition of recurring patterns within temporal databases or streaming data.

This dissertation proposal aims at developing and investigating efficient methods for the recognition of contextual patterns in multivariate time series in different application domains based on machine learning techniques. To this end, we propose a generic three-step approach that involves (1) feature extraction to build robust learning models based on significant time series characteristics, (2) segmentation to identify internally homogeneous time intervals and change points, as well as (3) clustering and/or classification to group the time series (segments) into the sub-population to which they belong to.

To support our proposed approach, we present and discuss first experiments on real-life vehicular data. Furthermore we describe a number of applications, where pattern recognition in multivariate time series is practical or rather necessary.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Monitoring of some functional parameters for an internal-combustion engine</title>
	  <abstract>This work presents the achievement of a monitoring, recording, processing and interpretation concept, at laboratory scale, of some functional parameters for a spark-ignition engine, based on an electronic equipment, which measures the values of 17 parameters that describe the operation of an internal-combustion engine. In order to be able to control the optimal operation of an internal-combustion engine, an important parameter that should be measured is the pressure in the combustion chamber, which has high values (50-70 bar), the developed temperature is high and the frequency of appearance is high (depending on the crankshaft's speed). The measured pressure depends on the crankshaft's position. Is used a pressure transducer with strain-gauge socket, and measurement of the crankshaft's position with a speed transducer with variable reluctance. From experimental data, on one hand, can be analyzed the pressure modification in the combustion chamber depending on the crankshaft's position, and on the other hand, the maximum pressure modification in the cylinder depending on the speed.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>
  <item>
    <title>Closed-loop control of EGR using ion currents</title>
	  <abstract>Two virtual sensors are proposed that use the spark-plug based ion current sensor for combustion engine control. The first sensor estimates combustion variability for the purpose of controlling exhaust gas recirculation (EGR) and the second sensor estimates the pressure peak position for control of ignition timing. Use of EGR in engines is important because the technique can reduce fuel consumption and NOx emissions, but recirculating too much can have the adverse effect with e.g. increased fuel consumption and poor driveability of the vehicle. Since EGR also affects the phasing of the combustion (because of the diluted gas mixture with slower combustion) it is also necessary to control ignition timing otherwise efficiency will be lost. The combustion variability sensor is demonstrated in a closed-loop control experiment of EGR on the highway and the pressure peak sensor is shown to handle both normal and an EGR condition.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>
  <item>
    <title>Verification &amp; validation of an agent-based forest fire simulation model</title>
	  <abstract>In this paper, we present the verification and validation of an agent-based model of forest fires. We use a combination of a Virtual Overlay Multi-Agent System (VOMAS) validation scheme with Fire Weather Index (FWI) to validate the forest fire Simulation. FWI is based on decades of real forest fire data and is now regarded as a standard index for fire probability with wide usage across Canada, New Zealand and Australia. VOMAS approach allows for flexible validation of agent-based simulation models. In the current work, it is used in the form of a simulation of a randomly deployed Wireless Sensor Network for forest monitoring. Here, each virtual "sensor" agent uses FWI to calculate fire probability and compares it with the simulation model. VOMAS verification and validation methodology for agent-based models allows for interactive design of Agent-Based Models involving both the Simulation Specialists as well as the Subject Matter Experts. The presented simulation model also uses weather parameters such as wind speed, rain, snow to calculate Indices such as Fire Weather Index (FWI), Build Up Index (BUI) and Initial Spread Index (ISI) in real time. We also study the effects of fires on the life of simulated VOMAS sensors. Using extensive simulations, we demonstrate the effectiveness and ease of use of VOMAS based Validation.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Data fusion in intelligent transportation systems: Progress and challenges - A survey</title>
	  <abstract>In intelligent transportation systems (ITS), transportation infrastructure is complimented with information and communication technologies with the objectives of attaining improved passenger safety, reduced transportation time and fuel consumption and vehicle wear and tear. With the advent of modern communication and computational devices and inexpensive sensors it is possible to collect and process data from a number of sources. Data fusion (DF) is collection of techniques by which information from multiple sources are combined in order to reach a better inference. DF is an inevitable tool for ITS. This paper provides a survey of how DF is used in different areas of ITS.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>
  <item>
    <title>Micro-Blog: sharing and querying content through mobile phones and social participation</title>
	  <abstract>Recent years have witnessed the impacts of distributed content sharing (Wikipedia, Blogger), social networks (Facebook, MySpace), sensor networks, and pervasive computing. We believe that significant more impact is latent in the convergence of these ideas on the mobile phone platform. Phones can be envisioned as people-centric sensors capable of aggregating participatory as well as sensory inputs from local surroundings. The inputs can be visualized in different dimensions, such as space and time. When plugged into the Internet, the collaborative inputs from phones may enable a high resolution view of the world. This paper presents the architecture and implementation of one such system, called Micro-Blog. New kinds of application-driven challenges are identified and addressed in the context of this system. Implemented on Nokia N95 mobile phones, Micro-Blog was distributed to volunteers for real life use. Promising feedback suggests that Micro-Blog can be a deployable tool for sharing, browsing, and querying global information.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>
  <item>
    <title>Partners in Freedom: Contributions of the Langley Research Center to U.S. Military Aircraft of the 1990''s</title>
	  <abstract>Established in 1917 as the nation''s first civil aeronautics research laboratory under the National Advisory Committee for Aeronautics (NACA), Langley was a small laboratory that solved the problems of flight for military and civil aviation. Throughout history, Langley has maintained a working partnership with the Department of Defense, U.S. industry, universities, and other government agencies to support the defense of the nation with research. During World War II, Langley directed virtually all of its workforce and facilities to research for military aircraft. Following the war, a balanced program of military and civil projects was undertaken. In some instances Langley research from one aircraft program helped solve a problem in another. At the conclusion of some programs, Langley obtained the research models for additional tests to learn more about previously unknown phenomena. The data also proved useful in later developmental programs. Many of the military aircraft in the U.S. inventory as of late 1999 were over 20 years old. Langley activities that contributed to the development of some of these aircraft began over 50 years prior. This publication documents the role - from early concept stages to problem solving for fleet aircraft - that Langley played in the military aircraft fleet of the United States for the 1990''s.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Injection engine as a control object. I. Schematic diagram of the engine and synthesis of a mathematical model</title>
	  <abstract>The paper is devoted to the analysis of injection engine as an object of automatic control by a built-in microprocessor system. The schematic diagram of the engine is presented; controlled, measured, and input variables are indicated; a mathematical model of the engine oriented to the use in analysis and synthesis of control systems is described. Transient processes and static characteristics of the injection engine V8 of Chervolet Corvette are given as an example.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Information systems and environmentally sustainable development: energy informatics and new directions for the is community</title>
	  <abstract>While many corporations and Information Systems units recognize that environmental sustainability is an urgent problem to address, the IS academic community has been slow to acknowledge the problem and take action. We propose ways for the IS community to engage in the development of environmentally sustainable business practices. Specifically, as IS researchers, educators, journal editors, and association leaders, we need to demonstrate how the transformative power of IS can be leveraged to create an ecologically sustainable society. In this Issues and Opinions piece, we advocate a research agenda to establish a new subfield of energy informatics, which applies information systems thinking and skills to increase energy efficiency. We also articulate how IS scholars can incorporate environmental sustainability as an underlying foundation in their teaching, and how IS leaders can embrace environmental sustainability in their core principles and foster changes that reduce the environmental impact of our community.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>MineFleet®: an overview of a widely adopted distributed vehicle performance data mining system</title>
	  <abstract>This paper describes the MineFleet distributed vehicle performance data mining system designed for commercial fleets. MineFleet analyzes high throughput data streams onboard the vehicle, generates the analytics, sends those to the remote server over the wide-area wireless networks and offers them to the fleet managers using stand-alone and web-based user-interface. The paper describes the overall architecture of the system, business needs, and shares experience from successful large-scale commercial deployments. MineFleet is probably one of the first commercially successful distributed data stream mining systems. This patented technology has been adopted, productized, and commercially offered by many large companies in the mobile resource management and GPS fleet tracking industry. This paper offers an overview of the system and offers a detailed analysis of what made it work.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Model-based control and learning control method for automobile idle speed control using electric throttle</title>
	  <abstract>Recently, it has become important to reduce the fuel consumption of automobile engines in order to reduce the amount of CO2 emission. Reduced consumption can be achieved by reducing the idle rotation speed. However, the idling stability at low idle speeds tends to be worse than that at higher idle speeds. Thus, more accurate idle speed control is required. In the present paper, we propose an engine model-based feed-forward idle control system. The model-based control system is demonstrated to achieve better control results. In addition, aging and environmental changes (such as temperature reduction) vary with the torque required for driving in-vehicle equipments and cause a deterioration of control for maintaining the idle speed. Regarding this problem, a new learning control method is developed and applied to an actual vehicle to verify that controllability can be improved.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>
  <item>
    <title>Greener aviation with virtual sensors: a case study</title>
	  <abstract>The environmental impact of aviation is enormous given the fact that in the US alone there are nearly 6 million flights per year of commercial aircraft. This situation has driven numerous policy and procedural measures to help develop environmentally friendly technologies which are safe and affordable and reduce the environmental impact of aviation. However, many of these technologies require significant initial investment in newer aircraft fleets and modifications to existing regulations which are both long and costly enterprises. We propose to use an anomaly detection method based on Virtual Sensors to help detect overconsumption of fuel in aircraft which relies only on the data recorded during flight of most existing commercial aircraft, thus significantly reducing the cost and complexity of implementing this method. The Virtual Sensors developed here are ensemble-learning regression models for detecting the overconsumption of fuel based on instantaneous measurements of the aircraft state. This approach requires no additional information about standard operating procedures or other encoded domain knowledge. We present experimental results on three data sets and compare five different Virtual Sensors algorithms. The first two data sets are publicly available and consist of a simulated data set from a flight simulator and a real-world turbine disk. We show the ability to detect anomalies with high accuracy on these data sets. These sets contain seeded faults, meaning that they have been deliberately injected into the system. The second data set is from real-world fleet of 84 jet aircraft where we show the ability to detect fuel overconsumption which can have a significant environmental and economic impact. To the best of our knowledge, this is the first study of its kind in the aviation domain.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>
  <item>
    <title>Communications of the ACM</title>
	  <abstract></abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>A New Approach to Humanitarian Demining. Part 1: Mobile Platform for Operation on Unstructured Terrain</title>
	  <abstract>Landmines can deprive whole areas of valuable resources, and continue to kill and cause injuries years after the end of armed conflicts. Armored vehicles are used for mine clearance, but with limited reliability. The final inspection of minefields is still performed by human deminers exposed to potentially fatal accidents. The aim of this research is to introduce automation as a way to improve the final level of humanitarian demining. This paper addresses mobility and manipulation, while sensing, communication and visualization shall be discussed in detail in a subsequent paper. After analyzing the merits and limitations of previous works, a new approach to tele-operated demining is considered, using off-road buggies equipped with combustion engines, and taking into account actual field requirements. Control of the automated buggies on rough terrain is also discussed, as well as the development of a new weight-balanced manipulator for landmine clearance operations.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Combining hierarchical and goal-directed speed-up techniques for dijkstra's algorithm</title>
	  <abstract>In recent years, highly effective hierarchical and goal-directed speed-up techniques for routing in large road networks have been developed. This article makes a systematic study of combinations of such techniques. These combinations turn out to give the best results in many scenarios, including graphs for unit disk graphs, grid networks, and time-expanded timetables. Besides these quantitative results, we obtain general insights for successful combinations.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Development of Stitched, Braided and Woven Composite Structures in the ACT Program and at Langley Research Center (1985 to 1997) Summary and Bibliography</title>
	  <abstract>Summary results are presented from the research conducted on woven, braided, knitted and stitched (textile) composites at the Langley Research Center and under the NASA Advanced Composites Technology (ACT) Program in the period from 1985 to 1997. The report also includes an annotated bibliography of 270 U.S. publications on textile composites (with their abstracts). Two major research areas are discussed: (1) the general research in textile composites performed throughout the period under the direction of the Langley Research Center and (2) the development of textile composite aircraft structures by industry under the NASA ACT Program. The annotated bibliography is organized in three subsections: (1) general textiles R&amp;D under the auspices of Langley, (2) ACT Program development of textile structural components, and (3) textiles research by individuals and organizations not associated with the ACT Program. An author index is provided for the reports and documents.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>ICT for green: how computers can help us to conserve energy</title>
	  <abstract>Information and communication technology (ICT) consumes energy, but is also an important means of conserving energy. Conventionally, it has done so by optimizing the performance of energy-using systems and processes in industry and commerce. In the near future, ICT will also play a critical role in supporting the necessary paradigm shifts within the energy sector towards more sustainable electricity generation. However, with the advent of "smart" technology from the field of ubiquitous computing, further ways of reducing growing levels of domestic energy consumption are now emerging. With this in mind, we discuss how getting consumers "into the loop" can achieve energy savings on top of the efficiency gains resulting from automated systems, and we describe a prototype application aimed at inducing behavioral change by providing direct feedback on household electricity consumption.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>
  <item>
    <title>Data Mining of NASA Boeing 737 Flight Data---Frequency Analysis of In-Flight Recorded Data</title>
	  <abstract>Data recorded during flights of the NASA Trailblazer Boeing 737 have been analyzed to ascertain the presence of aircraft structural responses from various excitations such as the engine, aerodynamic effects, wind gusts, and control system operations. The NASA Trailblazer Boeing 737 was chosen as a focus of the study because of a large quantity of its flight data records. The goal of this study was to determine if any aircraft structural characteristics could be identified from flight data collected for measuring non-structural phenomena. A number of such data were examined for spatial and frequency correlation as a means of discovering hidden knowledge of the dynamic behavior of the aircraft. Data recorded from on-board dynamic sensors over a range of flight conditions showed consistently appearing frequencies. Those frequencies were attributed to aircraft structural vibrations.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Intelligent GPS-based predictive engine control for a motor vehicle</title>
	  <abstract>An intelligent Global Positioning System (GPS) based control system utilises information about the current vehicle position and upcoming terrain in order to reduce vehicle fuel consumption as well as improve road safety and comfort. The development of such in-vehicle control systems has provided static and dynamic road information. The vehicle running parameters have been mathematically defined whilst the engine control algorithms were derived from a custom-built engine test-rig. As the vehicle travelled over a particular route, road information such as gradient and position was stored with the past trajectory using a Neuro-Fuzzy technique. This road information was continuously updated and replaced by new data as the vehicle moved along, thereby adjusting the engine control parameters to reflect the actual current vehicle running data. The control system essentially used a fuzzy logic derived relief map of the test route and this was further validated and corrected based on the past trajectory from the in-vehicle GPS sensor. The simulation model demonstrated the feasibility and robustness of the control system for motor vehicle control applications.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>
  <item>
    <title>CarMA: towards personalized automotive tuning</title>
	  <abstract>Wireless sensing and actuation have been explored in many contexts, but the automotive setting has received relatively little attention. Automobiles have tens of onboard sensors and expose several hundred engine parameters which can be tuned (a form of actuation). The optimal tuning for a vehicle can depend upon terrain, traffic, and road conditions, but the ability to tune a vehicle has only been available to mechanics and enthusiasts. In this paper, we describe the design and implementation of CarMA (Car Mobile Assistant), a system that provides high-level abstractions for sensing automobile parameters and tuning them. Using these abstractions, developers can easily write smart-phone "apps" to achieve fuel efficiency, responsiveness, or safety goals. Users of CarMA can tune their vehicles at the granularity of individual trips, a capability we call personalized tuning. We demonstrate through a variety of applications written on top of CarMA that personalized tuning can result in over 10% gains in fuel efficiency. We achieve this through route-specific or driver-specific customizations. Furthermore, CarMA is capable of improving user satisfaction by increasing responsiveness when necessary, and promoting vehicular safety by appropriately limiting the range of performance available to novice or unsafe drivers.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Empowered by wireless communication: Distributed methods for self-organizing traffic collectives</title>
	  <abstract>In recent years, tremendous progress has been made in understanding the dynamics of vehicle traffic flow and traffic congestion by interpreting traffic as a multiparticle system. This helps to explain the onset and persistence of many undesired phenomena, for example, traffic jams. It also reflects the apparent helplessness of drivers in traffic, who feel like passive particles that are pushed around by exterior forces; one of the crucial aspects is the inability to communicate and coordinate with other traffic participants.

We present distributed methods for solving these fundamental problems, employing modern wireless, ad-hoc, multi-hop networks. The underlying idea is to use these capabilities as the basis for self-organizing methods for coordinating data collection and processing, recognizing traffic phenomena, and changing their structure by coordinated behavior. The overall objective is a multi-level approach that reaches from protocols for local wireless communication, data dissemination, pattern recognition, over hierarchical structuring and coordinated behavior, all the way to large-scale traffic regulation.

In this article, we describe three types of results: (i) self-organizing and distributed methods for maintaining and collecting data (using our concept of Hovering Data Clouds); (ii) adaptive data dissemination for traffic information systems; (iii) methods for self-recognition of traffic jams. We conclude by describing higher-level aspects of our work.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Performance measurement systems of the scaled active steering railway vehicle using the telemetry systems</title>
	  <abstract>This paper describes the performance measurement system of the active steering railway vehicle with the scaled test bed using the acquisition system about the wheel lateral force. Active steering system of railway vehicles has proven its ability to bridge the gap between stability and curve friendliness. This scaled testbed system consists of two steering actuators, a steering controller, and various sensor systems to detect lateral displacement, vibration, track curvature, and sensor systems. To compare with the various control strategies, we installed the telemetry systems on the steering wheelsets to detect the wheel/rail lateral force. Running test results of 1/5 scaled active steering vehicle on the curved track show that the proposed measuring system has good performance.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>
Fuzzy controller design with the degree of non-uniformity for the scaled active steering testbed in the railway vehicle</title>
	  <abstract>In urban transit systems, rail passenger vehicles are often required to negotiate tight curves. Running curve railway, the wheelsets of conventional vehicles generally misalign radically with the track increasing wheel/rail contact forces and resulting in increased wheel and rail wear, outbreak of squeal noise, fuel consumption, and risk of derailment. To solve these problems, modified suspension system designs, application for alternate wheel profiles, active and semi-active steering techniques have been proposed. Active steering system has proven its ability to bridge the gap between stability and curve friendliness. This paper presents a technique for improving the performance of active steering fuzzy control systems by adequately changing the widths of triangular fuzzy membership functions. Experiment results show that the proposed fuzzy controller robustly yields good performance comparing with conventional controller.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Control of energy saving at industrial enterprises</title>
	  <abstract>Problems connected with improvement of control systems for power systems of industrial enterprises, which are most important elements of energy and fuel consumption in industry, are considered. The growth of energy and fuel cost, the increasing requirements to their saving determine the topical character of this study. The developed technical and software tools of information processing and decision making provide new capabilities for improving control. Multilevel structures of problems and control circuits, models and algorithms for their realization are proposed. The formulations of control problems, applied information and mathematical tools can be used at mechanical engineering enterprises and in other fields of industry.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Green sigma and the technology of transformation for environmental stewardship</title>
	  <abstract>Many enterprises have launched initiatives and benefited from improving their position as stewards of the environment. Strategies that help determine where to focus investments and resources in order to improve the environment are also becoming more common. Not surprisingly, methodologies designed to reduce waste are well suited to help companies that have environmental impact objectives such as reducing their carbon footprint and conserving natural resources. Green Sigma™, developed by IBM, is one such methodology that enables transformation for environmental stewardship by applying a proven process, robust analysis tools, and effective technology solutions. It utilizes aspects of the traditional Lean and Six Sigma™ methodologies while providing the necessary tools to identify, implement, and sustain improvements that have a positive impact on the environment. The Green Sigma methodology is explained and then applied to a manufacturing and warehouse facility in an actual case study. By following the five steps of Green Sigma, applying robust analysis tools, and creating a management dashboard system, an energy savings of at least 20% was identified and achieved.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>1</relevance>
  </item>
  <item>
    <title>Green multimedia: informing people of their carbon footprint through two simple sensors</title>
	  <abstract>In this work we discuss a new, but highly relevant, topic to the multimedia community; systems to inform individuals of their carbon footprint, which could ultimately effect change in community carbon footprint-related activities. The reduction of carbon emissions is now an important policy driver of many governments, and one of the major areas of focus is in reducing the energy demand from the consumers i.e. all of us individually.

In terms of CO2 generated from energy consumption, there are three predominant factors, namely electricity usage, thermal related costs, and transport usage. Standard home electricity and heating sensors can be used to measure the former two aspects, and in this paper we evaluate a novel technique to estimate an individual's transport-related carbon emissions through the use of a simple wearable accelerometer.

We investigate how providing this novel estimation of transport-related carbon emissions through an interactive web site and mobile phone app engages a set of users in becoming more aware of their carbon emissions. Our evaluations involve a group of 6 users collecting 25 million accelerometer readings and 12.5 million power readings vs. a control group of 16 users collecting 29.7 million power readings.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Application of braking/traction control systems to the scaled active steering testbed in the railway vehicle</title>
	  <abstract>Active steering system of railway vehicles has proven its ability to bridge the gap between stability and curve friendliness. Generally scaled railway vehicles were developed to reproduce the fundamental dynamic behavior of the full size railway vehicle in laboratory conditions. This paper describes the design of the braking/traction control systems as a part of a 1/5 scaled railway vehicle for active steering testbed which is to alleviate wheel/rail contact forces and to decrease wheel/rail wear. This system consists of two steering actuators, a steering controller, and various sensor systems to detect lateral displacement, vibration, track curvature, and so on. The control strategy of the braking/traction is founded on the velocity difference of two axle speed of the driving bogie. Running test results of 1/5 scaled active steering vehicle on the curved track show that the proposed braking/traction control systems has good performance.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>
  <item>
    <title>Injection engine as a control object. II. Problems of automatic control of the engine</title>
	  <abstract>Specific features of injection engine as a control object are discussed, strict formulations of problems of engine automatic control and principles of their solution are presented. Examples of solution of the problem of stabilization of air-fuel ratio and engine torque control problems are presented as illustrations for demonstration of application of modern methods of automatic control theory for solution of control problems of injection engines.</abstract>
	  <search_task_number>19</search_task_number>
	  <query>"fuel consumption" reduction sensor</query>
	  <relevance>0</relevance>
  </item>



	<item>
		<title>"Alone together?": exploring the social dynamics of massively multiplayer online games</title>
		<abstract>Massively Multiplayer Online Games (MMOGs) routinely attract millions of players but little empirical data is available to assess their players' social experiences. In this paper, we use longitudinal data collected directly from the game to examine play and grouping patterns in one of the largest MMOGs: World of Warcraft. Our observations show that the prevalence and extent of social activities in MMOGs might have been previously over-estimated, and that gaming communities face important challenges affecting their cohesion and eventual longevity. We discuss the implications of our findings for the design of future games and other online social spaces.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>The social side of gaming: a study of interaction patterns in a massively multiplayer online game</title>
		<abstract>Playing computer games has become a social experience. Hundreds of thousands of players interact in massively multiplayer online games (MMORPGs), a recent and successful genre descending from the pioneering multi-user dungeons (MUDs). These new games are purposefully designed to encourage interactions among players, but little is known about the nature and structure of these interactions. In this paper, we analyze player-to-player interactions in two locations in the game Star Wars Galaxies. We outline different patterns of interactivity, and discuss how they are affected by the structure of the game. We conclude with a series of recommendations for the design and support of social activities within multiplayer games.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Social regulation of online multiplayer games</title>
		<abstract>Social regulation of user behavior is a key aspect of the community maintenance required to ensure the continued success and well-being of virtual worlds as “Third Places” (Oldenburg, 1989). Accordingly, this dissertation focuses on social regulation within two fantasy-based game MUDs (Multi-user Dungeons) which will be referred to as Illusion and Odyssey. Briefly, social regulation can be defined as “social arrangements employed to keep the behavior of some people in line with the expectations of others” (Hewitt and Hewitt, 1996). One key focus of the work is on providing an ethnographic account of the work performed by the games' administrators, immortals, in order to regulate player behavior. Immortals were observed to use both situated and typified reasoning in order to evaluate situations in a tractable and manageable manner. Such evaluation differs between virtual and physical spaces with respect to both the fluidity of identity and the visibility of information cues.
The second key focus of this work is on the role of software code in regulating behavior. On the two studied systems, code is not merely used as a passive tool by the immortals in the form of specialized commands to monitor player behavior and issue punishments but it also plays a much more active, autonomous role. Both systems utilize specialized software routines that automatically enforce restrictions on behavior that were previously enforced by the immortals. Such code serves as the active agent of regulation by continuously monitoring behavior within the game world and taking regulatory action accordingly. The analysis of the use of coded rules for regulation focuses on differences in the ways in which immortals and coded rules perform regulation. These differences rest primarily in the range and type of information cues used; code was observed to use a narrow set of cues, while immortals considered a much wider set of cues including issues of intent and extenuating circumstances.
In summary, this dissertation presents a detailed account of the regulatory work performed directly by system administrators and by autonomous code so that virtual “third places” might continue to thrive and prosper.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>CAMEO: enabling social networks for massively multiplayer online games through continuous analytics and cloud computing</title>
		<abstract>Millions of people play Massively Multiplayer Online Games (MMOGs) and participate in the social networks built around MMOGs daily. These players turn into a collaborative community to exchange game news, advice, and expertise, but in return expect support such as player reports and clan statistics. Thus, the MMOG social networks need to collect and analyze MMOG data, in a process of continuous MMOG analytics. With the appearance of cloud computing, it has become attractive to use on-demand resources to run automated MMOG data analytics tools. In this work we present CAMEO, an architecture for Continuous Analytics for Massively multiplayEr Online games on cloud resources. Our architecture provides various mechanisms for MMOG data collection and continuous analytics of a pre-determined accuracy in real settings. We implement and deploy CAMEO to perform continuous analytics on data from RuneScape, a popular MMOG. Using resources from various real clouds, including the commercial cloud of Amazon, CAMEO can analyze the characteristics of a community of over 3,000,000 active players, and follow the progress of 500,000 of these players for over a week. Thus, we show evidence that CAMEO can support the social networks built around MMOGs.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>The effects of collective MMORPG (Massively Multiplayer Online Role-Playing Games) play on gamers' online and offline social capital</title>
		<abstract>This study examines the impact of collective MMORPG play on gamers' social capital in both the virtual world and the real world. Collective MMORPG play is conceptualized as the frequency of joint gaming actions and gamers' assessment of the experience in MMORPG guilds and groups. Social capital at the individual level refers to the resources and support provided by bonding and bridging social networks; collective-level social capital refers to people's civic engagement. A two-wave online survey was conducted to collect data from 232 Chinese MMORPG players. Two structural equation models were developed to test whether collective play influences offline social capital via the mediation of online social capital; the results did not demonstrate the existence of mediation effects. Specifically, collective play positively influences gamers' online bonding social capital, online bridging social capital and online civic engagement. The effect of collective play on offline bonding and bridging social capital is not significant; the effect of online bonding/bridging social capital on offline bonding/bridging social capital is not significant either. The study finds a significantly positive impact of collective play on offline civic engagement. The effect of online civic engagement on offline civic engagement is not significant. In contrast with collective play, the time of gaming is found to negatively influence online and offline social capital. This study contributes to the knowledge of social capital because it tests the effects of new media on online and offline social capital in the Chinese culture. In addition, this study provides empirical evidence for the positive effects of online games and highlights the social experience in MMORPG play and how it influences gamers' social networks and collective participation.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>The Social Behaviors of Experts in Massive Multiplayer Online Role-Playing Games</title>
		<abstract>We examine the social behaviors of game experts in Everquest II, a popular massive multiplayer online role-playing game (MMO). We rely on Exponential Random Graph Models (ERGM) to examine the anonymous privacy-protected social networks of 1,457 players over a five-day period. We find that those who achieve the most in the game send and receive more communication, while those who perform the most efficiently at the game show no difference in communication behavior from other players. Both achievement and performance experts tend to communicate with those at similar expertise levels, and higher-level experts are more likely to receive communication from other players.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Trends and Controversies</title>
		<abstract>The rich social media data generated in virtual worlds has important implications for business, education, social science, and society at large. Similarly, massively multiplayer online games (MMOGs) have become increasingly popular and have online communities comprising tens of millions of players. They serve as unprecedented tools for theorizing about and empirically modeling the social and behavioral dynamics of individuals, groups, and networks within large communities. Some technologists consider virtual worlds and MMOGs to be likely candidates to become the Web 3.0. AI can play a significant role, from multiagent avatar research and immersive virtual interface design to virtual world and MMOG Web mining and computational social science modeling. This issue includes articles with research examples from distinguished experts in social science and computer science. Each article presents a unique research framework, computational methods, and selected results.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>NFC Mobile Parlor Games Enabling Direct Player to Player Interaction</title>
		<abstract>Whilst there has been considerable research focus on the user experience of using NFC enabled mobile phones interacting with tags or objects, the ability to perform peer to peer contact has been given little consideration and is often dismissed as being only useful for setting up a connection or transferring business cards. In this research, we present two games that show how the intimate nature of this interaction can successfully be used as a game mechanic for social mobile games, where it is specifically designed to create a shared social experience and generating a near physical interaction between players. In particular, the two games presented use NFC within designs inspired by 19th century parlor games to illustrate how mobile games can benefit from the success console games such as Guitar Hero which move the focus of the player group from screen space to player space and allow a more intimate interaction and approach.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Expressing My Inner Gnome: Appearance and Behavior in Virtual Worlds</title>
		<abstract>Role-playing gamers take on the behavior they think appropriate for the "body" they inhabit in a virtual environment.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>The many faces of sociability and social play in games</title>
		<abstract>In the past, social interaction has been discussed mostly in the context of multiplayer games, ignoring the implicit forms of sociability in single player games. This paper distinguishes between the sociability around the playing of a game and the social play mediated by the game, and looks at single player, two player, multiplayer and massively multiplayer games as arenas for social interaction. The paper does not view social interaction as a new feature or a genre, but as a group of different, yet related, phenomena.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>A model of cognitive loads in massively multiplayer online role playing games</title>
		<abstract>Being one of the most commercially successful entertainment software applications, massively multiplayer online role playing games (MMORPGs) continue to expand in term of the revenue they generate as well as the involvement of users who congregate in their virtual space and form communities around them to support each other. Unlike conventional offline computer games, or networked games with limited numbers of players, MMORPGs are not merely software applications as they are usually seen as a space with complicated dynamics of social interactions. Hence, it is believed that playing these games might cause cognitive overload problems among the players as they have to constantly interact with the game world as well as with other users. We conducted an exploratory study using qualitative methods to explore cognitive overloads in Maple Story, a typical MMORPG. Our results reveal that several types of cognitive overloads emerge during the game playing. While some of these overloads pose serious problems even to expert players, players seem to develop strategies to overcome them. It is found that some forms of cognitive load are actually desirable in order to make the game challenging. We have also created a set of recommendations that can help game developers handle cognitive load problems in MMORPGs.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Exploring social interactions and attributes of casual multiplayer mobile gaming</title>
		<abstract>We explored the social and gaming attributes of interactive casual gaming on mobile phones. To do so, we developed a game called MoMENTus, a multiplayer mobile brain teaser game, and deployed it to 43 participants, divided into various multiplayer and team configurations, over the course of one week. Participants generally liked the interactive and mobile aspects of our casual game, while the team concept was unexpectedly less compelling. Our results suggest that mobile multiplayer casual gaming is a promising new direction, with game play fostering a feeling of community among players. Team play was found to be counter to the casual nature of the game, but competition acted as an effective motivator. Additionally, since mobile phones are "always-on-person," casual mobile gaming was successful in filling interstitial time. Findings address aspects of multiplayer mobile casual gaming, such as the role of competition, manner in which these games are played, and the impact of the type of content on such games. Implications for the design of interactive casual games for mobile phones are discussed.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Renegade gaming: practices surrounding social use of the Nintendo DS handheld gaming system</title>
		<abstract>Today's handheld gaming systems allow players to engage in multiplayer games via ad-hoc, wireless networking. They are also now sufficiently commonplace that it is possible to study how portability and ad-hoc wireless networking have affected the social gaming practices of owners of these systems. In this paper, we report findings from a qualitative study investigating the collocated multiplayer gaming practices of Nintendo DS owners. Based on interviews of nine DS owners and observations of three organized gaming events, we identified three major themes surrounding the social, multiplayer gaming practices of Nintendo DS users: renegade gaming, or the notion that users reappropriate contexts traditionally hostile to game play; pragmatic and social barriers to the formation of ad-hoc pick-up games, despite a clear desire for multiplayer, collocated gaming; and private gaming spheres, or the observation that the handheld device's form factor creates individual, privatized gaming contexts within larger social contexts. These findings lead to a set of implications for the design of future handheld gaming systems.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Virtual "Third Places": A Case Study of Sociability in Massively Multiplayer Games</title>
		<abstract>Georg Simmel [American Journal of Sociology 55:254---261 (1949)] is widely credited as the first scholar to have seriously examined sociability --- "the sheer pleasure of the company of others" and the central ingredient in many social forms of recreation and play. Later Ray Oldenburg [The Great Good Place. New York: Marlowe and Company (1989)] extended Simmel's work by focusing on a certain class of public settings, or "third places," in which sociability tends to occur, such as, bars, coffee shops, general stores, etc. But while Simmel and Oldenburg describe activities and public spaces in the physical world, their concepts may apply as well to virtual or online worlds. Today Massively Multiplayer Online Games (MMOGs) are extensive, persistent online 3D environments that are populated by hundreds of thousands of players at any given moment. The sociable nature of these online spaces is often used to explain their success: unlike previous video games, MMOGs require players to exchange information and collaborate in real-time to progress in the game. In order to shed light on this issue, we critically examine player-to-player interactions in a popular MMOG (Star Wars Galaxies). Based on several months of ethnographic observations and computerized data collection, we use Oldenburg's notion of "third places" to evaluate whether or not the social spaces of this virtual world fit existing definitions of sociable environments. We discuss the role online games can play in the formation and maintenance of social capital, what they can teach us about the evolution of sociability in an increasingly digitally connected social world, and what could be done to make such games better social spaces.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Design for emergence: experiments with a mixed reality urban playground game</title>
		<abstract>In this paper we present our work in the design of ubiquitous social experiences, aiming to foster group participation and spontaneous playful behaviours in a city environment. We outline our approach of design for emergence: to provide just enough of a game context and challenge for people to be creative, to extend and enrich the experience of play through their interaction in the real world. CitiTag is our mixed reality testbed, a wireless location-based multiplayer game based on the concept of playground ‘tag’. We describe the design and implementation of CitiTag and discuss results from two user studies.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Preventing bots from playing online games</title>
		<abstract>As multiplayer online gaming gains in economic and social importance, an increasingly large number of players is beginning to rely on bots (automated player agents) to gain unfair advantages in games. In this article we study the problem of restricting participation in online games to human players so they can enjoy the game without interference from the bots. We propose two broad approaches to prevent bots from playing online games. The first consists of seamlessly integrating software-based tests (known as reverse Turing tests or CAPTCHA tests) into online games to tell humans and computers apart. Our second contribution is to propose hardware instantiations of CAPTCHA tests. Our techniques are applicable in a wide variety of online games, from poker to "shoot'em ups." They are cost-effective, immune to cheating, and preserve the human players' enjoyment of each game. We conclude with a discussion of how approaches to deter the use of bots may complement our techniques to detect bots.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>The spellbound ones: illuminating everyday collaborative gaming practices in a MMORPG</title>
		<abstract>A common argument about computer games and learning is that the commitment gamers have might be transformed and used in educational practices. In order to unpack gamers' commitment, the present study investigates collaboration in a Multiplayer Online Role-Playing Game (MMORPG). It investigates gamers' practices in order to expose their everyday gaming activities and knowledge domains. Drawing on detailed descriptions of team gaming practices, the paper highlights that gamers' of MMORPGs are hands-on experts in handling a game interface. Their expertise is about skilled stances tied to gaming structures. Also, gamers are members in certain communities and adhere to both community specific epistemologies and to generic ones. These gaming stances are from certain educational approaches difficult to make-sense of, while gamers' commitments in other perspectives become means for learning. Lastly, in relation to MMORPGs and education, a neglected issue concerns social pressure in gaming communities, resulting in various forms of participation.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Was Vygotsky right? evaluating learning effects of social interaction in children internet games</title>
		<abstract>The social basis for learning, particularly in childhood, has been acknowledged since the seminal research of the Russian psychologist Lev Vygotsky. Although his theory is very often cited in HCI literature as a theoretical basis for the design of multi-user interactive artefacts, little empirical data is available that assess Vygotsky's thesis in this domain. This paper presents an empirical study that investigated the learning impact of social interaction in the context of children online edutainment. We developed "multiplayer" and "individual" configurations of an educational internet game and measured the learning benefits of "playing together" and "playing alone" in 54 children from a local elementary school. Not surprisingly, our findings confirm that Vygotsky was right. They provide some empirical evidence that in contexts of online gaming, the presence of interpersonal communication, collective goals, and social activities has measurable beneficial effects on children learning.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>A study of interaction patterns and awareness design elements in a massively multiplayer online game</title>
		<abstract>Massively multiplayer online games (MMOGs) have been known to create rich and versatile social worlds for thousands of millions of players to participate. As such, various game elements and advance technologies such as artificial intelligence have been applied to encourage and facilitate social interactions in these online communities, the key to the success of MMOGs. However, there is a lack of studies addressing the usability of these elements in games. In this paper, we look into interaction patterns and awareness design elements that support the awareness in LastWorld and FairyLand. Experimental results obtained through both in-game experiences and player interviews reveal that not all awareness tools (e.g., an in-game map) have been fully exploited by players. In addition, those players who are aware of these tools are not satisfied with them. Our findings suggest that awareness-oriented tools/channels should be easy to interpret and rich in conveying "knowledge" so as to reduce players-cognitive overload. These findings of this research recommend considerations of early stage MMOG design.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Social organization in virtual settings depends on proximity to human visual aspect</title>
		<abstract>Virtual environments are inherently social spaces, in which humans interact through avatars. However, the parameters which favor inter-individual social structuring in those settings are still far to be understood. Particularly, the putative influence of anthropomorphic similarity of visual aspect on social organization of avatars is a key issue to understand the cognitive processes used to form social interactions in virtual worlds. Using the highly popular massively multiplayer online role-playing game World of Warcraft as a model of socially-active virtual setting, we analyzed the social behavior of 11,649 avatars as a function of their visual aspect. Our results show that social structuring in virtual settings depends on proximity to human visual aspect. Social groups formed by human-like avatars display more homogeneity than what the optimal use of the interface would predict, while this effect is not observed for social groups formed by non-human avatars. Thus, immersion in virtual environments depends more on visually-triggered social dynamics (role-play) than on optimal use of the interface (game-play). Furthermore, social aspect may override the immediate reward of interface optimization, thus representing a major factor of immersion in virtual environments.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>The role of trait emotional intelligence in gamers' preferences for play and frequency of gaming</title>
		<abstract>This paper examines the role of trait emotional intelligence (trait EI) in gamers' preferences for play and frequency of gaming in a sample of 1051 young adult US/European gamers, who play frequently the online massively multiplayer game, World of Warcraft (WoW). Trait EI was shown to predict social and achievement preferences for play as well as frequency of gaming. In particular, trait EI was positively correlated to a preference for social practices per se and negatively correlated to a preference for achievement-oriented, instrumental practices. These findings advocate that gamers' preferences for play are in accordance with their emotion-related personality characteristics. Trait EI was also negatively associated with frequency of gaming suggesting that lower scorers on trait EI are more likely associated with more frequent game use.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>What we have here is a failure of companionship: communication in goal-oriented team-mate games</title>
		<abstract>There is a fairly common assumption about real-time, goal oriented, multiplayer games: communication is primarily appreciated (and used) for more effectively attaining goals. But an interesting question that does not seem to have been explored in the literature is whether the desire for companionship is a significant factor in people's desire for and use of communication channels in real-time, goal-oriented, cooperative games. A qualitative study was conducted in which 40 participants played variations of a real-time, goal-oriented, cooperative game with either human or artificial (AI) team-mates, using different communication modalities. Participants consistently expressed a strong desire for the ability to communicate with a team-mate, arguing that it made gameplay more effective and more enjoyable. The significant finding of this study is that in some cases, the strong desire for (and use of) communication channels in realtime, goal-oriented, cooperative games seems to actually be more of a desire for (and experience of) social companionship.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>My guild, my people: role of guilds in massively multiplayer online games</title>
		<abstract>Massively Multiplayer Online Games continue to grow and attract more users. The social aspect of MMOs differentiates them from single person games, increase user loyalty and often result in users spending increasing amounts of time in these virtual environments. We examine World of Warcraft guilds and identify three components of group identity: affective, behavioral and cognitive components. We present the results of our online survey indicating that the affective component, users liking each other and enjoying their interaction with each other is the strongest component of group identity. The result is significant in understanding user behavior and loyalty in MMOs.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Cross-modal compensation between name and visual aspect in socially active avatars</title>
		<abstract>Avatars serve for humans immersed in virtual settings as the interface between real and virtual worlds. The avatar-creation process involves numerous choices, including choice of visual representation, and choices to imbue the character with personality. Here, we hypothesised that these choices are not independent, and that a cross-modal talk may occur between the different components of the avatar identity. Specifically, we investigated whether name properties may be affected by the visual aspect (human vs. non-human) of the avatar. We analyzed names structure of players characters from the popular massively multiplayer online role-playing game (MMORPG) World of Warcraft, which display both human and non-human avatars. We selected 1261 names of characters actively engaged in the in-game and out-game social networks. Analysis of the names revealed that female names presented more variability than male names, and contained systematically more vowels than male names. However, the strategy used to enrich the vowel composition of female names differed between human-like and non-human characters, suggesting that a lesser proximity with human regular appearance was compensated by an increase of ''feminization'' of the name. Altogether, our results suggest that a cross-modal compensation occurs between name and visual aspect in the creation of socially active avatars.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Learning in Single-Versus Multiplayer Games: The More the Merrier?</title>
		<abstract>From the observations of successful entertainment games, it is hypothesized that implementing a single-player option may require a different approach from that in a multiplayer option, in terms of game design. To find out whether this could be true and to understand what the specific approaches could entail for educational games, three separate yet related investigations were conducted to examine single- versus multiplayer games: an investigation of two educational games designed and evaluated by the authors, a theoretical investigation from a game and learning perspective, and an empirical investigation of 23 case studies. From these three investigations, it turned out that a "single-player approach" is data intensive, has formal rules, and uses direct transfer and individual learning. On the other hand, a "multiplayer approach" is less straightforward. From a game perspective, it can be characterized as process intensive and having social rules. When related to learning, however, it could be positioned on any dimension. This exploration shows that the approaches differ to a large extent and that designers have to find a fit between what option they choose and approach they take.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>How computer gamers experience the game situation: a behavioral study</title>
		<abstract>Very little is known about computer gamers' playing experience. Most social scientific research has treated gaming as an undifferentiated activity associated with various factors outside the gaming context. This article considers computer games as behavior settings worthy of social scientific investigation in their own right and contributes to a better understanding of computer gaming as a complex, context-dependent, goal-directed activity. The results of an exploratory interview-based study of computer gaming within the "first-person shooter" (FPS) game genre are reported. FPS gaming is a fast-paced form of goal-directed activity that takes place in complex, dynamic behavioral environments where players must quickly make sense of changes in their immediate situation and respond with appropriate actions. Gamers' perceptions and evaluations of various aspects of the FPS gaming situation are documented, including positive and negative aspects of game interfaces, map environments, weapons, computer-generated game characters (bots), multiplayer gaming on local area networks (LANs) or the internet, and single player gaming. The results provide insights into the structure of gamers' mental models of the FPS genre by identifying salient categories of their FPS gaming experience. It is proposed that aspects of FPS games most salient to gamers were those perceived to be most behaviorally relevant to goal attainment, and that the evaluation of various situational stimuli depended on the extent to which they were perceived either to support or to hinder goal attainment. Implications for the design of FPS games that players experience as challenging, interesting, and fun are discussed.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>On Structure of Games</title>
		<abstract>Games, MMOG-s (Massively Multiplayer On-Line Games), on-line social sites etc have become a major cultural and economic force. The main distinguishing features and attraction of these applications is their interactivity and emergent quality --participants constantly change the state of affairs with their actions. Gameplay, the resulting dynamic flow of events, is fascinating: this is like execution of an algorithm, where elementary actions are defined by game rules, but the logic, the flowchart is composed “on-the-fly” by players. Our understanding of games and gameplay is still on the level of alchemy. Search for adequate formal methods for description and classification of games and gameplay has only started. We do not have established and widely accepted models and methods; the proposed models and classifications of games are mostly subjective.
Here is analyzed the current state of game research and proposed a game specification language, which allows to consider essential features of a new game before implementation. This together with proposed method for recording the “inner” working of games and gameplay allows better to understand games and gameplay and get objective data for studies. Some examples of game specification and recording of game engine's states demonstrate investigating dynamics and emergent qualities of gameplay.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>panOULU conqueror: pervasive location-aware multiplayer game for city-wide wireless network</title>
		<abstract>We present the design, implementation and evaluation of a novel pervasive location-aware multiplayer game. In the game teams of players try to score points by conquering the real-world access points of a large municipal wireless network. The game is implemented as a web service so that playing the game does not require any dedicated game software or hardware, but a general purpose WLAN device such as a laptop or a smart phone equipped with a web browser is sufficient. The game was empirically evaluated with a four-week long tournament involving 96 players in 31 teams. The players found pervasiveness, location-awareness, social interaction and addictivity as the best parts of the game. The main finding of our study is that location-awareness combined with a rather modest level of pervasiveness can go a long way in creating engaging gaming experiences.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Proximity-based chat in a first person shooter: using a novel voice communication system for online play</title>
		<abstract>Voice communication between players can have many benefits relative to text-based communication for game play and social experience in fast-paced multiplayer online games. However, previous research has highlighted some problems with existing implementations of voice-over-IP in online games and suggested the need to carefully design voice communication systems if they are to positively contribute to the game play and social experience of online multiplayer games. In this paper we present the results of a field trial of the "Immersive Communication Environment", a novel voice-over-IP system designed to support player communication in online games by simulating in the game world the way utterances travel through air in the physical world. We found that the proximity-based constraints imposed by this voice communication system created some advantage for players in terms of their game play and their experience of the game as a social event. The findings suggest that players benefit from voice communications systems that make socially salient information available to them according to interactional affordances and constraints that are sensibly designed and well understood.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Designing social videogames for educational uses</title>
		<abstract>In this paper we analyze the main areas of research into educational videogames and in the evolution of the technologies and design methodologies that are making these interactive systems increasingly natural, immersive and social. We present the design and development of a prototype for a collaborative educational videogame based on a Massively Multiplayer Online Role-Playing Game (MMORPG) engine for use in various educational contexts in (a) university education and (b) secondary education.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Agreeing to disagree – pre-game interaction and the issue of community</title>
		<abstract>Playing online multiplayer games entails matching oneself with other players. To do so, players must typically employ various types of communication tools that are part of the game or of game-external matching services. But despite the centrality of these tools they receive little attention in discussions of game design and game HCI. This paper seeks to rectify this situation by presenting an in-depth analysis of two pre-game interaction systems which represent influential approaches. Whereas one of these games allows for high player control and thus inspires negotiation, the other allows player communication mainly to help players pass time between matches. The two approaches are discussed in the light of HCI researcher Jenny Preece’s concept of “sociability” and zoologist Amotz Zahavi’s demonstration of criteria for “honest signalling”. The paper concludes with a discussion of the trade-off facing game designers between efficiency and community-supporting social interaction.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Koko: an architecture for affect-aware games</title>
		<abstract>The importance of affect in delivering engaging experiences in entertainment and educational games is well recognized. Yet, current techniques for building affect-aware games are limited, with the maintenance and use of affect in essence being handcrafted for each game. The Koko architecture describes a service-oriented middleware that reduces the burden of incorporating affect recognition into games, thereby enabling developers to concentrate on the functional and creative aspects of their applications. The Koko architecture makes three key contributions: (1) improving developer productivity by creating a reusable and extensible environment; (2) yielding an enhanced user experience by enabling independently developed games and other applications to collaborate and provide a more coherent user experience than currently possible; (3) enabling affective communication in multiplayer and social games. Further, Koko is intended to be used as an extension of existing game architectures. We recognize that complex games require additional third party libraries, such as game engines. To enable the required flexibility we define the interfaces of the Koko architecture in a formal manner, thereby enabling the implementation of those interfaces to readily adapt to the unique requirements of game's other architectural components and requirements.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Seriously fun: exploring how to combine promoting health awareness and engaging gameplay</title>
		<abstract>Combining engaging gameplay and educational aspects promoting health awareness gives an interesting challenge to game designers. This case study explores adolescents' (aged 13--16) technology usage, gaming habits and gaming motivations, as well as the elements affecting the user experience in serious games. Findings emphasize the importance of social aspects in gaming in form of presence of friends while gaming, or in offline or online multiplayer gaming. According to the findings games should offer challenges matching the player's competence and enable improvement, advancement and developing skills. Modifying characters, exploring the game world and physical activity were also important. The causes and consequences of selections in game and their relation to real-life were wished to be shown clearly in games. Design implications for designing games that promote health awareness are presented.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>The Road Rager: making use of traffic encounters in a mobile multiplayer game</title>
		<abstract>We present Road Rager, a prototype built in order to explore our hypothesis that proximity and a possibility to identify other players during temporary encounters could spur social interaction and enhance a mobile gaming experience. In this case, it is a multiplayer game designed to enable passengers in different cars to play against each other during a meeting in traffic. Using such meetings as resource opens new interesting possibilities for novel and engaging mobile experiences. In this paper we present the game concept, the implementation and the possibilities to interact - designed to successfully benefit from the dynamic and vivid mobile context created during a traffic encounter. We also present a technical test and some initial user feedback on the gaming experience.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>The changing dynamic of social interaction in World of Warcraft: the impacts of game feature change</title>
		<abstract>This study examines how changes in game features through patches and expansions can affect the social interaction within massively multiplayer games. Since patches and expansions are now commonplace within massively multiplayer games, understanding of the relationship would enable better production of social capital. In-depth interviews uncovered that the recent changes to the World of Warcraft affect in-game social interaction in 3 aspects. The areas of social interaction affected included interpersonal relationships, community size and social alienation. The discussions highlight how these findings can advance models of social interaction within MMOs.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Morphological annotation of a corpus with a collaborative multiplayer game</title>
		<abstract>In most of the natural language processing tasks, state-of-the-art systems usually rely on machine learning methods for building their mathematical models. Given that the majority of these systems employ supervised learning strategies, a corpus that is annotated for the problem area is essential. The current method for annotating a corpus is to hire several experts and make them annotate the corpus manually or by using a helper software. However, this method is costly and time-consuming. In this paper, we propose a novel method that aims to solve these problems. By employing a multiplayer collaborative game that is playable by ordinary people on the Internet, it seems possible to direct the covert labour force so that people can contribute by just playing a fun game. Through a game site which incorporates some functionality inherited from social networking sites, people are motivated to contribute to the annotation process by answering questions about the underlying morphological features of a target word. The experiments show that the 63.5% of the actual question types are successful based on a two-phase evaluation.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Principles of emergent design in online games: Mermaids phase 1 prototype</title>
		<abstract>This paper outlines the first phase prototype of Mermaids, a massively multiplayer online game (MMOG) being developed by Georgia Tech's Emergent Game Group {EGG}. We describe Mermaids in the context of the group's research mission, to develop specific games, techniques and design features that promote large-scale emergent social behavior in multiplayer games. We also discuss some of the innovative design features of the Mermaids game, and describe the rapid prototyping and iterative development process that enabled us to create a working prototype in a relatively short period of time on a zero budget project using a student-based development team. We also discuss the special challenges encountered when trying to develop a nontraditional game, one of whose stated research goals is to interrogate MMOG conventions, using a relatively conventional game engine.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Investigating the usability of social networking sites for teenagers with autism</title>
		<abstract>Teenagers with Autism Spectrum Disorder (ASD) tend to be socially isolated due to the interpersonal challenges of autism, yet they enjoy using computers. This study investigated the hypothesis that social networking sites provide the appropriate communication tools for teenagers with ASD. It concluded that although social networking sites remove extraneous stimuli which results in social anxiety among people with ASD, their design functionality does not provide the motivation required to initiate/conduct communication/social interaction among teenagers with ASD. Rather, people with ASD are motivated to communicate with others if the communication is part of an activity. Therefore, a multiplayer networking game has the potential of motivating teenagers with ASD to interact with others through entertainment.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Using player proximity in mobile multiplayer games: experiences from Sandman</title>
		<abstract>In addition to using Bluetooth as a communication channel, it can be used to discover other devices nearby. In games, such information can be used in many ways, such as to group people or direct player to player interaction for instance. In this paper, we explore the possibility to use social proximity in multiplayer gaming using mobile phones. As an example, we describe Sandman, which is a context-aware game built on the Multi-User Publishing Environment (MUPE) platform. The game is available for mobile phones with an access to the internet, Bluetooth, and Java MIDP 2.0</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Game design principles in everyday fitness applications</title>
		<abstract>The global obesity epidemic has prompted our community to explore the potential for technology to play a stronger role in promoting healthier lifestyles. Although there are several examples of successful games based on focused physical interaction, persuasive applications that integrate into everyday life have had more mixed results. This underscores a need for designs that encourage physical activity while addressing fun, sustainability, and behavioral change. This note suggests a new perspective, inspired in part by the social nature of many everyday fitness applications and by the successful encouragement of long term play in massively multiplayer online games. We first examine the game design literature to distill a set of principles for discussing and comparing applications. We then use these principles to analyze an existing application. Finally, we present Kukini, a design for an everyday fitness game.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Behavioral game play: social narrative of peer group observations</title>
		<abstract>Games are seldom played in isolation and there is invariably a social context. This does not just apply to multiplayer or online games, but also to games played in private where they form part of the shared experience of many peer groups. What is clear from the literature is that peer groups also exist between game players, on the internet or in a HAL (Home Area LAN) situation. The literature however seems sparse on how HAL scenarios affect relationships within the peer group; are the same status criteria in effect while game playing? Does the social standing within the group affect the style and competence of play? Do modern games support the peer group development, or do they foster situations which strain the relationships and/or cause fragmentation within the group? This paper is the result of an eight month observational study to identify factors which may influence the behavioural gameplay within a peer group.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Assessment of MANET broadcast schemes in the application context of multiplayer video games</title>
		<abstract>Taking into consideration the popularity of multiplayer games and the tremendous success of wireless networks, we strongly believe that future game engines will benefit from the integration of MANET (Mobile Ad hoc NETworks) technology [1]. Indeed, in addition to the social interests of playing with close located persons, MANET technology will allow players, like a group of kids in a playground, to easily improvise a LAN multiplayer party without any need for an existing wireless network infrastructure. In order to make such scenario a common based reality, several issues still have to be addressed. Among which is the MANET's broadcast problem. In fact, broadcast is a fundamental network service for multiplayer LAN mode. But up to now, the most used scheme for broadcasting in MANET relies on flooding. Unfortunately, this simple scheme leads to high redundancy and collisions. In the context of a multiplayer party supported by mobile terminals, this broadcast storm problem [10], could result into a dramatic reduction of the network energy-based lifetime and to a high variability of the transmission delay. The latter one is know to be a crucial QoS parameter which can significantly affect the game play. In this paper, we evaluate the performance of different MANET broadcast schemes in the context of multiplayer video games. We analyze the capacity of simple flooding, distance based, probability based and self pruning broadcast schemes to satisfy the QoS constraints required by multiplayer games like FPS. Assessment simulations are conducted for both Client/Server and P2P network game architectures.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Does it matter if you don't know who's talking?: multiplayer gaming with voiceover IP</title>
		<abstract>Voiceover IP (VoIP) now makes it possible for people in distributed online multiplayer games to talk to each other. This might not only influence game performance, but also social interaction. However, using VoIP in multiplayer games can often make it hard to know who is talking, an issue that other researchers have found to be problematic. In a 10-week study of a fixed group of adult gamers, we found that not knowing who is talking affects game performance differently according to the type of game. In team-based war games, it can have a negative effect both on learning and coordination, but in race games, where individual rather than teams compete, it appears generally not to matter. In contrast, the impact of not knowing who is talking on social interaction is the same regardless of game type: while the social experience can be highly enjoyable, it is difficult for gamers to get to know each other. We consider the design implications for enhancing both game performance and social interaction.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>MMORPG map evaluation using pedestrian agents</title>
		<abstract>Massive Multiplayer Online Role-Playing Games (MMORPG) increasingly become places of social engagement, by providing spaces for social interaction and relationship beyond home and workplace. The center of this is the virtual environment, in which players interact. Today's savvy players are demanding progressive, more playable and navigable game environments. Traditional methods of the game map evaluation, though reliable, do not offer a quantitative measure of players discomfort and efficiency. This paper introduces a complementary method of the MMORPG map evaluation by utilizing pedestrian agents. The agents employ calculable measures of walking efficiency and discomfort, providing objective criteria, against which the game maps are evaluated. Thus, they promote social interaction by improving the space wherein the players interact.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>An actor-network approach to games and virtual environments</title>
		<abstract>In this paper we apply some of the insights of Bruno Latour and actor network theory to suggest that games and virtual spaces can be interpreted as aesthetic forms which are established and stabilised by a 'collective' of humans and technologies. The 'agents' that comprise any collective or network -- whether it be a simple human-tool relation or a far more complex assemblage of actors in massively multiplayer games - are equally human and non human, social and material, corporeal and technical. Yet the collective impact of these factors is not often given serious attention in the discourses of ludology and game studies, which we argue can be attributed to a number of historical and technical reasons. The application of actor-network theory to games and virtual environments aims to facilitate a nuanced understanding that exceeds more conventional user-- and viewer-centred interpretations in game studies, and is therefore more organic to the open-ended and constantly changing nature of our engagement with online games and virtual environments.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>From buddyspace to CitiTag: large-scale symbolic presence for community building and spontaneous play</title>
		<abstract>In this paper we discuss the conceptual framework and principles that guide our work in the design of large-scale informal environments for collaborative work, learning and play, aiming to foster social bonds and to provide an exciting testbed for emergent social behaviours. We present three different applications we have developed: Buddyspace, an Instant Messaging environment for community building, BumperCars, an online presence-based multiplayer game and CitiTag, an experimental wireless mixed reality game.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Project massive: a study of online gaming communities</title>
		<abstract>Massively Multiplayer Online Games (MMOGs) continue to be a popular and lucrative sector of the gaming market. Project Massive was created to assess MMOG players' social experiences both inside and outside of their gaming environments and the impact of these activities on their everyday lives. The focus of Project Massive has been on the persistent player groups or "guilds" that form in MMOGs. The survey has been completed online by 1836 players, who reported on their play patterns, commitment to their player organizations, and personality traits like sociability, extraversion and depression. Here we report our cross-sectional findings and describe our future longitudinal work as we track players and their guilds across the evolving landscape of the MMOG product space.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>PartyPeer: a P2P massively multiplayer online game</title>
		<abstract>Using peer-to-Peer (P2P) architectures for large scale interactive applications such as Massively Multiplayer Online Games (MMOG) is very challenging because of the difficulties to maintain a consistent game world in a distributed topology and exchange game state information in the P2P network without a central sever. In this demo proposal we present the innovative design and implementation of PartyPeer, an online social game which supports a massive number of users using our P2P based streaming network called ACTIVE+. We also discuss some of the implementation challenges when building this real-world P2P based game.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Front Matter and Contents</title>
		<abstract>Design for emergence investigates spontaneous, unpredictable uses of technology that are driven by social contexts and collaborative processes, based on our ability to communicate our presence, both virtual and physical, in symbolic ways. In light of the fact that social dynamics and unexpected uses of technology can inspire innovation, this book proposes a research model of design for emergence, focusing on emergent phenomena as part of an iterative design process. By providing playful, technology-mediated experiences with minimal structure, unpredictable user behaviours can emerge through exploration, resulting in a richer and more complex, social experience. The research methodology is practice-based; two interactive prototypes were designed, implemented and evaluated in different contexts: an online multiplayer BumperCar game and a wireless, location-based urban game of 'tag', called CitiTag. User studies showed that collaborative, spontaneous play can enhance the sense of social participation in a group activity. Collective and individual behaviours and creative uses of technology emerged from a simply designed application based on symbolic presence, both in the virtual and the physical world.
CitiTag experiments showed that virtual elements in a mixed reality game can instigate novel experiences in the context of our everyday physical and social environment, with often unexpected results. The observed emergent behaviours are personal and collective extensions of the virtual experience in the real world. The book concludes with a positive view of ubiquitous and social computing, in which the virtual world becomes a 'first class citizen' rather than a substitute for the real world, creating new situations and engaging experiences in the setting of our daily life that were not possible before.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Wireless education platform open source way to create mobile education</title>
		<abstract>This paper describes the Wireless Education Platform (WEP) -- a platform for mobile multi-user educational services. The platform can be used for multilingual, social multi-user educational gaming experiences on mobile platforms.
The platform is at the initial stages of development, and it currently consists of two exercise types: mathematics and memory. One solo-training mode and three different types of multiplayer games have been implemented. Currently, no real user studies have been done, although the application has been used by pupils.
This paper discusses the design principles behind WEP; the modular extensible implementation of it, and the multi-user aspects of the platform.</abstract>
		<search_task_number>18</search_task_number>
		<query>social multiplayer game</query>
		<relevance>0</relevance>
	</item>



  <item>
    <title>RearType: text entry using keys on the back of a device</title>
	<abstract>RearType is a text input system for mobile devices such as Tablet PCs, using normal keyboard keys but on the reverse side of the device. The standard QWERTY layout is split and rotated so that hands gripping the device from either side have the usual keys under the fingers. This frees up the front of the device, maximizing the use of the display for visual output, eliminating the need for an onscreen keyboard and the resulting hand occlusion, and providing tactile and multi-finger text entry - with potential for knowledge transfer from QWERTY. Using a prototype implementation which includes software visualization of the keys to assist with learning, we conducted a study to explore the initial learning curve for RearType. With one hour's training, RearType typing speed was an average 15 WPM, and was not statistically different to a touchscreen keyboard.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Proceedings of the 27th international conference extended abstracts on Human factors in computing systems</title>
	<abstract>Welcome to CHI 2009! CHI comprises many events, ranging from archival material stored in the ACM digital library, to transient interactions such as the presentations, panels, and poster discussions, to the many social interactions and activities that make CHI a collegial and intimate experience. All parts are important, but it is the archival material -- especially the papers and notes -- that establishes CHI as the leading academic conference in Human Computer Interaction. Yet there are significant challenges in managing paper and notes within CHI. The HCI field has been very successful at creating new generations of research and practitioners over the years. The many people who are part of this community see CHI as the place to share their knowledge and experiences with others, primarily by publishing and presenting papers and notes. This has stressed the system in many ways. As submissions increase, so do the difficulties in managing the review process, finding good reviewers and other volunteers, matching papers to those competent in the subject matter, deciding which papers to accept or reject, maintaining consistent standards across both paper and notes, and not falling into the trap of overly narrowing our view of what is an 'acceptable' CHI paper. This year, we introduced several large changes to the CHI papers/notes process to mitigate these challenges, most which will be transparent to attendees. First, we reorganized the CHI program committee into nine topical subcommittees - each a mini program committee - comprising sub-committee chairs (SCs) and various associate chairs (ACs) knowledgeable on the topic. Authors could select the subcommittee that he or she felt could best handle their submission. We did this to improve the match of a submission to AC and ultimately to reviewers, to have more focused and relevant discussions in the program committee meeting, and to minimize the load on individual volunteers. Second, we combined papers and notes, where all were handled in exactly the same way. We did this to ensure a consistent decision standard across both submission types. Third, we introduced contribution types, where each type described a different way that a CHI submission could contribute to the field as well as typical questions such a contribution should address. Authors identified their submission by contribution type, and (hopefully) used the information to help structure their paper. The idea is that we wanted to encourage a broad variety of submissions from authors (rather than 'formula' papers), while also providing guidance to referees by supplying criteria appropriate to the type of contribution the submission was making. It will likely take several years before the full impact of these changes are known. We know that subcommittees did help us manage the large number of submissions. We also believe there was an overall better match between referees and submissions, and that papers and notes were handled consistently. We don't yet know about the effect of contribution types: this is a cultural change where we are hoping that authors will be more willing to write papers that don't match a particular formula, and that reviewers will be more accepting of those submissions. Now for the numbers. This year, there were 1130 submissions, comprising 711 full papers and 419 notes. This is the highest number of submissions ever to CHI. Of these, we accepted 24.5%. The papers/notes committee involved 107 volunteers: the 2 co-chairs, 10 sub-committee chairs, and 95 associate chairs (ACs). Each AC managed 10-14 submissions, and personally recruited at least three -- sometimes more -- referees knowledgeable in the paper's topic. Refereeing was through blind review. Each referee returned a recommendation along with a detailed review, and authors had opportunity to rebut these reviews. Additional reviews were sometimes solicited. Almost all program committee members then attended a two day meeting in Boston in December. Rigorous discussions took place at the PC meeting, and the majority of papers were read by a second AC as well. The decision process was highly visible so that the committee could calibrate itself. Finally, the various committees nominated 5% of the submissions as potential best papers. A separate committee deliberated over these papers, where only 1% of papers and notes received a best paper award. In total, as you will see in the program, 32 papers and four notes were designated as honorable mentions, while seven papers and four notes honored as best papers. Congratulations to all authors who achieved this significant status!</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Movement model, hits distribution and learning in virtual keyboarding</title>
	<abstract>In a ten-session experiment, six participants practiced typing with an expanding rehearsal method on an optimized virtual keyboard. Based on a large amount of in-situ performance data, this paper reports the following findings. First, the Fitts-digraph movement efficiency model of virtual keyboards is revised. The format and parameters of Fitts' law used previously in virtual keyboards research were incorrect. Second, performance limit predictions of various layouts are calculated with the new model. Third, learning with expanding rehearsal intervals for maximum memory benefits is effective, but many improvements of the training algorithm used can be made in the future. Finally, increased visual load when typing previously practiced text did not significantly change users' performance at this stage of learning, but typing unpracticed text did have a performance effect, suggesting a certain degree of text specific learning when typing on virtual keyboards</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Ability-Based Design: Concept, Principles and Examples</title>
	<abstract>Current approaches to accessible computing share a common goal of making technology accessible to users with disabilities. Perhaps because of this goal, they may also share a tendency to centralize disability rather than ability. We present a refinement to these approaches called ability-based design that consists of focusing on ability throughout the design process in an effort to create systems that leverage the full range of human potential. Just as user-centered design shifted the focus of interactive system design from systems to users, ability-based design attempts to shift the focus of accessible design from disability to ability. Although prior approaches to accessible computing may consider users’ abilities to some extent, ability-based design makes ability its central focus. We offer seven ability-based design principles and describe the projects that inspired their formulation. We also present a research agenda for ability-based design.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Involving psychometric tests for input device evaluation with older people</title>
	<abstract>This paper presents a preliminary study of using psychometric tests when testing input devices with older people. An experiment with twelve older computer users evaluating three commonly used input devices (mouse, touch screen and tablet-with-stylus) in two common computer tasks (browsing and playing solitaire), preceded by a questionnaire and psychometric tests (Simple Reaction Time, Mini Mental State Exam and Identical Picture), and concluded with debriefing interviews, is described. The paper concludes that psychometric tests can provide quantitative data that complement the information collected through the questionnaire and interview and that some psychometric data were the best predictor of task performance.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Comparing cursor orientations for mouse, pointer, and pen interaction</title>
	<abstract>Most graphical user interfaces provide visual cursors to facilitate interaction with input devices such as mice, pointers, and pens. These cursors often include directional cues that could influence the stimulus-response compatibility of user input. We conducted a controlled evaluation of four cursor orientations and an orientation-neutral cursor in a circular menu selection task. Mouse interaction on a desktop, pointer (i.e. wand) interaction on a large screen, and pen interaction on a Tablet PC were evaluated. Our results suggest that choosing appropriate cursors is especially important for pointer interaction, but may be less important for mice or pens. Cursors oriented toward the lower-right corner of a display yielded the poorest performance overall while orientation-neutral cursors were generally the best. Advantages were found for orientations aligned with the direction of movement. We discuss these results and suggest guidelines for the appropriate use of cursors in various input and display configurations.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Evaluating swiftpoint as a mobile device for direct manipulation input</title>
	<abstract>This paper presents a promising new computer pointing device, called Swiftpoint, that is designed primarily for mobile computer (for example, laptop) users in constrained space. Swiftpoint has many advantages over current pointing devices: it is small, ergonomic, has a digital ink mode, and can be used over a flat keyboard. We present the results of a formal evaluation conducted to compare Swiftpoint to two of the most common pointing devices with today's mobile computers: the mouse, and touchpad. Two laws commonly used in evaluating pointing devices, Fitts' Law and the Steering Law, were used to evaluate Swift-point. Results showed that Swiftpoint was faster and more accurate than the touchpad. The performance of the mouse was however, superior to both the touchpad and Swiftpoint.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>An introduction to 3D spatial interaction with video game motion controllers</title>
	<abstract>3D spatial interfaces [Bowman et al. 2004] give users the ability to spatially interact with 3D virtual worlds because they provide natural mappings from human movement to interface controls. These interfaces, common in virtual and augmented reality applications, give users, rich, immersive, and interactive experiences that can mimic the real world or provide magical, larger than life interaction metaphors [Katzourin et al. 2006].</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>“Writing with music”: Exploring the use of auditory feedback in gesture interfaces</title>
	<abstract>We investigate the use of auditory feedback in pen-gesture interfaces in a series of informal and formal experiments. Initial iterative exploration showed that gaining performance advantage with auditory feedback was possible using absolute cues and state feedback after the gesture was produced and recognized. However, gaining learning or performance advantage from auditory feedback tightly coupled with the pen-gesture articulation and recognition process was more difficult. To establish a systematic baseline, Experiment 1 formally evaluated gesture production accuracy as a function of auditory and visual feedback. Size of gestures and the aperture of the closed gestures were influenced by the visual or auditory feedback, while other measures such as shape distance and directional difference were not, supporting the theory that feedback is too slow to strongly influence the production of pen stroke gestures. Experiment 2 focused on the subjective aspects of auditory feedback in pen-gesture interfaces. Participants' rating on the dimensions of being wonderful and stimulating was significantly higher with musical auditory feedback. Several lessons regarding pen gestures and auditory feedback are drawn from our exploration: a few simple functions such as indicating the pen-gesture recognition results can be achieved, gaining performance and learning advantage through tightly coupled process-based auditory feedback is difficult, pen-gesture sets and their recognizers can be designed to minimize visual dependence, and people's subjective experience of gesture interaction can be influenced using musical auditory feedback. These lessons may serve as references and stepping stones toward future research and development in pen-gesture interfaces with auditory feedback.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Interface with pre-typing visual feedback for touch-sensitive keyboard</title>
	<abstract>In this paper, a method is described that detects and displays what key a user has touched with his fingertip before the key is pressed. The proposed method is based on the use of a touch-sensitive cover on a pushbutton key set. The identity of the touched key is displayed exactly at the point where the I-beam pointer indicates at the inputting string, so that correct data entry will be made simpler without looking at the keyboard or needing training.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A Meta-Analytical Review of Empirical Mobile Usability Studies</title>
	<abstract>In this paper we present an adapted usability evaluation framework to the context of a mobile computing environment. Using this framework, we conducted a qualitative meta-analytical review of more than 100 empirical mobile usability studies. The results of the qualitative review include (a) the contextual factors studied; (b) the core and peripheral usability dimensions measured; and (c) key findings in the form of a research agenda for future mobile usability research, including open and unstructured tasks are underutilized, interaction effects between interactivity and complexity warrant further investigation, increasing research on accessibility may improve the usability of products and services for often overlooked audiences, studying novel technology and environmental factors will deepen contextual mobile usability knowledge, understanding which hedonic factors impact the aesthetic appeal of a mobile device or service and in turn usability, and a high potential for neuroscience research in mobile usability. Numerous additional findings and takeaways for practitioners are also discussed.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Adapting paper prototyping for designing user interfaces for multiple display environments</title>
	<abstract>A multiple display environment (MDE) networks personal and shared devices to form a virtual workspace, and designers are just beginning to grapple with the challenges of developing interfaces tailored for these environments. To develop effective interfaces for MDEs, designers must employ methods that allow them to rapidly generate and test alternative designs early in the design process. Paper prototyping offers one promising method, but needs to be adapted to effectively simulate the use of multiple displays and allow testing with groups of users. In this paper, we share experiences from two projects in which paper prototyping was utilized to explore interfaces for MDEs. We identify problems encountered when applying the traditional method, describe how these problems were overcome, and distill our experiences into recommendations that others can draw upon. By following our recommendations, designers need only make minor modifications to the existing method to better realize benefits of paper prototyping for MDEs.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Unipad: single stroke text entry with language-based acceleration</title>
	<abstract>A stylus-based text entry technique called Unipad is presented. Unipad combines single-stroke text input with language-based acceleration techniques, including word completion, suffix completion, and frequent word prompting. In a study with ten participants, entry rates averaged 11.6 wpm with 0.90% errors after two hours of practice. In follow-on sessions to establish the expert potential, four users entered "the quick brown fox" phrase repeatedly for four blocks of 15 minutes each. Average rates on the last block ranged from 17.1 to 35.1 wpm, with peak rates reaching 48 wpm.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The state of the market: a laptop buying guide</title>
	<abstract>LJ helps you find the right Linux laptop.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Mobile application for determination of users' text entry speed</title>
	<abstract>The report examines the existing metrics for determination of users' text entry speed. The functional and ergonomic requirements for a mobile application that can perform such task are defined. The realization of a concrete mobile application (m-TESM) is described and appropriate conclusions are made.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Pen-based computing</title>
	<abstract>Pens may seem old-fashioned, but some researchers think they are the future of interaction. Can they teach this old dog some new tricks?</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Design and validation of two-handed multi-touch tabletop controllers for robot teleoperation</title>
	<abstract>Controlling the movements of mobile robots, including driving the robot through the world and panning the robot's cameras, typically requires many physical joysticks, buttons, and switches. Operators will often employ a technique called "chording" to cope with this situation. Much like a piano player, the operator will simultaneously actuate multiple joysticks and switches with his or her hands to create a combination of complimentary movements. However, these controls are in fixed locations and unable to be reprogrammed easily. Using a Microsoft Surface multi-touch table, we have designed an interface that allows chording and simultaneous multi-handed interaction anywhere that the user wishes to place his or her hands. Taking inspiration from the biomechanics of the human hand, we have created a dynamically resizing, ergonomic, and multi-touch controller (the DREAM Controller). This paper presents the design and testing of this controller with an iRobot ATRV-JR robot.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Proceedings of the 2011 annual conference on Human factors in computing systems</title>
	<abstract>Over the last year or so, we have been blessed with the challenge, the opportunity, and the distinct pleasure of organizing the CHI 2011 Conference on Human Factors in Computing Systems, the premier international conference for the field of human-computer interaction. CHI 2011 takes place in gorgeous, energetic, sophisticated Vancouver BC, a city renowned for its innovation in entertainment, sustainability, accessibility, and inclusivity. The New York Times calls it, "a liquid city, a tomorrow city, equal parts India, China, England, France and the Pacific Northwest." Vancouver lays a beautiful backdrop for our conference, which boasts nearly 30 years of wonderful work. Behind the success of the conference is our diverse community of faculty and students, of researchers and practitioners, of young and, well, also of experienced. It is a community of designers, technologists, psychologists, social scientists, biologists, artists, engineers, anthropologists, musicians; the list goes on. Wherever we are, we are always a community of near and far. Most impressively, ours is a community that cares deeply about innovating, learning, sharing, and interacting; all with the common goal of using technology to shape the way people around the world live and play. Returning attendees will recognize the general conference format - 2 days of small intimate workshops, followed by 4 days of technical content, all surrounded by social and intellectual exchanges. In addition to the familiar venues that form the core of the conference, we have also arranged various special events, such an keynotes by Howard Rheingold and Ethan Zuckerman; invited talks by ACM SIGCHI award winners Terry Winograd, Larry Tesler, Alan Newell, and Clayton Lewis; an HCI museum exhibit hosted by Bill Buxton; and a panel celebrating Stu Card's achievements and contributions to the field of HCI. With the record number of submissions and accepted content this year, we hope that you will utilize the print and electronic programs, but also the daily CHI Madness presentations that provide a glimpse of the day ahead. In the interest of continuing to evolve the conference to best serve our needs, we will experiment with shorter talks this year (20 minute slots for long pieces of content and 10 for shorter ones) to infuse even more energy into the program. We will also have a pretty full slate of social media applications to help you connect with other attendees and to provide you with the fullest experience possible. </abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Attribute gates</title>
	<abstract>Attribute gates are a new user interface element designed to address the problem of concurrently setting attributes and moving objects between territories on a digital tabletop. Motivated by the notion of task levels in activity theory, and crossing interfaces, attribute gates allow users to operationalize multiple subtasks in one smooth movement. We present two configurations of attribute gates; (1) grid gates which spatially distribute attribute values in a regular grid, and require users to draw trajectories through the attributes; (2) polar gates which distribute attribute values on segments of concentric rings, and require users to align segments when setting attribute combinations. The layout of both configurations was optimised based on targeting and steering laws derived from Fitts' Law. A study compared the use of attribute gates with traditional contextual menus. Users of attribute gates demonstrated both increased performance and higher mutual awareness.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The impact on musculoskeletal system during multitouch tablet interactions</title>
	<abstract>HCI researchers and technologists have heralded multitouch interaction as the technology to drive computing systems into the future. However, as we move towards a world where interaction is based on human body movements that are not well documented or studied, we face a serious and a grave risk of creating technology and systems that may lead to musculoskeletal disorders (MSD's). Designers need to be empowered with objective data on the impact of multitouch interactions on the musculoskeletal system to make informed choices in interaction design. In this paper we present an experiment that documents kinematic (movement) and kinetic measures (EMG) when interacting with a multitouch tablet. Results show that multitouch interaction can induce significant stress that may lead to MSDs and care must be taken when designing multitouch interaction.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Effectiveness of annotating by hand for non-alphabetical languages</title>
	<abstract>Unlike documents, annotation for multimedia information needs to be input as text, not in the form of symbols such as underlines and circles. This is problematic with keyboard input for non-alphabetical languages, especially the East Asian languages such as Chinese and Japanese, because it is labor intensive and imposes a high cognitive load. This study provides a quantitative analysis of the effectiveness of making annotations by hand during a note-taking task in Japanese. Although the lessons learned from this study come from Japanese text input, they are also generally applicable to other East Asian Languages which use ideographic characters such as Chinese. In our study, we focused on both the ergonomic and cognitive aspects and found that during annotation and note-taking task input by hand is more effective than input by keyboard. Finally, we anatomized the keyboard input problem and discuss it in this paper.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Motivating mobility: designing for lived motivation in stroke rehabilitation</title>
	<abstract>How to motivate and support behaviour change through design is becoming of increasing interest to the CHI community. In this paper, we present our experiences of building systems that motivate people to engage in upper limb rehabilitation exercise after stroke. We report on participatory design work with four stroke survivors to develop a holistic understanding of their motivation and rehabilitation needs, and to construct and deploy engaging interactive systems that satisfy these. We reflect on the limits of motivational theories in trying to design for the lived experience of motivation and highlight lessons learnt around: helping people articulate what motivates them; balancing work, duty, fun; supporting motivation over time; and understanding the wider social context. From these we identify design guidelines that can inform a toolkit approach to support both scalability and personalisability.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Using paper mockups for evaluating soft keyboard layouts</title>
	<abstract>Five experiments were conducted to compare soft keyboard layouts. The methodology involved paper mockups and manual timing in a classroom situation. Students worked in pairs - one as experimenter, one as participant - swapping roles midway through the experiment. Participants used a stylus to tap the well-known "quick brown fox" phrase five times on each layout. Entry speeds, computed from the measured time to enter the phrase, were 26.5 to 34.5 wpm for the Qwerty keyboard layout, 12.3 to 14.7 wpm for the Opti layout, 15.7 wpm for the Fitaly layout, 12.1 and 12.3 for a Qwerty-Phone (QP) hybrid layout, and 19.0 to 23.0 wpm for the standard phone keypad layout. The merits and limitations of the evaluation method are discussed.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Interactive ASR error correction for touchscreen devices</title>
	<abstract>We will demonstrate a novel graphical interface for correcting search errors in the output of a speech recognizer. This interface allows the user to visualize the word lattice by "pulling apart" regions of the hypothesis to reveal a cloud of words simlar to the "tag clouds" popular in many Web applications. This interface is potentially useful for dictation on portable touchscreen devices such as the Nokia N800 and other mobile Internet devices.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Speech interaction in a multimodal tool for handwritten text transcription</title>
	<abstract>STATE is a multimodal tool for document processing and text transcription. Its graphical front-end can be easily connected to different text recognition back-ends. New features and improvements are presented in this work: the interactive correction of one word in the transcribed line has been improved to reestimate the entire transcription line using the user feedback and speech input has been integrated in the multimodal interface enabling the user to also utter the word to be corrected, giving the user the possibility to use the interface according to her preferences or the task at hand. Thus, at the current version of STATE, the user can type, write on the screen with a stylus, or utter the incorrectly recognized word, and then, the system uses the user feedback in any of the proposed modalities to reestimate the transcribed line so as to hopefully correct other errors which could be caused by the mistaken word the user has corrected.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The anatomy of prototypes: Prototypes as filters, prototypes as manifestations of design ideas</title>
	<abstract>The role of prototypes is well established in the field of HCI and Design. A lack of knowledge, however, about the fundamental nature of prototypes still exists. Researchers have attempted to identify different types of prototypes, such as low- vs. high-fidelity prototypes, but these attempts have centered on evaluation rather than support of design exploration. There have also been efforts to provide new ways of thinking about the activity of using prototypes, such as experience prototyping and paper prototyping, but these efforts do not provide a discourse for understanding fundamental characteristics of prototypes. In this article, we propose an anatomy of prototypes as a framework for prototype conceptualization. We view prototypes not only in their role in evaluation but also in their generative role in enabling designers to reflect on their design activities in exploring a design space. We base this framework on the findings of two case studies that reveal two key dimensions: prototypes as filters and prototypes as manifestations. We explain why these two dimensions are important and how this conceptual framework can benefit our field by establishing more solid and systematic knowledge about prototypes and prototyping.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Two-handed virtual manipulation</title>
	<abstract>We discuss a two-handed user interface designed to support three-dimesional neurosurgical visualization. By itself, this system is a “point design,” an example of an advanced user interface technique. In this work, we argue that in order to understand why interaction techniques do or do not work, and to suggest possibilities for new techniques, it is important to move beyond point design and to introduce careful scientific measurement of human behavioral principles. In particular, we argue that the common-sense viewpoint that “two hands save time by working in parallel” may not always be an effective way to think about two-handed interface design because the hands do not necessarily work in parallel (there is a structure to two-handed manipulation) and because two hands do more than just save time over one hand (two hands provide the user with more information and can structure how the user thinks about a task). To support these claims, we present an interface design developed in collaboration with neurosurgeons which has undergone extensive informal usability testing, as well as a pair of formal experimental studies which investigate behavioral aspects of two-handed virtual object manipulation. Our hope is that this discussion will help others to apply the lessons in our neurosurgery application to future two-handed user interface designs.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Touch-display keyboards: transforming keyboards into interactive surfaces</title>
	<abstract>In spite of many advances in GUI workstations, the keyboard has remained limited to text entry and basic command invocation. In this work, we introduce the Touch-Display Keyboard (TDK), a novel keyboard that combines the physical-ergonomic qualities of the conventional keyboard with dynamic display and touch-sensing embedded in each key. The TDK effectively transforms the keyboard into an interactive surface that is seamlessly integrated with the interaction space of GUIs, extending graphical output, mouse interaction and three-state input to the keyboard. This gives rise to an entirely new design space of interaction across keyboard, mouse and screen, for which we provide a first systematic analysis in this paper. We illustrate the emerging design opportunities with a host of novel interaction concepts and techniques, and show how these contribute to expressiveness of GUIs, exploration and learning of keyboard interfaces, and interface customization across graphics display and physical keyboard.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Shifting the focus from accuracy to recallability: A study of informal note-taking on mobile information technologies</title>
	<abstract>Mobile information technologies are theoretically well-suited to digitally accomodate informal note-taking, with the notes often recorded quickly and under less than ideal circumstances. Unfortunately, user adoption of mobile support for informal note-taking has been hindered in large part by slow text entry techniques. Building on research confirming people's ability to recognize erroneous text, this study explores two simple modifications to Graffiti-based text entry with the goal of increasing text entry speed: disabling text correction and disabling visual feedback. As expected, both modifications improved text entry speed at the cost of recognizability. To address the decrease in recognizability, a multiapproach text-enhancement algorithm is introduced with the goal of modifying the erroneous note to facilitate the process of recalling the event or activity that originally motivated the note. A study with 75 participants confirmed that the proposed approach of discouraging user-initiated error correction during note-taking, enhancing the resulting erroneous notes, and facilitating recall with enhanced alternative lists, increased note-taking speed by 47% with no negative impact on the participants' ability to recall important details about the scenarios which prompted the note-taking activities. This research highlighs the importance and efficacy of shifting the focus from accuracy to recallability when examining the overall efficacy of informal notes. The proposed modifications and adaptations produce significant benefits and have important implications for how mobile technologies are designed to support both informal note-taking and text entry in general.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>[hid] toolkit: a unified framework for instrument design</title>
	<abstract>The [hid] toolkit is a set of software objects for designing gestural instruments. All too frequently, computer performers are tied to the keyboard/mouse/monitor model, narrowly constraining the range of possible gestures. A multitude of off-the-shelf input devices are readily available, making it easy to utilize a broader range of gestures. Human Interface Devices (HIDs) such as joysticks, tablets, and gamepads are cheap and can be good musical controllers, with some even provide haptic feedback. The [hid] toolkit provides a unified, consistent framework for getting gestural data from these devices, controlling the feedback, and mapping this data to the desired output. The [hid] toolkit is built in Pd, which provides an ideal platform for this work, combining the ability to synthesize and control audio, video, and other media. The addition of easy access to gestural data allows for rapid prototypes. A usable environment also makes computer music instrument design accessible to novices.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Towards maximizing the accuracy of human-labeled sensor data</title>
	<abstract>We present two studies that evaluate the accuracy of human responses to an intelligent agent's data classification questions. Prior work has shown that agents can elicit accurate human responses, but the applications vary widely in the data features and prediction information they provide to the labelers when asking for help. In an initial analysis of this work, we found the five most popular features, namely uncertainty, amount and level of context, prediction of an answer, and request for user feedback. We propose that there is a set of these data features and prediction information that maximizes the accuracy of labeler responses. In our first study, we compare accuracy of users of an activity recognizer labeling their own data across the dimensions. In the second study, participants were asked to classify a stranger's emails into folders and strangers' work activities by interruptibility. We compared the accuracy of the responses to the users' self-reports across the same five dimensions. We found very similar combinations of information (for users and strangers) that led to very accurate responses as well as more feedback that the agents could use to refine their predictions. We use these results for insight into the information that help labelers the most.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Feel-good touch: finding the most pleasant tactile feedback for a mobile touch screen button</title>
	<abstract>Earlier research has shown the benefits of tactile feedback for touch screen widgets in all metrics: performance, usability and user experience. In our current research the goal was to go deeper in understanding the characteristics of a tactile click for virtual buttons. More specifically we wanted to find a tactile click which is the most pleasant to use with a finger. We used two actuator solutions in a small mobile touch screen: piezo actuators or a standard vibration motor. We conducted three experiments: The first and second experiments aimed to find the most pleasant tactile feedback done with the piezo actuators or a vibration motor, respectively, and the third one combined and compared the results from the first two experiments. The results from the first two experiments showed significant differences for the perceived pleasantness of the tactile clicks, and we used these most pleasant clicks in the comparison experiment in addition to the condition with no tactile feedback. Our findings confirmed results from earlier studies showing that tactile feedback is superior to a nontactile condition when virtual buttons are used with the finger regardless of the technology behind the tactile feedback. Another finding suggests that the users perceived the feedback done with piezo actuators slightly more pleasant than the vibration motor based feedback, although not statistically significantly. These results indicate that it is possible to modify the characteristics of the virtual button tactile clicks towards the most pleasant ones, and on the other hand this knowledge can help designers to create better touch screen virtual buttons and keyboards.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A study on PDAs for onboard applications and technologies and methodologies</title>
	<abstract>In 2005, the European Space Agency carried out a study intended to clarify the use of PDAs for onboard space operations. Being this a rather new application domain due to the introduction of uncommon technology onboard, a need for feasibility clarification was perceived. The study narrowed its unbounded technological dimension by focusing on Java technologies. Furthermore, at a software engineering level an "agile" approach was chosen for evaluation because of the high degree of user interaction and their ability to cope with general requirement changes like those resulting from the uncertainty about the actual features and quality of specific PDA platforms and COTS. Two studies were performed in parallel with the same objectives but obtaining different solutions. Suitable "agile-like" approaches compatible with space software standards were defined and tried. A space crew representative was involved from the early phases of the studies, and a final evaluation exercise at the European Astronaut Centre training facilities was performed. Feedback on the suitability of the technologies and the MMI design choices were obtained together with other unexpected lessons. The paper is concluded with an enumeration of lessons learnt and hypotheses about "agile" approaches of interest for the software engineering community.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A Fitts Law comparison of eye tracking and manual input in the selection of visual targets</title>
	<abstract>We present a Fitts' Law evaluation of a number of eye tracking and manual input devices in the selection of large visual targets. We compared performance of two eye tracking techniques, manual click and dwell time click, with that of mouse and stylus. Results show eye tracking with manual click outperformed the mouse by 16%, with dwell time click 46% faster. However, eye tracking conditions suffered a high error rate of 11.7% for manual click and 43% for dwell time click conditions. After Welford correction eye tracking still appears to outperform manual input, with IPs of 13.8 bits/s for dwell time click, and 10.9 bits/s for manual click. Eye tracking with manual click provides the best tradeoff between speed and accuracy, and was preferred by 50% of participants. Mouse and stylus had IPs of 4.7 and 4.2 respectively. However, their low error rate of 5% makes these techniques more suitable for refined target selection.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>tranSticks: physically manipulatable virtual connections</title>
	<abstract>A virtually connected medium called tranStick is described that functions both as a "virtual wire" and as a "memory card" containing a shared space. A user can connect two networked devices by simply placing one of a pair of tranSticks with the same identifier into each device. The tranSticks provide feedback indicating that the devices are connected; the connection to be closed or changed in the same way it would be if the devices were connected by a physical cable. A user can also access to a shared space on a network as if the space were in the tranStick. Since tranSticks contain long secret keys, the process of finding another tranStick with the same identifier can be encrypted. The tranStick approach differs from other approaches in that it provides feedback from the connection as well as serving as a medium for establishing a connection, and it enables disconnection and switchover to be done intuitively because the operations are reversible.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Lucid touch: a see-through mobile device</title>
	<abstract>Touch is a compelling input modality for interactive devices; however, touch input on the small screen of a mobile device is problematic because a user's fingers occlude the graphical elements he wishes to work with. In this paper, we present LucidTouch, a mobile device that addresses this limitation by allowing the user to control the application by touching the back of the device. The key to making this usable is what we call pseudo-transparency: by overlaying an image of the user's hands onto the screen, we create the illusion of the mobile device itself being semi-transparent. This pseudo-transparency allows users to accurately acquire targets while not occluding the screen with their fingers and hand. Lucid Touch also supports multi-touch input, allowing users to operate the device simultaneously with all 10 fingers. We present initial study results that indicate that many users found touching on the back to be preferable to touching on the front, due to reduced occlusion, higher precision, and the ability to make multi-finger input.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>CINCH: a cooperatively designed marking interface for 3D pathway selection</title>
	<abstract>To disentangle and analyze neural pathways estimated from magnetic resonance imaging data, scientists need an interface to select 3D pathways. Broad adoption of such an interface requires the use of commodity input devices such as mice and pens, but these devices offer only two degrees of freedom. CINCH solves this problem by providing a marking interface for 3D pathway selection. CINCH interprets pen strokes as pathway selections in 3D using a marking language designed together with scientists. Its bimanual interface employs a pen and a trackball (see Figure 1), allowing alternating selections and scene rotations without changes of mode. CINCH was evaluated by observing four scientists using the tool over a period of three weeks as part of their normal work activity. Event logs and interviews revealed dramatic improvements in both the speed and quality of scientists' everyday work, and a set of principles that should inform the design of future 3D marking interfaces. More broadly, CINCH demonstrates the value of the iterative, participatory design process that catalyzed its evolution.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Hard lessons: effort-inducing interfaces benefit spatial learning</title>
	<abstract>Interface designers normally strive for a design that minimises the user's effort. However, when the design's objective is to train users to interact with interfaces that are highly dependent on spatial properties (e.g. keypad layout or gesture shapes) we contend that designers should consider explicitly increasing the mental effort of interaction. To test the hypothesis that effort aids spatial memory, we designed a "frost-brushing" interface that forces the user to mentally retrieve spatial information, or to physically brush away the frost to obtain visual guidance. We report results from two experiments using virtual keypad interfaces -- the first concerns spatial location learning of buttons on the keypad, and the second concerns both location and trajectory learning of gesture shape. The results support our hypothesis, showing that the frost-brushing design improved spatial learning. The participants' subjective responses emphasised the connections between effort, engagement, boredom, frustration, and enjoyment, suggesting that effort requires careful parameterisation to maximise its effectiveness.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Eye-based head gestures</title>
	<abstract>A novel method for video-based head gesture recognition using eye information by an eye tracker has been proposed. The method uses a combination of gaze and eye movement to infer head gestures. Compared to other gesture-based methods a major advantage of the method is that the user keeps the gaze on the interaction object while interacting. This method has been implemented on a head-mounted eye tracker for detecting a set of predefined head gestures. The accuracy of the gesture classifier is evaluated and verified for gaze-based interaction in applications intended for both large public displays and small mobile phone screens. The user study shows that the method detects a set of defined gestures reliably.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Security, privacy, and personalization: Informing next-generation interaction concepts for interactive TV systems</title>
	<abstract>The success of the next-generation of interactive TV systems will depend on the type and form of user interaction these systems can support. New services like social TV, direct image, and data up- and download from the set-top box, connectivity of the iTV system to the PC and other mobile media devices, pose the question of how to support security, privacy, and personalization. To investigate the (sometimes naïve) concepts of users of what changes an interactive TV system will bring for their living room behaviors in terms of security and privacy, and how to support these aspects regarding user interaction, we set up two ethnographic studies. Study one investigated users' assumptions and ideas on the concepts of security, privacy, and personalization in households that did currently not use interactive TV. This allowed us to understand possible problems in terms of security and privacy before the introduction of interactive TV. Study two investigated current practices and usages of media and devices, to understand what type of interaction concept would support the practices of the user best. The results show that new forms of user interaction must support personalized access to the content on the TV, that they must support security and privacy, and that they should enable new forms of connectivity for all devices used in the living room.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>3D input for 3D worlds</title>
	<abstract>Virtual Worlds present a 3D space to the user. However, input devices are typically 2D. This unnatural mapping reduces the engagement of the experience. We are exploring using Wii controllers to provide 3D gesture-based input to the 3D virtual world, Second Life. By evaluating its usability, we found that gesture-based interfaces are appealing and natural for hand gestures such as wave but difficult to map to facial expressions.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>LensMouse: augmenting the mouse with an interactive touch display</title>
	<abstract>We introduce LensMouse, a novel device that embeds a touch-screen display -- or tangible 'lens' -- onto a mouse. Users interact with the display of the mouse using direct touch, whilst also performing regular cursor-based mouse interactions. We demonstrate some of the unique capabili-ties of such a device, in particular for interacting with auxil-iary windows, such as toolbars, palettes, pop-ups and dia-log-boxes. By migrating these windows onto LensMouse, challenges such as screen real-estate use and window man-agement can be alleviated. In a controlled experiment, we evaluate the effectiveness of LensMouse in reducing cursor movements for interacting with auxiliary windows. We also consider the concerns involving the view separation that results from introducing such a display-based device. Our results reveal that overall users are more effective with LenseMouse than with auxiliary application windows that are managed either in single or dual-monitor setups. We conclude by presenting other application scenarios that LensMouse could support.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The eLabBench: an interactive tabletop system for the biology laboratory</title>
	<abstract>We present the eLabBench -- a tabletop system supporting experimental research in the biology laboratory. The eLabBench allows biologists to organize their experiments around the notions of activities and resources, and seamlessly roam information between their office computer and the digital laboratory bench. At the bench, biologists can pull digital resources, annotate them, and interact with hybrid (tangible + digital) objects such as racks of test tubes. This paper focuses on the eLabBench's design, and presents three main contributions: First, based on observations we highlight a set of characteristics digital benches should support in a laboratory. Second, we describe the eLabBench, including a simple implementation of activity-based computing for tabletop environments, with support for activity roaming, note-taking, and hybrid objects. Third, we present preliminary feedback of the eLabBench based on a ongoing deployment in a biology laboratory, and propose a design space definition for the design of single-user, work-oriented, tabletop systems.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Text entry from power wheelchairs: edgewrite for joysticks and touchpads</title>
	<abstract>Power wheelchair joysticks have been used to control a mouse cursor on desktop computers, but they offer no integrated text entry solution, confining users to point-and-click or point-and-dwell with on-screen keyboards. But on-screen keyboards reduce useful screen real-estate, exacerbate the need for frequent window management, and impose a second focus of attention. By contrast, we present two integrated gestural text entry methods designed for use from power wheelchairs: one for joysticks and the other for touchpads. Both techniques are adaptations of EdgeWrite, originally a stylus-based unistroke method designed for people with tremor. In a preliminary study of 7 power wheelchair users, we found that touchpad EdgeWrite was faster than joystick WiVik, and joystick EdgeWrite was only slightly slower after minimal practice. These findings reflect "walk up and use"-ability and warrant further investigation into extended use.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>TouchCuts and TouchZoom: enhanced target selection for touch displays using finger proximity sensing</title>
	<abstract>Although touch-screen laptops are increasing in popularity, users still do not comfortably rely on touch in these environments, as current software interfaces were not designed for being used by the finger. In this paper, we first demonstrate the benefits of using touch as a complementary input modality along with the keyboard and mouse or touchpad in a laptop setting. To alleviate the frustration users experience with touch, we then design two techniques, TouchCuts, a single target expansion technique, and ,i>TouchZoom,/i>, a multiple target expansion technique. Both techniques facilitate the selection of small icons, by detecting the finger proximity above the display surface, and expanding the target as the finger approaches. In a controlled evaluation, we show that our techniques improve performance in comparison to both the computer mouse and a baseline touch-based target acquisition technique. We conclude by discussing other application scenarios that our techniques support.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The reification of metaphor as a design tool</title>
	<abstract>Despite causing many debates in human-computer interaction (HCI), the term “metaphor” remains a central element of design practice. This article investigates the history of ideas behind user-interface (UI) metaphor, not only technical developments, but also less familiar perspectives from education, philosophy, and the sociology of science. The historical analysis is complemented by a study of attitudes toward metaphor among HCI researchers 30 years later. Working from these two streams of evidence, we find new insights into the way that theories in HCI are related to interface design, and offer recommendations regarding approaches to future UI design research.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Taxonomy of microinteractions: defining microgestures based on ergonomic and scenario-dependent requirements</title>
	<abstract>This paper explores how microgestures can allow us to execute a secondary task, for example controlling mobile applications, without interrupting the manual primary task, for instance, driving a car. In order to design microgestures iteratively, we interviewed sports- and physiotherapists while asking them to use task related props, such as a steering wheel, a cash card, and a pen for simulating driving a car, an ATM scenario, and a drawing task. The primary objective here is to define microgestures that are easily performable without interrupting or interfering the primary task. Using expert interviews, we developed a taxonomy that classifies these gestures according to their task context. We also assessed the ergonomic and attentional attributes that influence the feasibility and task suitability of microinteractions, and evaluated their level of resources required. Accordingly, we defined 21 microgestures that allow performing microinteractions within a manual, dual task context. Our taxonomy poses a basis for designing microinteraction techniques.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>AccessibilityUtil: a tool for sharing experiences about accessibility of web artifacts</title>
	<abstract>Existing guidelines on how to develop web applications with accessibility are currently not applied in many organizations. The lack of "closeness" of these guidelines to the developers is one of the most significant reasons for this scenario, as developers often do not know them or find it difficult to apply them. As a proposed solution to increase the use of guidelines in development practices, we present a proposal of a collaborative tool for sharing and disseminating experiences. The proposal is based on capturing and disseminating Design Rationale (DR) related to experiences with developing web artifacts, following the Web Content Accessibility Guidelines (WCAG) 2.0.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Human-centered fingertip mandarin input system using single camera</title>
	<abstract>Designing a user friendly Chinese input system is a challenging task due to the logographic nature of Chinese characters. Using fingertips and cameras to replace pens and touch panels as input devices could reduce the cost and improve ease-of-use and comfort of the computer-human interface. In this work, Chinese character entry is achieved using Mandarin Phonetic Symbol (MPS) recognition via on-line fingertip tracking. In the proposed system, particle filters are applied for robust fingertip tracking. Afterwards, MPS recognition is performed on the tracked fingertip trajectories using Hidden Markov Models. In the proposed system, the challenges of entering, leaving, and virtual strokes caused by video-based fingertip input can be overcome. We conduct experiments to validate that the MPS symbols written by fingertips are successfully and efficiently recognized using the proposed framework.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Writing with a joystick: a comparison of date stamp, selection keyboard, and EdgeWrite</title>
	<abstract>A joystick text entry method for game controllers and mobile phones would be valuable, since these devices often have joysticks but no conventional keyboards. But prevalent joystick text entry methods are slow because they are selection-based. EdgeWrite, a new joystick text entry method, is not based on selection but on gestures from a unistroke alphabet. Our experiment shows that this new method is faster, leaves fewer errors, and is more satisfying than date stamp and selection keyboard (two prevalent selection-based methods) for novices after minimal practice. For more practiced users, our results show that EdgeWrite is at least 1.5 times faster than selection keyboard, and 2.4 times faster than date stamp.</abstract>
	<search_task_number>16</search_task_number>
	<query>ergonomics tablet typing</query>
	<relevance>0</relevance>
  </item>


  <item>
    <title>
      Intercultural work assignments in global software development preparation for entry and re-entry
    </title>
    <abstract>
      Global software is needed to support global business activities for companies whose processes, staffs, markets, and customers are increasingly worldwide in scope. This development creates a need for intercultural work assignments. While some development is done by virtual global teams, co-location of IS professionals in other cultures during the deployment of systems is also common. This research study in progress involves semi-structured interviews with IS managers and developers and a mailed survey of IS personnel, and focuses on the adjustment to and from an intercultural work assignment for IS professionals. It also explores research questions dealing with issues involving: types of preparation of IS professionals for intercultural work assignments and repatriation; individual differences between IS professionals, including preparedness for intercultural work, that predict success in such assignments and repatriation; and the impact of cultural novelty on successful transitions to and from overseas work. The contributions anticipated from the study are an understanding of how the nature of preparation for overseas assignments and individual differences impact the success of co-located IS professionals.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Teaching HCI: a challenging intercultural, interdisciplinary, cross-field experience
    </title>
    <abstract>
      The extraordinary development of the Human-Computer Interaction (HCI) is still poorly reflected in South America, with the notable exception of Brazil. Our interest in HCI came from practice, as we have an extensive experience in software development. We are teaching HCI in a Chilean university (Pontificia Universidad Católica de Valparaíso) since 2003. Teaching HCI was a highly challenging intercultural, interdisciplinary, cross-field, but very rewarding experience. It was an intercultural experience, as we were born, raised, educated, with work experience both as professionals and professors in an East-European ex-communist country (Romania), but we taught HCI in a Latin-American country (Chile). Moreover, we did it in English, for Spanish speaker students. It was an interdisciplinary experience as HCI is a highly interdisciplinary science itself. It was a cross-field experience, as it allowed us to build a bridge between theory and practice.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      A model of pedagogical collaboration: eTwinning in Romania
    </title>
    <abstract>
      The importance of information technologies for education is widely recognized, having become in the past decades a central point in the education at all levels - curriculum and teaching practice, teacher training, development of educational institutions, educational policies.Progress is significant, from providing educational institutions with technology to educational software development, support materials development and teacher training programmes.One of the most important steps of European Union action in this field is eTwinning, part of the Lifelong Learning Programme of the European Commission and accompanying measure for the Comenius Branch Programme. eTwinning was launched in 2005 and its aims are to facilitate communication and development among schools in the pre-university system of all UE states, new educational product development by international teams, improvement of both teachers and students in their ability to use new technologies, communication improvement using foreign languages and intercultural awareness. Today, approximately 70,000 schools from 32 states are part of the system and they are involved in over 5,000 ongoing projects.eTwinning started in Romania in October 2007 and on the 15th of April 2009, 5484 teachers from 3549 schools were enrolled, having 926 ongoing projects.The purpose of the article is to present the main objectives and actions taken by the Romanian Science Institute of Education, as a National Support Services for the eTwinning in collaboration with the Center for Innovation in Education.At the same time we have mentioned the most important campaigns/projects developed within the programme in the preuniversitary institutions in Romania and the outcome trying to emphasize the benefits of co-working in such projects using eTwinning platform tools.Some of the approximately 850 projects carried out so far enjoyed international recognition, schools in Romania being among the finalists/winners or receiving either annual eTwinning prizes or European Quality Certifications from the Central Support Services of eTwinning in Brussels.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Comparing privacy attitudes of knowledge workers in the U.S. and India
    </title>
    <abstract>
      We compared privacy attitudes of knowledge workers from the U.S. and India who were involved in a collaborative software development project distributed across five sites of a multinational corporation. Prior studies on consumer privacy suggest that privacy concerns in India are lower than those in the U.S. While our work largely confirmed these findings, we found unexpectedly that knowledge workers in India expressed higher interpersonal privacy concerns compared with their U.S. colleagues. Our study points to a number of explanatory factors for the elevated privacy concerns in the Indian knowledge workplace: nature of interpersonal relationships, associations with privacy, competition among team members, management style and hierarchy, and differences in the physical characteristics of the workplace. Our findings highlight the challenges in satisfying privacy needs when individuals and teams collaborate with knowledge workers in India. An understanding of these issues is important for building and deploying systems for intercultural collaboration that can accommodate differences in privacy concerns.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Collaborating across cultural and technological boundaries: team culture and information use in a map navigation task
    </title>
    <abstract>
      The increased globalization of the workplace and the availability of collaboration technologies are making CMC a necessary aspect of teamwork [27]. Culturally diverse teams are becoming the norm in knowledge-intensive projects that involve making sense of incomplete, ambiguous, and complex information (e.g., software development, new product design, customer service). The ability of teams to perform such tasks effectively is often a function of the media they use to collaborate and the culturally conditioned expectations of team members. We conducted a laboratory study to examine how different collaboration media and cultural backgrounds influence the sense-making process of culturally mixed and homogenous dyads. American, Chinese, and intercultural American-Chinese pairs of participants collaborated on two map navigation tasks using one of three technologies: video, audio, or IM. As predicted, culture and media interacted to affect the content and pattern of participants' communication.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Where did we turn wrong?: unpacking the effect of culture and technology on attributions of team performance
    </title>
    <abstract>
      Computer-mediated collaboration is becoming an increasingly prevalent form of work ([22]). At the same time, organizations are relying more and more on culturally diverse teams to staff knowledge-intensive projects (e.g., software development, customer service, corporate training,). We conducted a laboratory study examining the role of collaborative technologies and culture on 2-person team members' attributions of causes for their collaborative performance. Pairs of American, Chinese, and intercultural American-Chinese students collaborated on two map navigation tasks using one of three technologies: video, audio, or IM. As predicted, culture and technology interacted to affect the extent to which members attributed performance to dispositional factors (e.g., personality or mood) vs. situational factors (e.g., the technology or task difficulty). We discuss the implications of our results forcross-cultural collaborative work
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Antecedents of ISD Offshoring Outcomes: Exploring Differences between India and China
    </title>
    <abstract>
      Prior research in IS offshoring has highlighted issues in software development projects arising from differences in culture, from status differences and resource inequalities, as well as from asset-related characteristics. Based on Practice Theory and Transaction Cost Economics, we integrate these three perspectives into a single research model explaining the relation between the outcome of offshore ISD projects and cultural, social, and asset-related characteristics of the projects. We substantiate our model with a multiple-case study in two settings in which German companies have offshored ISD projects to India and China. Thereby, we also address a severe drawback of contemporary intercultural IS offshoring research: the neglect of China as the most rapidly growing IS offshoring location at present.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Design and development of a pictogram communication system for children around the world
    </title>
    <abstract>
      Pangaea develops an intercultural collaboration environment using ICT (Information and Communication Technology) called the Universal Playground where children around the world can foster personal bonds regardless of their location, language, and cultural background, through a playful activity called "Pangaea Activity." Pangaea is a unique organization in that it has on-going global fields for local children and has developed its own ICT system. This case paper reports how the Communicator, the pictogram communication software, was designed and developed. Development of the software and ICT system comes together through the Pangaea Activity menu, facilitation knowhow, and field operation flow in order to bring the best performance toward its mission. As human-resources, funding, and time are limited, internal qualitative evaluations were conducted actively and quantitative evaluations were done in cooperation with external research groups.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Software engineering research agendas panel (SERA) (panel session): “what can't we do, but need to learn how to do?”
    </title>
    <abstract>
      The software challenges of the new millennium include more mature users expecting functioning software, more critical technical and business applications requiring dependable software, globalization requiring distributed development teams, and paradigm clashes between new and old economy firms. Software engineering has to be evaluated anew in terms of “what cant we do today, and what do we have to learn how do in order to meet those challenges”. This panel discusses and proposes urgent research topics as well as research programs to address those topics. Although, such discussions are going on in all different countries (e.g., PITAC in the US, similar activities in Germany, this international panel will add new perspectives thought intercultural cross-fertilization. The panel will consist of three parts: Position statements and brief discussion among panelists, questions/answers from the audience, and summary.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      The Language Grid: Service-Oriented Collective Intelligence for Language Resource Interoperability, 1st edition
    </title>
    <abstract>
      There is increasing interaction among communities with multiple languages, thus we need services that can effectively support multilingual communication. The Language Grid is an initiative to build an infrastructure that allows end users to create composite language services for intercultural collaboration. The aim is to support communities to create customized multilingual environments by using language services to overcome local language barriers. The stakeholders of the Language Grid are the language resource providers, the language service users, and the language grid operators who coordinate the former.This book includes 18 chapters in six parts that summarize various research results and associated development activities on the Language Grid. The chapters in Part I describe the framework of the Language Grid, i.e., service-oriented collective intelligence, used to bridge providers, users and operators. Two kinds of software are introduced, the service grid server software and the Language Grid Toolbox, and code for both is available via open source licenses. Part II describes technologies for service workflows that compose atomic language services. Part III reports on research work and activities relating to sharing and using language services. Part IV describes various applications of language services as applicable to intercultural collaboration. Part V contains reports on applying the Language Grid for translation activities, including localization of industrial documents and Wikipedia articles. Finally, Part VI illustrates how the Language Grid can be connected to other service grids, such as DFKI's Heart of Gold and smart classroom services in Tsinghua University in Beijing.The book will be valuable for researchers in artificial intelligence, natural language processing, services computing and human--computer interaction, particularly those who are interested in bridging technologies and user communities.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Collaboration and Intercultural Issues on Requirements: Communication, Understanding and Softskills (CIRCUS)
    </title>
    <abstract>
      Software and system development nowadays is increasingly a globally-distributed undertaking. Development teams are often distributed over multiple sites and countries. Geographic separation, different time zones and cultural differences have brought new challenges to Requirements Engineering, which require improved skills in communication, collaboration and dealing with intercultural issues. This paper presents the idea of the CIRCUS workshop held in conjunction with RE'09 and summarizes organizational prerequisites and proven success factors for Global Requirements Engineering.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Dark Fiber (Electronic Culture: History, Theory, and Practice Series): Tracking Critical Internet Culture
    </title>
    <abstract>
      From the Publisher:According to media critic Geert Lovink, the Internet is being closed off by corporations and governments intent on creating a business and information environment free of dissent. Calling himself a radical media pragmatist, Lovink envisions an Internet culture that goes beyond the engineering culture that spawned it to bring the humanities, user groups, social movements, non-governmental organizations (NGOs), artists, and cultural critics into the core of Internet development.In Dark Fiber, Lovink combines aesthetic and ethical concerns and issues of navigation and usability without ever losing sight of the cultural and economic agendas of those who control hardware, software, content, design, and delivery. He examines the unwarranted faith of the cyber-libertarians in the ability of market forces to create a decentralized, accessible communication system. He studies the inner dynamics of hackers' groups, Internet activists, and artists, seeking to understand the social laws of online life. Finally, he calls for the injection of political and economic competence into the community of freedom-loving cyber-citizens, to wrest the Internet from corporate and state control.The topics include the erosion of email, bandwidth for all, the rise and fall of doctom mania, techno-mysticism, sustainable social networks, the fight for a public Internet time standard, and collaborative text filtering. Stressing the importance of intercultural collaboration, Lovink includes reports from Albania, where NGOs and artists use new media to combat the country's poverty and isolation; from Taiwan, where the September 1999 earthquake highlighted the cultural politics of the Internet; and from Delhi, where a new media center explores free software, public access, and Hindi interfaces.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Subjectivity and information ethics
    </title>
    <abstract>
      In “A Brief History of Information Ethics,” Thomas Froehlich (2004) quickly surveyed under several broad categories some of the many issues that constitute information ethics: under the category of librarianship—censorship, privacy, access, balance in collections, copyright, fair use, and codes of ethics; under information science, which Froehlich sees as closely related to librarianship—confidentiality, bias, and quality of information; under computer ethics—intellectual property, privacy, fair representation, nonmaleficence, computer crime, software reliability, artificial intelligence, and e-commerce; under cyberethics (issues related to the Internet, or “cyberspace”)—expert systems, artificial intelligence (again), and robotics; under media ethics—news, impartiality, journalistic ethics, deceit, lies, sexuality, censorship (again), and violence in the press; and under intercultural information ethics—digital divide, and the ethical role of the Internet for social, political, cultural, and economic development. Many of the debates in information ethics, on these and other issues, have to do with specific kinds of relationships between subjects. The most important subject and a familiar figure in information ethics is the ethical subject engaged in moral deliberation, whether appearing as the bearer of moral rights and obligations to other subjects, or as an agent whose actions are judged, whether by others or by oneself, according to the standards of various moral codes and ethical principles. Many debates in information ethics revolve around conflicts between those acting according to principles of unfettered access to information and those finding some information offensive or harmful. Subjectivity is at the heart of information ethics. But how is subjectivity understood? Can it be understood in ways that broaden ethical reflection to include problems that remain invisible when subjectivity is taken for granted and when how it is created remains unquestioned? This article proposes some answers by investigating the meaning and role of subjectivity in information ethics. (In an article on cyberethics (2000), I asserted that there was no information ethics in any special sense beyond the application of general ethical principles to information services. Here, I take a more expansive view.)This article originated as a presentation given on September 16, 2005, to the Nordic Research School in Library and Information Science Workshop “Structures of Power: Information, Knowledge, and Property,” held at the Department of Archival Science, Library and Information Science, Museology (ALM), Uppsala University, Sweden, September 15–17, 2005. It has been extensively revised. © 2008 Wiley Periodicals, Inc.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Multi-site distributed software development: issues, solutions, and challenges
    </title>
    <abstract>
      Tackling the disadvantages associated with remote communication is a challenge in multi-site software development. The challenge includes all these factors: awareness of the work that is being done according to the project plan, the work that is being done to co-operate between teams, the issues that have been currently raised, the issues that have been clarified, and a means of discussing issues and arriving at a solution within a multi-site distributed environment. If everyone working on a certain project is located in the same area, then situational awareness is relatively straightforward, but the overheads in communication can be great when team members need to meet in order to discuss the problems, to raise issues, to make decisions and to find answers in a multi-site distributed environment. In this paper, we present issues, solutions, and challenges in Multi-site Software Development. The solution main idea is to have knowledge and agreement explicitly interpreted by software tools rather than just being implicitly interpreted by human developers. The representation of software engineering concepts, software development tasks, software development models, software development processes, software development issues and development solutions, as well as software development documentation in the digital form, provides intuitive, clear, precise concepts and ideas, knowledge. Sharing knowledge facilitates a common understanding of the knowledge. This enables effective ways of reaching a consensus of understanding which is of benefit to remote team members in a distributed environment. The common knowledge is semantically shared not only among remote team members but also among software systems.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Global software development: building a research community
    </title>
    <abstract>
      While the field of global software development (GSD) remains in its relative infancy, a group of GSD researchers are on the brink of building a stronger research community that will be able to collectively address many of the current challenges in the field. This paper details emerging issues in the GSD community while discussing contributions made at the latest GSD workshop held at ICSE 2004. These issues include: the need for community building and increased collaboration between researchers, the importance of more systematic application and documentation of research techniques, and the opportunity to build defined models and theories, and, in doing so, define the state of the practice. Brief summaries of workshop papers are incorporated, along with discussions of the topics addressed during the workshop. These topics include: Feasibility of GSD, Strategies for Success of GSD, and Research Methods and Challenges in GSD.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Exploring the communication behaviour among global software development learners
    </title>
    <abstract>
      This study examines communication behaviours in global software learning teams. Using a coding scheme developed in previous research (Curtis and Lawson, 2001), the paper characterises communication behaviours of students engaged in a software development project. The paper reports the results of two pilot projects done with students in USA, England, Turkey and Panama. Through content analysis we identify distinct patterns of interactions and examine how these patterns are associated with task, culture, or performance. Our results suggest that communication patterns among global software learners may be related to task type, culture and levels of performance.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Online collaboration: Collaborative behavior patterns and factors affecting globally distributed team performance
    </title>
    <abstract>
      Studying the collaborative behavior of online learning teams and how this behavior is related to communication mode and task type is a complex process. Research about small group learning suggests that a higher percentage of social interactions occur in synchronous rather than asynchronous mode, and that students spend more time in task-oriented interaction in asynchronous discussions than in synchronous mode. This study analyzed the collaborative interaction patterns of global software development learning teams composed of students from Turkey, US, and Panama. Data collected from students' chat histories and forum discussions from three global software development projects were collected and compared. Both qualitative and quantitative analysis methods were used to determine the differences between a group's communication patterns in asynchronous versus synchronous communication mode. K-means clustering with the Ward method was used to investigate the patterns of behaviors in distributed teams. The results show that communication patterns are related to communication mode, the nature of the task, and the experience level of the leader. The paper also includes recommendations for building effective online collaborative teams and describes future research possibilities.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Trust dynamics in global software engineering
    </title>
    <abstract>
      Trust is one of the key factors that determines success or failure of any software project. However, achieving and maintaining trust in distributed software projects, when team members are geographically, temporally and culturally distant from each other, is a remarkable challenge. This paper explores the dynamics of trust and best practices performed in software organizations to address trust-related issues in global software engineering. Semi-structured interviews were conducted in six different distributed software development organizations and a resulting trust dynamics model is presented. Based on the findings, the paper also provides suggestions for the industry to achieve trust in distributed collaborations.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Cultural patterns in software process mishaps: incidents in global projects
    </title>
    <abstract>
      This paper describes a current and ongoing research project being conducted at the University of British Columbia, Canada. The paper begins by briefly describing past anthropological and sociological culture research. This research will inform our current exploration into the issues surrounding culture and its role in Global Software Development efforts. It then clarifies why this research is particularly important. The paper continues with a description of the current phase of this research, which is an exploratory qualitative approach rooted in Grounded Theory, and of the next phase, which will be a more quantitative approach looking at specific "problem areas" that were identified during the first phase.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Global software development for the practitioner
    </title>
    <abstract>
      This International Workshop on Global Software Development for the Practitioner (GSD2006) was held in conjunction with the 28thInternational Conference on Software Engineering (ICSE 2006) on May 23rd, 2006 in Shanghai, China. The workshop was motivated by the industry trend towards developing software in globally distributed settings: geographically distributed teams, or outsourcing parts of the software development to other organizations in other parts of the world. Topics presented and discussed in the workshop focused on grounded, practical strategies and techniques that address the geographic, temporal, organizational, and cultural boundaries inherent in global software projects.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Introduction to the 1ST international workshop on global software development for the practitioner
    </title>
    <abstract>
      This International Workshop on Global Software Development for the Practitioner (GSD2006) is held in conjunction with the 28TH International Conference on Software Engineering (ICSE 2006) on May 23RD , 2006 in Shanghai, China. The workshop is motivated by the industry trend toward developing software in globally distributed settings: geographically distributed teams or outsourcing part of software development to other organizations in other parts of the world. Topics presented and discussed in the workshop focus on grounded, practical strategies and techniques that address the geographical, temporal, organizational and cultural boundaries inherent in global software projects.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Digital city shanghai: concepts, foundations, and current state
    </title>
    <abstract>
      The fastest developing city in China is Shanghai; its annual growth rate (GDP) exceeded 11.9% average in every year from 1992 and GDP per person in 2003 reached 4909 USD. Economists, industries and the government have been considering how to keep Shanghai developing in a sustainable manner. In 1994, the Shanghai Municipal Government put forward the concept of ”information harbor” and set the goal of establishing a series of information infrastructures in the following five years. The Shanghai city informatization project started in 1999, and by the end of 2001 it had spread across Shanghai. At the same time, some applications based on Internet technologies were designed and developed. In the beginning of 2002, based on the current information infrastructures and systems, Shanghai Municipal Government announced the ”Digital City Shanghai” strategy. This chapter will introduce the background of the Digital City Shanghai project, the current state of the information infrastructure in Shanghai, and the concept of system design &amp; user interface. The Digital City Shanghai project will be described in four parts: Spatial Data Infrastructure, City Informatization, Shanghai City Grid, and Shanghai Logistics Information Platform. We will also introduce some research projects. In addition to the strategy, we describe and discuss social perspective and problems.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Internal and contextual factors, knowledge processes and performance: From the Chinese provider's perspective
    </title>
    <abstract>
      This paper explores the influences of two internal factors, i.e. supplier team's IT-based skills and communication with client's team, and two contextual factors, i.e. supplier team's understanding of client's culture and collaboration with client's team, on knowledge processes and performance in global sourcing of IT services from the Chinese provider's perspective. Knowledge processes are characterized by knowledge sharing, knowledge-based coordination and expertise management, and performance is measured by product success and personal satisfaction. Data have been collected in 13 companies in Xi'an Software Park, with 26 in-depth, semi-structured interviews held with top and middle managers, and 200 structured questionnaires distributed to knowledge workers who are involved in global sourcing projects. The results indicate that supplier team's IT-based skills, communication with client's team, cultural understanding of client's culture and collaboration with client's team are positively associated with knowledge process and performance. Also, knowledge sharing, knowledge-based coordination and expertise management are found to be crucial for those influential factors to function positively and contribute to the performance. The findings of this study suggest that the effects of key factors on knowledge processes and performance in global sourcing of IT services appear to transcend the social and cultural differences; however, contextual factors seem to have more significant influences on knowledge processes and performance in global sourcing of IT services.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Intercultural collaboration support system using disaster safety map and machine translation
    </title>
    <abstract>
      Natural Disaster Youth Summit (NDYS) is an intercultural collaboration project promoted by a NPO called JEARN (Japan Education And Resource Network). They are working on collaborative learning about disaster prevention by exchanging disaster safety maps created by students over the world and by discussing over BBS's and video conference systems on the Internet. Language barrier is the most difficult issue in this activity, and in order to communicate freely in their own native language, linguistic support, such as using machine translation systems, is necessary. We develop CoSMOS, an intercultural collaboration support system using disaster safety map and machine translation, and discuss collaboration supports using CoSMOS from four viewpoints; handling high-definition digital images, linking disaster safety maps to the world map, supporting collaborative learning, and supporting multiple languages.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      The role of annotation in intercultural communication
    </title>
    <abstract>
      In intercultural communication, there are large barriers when the languages and the cultures are different. It is undoubtedly preferable for people to have smooth communications using their mother language. Therefore, we have developed a chat system called AnnoChat. AnnoChat has an annotation function for smooth intercultural communications. We applied AnnoChat in experiments with Japanese, Chinese, and Korean speakers. The results of the experiments showed that about 70% of the added annotations were reusable as intercultural knowledge information. About 20% of the added annotations were used to supplement information that could not be described while chatting. It is thought to be an effective example of applying annotation in intercultural communications.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Collaborative platform for multilingual resource development and intercultural communication
    </title>
    <abstract>
      In the present borderless information society, we need a lot of fundamental linguistic tools as well as the standard reference resources to facilitate our daily communications across the languages and cultures for better understanding or smoothing the communications. Online collaborative works are efficiently conducted among expert groups via many existing services such as Sourceforge, Wiki or Weblog. However, in the process of multilingual resource development and intercultural communication we still need to fulfill the requirements in well-structured design of the database, and communication tools that provide necessary linkages between records of intention to particular assertions, and functions to realize selectional preference in case that there are more than one assertion. In this paper, we propose a new platform, called Knowledge Unifying Initiator (KUI). We conducted a study on multilingual medical text collaborative translation and the initiative in Asian WordNet development to evaluate our proposed platform.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Atoms of bonding: communication components bridging children worldwide
    </title>
    <abstract>
      Connecting children around the world using ICT with the mind of respecting various cultures and language, NPO Pangaea is challenging to create "universal playground" where children can feel a bond regardless of their physical locations, languages spoken, or economic circumstances. We develop Package consisting contents, or activities, facilitator training program, and net environment utilizing pictogram designed by adults and children. Two years of conducting activities, over 100 occasions, to create bonds among children, four major communication components became apparent. 1) Shared tasks, 2) Shared personal information, 3) Enjoyable face to face meeting, 4) Attractive communication method. Lacking any one of these four components, it is difficult for participants, aged 9-17, to actively seek the opportunities to bond. Pangaea activities now taking places in Tokyo, Kyoto, Seoul, Vienna, and Kenya, what works for intercultural communication activities will be presented focusing on four components.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Multilingual communication support using the language grid
    </title>
    <abstract>
      Our proposed "Language Grid" infrastructure supports multilingual communication by combining in new way language resources, such as machine translators, morphological analyzers, and dictionaries specific to user communities. We developed the Language Grid as a language infrastructure on the Internet. The Language Grid enables user communities to combine two or more machine translators and their community dictionaries by workflows, and to easily create new multilingual services specific to the communities. Because the quality of language services is not often defined, however, we need to confirm that the created multilingual service is really useful. We need to extend the process of general usability testing to the multilingual environment. For example, cooperation between user communities and language grid providers can significantly improve the accuracy of machine translation: it turns out that machine translations can be useful for interactive communication in the field of inter-cultural collaboration.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Evaluation and usability of back translation for intercultural communication
    </title>
    <abstract>
      When users communicate with each other via machine translation, it is important to improve the quality of the translations. The "Back Translation" technique can improve the translation accuracy. A back translation, first, translates the input language into the target language (outward), and then translates the target language into the input language (homeward). This allows the users to confirm the accuracy of the machine translation by themselves. If the user finds that his input sentence is unsuitable for machine translator, he can rewrite the input sentence. For effective multilingual communication, it is important that the back translation offer good accuracy and good usability. This paper focuses on these two points; we evaluated the accuracy of back translation, and developed a user interface that improves the usability of back translation. The outward and homeward translations show a correlation. Back translation can improve the accuracy of outward translation for users.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Effects of machine translation on collaborative work
    </title>
    <abstract>
      Even though multilingual communities that use machine translation to overcome language barriers are increasing, we still lack a complete understanding of how machine translation affects communication. In this study, eight pairs from three different language communities--China, Korea, and Japan--worked on referential tasks in their shared second language (English) and in their native languages using a machine translation embedded chat system. Drawing upon prior research, we predicted differences in conversational efficiency and content, and in the shortening of referring expressions over trials. Quantitative results combined with interview data show that lexical entrainment was disrupted in machine translation-mediated communication because echoing is disrupted by asymmetries in machine translations. In addition, the process of shortening referring expressions is also disrupted because the translations do not translate the same terms consistently throughout the conversation. To support natural referring behavior in machine translation-mediated communication, we need to resolve asymmetries and inconsistencies caused by machine translations.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Agent and grid technologies for intercultural collaboration
    </title>
    <abstract>
      After September 11, we remember there was a clear conflict in public opinions within western countries. While 77% of those interviewed in France opposed to military intervention against Iraq (2003.1.9 Le Figaro), 63% of the U.S. population were proud of the U.S. role in the war (2003.3.22 CBS News). Conflicts in governmental policies are common, but conflicts in public opinion between western countries have not been observed before. Though we all share information on the Web, similar conflicts arose recently in East Asia. While about 90 percent of Chinese polled blamed Japan, more than half of Japanese polled said it was hard to tell who bore responsibility (2005.8.24 Genron NPO and Peking University). According to Global Reach, the ratio of English speaking people online has decreased to 35.2% in 2004. To increase mutual understanding between different cultures and of opinions in different languages, it is essential to build a language infrastructure on top of the Internet.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Collaborative translation by monolinguals with machine translators
    </title>
    <abstract>
      In this paper, we present the concept for collaborative translation, where two non-bilingual people who use different languages collaborate to perform the task of translation using machine translation (MT) services, whose quality is imperfect in many cases. The key idea of this model is that one person, who handles the source language (source lan-guage side) and another person, who handles the target language (target language side), play different roles: the target language side modifies the translated sentence to improve its fluency, and the source language side evaluates its adequacy. We demonstrated the effectiveness and the practicality of this model in a tangible way.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Effects of undertaking translation repair using back translation
    </title>
    <abstract>
      Translation repair plays an important role in intercultural communication that involves machine translation. It can be used to create messages that have very few translation mistakes. The accuracy of the results of translation repair when an original sentence is rewritten has not yet been evaluated. The improvement brought about by translation repair has to be demonstrated in order to apply translation repair to multilingual communication. Therefore, we have evaluated the translation repair of Japanese-English, Japanese-Chinese, and Japanese-Korean translations using back translation. We have used test sentences with a character count ranging from 15 to 32. On the basis of these evaluation experiments, we have estimated the accuracy and the cost of translation repair. (1) After nearly six rounds of translation repair work in three languages, the average translation accuracy of the sentences used in the experiment was improved (the meaning of the translated sentences was almost the same as that of the original sentences). In the experiment, 65% of the sentences were improved to the level of highly accurate. Moreover, 99% of the sentences were improved to the level of moderately accurate. 2) The cost of repairing a sentence depended on the number of translation-difficult words or phrases that were contained in the sentence. When the quality of a translation was low, finding the word or phrase to be modified was a difficult task. Thus, the cost corresponded to the quality of the translation.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Design of multilingual participatory gaming simulations with a communication support agent
    </title>
    <abstract>
      People communicating through machine translators cannot tell what the purpose of their communication is or what other people are thinking because of the poor quality of translation services. If they are able to share their understanding within a "common ground" like a communicative or behavioral protocol, they can overcome their difficulties in communication, and we can improve information systems to help them improve mutual understanding. We designed a multilingual participatory gaming simulation, and conducted multilingual gaming experiments with Japanese and Korean participants. We extracted the protocol for conversation with mistranslations from the game logs and designed an agent to support conversation. Then, Japanese and Chinese played it and we observed and analyzed the behaviors of agents and the interaction between players and agents. Consequently, we obtained two main sets of results: (1) an agent function that notified players of the time that had elapsed since the conversation had broken down effectively speeded up their negotiations and achieved more active communications. (2) Tagging by participants was difficult and ineffective in leading to specific protocols and conversations when mistranslations occurred.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Adoption of translation support technologies in a multilingual work environment
    </title>
    <abstract>
      We study the adoption of translation support technologies by professors at a multilingual university, using the framework of the Technology Adoption Model (TAM). TAM states that a user's perceived usefulness and ease of use for the technology ultimately determines her actual use of it. Through a survey and a set of interviews with our subjects, we find that there is evidence for TAM in the context of translation support tools. However, we also find that user adoption of these tools is a bit more complicated. Users who are able to successfully employ these tools have not only developed strategies to overcome their inaccuracies (e.g. by post-editing machine translated text), they also often compensate for the weaknesses of a given technology by combining the use of multiple tools.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Multilingual knowledge management
    </title>
    <abstract>
      Although there has been substantial research in knowledge management, there has been limited work in the area of multilingual knowledge management. The purpose of this chapter is to review and summarize some of the existing and supporting literature surrounding the emerging field of multilingual knowledge management. It does that by reviewing recent applications from multiple fields and the presentation of multilingual information. The chapter uses a theory about knowledge management and also examines supporting literature in translation, collaboration, ontologies and search.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      User-Centered Design Stories: Real-World UCD Case Studies
    </title>
    <abstract>
      Intended for both the student and the practitioner, this is the first user-centered design casebook. It follows the Harvard Case study method, where the reader is placed in the role of the decision-maker in a real-life professional situation. In this book, the reader is asked to perform analysis of dozens of UCD work situations and propose solutions for the problem set.The problems posed in the cases cover a wide variety of key tasks and issues facing practitioners today, including those that are related to organizational/managerial topics, UCD methods and processes, and technical/ project issues. The benefit of the casebook and its organization is that it offers the new practitioner (as well as experienced practitioners working in new settings) the valuable practice in decision-making that one cannot get by reading a book or attending a seminar.*The first User-Centered Design Casebook, with cases covering the key tasks and issues facing UCD practitioners today.*Each chapter based on real world cases with complex problems, giving readers as close to a real-world experience as possible.* Offers "the things you don't learn in school," such as innovative and hybrid solutions that were actually used on the problems discussed.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Software Development Cultures and Cooperation Problems: A Field Study of the Early Stages of Development of Software for a Scientific Community
    </title>
    <abstract>
      In earlier work, I identified a particular class of end-user developers, who include scientists and whom I term `professional end-user developers', as being of especial interest. Here, I extend this work by articulating a culture of professional end-user development, and illustrating by means of a field-study how the influence of this culture causes cooperation problems in an inter-disciplinary team developing a software system for a scientific community. My analysis of the field study data is informed by some recent literature on multi-national work cultures. Whilst acknowledging that viewing a scientific development through a lens of software development culture does not give a full picture, I argue that it nonetheless provides deep insights.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Mobile Technology for Children: Designing for Interaction and Learning
    </title>
    <abstract>
      With the goal of improving the design of mobile technology for children, On the Move brings together contributions from HCI leaders in research, and industry, and technology and education based policy experts to analyze and evaluate and present solutions.  To show readers how they can apply each design problem and case study to their HCI professional or academic work, each chapter will contain best practice advice.In HCI, social implications, in addition to interface design, usability, and performance, are all part of an informed design solution. Chauncey Wilson, Senior User Researcher, Autodesk, Inc., and forthcoming MK author, states, “The design of mobile devices is not an algorithmic process, it is must be considered in a social context that examines culture, changing trends, and other factors. This proposal provides a solid foundation of the social and cultural factors that are critical in the design of mobile products for children.”There are many technological solutions to consider, many contexts to explore user scenarios and many goals for supporting learning. It contains the work of 43 authors from 9 countries, each deeply invested in improving and analyzing design of childrens mobile products.  These authors have diverse points of view, but it is a subject that deserves debate.  The trends, design and use of these products has been both lauded and criticized, and the debate is far from over.  The need has never been greater for an evaluation of the design and the affects of the design mobile technology as it pertains to children's products and learning – the good and the bad – especially for and by the people who conduct research, develop and design the products.*First book for HCI practitioners and researchers to present a multitude of voices on the design, technology, and impact of mobile devices for children from global perspective*Features contributions from leading HCI academics, professionals, and childrens technology policy leaders from nine countries*Each contribution and case study is followed by a best practice overview to help readers improve future research and design and for a quick reference at a later date
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Design guidelines for software processes knowledge repository development
    </title>
    <abstract>
      Context: Staff turnover in organizations is an important issue that should be taken into account mainly for two reasons: 1.Employees carry an organization's knowledge in their heads and take it with them wherever they go 2.Knowledge accessibility is limited to the amount of knowledge employees want to share  Objective: The aim of this work is to provide a set of guidelines to develop knowledge-based Process Asset Libraries (PAL) to store software engineering best practices, implemented as a wiki. Method: Fieldwork was carried out in a 2-year training course in agile development. This was validated in two phases (with and without PAL), which were subdivided into two stages: Training and Project. Results: The study demonstrates that, on the one hand, the learning process can be facilitated using PAL to transfer software process knowledge, and on the other hand, products were developed by junior software engineers with a greater degree of independence. Conclusion: PAL, as a knowledge repository, helps software engineers to learn about development processes and improves the use of agile processes.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Global software development and collaboration: barriers and solutions
    </title>
    <abstract>
      While organisations recognise the advantages offered by global software development, many socio-technical barriers affect successful collaboration in this inter-cultural environment. In this paper, we present a review of the global software development literature where we highlight collaboration problems experienced by a cross-section of organisations in twenty-six studies. We also look at the literature to answer how organisations are overcoming these barriers in practice. We build on our previous study on global software development where we define collaboration as four practices related to agreeing, allocating, and planning goals, objectives, and tasks among distributed teams.We found that the key barriers to collaboration are geographic, temporal, cultural, and linguistic distance; the primary solutions to overcoming these barriers include site visits, synchronous communication technology, and knowledge sharing infrastructure to capture implicit knowledge and make it explicit.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      A review of non-technical issues in global software development
    </title>
    <abstract>
      Owing to globalisation and Information and Communication Technologies (ICT) proliferation, Global Software Development (GSD) is increasingly omnipresent. Many organisations have turned to it in the quest for higher quality software delivered on time economically. GSD is facing a variety of challenges, including non-technical challenges such as cross-cultural management, communication, collaboration, coordination, distance and time, team dynamics, trust, etc. In this paper these significant non-technical issues are reviewed. It is found that non-technical areas such as team dynamics and cross-cultural risk management have received scant attention and need further studies. Implications drawn from the review will provide knowledge to facilitate further empirical studies in these areas.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Knowledge-oriented software engineering process in a multi-cultural context
    </title>
    <abstract>
      In software engineering, leading trends can be detected that will affect the characteristic features of a product and its development process. On a product level, the growth of size and complexity is apparent--but on the one hand only. On the other hand, there is also a growing demand for simple and reasonable small software products executed by handheld terminals and smartphones; these applications are in many cases expected to collaborate with databases over the Internet. In addition, different kinds of service concepts (ASP, SaaS) are becoming recognized alternatives to the traditional way of buying software. Increasingly, software products are also distributed in a wide geographical scope to users with different cultural backgrounds and expectations. In software engineering work, as a consequence of this growth in size and complexity, the development work is more and more often distributed. The software business itself is becoming global because of acquisitions, offshoring, and international subcontracting. The globalization of work sets new requirements to the engineering processes: in international teams the organisational and cultural differences of the development subteams have to be recognized. In this paper, the focus is on the software development and its global dimension--especially the roles of multi-cultural and cross-organizational issues in software engineering. Our paper presents the results of the first phase of our three phases research project related to "Culture-Aware Software Engineering." The main result of the first phase is the multi-cultural software engineering working model introduced in our paper. Culture is seen as one example of the context, i.e. the situation at hand. The concept of culture has also different meanings, which have to be understood in well-organized software engineering. Software engineering work is analyzed as a knowledge creation process, in which both explicit and tacit knowledge are recognized and the transformation between these establishes baselines along the development life cycle.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Cross-cultural user-experience design
    </title>
    <abstract>
      User interfaces for desktop, Web, mobile, and vehicle platforms extend across culturally diverse user communities, sometimes within a single country or language group, and certainly across the globe. If user interfaces are to be usable, useful, and appealing to such a wide range of users, user-interface/user-experience developers must account for cultural aspects in globalizing/localizing products and services. In this course, participants will learn practical principles and techniques that are immediately useful for both analysis and design tasks. Where time permits, they will have an opportunity to put their understanding into practice through a series of group exercises. Some handout materials are available in Mandarin.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Risk-driven Management Contingency Policies in Collaborative Software Development
    </title>
    <abstract>
      Inter-organisational Collaborative Software Development (CSD), like other inter-organisational collaborative efforts that span national, language and cultural boundaries, raises significant challenges and risks. Its study offers useful insight with regard to the more complex, collaborative, and interdependent operational environment, where new risks arise and interfere with software projects even when all traditional risks are well controlled. In this paper, we outline a collaborative risk management framework, and focus on a set of governing policies and management contingency processes required to support the framework and to manage the interactions with other critical project activities and goals.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      A Collaboration Model for Global Multicultural Software Development
    </title>
    <abstract>
      Software development projects seem to raise many challenges and issues. These issues are exacerbated when the projects are distributed globally and thus software development projects are multicultural. Globalization has increased the need to estimate the effectiveness and cost savings of every project. That is why many projects are outsourced or in other ways distributed to cheaper countries. The main problems of software development projects are related to knowledge sharing, communication, and cultural issues. This paper studies the challenges in global multicultural software development projects and the kinds of collaborative tools available for software development. This paper presents an examination of a collaboration model for global multicultural software development. The model is based on the authors' own work experience and literature research. We propose that this model could be used as a reference model when planning global software projects.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      A role-based qualification and certification program for software architects: an experience report from Siemens
    </title>
    <abstract>
      In this experience report, we describe the motivation, experience, lessons learned, and future directions of a software engineering curriculum used at a large international company. The "Curriculum for Software Engineers" project, which developed the content and a role-based qualification and certification program, was started at Siemens in 2006. This paper includes an overview of various kinds of certification in the software engineering area and why we chose the knowledge- and experience-based type of certification. The experience report part focuses mainly on the "certified senior software architect" role, as this role has the longest history and participants from many different business units and countries.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Methodological reflections on a field study of a globally distributed software project
    </title>
    <abstract>
      Context: We describe the methodology of a field study of a globally distributed software development project in a multinational corporation. The project spanned four sites in the US and one in India, and is a representative example of the complexities and intricacies of global corporate software development. Objective: Our goal is to provide the rationale behind the methodological choices and derive insights to inform the methodology of future studies of global software engineering teams. The paper also aims to provide an illustrative case of a typical geographically distributed corporate software project, through an in-depth description that emerged by applying the methods. Method: We reflect upon the reasons for choosing each of our methods, viz., non-participant observation, site visits, interviews, and an online questionnaire. We then discuss what we learned from the experience of applying the methods. Results: During and after the study, the discussions surrounding our methodological choices yielded important insights. The dynamics of software engineering practice and the geographical distribution of the project impacted factors such as access, costs, and cultural and linguistic diversity, and influenced the choice of methods. Our experience makes a case for methodological breadth and plurality as a means to a broad understanding of a global project. This understanding could then be linked to the specific research questions under consideration. Conclusion: The in-depth contextual description of the project that emerged from our methods highlights the utility of our methodological approach and provides an illustration of the complex nature of these projects. Our systematic reflection also yielded several methodological insights and provides important implications for future empirical studies of global corporate software development. Our experience can serve as a useful resource in methodological choices for research on globally distributed software engineering teams, or collaborative knowledge work in general.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Obstacles to usability evaluation in practice: a survey of software development organizations
    </title>
    <abstract>
      This paper reports from a combined questionnaire survey and interview study of obstacles for deploying usability evaluation in software development organizations. It was conducted in a limited geographical area. The purpose of the questionnaire survey was to determine whether software development organizations in that area were evaluating the usability of their software and to identify key obstacles. It revealed that 29 of 39 software development organizations conducted some form of usability evaluation. The purpose of the interview study was to gain more insight into the obstacles that were expressed. It involved 10 of the 39 software development organizations. Our results show, that the understanding of usability evaluation is a major obstacle. Furthermore, the two most significant obstacles were resource demands and the mindset of developers. These obstacles were not only an obstacle for more organizations to deploy usability evaluation, but also a concern for the software development organizations, that had deployed usability evaluations in their development process.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      The brave new world of design requirements
    </title>
    <abstract>
      Despite its success over the last 30 years, the field of Requirements Engineering (RE) is still experiencing fundamental problems that indicate a need for a change of focus to better ground its research on issues underpinning current practices. We posit that these practices have changed significantly in recent years. To this end we explore changes in software system operational environments, targets, and the process of RE. Our explorations include a field study, as well as two workshops that brought together experts from academia and industry. We recognize that these changes influence the nature of central RE research questions. We identify four new principles that underlie contemporary requirements processes, namely: (1) intertwining of requirements with implementation and organizational contexts, (2) dynamic evolution of requirements, (3) emergence of architectures as a critical stabilizing force, and (4) need to recognize unprecedented levels of design complexity. We recommend a re-focus of RE research based on a review and analysis of these four principles, and identify several theoretical and practical implications that flow from this analysis.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural software development
    </query>
    <relevance>
      0
    </relevance>
  </item>


  <item>
    <title>An introduction to 3D spatial interaction with video game motion controllers</title>
	<abstract>3D spatial interfaces [Bowman et al. 2004] give users the ability to spatially interact with 3D virtual worlds because they provide natural mappings from human movement to interface controls. These interfaces, common in virtual and augmented reality applications, give users, rich, immersive, and interactive experiences that can mimic the real world or provide magical, larger than life interaction metaphors [Katzourin et al. 2006].</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Systems, interactions, and macrotheory</title>
	<abstract>A significant proportion of early HCI research was guided by one very clear vision: that the existing theory base in psychology and cognitive science could be developed to yield engineering tools for use in the interdisciplinary context of HCI design. While interface technologies and heuristic methods for behavioral evaluation have rapidly advanced in both capability and breadth of application, progress toward deeper theory has been modest, and some now believe it to be unnecessary. A case is presented for developing new forms of theory, based around generic “systems of interactors.” An overlapping, layered structure of macro- and microtheories could then serve an explanatory role, and could also bind together contributions from the different disciplines. Novel routes to formalizing and applying such theories provide a host of interesting and tractable problems for future basic research in HCI.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Improving graphical information system model use with elision and connecting lines</title>
	<abstract>Graphical information system (IS) models are used to specify and design IS from several perspectives. Due to the growing size and complexity of modern information systems, critical design information is often distributed via multiple diagrams. This slows search performance and results in reading errors that later cause omissions and inconsistencies in the final designs. We study the impact of large screens and the two promising visual integration techniques of elision and connecting lines that can decrease the designers' cognitive efforts to read diagrams. We conduct a laboratory experiment using 84 computer science students to investigate the impact of these techniques on the accuracy of the subjects' search and recall with entity-relationship diagrams and data flow diagrams. The search tasks involve both vertical and horizontal searches on a moderately complex IS model that consists of multiple diagrams. We also examine the subjects' spatial visualization abilities as a possible covariant for observed search performance. These visual integration techniques significantly reduced errors in both the search and the recall of diagrams, especially with respect to individuals with low spatial visualization ability.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>The interior spatial environment: dynamic 0g environments and human places</title>
	<abstract>In outer space, human experiences interject with technology in ways not experienced on Earth. We will examine the human aspects of living both on Earth and in the interior 0g environment, how technology interacts with astronauts, and how the design of the interior environments in outer space considers the interaction of technology and humanity from an astronaut's perspective. These issues are explored with Marc Garneau, the first Canadian astronaut in outer space, and Travis Baldwin, 0g environment designer, who has worked with astronauts in the USA. Both discuss the complexity of living in space and how the ways that we live on Earth in terms of interior spatial environment characteristics might be interesting to consider. Within interior environments on Earth, function is always affected and mediated by social and dynamic human relationships and people's experience and perceptions. When these experiences are transferred into the 0g environment, small, confined, and highly technological environments augment and intensify human relations. And yet these are rarely considered in the quest for secure, functional environments in space transport or on stations. These issues are discussed from both theoretical and practical perspectives in conversation with Garneau and Baldwin. In the interview with Marc Garneau, the author validates certain theoretical assumptions about the complex and dynamic interactions that occur in space, understanding the ways that astronauts cope in space and become creative in figuring out how to "mold" the environment to become their own, and what design factors are important to consider. Finally, examples of potential design ideas for integrating and humanizing the interior 0g space environment are explored and ideas about how to integrate human social elements are proposed.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>The changing face of technical communication: new directions for the field in a new millennium</title>
	<abstract>In this panel session, the authors identify four different factors shaping the future of technical communication: user-centered design, corporate universities, cross-disciplinary collaboration, and knowledge management. The authors each address how factors once considered external to the field of technical communication are now becoming thoroughly integrated with it. These four studies, in conjunction, suggest how the field of technical communication is becoming increasingly complex and how participants (practitioners, researchers, and educators) will need to adapt to this new terrain.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
   <item>
    <title>Accessible Multimodal Media Center Application for Blind and Partially Sighted People</title>
	<abstract>We present a multimodal media center interface designed for blind and partially sighted people. It features a zooming focus-plus-context graphical user interface coupled with speech output and haptic feedback. A multimodal combination of gestures, key input, and speech input is utilized to interact with the interface. The interface has been developed and evaluated in close cooperation with representatives from the target user groups. We discuss the results from longitudinal evaluations that took place in participants’ homes, and compare the results to other pilot and laboratory studies carried out previously with physically disabled and nondisabled users.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Designing human-computer interfaces for quadriplegic people</title>
	<abstract>The need for participation in an emerging Information Society has led to several research efforts for designing accessibility solutions for disabled people. In this paper we present a method for developing Human-Computer Interfaces (HCIs) for quadriplegic people in modern programming environments. The presented method accommodates the design of scanning interfaces with modern programming tools, leading to flexible interfaces with improved appearance and it is based on the use of specially designed software objects called "wifsids" (Widgets For Single-switch Input Devices). The wifsid structure is demonstrated and 4 types of wifsids are analyzed. Developed software applications are to be operated by single-switch activations that are captured through the wifsids, with the employment of several modes of the scanning technique. We also demonstrate the "Autonomia" software application, that has been developed according to the specific methodology. The basic snapshots of this application are analyzed, in order to demonstrate how the wifsids cooperate with the scanning process in a user-friendly environment that enables a quadriplegic person to access an ordinary computer system.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Game development documentation and institutional collection development policy</title>
	<abstract>Videogames and other new media artifacts constitute an important part of our cultural and economic landscape and collecting institutions have a responsibility to collect and preserve these materials for future access. Unfortunately, these kinds of materials present unique challenges for collecting institutions including problems of collection development, technological preservation, and access. This paper presents findings from a grant-funded project focused on examining documentation of the creative process in game development. Data includes twelve qualitative interviews conducted with individuals involved in the game development process, spanning a number of different roles and institution types. The most pressing findings are related to the nature of documentation in the videogame industry: project interviews indicate that the game development process does produce significant and important documentation as traditionally conceived by collecting institutions, ranging from game design documents to email correspondence and business reports. However, while it does exist, traditional documentation does not adequately, or even, at times, truthfully represent the project or the game creation process as a whole. In order to adequately represent the development process, collecting institutions also need to seek out and procure numerous versions of games and game assets as well as those game assets that are natural byproducts of the design process like gamma and beta versions of the game, for example, vertical slices, or different renderings of graphical elements.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>A survey of research on context-aware homes</title>
	<abstract>The seamless integration of people, devices and computation will soon become part of our daily life. Sensors, actuators, wireless networks and ubiquitous devices powered by intelligent computation will blend into future environments in which people will live. Despite showing great promise, research into future computing technologies is often far removed from the needs of users. The nature of such future systems is often too obtrusive, seemingly denying their purpose. Furthermore, most research on context-aware environments and ubiquitous computing conducted so far has concentrated on supporting people while at work. This paper presents research issues that need to be addressed to enhance the quality of life for people living in context-aware homes. We survey current research and present strategies that facilitate the diffusion of information technology into homes in order to inspire positive emotions, encourage effortless exploration of content and help occupants to achieve tasks at hand.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Easy doesn't do it: skill and expression in tangible aesthetics</title>
	<abstract>In this paper, we articulate the role of movement within a perceptual-motor view of tangible interaction. We argue that the history of human---product interaction design has exhibited an increasing neglect of the intrinsic importance of movement. On one hand, human---product interaction design has shown little appreciation in practice of the centrality of our bodily engagement in the world. This has resulted in technologies that continue to place demands on our cognitive abilities, and deny us the opportunity of building bodily skill. On the other hand, the potential for movement in products to be a meaningful component of our interaction with them has also been ignored. Both of these directions (design for bodily engagement and the expressiveness of product movements) are sketched out, paying particular respect for their potential to impact both interaction aesthetics and usability. We illustrate a number of these ideas with examples.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
   <item>
    <title>MAUI: a multimodal affective user interface</title>
	<abstract>Human intelligence is being increasingly redefined to include the all-encompassing effect of emotions upon what used to be considered 'pure reason'. With the recent progress of research in computer vision, speech/prosody recognition, and bio-feedback, real-time recognition of affect will enhance human-computer interaction considerably, as well as assist further progress in the development of new emotion theories.In this article, we describe how affect, moods and emotions closely interact with cognition and how affect and emotion are the quintessential multimodal processes in humans. We then propose an adaptive system architecture designed to sense the user's emotional and affective states via three multimodal subsystems (V, K, A): namely (1) the Visual (from facial images and videos), (2) Kinesthetic (from autonomic nervous system (ANS) signals), and (3) Auditory (from speech). The results of the system sensing are then integrated into the multimodal perceived multimodal anthropomorphic interface agent then adapts its interface by responding most appropriately to the current emotional states of its user, and provides intelligent multi-modal feedback to the user.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Cognitive science implications for enhancing training effectiveness in a serious gaming context</title>
	<abstract>Serious games use entertainment principles, creativity, and technology to meet government or corporate training objectives, but these principles alone will not guarantee that the intended learning will occur. To be effective, serious games must incorporate sound cognitive, learning, and pedagogical principles into their design and structure. In this paper, we review cognitive principles that can be applied to improve the training effectiveness in serious games and we describe a process we used to design improvements for an existing game-based training application in the domain of cyber security education.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Dynamic positioning systems: usability and interaction styles</title>
	<abstract>This paper describes the first steps of a research project directed towards human computer interaction (HCI) within the maritime environment and on maritime equipment. The focus is at this stage mainly on interaction with Dynamic Positioning Systems (DP) and how new interaction styles can be introduced to make the interaction more efficient and less faulty in both standard operations and in safety-critical situations. The initial experiment looks into how a DP operator can operate a DP system by using bi-manual interaction/multi-touch combined with hand-gestures to create a new type of user-experience. The aim for this research is to investigate which gestures feel natural to the DP operator and how/if they can be implemented into a real-life DP system.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>'Teen-scape': designing participations for the design excluded</title>
	<abstract>Aside from designing artefacts, designers can also design participations with people. This paper is a reflection of an eighteen-month design experiment by a design researcher with different designers aiming to develop a relationship with staff members and students of a secondary school. From this responsive experiment, four identified types or steps of design participations were identified: 1) innovation led by designers; 2) collaboration between designers and the 'users'; 3) emancipation focuses on how users invite professional designers to share design thinking and finally, 4) motivation is about projects initiated by 'users' to invite designers to co-designing. The result of this period of engagement is 'Teen-scape', a new school playground, designed by a design graduate, through exposure to methods for design inclusion. The introduction of creative thinking from design studies into the secondary school environment, through this design engaging process, demonstrates how design can instigate a transformation of lives, whilst highlighting the importance of people participation and the role of design facilitators who instigate and inform the participation. The main aim is to urge a new design discipline, entitled the Design Participations, which is a design study area extending creative thinking to design processes that engage people in design.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Memory fragments of the industrial landscape</title>
	<abstract>Motivation -- To enhance the awareness of places of a city industrial heritage by exposing in situ its identities and stories through the personal memories of its inhabitants.

Research approach -- Analysis followed by an explorative survey based on interviews in which museum visitors are asked to discuss their impressions about an exhibition in which personal memories are reminisced.

Findings/Design -- Personal narrative seems to trigger criticism, curiosity, and engage visitors in actively discussing and critically questioning views on places and their past. Based on these findings we propose concepts for possible designs.

Research limitations/Implications -- The survey was based on a limited number of participants and entails a single narrative.

Originality/Value -- This work contributes to research and debate on interpretation and representation of material and cultural heritage, with particular focus on industrial landscapes and modern vestiges left over by the industry of the 19th century.

Take away message -- To tell what we remember, and to keep on telling it, is to keep the past alive in the present (Gruchow, 1995).</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
   <item>
    <title>An actor-network approach to games and virtual environments</title>
	<abstract>In this paper we apply some of the insights of Bruno Latour and actor network theory to suggest that games and virtual spaces can be interpreted as aesthetic forms which are established and stabilised by a 'collective' of humans and technologies. The 'agents' that comprise any collective or network -- whether it be a simple human-tool relation or a far more complex assemblage of actors in massively multiplayer games - are equally human and non human, social and material, corporeal and technical. Yet the collective impact of these factors is not often given serious attention in the discourses of ludology and game studies, which we argue can be attributed to a number of historical and technical reasons. The application of actor-network theory to games and virtual environments aims to facilitate a nuanced understanding that exceeds more conventional user-- and viewer-centred interpretations in game studies, and is therefore more organic to the open-ended and constantly changing nature of our engagement with online games and virtual environments.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Auditory icon and earcon mobile service notifications: intuitiveness, learnability, memorability and preference</title>
	<abstract>With an ever increasing number of mobile services, meaningful audio notifications could effectively inform users of the incoming services while minimising undesired and intrusive interruptions. Therefore, careful design of mobile service notification is needed. In this paper we evaluate two types of audio (auditory icons and earcons) as mobile service notifications, by comparing them on 4 measures: intuitiveness, learnability, memorability and user preference. A 4-stage longitudinal evaluation involving two lab experiments, a field study and a web-based experiment indicated that auditory icons performed significantly better in all measures. Implications for mobile audio notification design are presented.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Hybrid documents ease text corpus analysis for literary scholars</title>
	<abstract>We present a study that explores how literary scholars interact with physical and digital documents in their daily work. Motivated by findings from this study, we propose refactoring the working environment of our target audience to improve the integration of digital material into established paper-centric processes. This is largely facilitated through the use of hybrid documents, i.e., cross-modal compound documents that employ a printed book for rich, tangible interaction in tandem with a digital component for matching interactive augmentation on a digital workbench. The results from two user studies in which we evaluated increasingly detailed prototypes demonstrate that this design offers better support for central workflows in literary studies than currently prevalent approaches.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Investigating teamwork and taskwork in single- and multi-display groupware systems</title>
	<abstract>Multi-display groupware (MDG) systems, which typically comprise both public and personal displays, promise to enhance collaboration, yet little is understood about how they differ in use from single-display groupware (SDG) systems. While research has established the technical feasibility of MDG systems, evaluations have not addressed the question of how users' behave in such environments, how their interface design can impact group behavior, or what advantages they offer for collaboration. This paper presents a user study that investigates the impact of display configuration and software interface design on taskwork and teamwork. Groups of three completed a collaborative optimization task in single- and multi-display environments, under different task interface constraints. Our results suggest that MDG configurations offer advantages for performing individual task duties, whereas SDG conditions offer advantages for coordinating access to shared resources. The results also reveal the importance of ergonomic design considerations when designing co-located groupware systems.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Participatory ergonomics using VR integrated with analysis tools</title>
	<abstract>This paper presents our work on the integrated use of simulation tools in real time for participatory occupational ergonomic studies. The focus of this paper is a synergistic system that consists of an interactive immersive simulation tool that has been developed in-house and integrated with a commercial human modeling simulation system, Jack™. The impetus of the real-time integration is to allow the complementary use of two powerful simulation tools by allowing the user to perform the task naturally in an immersive environment, while the body posture information is continuously and automatically passed to the human modeling system for a continuous (and not discrete) analysis of the participatory ergonomic issues under consideration. This facilitates integration of ergonomic issues early in the design and planning phases of workplace layouts, even where the physical facility does not exist. The proposed integration is demonstrated using a manufacturing example.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
   <item>
    <title>Enhanced auditory menu cues improve dual task performance and are preferred with in-vehicle technologies</title>
	<abstract>Auditory display research for driving has mainly focused on collision warning signals, and recent studies on auditory in-vehicle information presentation have examined only a limited range of tasks (e.g., cell phone operation tasks or verbal tasks such as reading digit strings). The present study used a dual task paradigm to evaluate a plausible scenario in which users navigated a song list. We applied enhanced auditory menu navigation cues, including spearcons (i.e., compressed speech) and a spindex (i.e., a speech index that used brief audio cues to communicate the user's position in a long menu list). Twenty-four undergraduates navigated through an alphabetized song list of 150 song titles---rendered as an auditory menu---while they concurrently played a simple, perceptual-motor, ball-catching game. The menu was presented with text-to-speech (TTS) alone, TTS plus one of three types of enhanced auditory cues, or no sound at all. Both performance of the primary task (success rate of the game) and the secondary task (menu search time) were better with the auditory menus than with no sound. Subjective workload scores (NASA TLX) and user preferences favored the enhanced auditory cue types. Results are discussed in terms of multiple resources theory and practical IVT design applications.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>PETA: a pedagogical embodied teaching agent</title>
	<abstract>We describe a hybrid real and virtual system that monitors and teaches children in an everyday classroom environment without requiring any special virtual reality set ups or any knowledge that there is a computer involved. This system is truly pervasive in that it interacts with a child who is playing with normal physical toys using speech. A simulated virtual head provides a focus and the opportunity for microphonological language teaching, whilst a simulated world allows the teacher to demonstrate using her set of blocks -- much as a teacher might demonstrate at the front of the class. However this system allows individual students, or pairs of students, to interact with the task in a computer-free way and receive feedback from PETA, the Teaching Head, as if from their own private tutor.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Enabling flexible manufacturing systems by using level of automation as design parameter</title>
	<abstract>Handling flexibility in an ever changing manufacturing environment is one of the key challenges for a successful industry. By using tools for virtual manufacturing, industries can analyze and predict outcomes of changes before taking action to change the real manufacturing systems. This paper describes a simulation tool that can be used to study the effect of level of automation issues on the design of manufacturing systems, including their effect on the overall system performance, ergonomics, environment, and economic measures. Determining a suitable level of automation can provide a manufacturing system with the flexibility needed to respond to the unpredictable events that occur in factory systems such as machine failures, lack of quality, lack of materials, lack of resources, etc. In addition, this tool is designed to use emerging simulation standards, allowing it to provide a neutral interface for both upstream and downstream data sources.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Lo-Fi matchmaking: a study of social pairing for backpackers</title>
	<abstract>It is technically feasible for mobile social software such as pairing or ‘matchmaking' systems to introduce people to others and assist information exchange. However, little is known about the social structure of many mobile communities or why they would want such pairing systems. While engaged in other work determining requirements for a mobile travel assistant we saw a potentially useful application for a pairing system to facilitate the exchange of travel information between backpackers. To explore this area, we designed two studies involving usage of a low-fidelity role prototype of a social pairing system for backpackers. Backpackers rated the utility of different pairing types, and provided feedback on the social implications of being paired based on travel histories. Practical usage of the social network pairing activity and the implications of broader societal usage are discussed.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Controlling interruptions: awareness displays and social motivation for coordination
</title>
	<abstract>Spontaneous communication is common in the workplace but can be disruptive. Such communication usually benefits the initiator more than the target of the disruption. Previous research has indicated that awareness displays showing the workload of a target can reduce the harm interruptions inflict, but can increase the cognitive load on interrupters. This paper describes an experiment testing whether team membership influences interrupters' motivation to use awareness displays and whether the informational-intensity of a display influences its utility and cost. Results indicate interrupters use awareness displays to time communication only when they and their partners are rewarded as a team and that this timing improves the target's performance on a continuous attention task. Eye-tracking data shows that monitoring an information-rich display imposes a substantial attentional cost on the interrupters, and that an abstract display provides similar benefit with less distraction.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
   <item>
    <title>The role of spatial contextual factors in mobile personalization at large sports events</title>
	<abstract>This paper presents three field studies undertaken at large sports events in UK and China, with the aim of improving the user experience at these types of events through the design of personally relevant mobile services. These field studies investigated: which aspects of spatial context were relevant within the confines of a large sporting event, how their relevance differed according to sports event and language/culture, and how they could be used to prescribe the behaviour of a personalizable/adaptive mobile device. Spatial aspects of context were found to be highly significant within the large sports arena. They can be used to maximize the relevance of information and communication services delivered to a spectator over a mobile device. A range of design implications are discussed.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Back to the Holodeck: new life for virtual reality?</title>
	<abstract>This paper examines the gap between optimistic 1990s predictions of "immersive" Virtual Reality and the fact that these systems are not in widespread use today (2007). In particular, Janet Murray's 1997 adoption of the science fiction device "The Holodeck" as a model for compelling interactive narrative is re-examined in light of a decade of digital technology development. Comparison is made between the rise of games as "desktop VR", and the development of other media forms. Alternative definitions of Virtual Reality are discussed as a means of revealing possible technical and societal reasons for the lack of commonplace "immersive" systems. This paper then presents the global changes that might trigger the development of practical 1990s style "immersive" VR for commonplace games and other entertainment systems. The conclusion then examines the technical and cultural feasibility of various possible directions for VR systems design, and calls for a new realization by designers of the limitations they are currently working with.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>A virtual human architecture that integrates kinematic, physical and behavioral aspects to control H-Anim characters</title>
	<abstract>Virtual humans are being increasingly used in different domains. Virtual human modeling requires to consider aspects belonging to different levels of abstractions. For example, at lower levels, one has to consider aspects concerning the geometric definition of the virtual human model and appearance while, at higher levels, one should be able to define how the virtual human behaves into an environment. H-Anim, the standard for representing humanoids in X3D/VRML worlds, is mainly concerned with low-level modeling aspects. As a result, the developer has to face the problem of defining the virtual human behavior and translating it into lower levels (e.g. geometrical and kinematic aspects). In this paper, we propose VHA (Virtual Human Architecture), a software architecture that allows one to easily manage an interactive H-Anim virtual human into X3D/VRML worlds. The proposed solution allows the developer to focus mainly on high-level aspects of the modeling process, such as the definition of the virtual human behavior.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>MokE: a tool for Mobile-ok evaluation of web content</title>
	<abstract>The ever-growing corpus of web content is now accessible from mobile devices. But is web content ready for mobile access? Which characteristics provide an acceptable user experience when using a mobile device? Based on W3C's MobileOK best practices and tests we present MokE (Mobile OK Evaluator), a tool that helps creators of web content, webmasters and site administrators to evaluate how a mobile user will experience browsing of their content. By making use of W3C's basic tests this tool is comprised of a set of modules that crawl a web site, analyze its content (including a hidden web capability) and provide an evaluation of its appropriateness for mobile access.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Mobile service audio notifications: intuitive semantics and noises</title>
	<abstract>It is hoped that context-aware systems will present users with an increasing number of relevant services in an increasingly wide range of contexts. With this expansion, numerous service notifications could overwhelm users. Therefore, careful design of the notification mechanism is needed. In this paper, we investigate how semantic richness of different types of audio stimuli can be utilised to shape the intuitiveness of mobile service notifications. In order to do so, we first develop a categorisation of mobile services so that clustered services can share the same notifications. Not surprisingly, it was found that overall speech performed better than non-speech sounds, and auditory icons performed overall better than earcons. However, exceptions were observed when richer semantics were utilised in the seemingly poorer medium. We argue that success and subjective preference of auditory mobile service notifications heavily depends on the success and level of directness of the metaphors used.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
   <item>
    <title>Pick-by-Vision comes on age: evaluation of an augmented reality supported picking system in a real storage environment</title>
	<abstract>Order picking is one of the most important process steps in logistics. Because of their flexibility human beings cannot be replaced by machines. But if workers in order picking systems are equipped with a head-mounted display, Augmented Reality can improve the information visualization.

In this paper the development of such a system -- called Pick-by-Vision - is presented. The system is evaluated in a user study performed in a real storage environment. Important logistics figures as well as subjective figures were measured. The results show that a Pick-by-Vision system can improve considerably industrial order picking processes.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>A sufism-inspired model for embodied interaction design</title>
	<abstract>This paper presents a concise overview of argumentations about the body and the concept of embodiment in the field of technology by also referencing to the similar debates in philosophy, critical theory and social studies. In view of the observation that advances in technology call for a re-definition of embodiment, and in many different fields of life embodied approaches are increasingly looked for, it briefly revisits Modernity's ideal of disembodiment, which for those of us who live in modern societies, still dominates our approach to life and our understanding of existence, and as a corollary, our daily interactions with technologies as well. Then drawing from Sufism, the mystical tradition of Islam, and from a phenomenological interpretation of its approach to human body, the authors finally propose a four-layer understanding of embodiment as a model to be implemented in embodied interaction design.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Multimedia content analysis for consumer electronics</title>
	<abstract>The last few years have seen an explosion not only in the amount of content produced but also in the amount of content available. However, while it is great that users can watch virtually anything they want, it becomes increasingly difficult to find content that is actually relevant to the user. This "curse" of content availability is very important for consumer electronic devices since they are as valuable as the value they add to a user's life: if the only thing they do is to increase exponentially the users' choices of what to watch, listen to or consume in any way, then they add, paradoxically, more complication to their lives. Content analysis can be used to help the user select what to watch and is thus essential in modern multimedia devices. However, there are technical and human restrictions to the technology developed for such devices.

In this paper we discuss these restrictions and how they affect the work on content analysis done in an industrial research environment. We will present three case studies that show how these restrictions influence the choices made from the moment content analysis technology is designed, until the moment it is tested with users.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>An evaluation of sunlight-viewable displays</title>
	<abstract>There is a need for displays that can be read even in adverse environmental conditions such as under high intensity, directed sunlight. This is particularly important in self-service terminals that may be placed in an exterior location. These terminals are intended to be used by the general public.

This paper reports the results of a study comprising of four evaluations of sunlight viewable displays. These evaluations consisted of 16 and 20 participants, with two additional expert reviews. A comparison is made between two different types of display technology, namely, transflective and high-bright backlit displays, and results are presented of this comparison. Secondary comparisons of different approaches for managing glare and reflections are also presented and discussed. Transflective displays were found to be at least as good as high-bright alternatives and are therefore promoted as a viable technology for daylight-viewable displays to be placed in an exterior environment.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>The role of physicality in tangible and embodied interactions</title>
	<abstract>The boundaries between "the digital" and our everyday physical world are dissolving as we develop more physical ways of interacting with computing. This forum presents some of the topics discussed in the colorful multidisciplinary field of tangible and embodied interaction. Eva Hornecker, Editor</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
   <item>
    <title>Tracking real-time user experience (TRUE): a comprehensive instrumentation solution for complex systems</title>
	<abstract>Automatic recording of user behavior within a system (instrumentation) to develop and test theories has a rich history in psychology and system design. Often, researchers analyze instrumented behavior in isolation from other data. The problem with collecting instrumented behaviors without attitudinal, demographic, and contextual data is that researchers have no way to answer the 'why' behind the 'what'. We have combined the collection and analysis of behavioral instrumentation with other HCI methods to develop a system for Tracking Real-Time User Experience (TRUE). Using two case studies as examples, we demonstrate how we have evolved instrumentation methodology and analysis to extensively improve the design of video games. It is our hope that TRUE is adopted and adapted by the broader HCI community, becoming a useful tool for gaining deep insights into user behavior and improvement of design for other complex systems.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Measuring user experience in the living room: results from an ethnographically oriented field study indicating major evaluation factors</title>
	<abstract>This paper is focusing on the question how to best evaluate the User Experience (UX) of interaction technologies in the context of the living room. In this setting, other factors may be of importance and influence the user's experience of the interaction than for example in a work environment, like e.g. haptic properties of the remote control. In order to better understand these factors and their impact on UX, and thus being able to improve evaluation methods for this context in the future, a large scale ethnographic study has been conducted, focusing on the identification of factors that influence the UX of interaction technologies in a living room setting from a user's perspective. This paper presents the UX-related and contextual factors (e.g. aesthetic experience, emotional response) that were identified during this study and discusses these results and their implications for UX evaluation in the living room in detail.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Designing kinetic interactions for organic user interfaces</title>
	<abstract>Considering the future of kinetic design in user interfaces.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Pen-based computing</title>
	<abstract>Pens may seem old-fashioned, but some researchers think they are the future of interaction. Can they teach this old dog some new tricks?</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Speciation in the computing sciences: digital forensics as an emerging academic discipline</title>
	<abstract>In evolutionary terms, speciation occurs when members of a species, pressured by the environment, diverge to the point where they are recognizable as separate species. This paper explores the concept as applied to Digital Forensics as a distinct academic discipline from other computing sciences. What are the pressures that have resulted in the emergence of Digital Forensics, how might the domain be defined and, what interdisciplinary connections does Digital Forensics have?This paper examines the domain content of Digital Forensics and its potential in the development of viable undergraduate and graduate degree programs to satisfy professional and scholarly demand.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
   <item>
    <title>IQ-Station: a low cost portable immersive environment</title>
	<abstract>The emergence of inexpensive 3D-TVs, affordable input and rendering hardware and open-source software has created a yeasty atmosphere for the development of low-cost immersive systems. A low cost system (here dubbed an IQ-station), fashioned from commercial off-theshelf technology (COTS), coupled with targeted immersive applications can be a viable laboratory instrument for enhancing scientific workflow for exploration and analysis. The use of an IQ-station in a laboratory setting also has the potential of quickening the adoption of a more sophisticated immersive environment as a critical enabler in modern scientific and engineering workflows. Prior work in immersive environments generally required special purpose display systems, such as a head mounted display (HMD) or a large projector-based implementation, which have limitations in terms of cost, usability, or space requirements. The alternative platform presented here effectively addresses those limitations. This work brings together the needed hardware and software components to create a fully integrated immersive display and interface system that can be readily deployed in laboratories and common workspaces. By doing so, it is now feasible for immersive technologies to be included in researchers' day-to-day workflows. The IQ-station sets the stage for much wider adoption of immersive interfaces outside the small communities of virtual reality centers. In spite of this technical progress, the long-term success of these systems depends on resolving several important issues related to users and support. Key among these issues are: to what degree should hardware and software be customized; what applications and content are available; and how can a community be developed?</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Ontology models for interaction design: case study of online support</title>
	<abstract>We report a case study for online self-support, which illustrates an advanced form of work modeling based on ontology technology. This new method enables a much earlier understanding of the design problem and promotes interdisciplinary design collaboration. A functional prototype was implemented for user testing and showed significant improvement in content discovery.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Assertions to better specify the amazon bug</title>
	<abstract>Modern Web applications are mainly distributed systems that exploit the Internet as communication means and the Web as neutral interface to access services and data. The addition of services to Web applications poses problems that are usually tackled at the technology level, but that should be addressed during design to deliver quality Web applications. A typical example of these problems is the Amazon bug, an annoying problem that the user could encounter if after adding products to his shopping cart, he rolls back to a page with a previous version of the cart and tries to buy it. This would make the user buy the last version of the cart's contents, which in some subtle cases could be different from what expected.In this paper, we do not want to discuss all design aspects, but only how provided services/operations should jointly be designed with the rest of the system. We propose a new reference model for Web applications: Operations require a more complex model where they are not simply appended to information and navigation elements, but they can cooperate with them. Besides the reference model, the paper proposes the use of assertions to constraint the behavior of designed operations. Assertions do not only predicate on how data should be modified, but must also take into account how presentation and navigation could be affected by the execution of the operation.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Vrije Universiteit</title>
	<abstract>Multimedia and Culture is a new master's program in computer science that takes four years of full-time study. About half of the curriculum consists of computer science classes, and the other half is made up of highly multidisciplinary content including humanities (psychology, anthropology, and ethnography), human-computer interaction (theory, task analysis, and practical design projects), artistic design and crafts, and modern culture (relation between literature and visual arts, and music).</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Manufacturing modeling methods: an approach and interface for building generic manufacturing kanban-systems models</title>
	<abstract>Simulation of manufacturing systems, historically the first major application area of discrete-event process simulation, is becoming a steadily more proactive and important strategy for achieving manufacturing efficiency. Concurrently, lean manufacturing has become a nearly essential corporate strategy to compete successfully in an increasingly austere and global business environment. Furthermore, industrial engineers responsible for supporting successfully competitive manufacturing operations have less and less time available for manipulating details deep within a simulation model in order to evaluate numerous complex alternatives. Convergence among these trends motivated the development of a generic manufacturing kanban-systems simulator that has Kanban inventory optimization capability, and an accompanying interface, described in this paper.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
   <item>
    <title>Challenges for Norwegian PC-users with Parkinson's disease - a survey</title>
	<abstract>This paper presents the results from a Norwegian survey on the computer use of people with Parkinson's disease (PD), conducted through the PIKT project in the summer of 2008. The study shows that nearly 80 % of the user population has significant and severe PD related challenges using a computer. Frequent problem areas include inertia, muscle stiffness, using a computer mouse, tremor and issues related to keyboard and ergonomics. Assistance in optimizing computer use is severely lacking, non-systematic and coincidental, and the employment rate among the respondents is about half of the national average. This paper describes the main findings from this study. It highlights main challenges for the user group in question, confirms national findings on gender, computer illiteracy and usage aspects, paints a picture of the current situation for PC-users with Parkinson's disease in Norway and discusses the implications of the findings.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Tangible interfaces in virtual environments for industrial design</title>
	<abstract>In the fields of industrial design and car manufacturing the creation of 3D curves plays a fundamental role within the design process: it allows the improvement of the visual appeal of artifacts, it enhances ergonomics and the product's commercial competitiveness through product differentiation. When flexibility and intuition are to be privileged it is fundamental to achieve natural, intuitive, mathematically correct, creation and modification of surfaces.The scientific aim of this research is the development of an innovative metaphor to modeling of 3D curves which maintains the natural expertise of the designer. The major contribution of this paper is the capability of the system to create and modify the curve naturally, without mathematical artifices, within the limits set by the use of Bezier curves.The proposed metaphor combines the benefits of two acknowledged techniques referred to as the Digital Tape Drawing and the Eraser Pen, which allow the real-time modification of the curve. The integrated adoption of tangible interfaces and innovative mathematical tools, combined with the adoption of semi-immersive environment and lightweight interaction devices, delivers intuitive curve creation for free-form modeling within the virtual scene. The paper describes the details of the algorithm developed and it highlights its strengths during the styling phase.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>A pervasive wiki application based on VoiceXML</title>
	<abstract>In this paper, we describe the design and implementation of an audio wiki application accessible via the Public Switched Telephone Network (PSTN) and the Internet for educational purposes. The application exploits mature World Wide Web Consortium standards such as VoiceXML, Speech Synthesis Markup Language (SSML) and Speech Recognition Grammar Specification (SRGS). The purpose of such an application is to assist visually impaired, technologically uneducated, and underprivileged people in accessing information originally intended to be accessed visually via a Personal Computer. Users may access wiki content via wired or mobile phones, or via a Personal Computer using a Web Browser or a Voice over IP service. This feature promotes pervasiveness to educational material to an extremely large population, i.e. those who simply own a telephone line.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Controlling creatures with Linux</title>
	<abstract>Is that movie character animatronic orcomputer-generated? Find out how the same Linux-based system can let oneperson control either one.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Ethics and usability testing in computer science education</title>
	<abstract>Ethics and usability testing are increasingly important parts of a modern computer science education, given the changing profile of computer science employment and the increased focus on privacy. This paper introduces the concept of a participant pool, a means to recruit participants for research, which can be a valuable tool in teaching usability testing and ethics. It has a role in an overall emphasis on software quality and the importance of research. It can also help to increase the profile of this research and the desirability of a Computer Science degree to the larger population.</abstract>
	<search_task_number>16</search_task_number>
	<query>Ergonomics and modern devices</query>
	<relevance>0</relevance>
  </item>


  <item>
    <title>Enhancing query translation with relevance feedback in translingual information retrieval</title>
	<abstract>As an effective technique for improving retrieval effectiveness, relevance feedback (RF) has been widely studied in both monolingual and translingual information retrieval (TLIR). The studies of RF in TLIR have been focused on query expansion (QE), in which queries are reformulated before and/or after they are translated. However, RF in TLIR actually not only can help select better query terms, but also can enhance query translation by adjusting translation probabilities and even resolving some out-of-vocabulary terms. In this paper, we propose a novel relevance feedback method called translation enhancement (TE), which uses the extracted translation relationships from relevant documents to revise the translation probabilities of query terms and to identify extra available translation alternatives so that the translated queries are more tuned to the current search. We studied TE using pseudo-relevance feedback (PRF) and interactive relevance feedback (IRF). Our results show that TE can significantly improve TLIR with both types of relevance feedback methods, and that the improvement is comparable to that of query expansion. More importantly, the effects of translation enhancement and query expansion are complementary. Their integration can produce further improvement, and makes TLIR more robust for a variety of queries.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Simultaneous multilingual search for translingual information retrieval</title>
	<abstract>We consider the problem of translingual information retrieval, where monolingual searchers issue queries in a different language than the document language(s) and the results must be returned in the language they know, the query language. We present a framework for translingual IR that integrates document translation and query translation into the retrieval model. The corpus is represented as an aligned, jointly indexed "pseudo-parallel" corpus, where each document contains the text of the document along with its translation into the query language. The queries are formulated as multilingual structured queries, where each query term and its translations into the document language(s) are treated as synonym sets. This model leverages simultaneous search in multiple languages against jointly indexed documents to improve the accuracy of results over search using document translation or query translation alone. For query translation, we compared a statistical machine translation (SMT) approach to a dictionary-based approach. We found that using a Wikipedia-derived dictionary for named entities combined with an SMT-based dictionary worked better than SMT alone. Simultaneous multilingual search also has other important features suited to translingual search, since it can provide an indication of poor document translation when a match with the source document is found. We show how close integration of CLIR and SMT allows us to improve result translation in addition to IR results.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Mandarin-English Information (MEI): investigating translingual speech retrieval</title>
	<abstract>We describe a system which supports English text queries searching for Mandarin Chinese spoken documents. This is one of the first attempts to tightly couple speech recognition with machine translation technologies for cross-media and cross-language retrieval. The Mandarin Chinese news audio are indexed with word and subword units by speech recognition. Translation of these multi-scale units can effect cross-language information retrieval. The integrated technologies will be evaluated based on the performance of translingual speech retrieval.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Term selection for information retrieval applications</title>
	<abstract>The selection and identification of terms is an important part of many natural language applications. In the information retrieval domain documents are often abbreviated to their most salient terms in order to reduce storage requirements and processing time and also to make algorithms more efficient. The quality of search results is a direct reflection of the quality of these representative features. In translingual applications translation dictionaries must be built in order to bridge the gap between source and target languages. With limited time and resources the most effective terms for translation must somehow be chosen. Techniques for term selection are also fundamental to a number of other tasks including automatic generation of indices, concordances In this dissertation we investigate methods for selecting terms in the context of a number of specific tasks. As a practical test-case for some of the approaches developed here, we participate in the formal evaluations of Topic Detection and Tracking. In the spirit of residual-idf, a metric which measures deviation from Poisson, we develop a sum-log-ratios metric which improves upon residual-idf in two significant ways—it incorporates document length normalization and it is a function of the entire within-document term count distribution. Also developed here is the idea of a “universal dictionary” as a basis for translingual information retrieval tasks. In the methods section, we describe a suffix array based indexing scheme ideally suited to efficiently calculate within-document term counts for ngrams in very large corpora.
We test our methods in a number of real-world applications. In the formal evaluations of TDT2 we show that the simple vector space model performs as well as much more complicated models. In the context of building a “universal dictionary”, we use our method of term selection to choose a vocabulary of less than 10,000 terms which is essentially as effective for topic tracking as an unlimited vocabulary of over 300,000 terms. We demonstrate that this same method extends well to other applications, employing it as a novel approach to multi-word terminology and collocation extraction.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Terminology Retrieval: Towards a Synergy between Thesaurus and Free Text Searching</title>
	<abstract>Multilingual Information Retrieval usually forces a choice between free text indexing or indexing by means of multilingual thesaurus. However, since they share the same objectives, synergy between both approaches is possible. This paper shows a retrieval framework that make use of terminological information in free-text indexing. The Automatic Terminology Extraction task, which is used for thesauri construction, shifts to a searching of terminology and becomes an information retrieval task: Terminology Retrieval. Terminology Retrieval, then, allows cross-language information retrieval through the browsing of morpho-syntactic, semantic and translingual variations of the query. Although terminology retrieval doesn't make use of them, controlled vocabularies become an appropriate framework for terminology retrieval evaluation.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Comparing cross-language query expansion techniques by degrading translation resources</title>
	<abstract>The quality of translation resources is arguably the most important factor affecting the performance of a cross-language information retrieval system. While many investigations have explored the use of query expansion techniques to combat errors induced by translation, no study has yet examined the effectiveness of these techniques across resources of varying quality. This paper presents results using parallel corpora and bilingual wordlists that have been deliberately degraded prior to query translation. Across different languages, translingual resources, and degrees of resource degradation, pre-translation query expansion is tremendously effective. In several instances, pre-translation expansion results in better performance when no translations are available, than when an uncompromised resource is used without pre-translation expansion. We also demonstrate that post-translation expansion using relevance feedback can confer modest performance gains. Measuring the efficacy of these techniques with resources of different quality suggests an explanation for the conflicting reports that have appeared in the literature.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Challenges and solutions of multilingual and translingual information service systems</title>
	<abstract>In this paper, we present a survey of challenges and solutions of multilingual and translingual information service systems. In contrast to the computational linguistics literature on such systems, we are approaching the theme here from an HCI perspective. We will argue for a strategy that reduces reliance on automatic free-text translation, language input and classical information retrieval while not giving up these less reliable technologies altogether. We will also opt for a close situation-driven integration of information and communication functionalities. The described solutions have been incorporated into a novel mobile combined information and communication system for foreign tourists that has been tested under realistic conditions by users from several countries. The system is developed by the German-Chinese cooperation project COMPASS20081, a research action within the Digital Olympics framework.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Automatic extraction of named entity translingual equivalence based on multi-feature cost minimization</title>
	<abstract>Translingual equivalence refers to the relationship between expressions of the same meaning from different languages. Identifying translingual equivalence of named entities (NE) can significantly contribute to multilingual natural language processing, such as crosslingual information retrieval, crosslingual information extraction and statistical machine translation. In this paper we present an integrated approach to extract NE translingual equivalence from a parallel Chinese-English corpus.Starting from a bilingual corpus where NEs are automatically tagged for each language, NE pairs are aligned in order to minimize the overall multi-feature alignment cost. An NE transliteration model is presented and iteratively trained using named entity pairs extracted from a bilingual dictionary. The transliteration cost, combined with the named entity tagging cost and word-based translation cost, constitute the multi-feature alignment cost. These features are derived from several information sources using unsupervised and partly supervised methods. A greedy search algorithm is applied to minimize the alignment cost. Experiments show that the proposed approach extracts NE translingual equivalence with 81% F-score and improves the translation score from 7.68 to 7.74.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Multilingual access to large spoken archives</title>
	<abstract>Spoken word collections promise access to unique and compelling content, and most of the technology needed to realize that promise is now in place. Decreasing storage costs, increasing network capacity, and the availability of software to encode and exchange digital audio make possible physical access to spoken word collections at a previously unimaginable scale. Effective support for intellectual access --- the problem of finding what you are looking for --- is much more challenging, however. In this talk I will briefly describe work that has been done on this problem at the Text Retrieval Conferences, the Topic Detection and Tracking evaluations, and in individual research projects around the world. I will then describe a unique resource, a collection of 116,000 hours of oral history interviews recorded in 32 languages in 57 countries that has been assembled by the Survivors of the Shoah Visual History Foundation. Nearly 10,000 hours of this audio has been manually segmented, summarized and indexed, making this an unrivaled resource with which we can explore a broad array of data-driven techniques. My main focus will be to explain how we are leveraging this exceptional resource to develop the ability to index similar materials automatically.

The project we call MALACH (Multilingual Access to Large spoken ArCHives) builds on a long heritage of increasingly demanding applications for speech recognition technology. The accented, emotional and elderly speech in the Shoah Foundation's collection are so challenging that state-of-the-art systems initially yielded a 90% word error rate! We now have speech recognition systems that achieve better than half that error rate for two languages, English and Czech. That's nowhere near good enough to produce readable transcripts, but it is approaching a point where other language technologies can begin to make headway. I'll illustrate that point with our latest results from across the project on speech recognition, natural language processing components, and information retrieval system design.

The scope of this one project is breathtaking, directly involving nine research teams from six institutions on two continents (Charles University, IBM T.J. Watson Research Lab, Johns Hopkins University, the Shoah Foundation, the University of Maryland, and the University of West Bohemia), with interests that range from the information needs of historians to the modeling of Czech colloquial pronunciation. Virtually every topic in computational linguistics finds expression in that range. We plan to ultimately build speech recognition systems in at least five languages (adding Russian, Polish and Slovak to what we have now), so morphology and language modeling are critical issues. The diverse range of languages in the collection make translation and translingual search essential capabilities. The sheer size of the collection and the strict linearity of the audio medium call for effective summarization. References to named entities are important hooks for many information seeking strategies, so named entity detection and co-reference resolution techniques that are robust in the face of pronunciation variations are needed. An interview is a dialog, and these interviews contain a rich discourse structure, thus effective discourse and dialog analysis could lead to new ways of supporting access. And, of course, progress on all of this depends fundamentally on evaluation.

As with any collection, we must respect the wishes of its creators when using it in our research. In this case, more than 50,000 people contributed their life stories. The stories speak of some of the greatest inhumanity ever witnessed, and many of those who told those stories still walk among us. Much as we might wish that ELRA or the LDC could obtain and release the entire collection, that is not likely to happen any time soon. But the Shoah Foundation does hope to begin the process of clearing subsets of the collection for research use over the next year or so, and we are gearing our annotation and test collection development efforts to maximize the overlap with what they will ultimately release. Now is therefore a propitious time to begin to think about how these unique materials might be used in your own research. Since the dawn of language, the oral tradition has been the dominant form in which we have told our stories and passed on our culture. Over the past few thousand years, the written form has moved to the fore, principally because access to the written word has been more easily supported by the available technology. We now stand on the verge of restoring the balance and building an oral tradition that gives lasting voice to those who choose not to write their stories. I invite you to join us in that quest!</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Advanced Information Retrieval</title>
	<abstract>In this paper we explore some of the most important areas of advanced information retrieval. In particular, we look at cross-lingual information retrieval, multimedia information retrieval and semantic-based information retrieval. Cross-lingual information retrieval deals with asking questions in one language and retrieving documents in one or more different languages. With an increasingly globalized economy, the ability to find information in other languages is becoming a necessity. Multimedia information retrieval deals with finding media other than text, i.e. music and pictures. With the explosion of digital media that is available on the Internet and present on users' computers techniques for quickly and accurately finding desired media is important. Semantic based information retrieval goes beyond classical information retrieval and uses semantic information to understand the documents and queries in order to aid retrieval. Semantic based information retrieval goes beyond standard surface information by using the concepts represented in documents and queries to improve retrieval performance.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Embedding web-based statistical translation models in cross-language information retrieval</title>
	<abstract>Although more and more language pairs are covered by machine translation (MT) services, there are still many pairs that lack translation resources. Cross-language information retrieval (CLIR) is an application that needs translation functionality of a relatively low level of sophistication, since current models for information retrieval (IR) are still based on a bag of words. The Web provides a vast resource for the automatic construction of parallel corpora that can be used to train statistical translation models automatically. The resulting translation models can be embedded in several ways in a retrieval model. In this article, we will investigate the problem of automatically mining parallel texts from the Web and different ways of integrating the translation models within the retrieval process. Our experiments on standard test collections for CLIR show that the Web-based translation models can surpass commercial MT systems in CLIR tasks. These results open the perspective of constructing a fully automatic query translation device for CLIR at a very low cost.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Multilingual Information Retrieval Using Machine Translation, Relevance Feedback and Decompounding</title>
	<abstract>Multilingual retrieval (querying of multiple document collections each in a different language) can be achieved by combining several individual techniques which enhance retrieval: machine translation to cross the language barrier, relevance feedback to add words to the initial query, decompounding for languages with complex term structure, and data fusion to combine monolingual retrieval results from different languages. Using the CLEF 2001 and CLEF 2002 topics and document collections, this paper evaluates these techniques within the context of a monolingual document ranking formula based upon logistic regression. Each individual technique yields improved performance over runs which do not utilize that technique. Moreover the techniques are complementary, in that combining the best techniques outperforms individual technique performance. An approximate but fast document translation using bilingual wordlists created from machine translation systems is presented and evaluated. The fast document translation is as effective as query translation in multilingual retrieval. Furthermore, when fast document translation is combined with query translation in multilingual retrieval, the performance is significantly better than that of query translation or fast document translation.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>User-model based personalized summarization</title>
	<abstract>The potential of summary personalization is high, because a summary that would be useless to decide the relevance of a document if summarized in a generic manner, may be useful if the right sentences are selected that match the user interest. In this paper we defend the use of a personalized summarization facility to maximize the density of relevance of selections sent by a personalized information system to a given user. The personalization is applied to the digital newspaper domain and it used a user-model that stores long and short term interests using four reference systems: sections, categories, keywords and feedback terms. On the other side, it is crucial to measure how much information is lost during the summarization process, and how this information loss may affect the ability of the user to judge the relevance of a given document. The results obtained in two personalization systems show that personalized summaries perform better than generic and generic-personalized summaries in terms of identifying documents that satisfy user preferences. We also considered a user-centred direct evaluation that showed a high level of user satisfaction with the summaries.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Anchor text mining for translation of Web queries: A transitive translation approach</title>
	<abstract>To discover translation knowledge in diverse data resources on the Web, this article proposes an effective approach to finding translation equivalents of query terms and constructing multilingual lexicons through the mining of Web anchor texts and link structures. Although Web anchor texts are wide-scoped hypertext resources, not every particular pair of languages contains sufficient anchor texts for effective extraction of translations for Web queries. For more generalized applications, the approach is designed based on a transitive translation model. The translation equivalents of a query term can be extracted via its translation in an intermediate language. To reduce interference from translation errors, the approach further integrates a competitive linking algorithm into the process of determining the most probable translation. A series of experiments has been conducted, including performance tests on term translation extraction, cross-language information retrieval, and translation suggestions for practical Web search services, respectively. The obtained experimental results have shown that the proposed approach is effective in extracting translations of unknown queries, is easy to combine with the probabilistic retrieval model to improve the cross-language retrieval performance, and is very useful when the considered language pairs lack a sufficient number of anchor texts. Based on the approach, an experimental system called LiveTrans has been developed for English--Chinese cross-language Web search.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A hybrid approach for indexing and retrieval of archaeological textual information</title>
	<abstract>This paper focuses on the problem of archaeological textual information retrieval, covering various field-related topics, and investigating different issues related to special characteristics of Arabic.

The suggested hybrid retrieval approach employs various clustering and classification methods that enhances both retrieval and presentation, and infers further information from the results returned by a primary retrieval engine, which, in turn, uses Latent Semantic Analysis (LSA) as a primary retrieval method. In addition, a stemmer for Arabic words was designed and implemented to facilitate the indexing process and to enhance the quality of retrieval.

The performance of our module was measured by carrying out experiments using standard datasets, where the system showed promising results with many possibilities for future research and further development.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Preliminary study into query translation for patent retrieval</title>
	<abstract>Patent retrieval is a branch of Information Retrieval (IR) aiming to support patent professionals in retrieving patents that satisfy their information needs. Often, patent granting bodies require patents to be partially translated into one or more major foreign languages, so that language boundaries do not hinder their accessibility. This multilinguality of patent collections offers opportunities for improving patent retrieval. In this work we exploit these opportunities by applying query translation to patent retrieval. We expand monolingual patent queries with their translations, using both a domain-specific patent dictionary that we extract from the patent collection, and a general domain-free dictionary. Experimental evaluation on a standard CLEF-IP dataset shows that using either translation dictionary fetches similar results: query translation can help patent retrieval, but not always, and without great improvement compared to standard statistical monolingual query expansion (Rocchio). The improvement is greater when the source language is English, as opposed to French or German, a finding partly due to the effect of the complex French and German morphology upon translation accuracy, but also partly due to the prevalence of English in the collection. A thorough per-query analysis reveals that cases where standard query expansion fails (e.g. zero recall) can benefit from query translation.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Cross-language plagiarism detection</title>
	<abstract>Cross-language plagiarism detection deals with the automatic identification and extraction of plagiarism in a multilingual setting. In this setting, a suspicious document is given, and the task is to retrieve all sections from the document that originate from a large, multilingual document collection. Our contributions in this field are as follows: (1) a comprehensive retrieval process for cross-language plagiarism detection is introduced, highlighting the differences to monolingual plagiarism detection, (2) state-of-the-art solutions for two important subtasks are reviewed, (3) retrieval models for the assessment of cross-language similarity are surveyed, and, (4) the three models CL-CNG, CL-ESA and CL-ASA are compared. Our evaluation is of realistic scale: it relies on 120,000 test documents which are selected from the corpora JRC-Acquis and Wikipedia, so that for each test document highly similar documents are available in all of the six languages English, German, Spanish, French, Dutch, and Polish. The models are employed in a series of ranking tasks, and more than 100 million similarities are computed with each model. The results of our evaluation indicate that CL-CNG, despite its simple approach, is the best choice to rank and compare texts across languages if they are syntactically related. CL-ESA almost matches the performance of CL-CNG, but on arbitrary pairs of languages. CL-ASA works best on "exact" translations but does not generalize well.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Relevancy based semantic interoperation of reuse repositories</title>
	<abstract>Software reuse is a promising solution to the software crisis. Reuse repositories are the basic infrastructure for software reuse. During the past decade, various academic, commercial, governmental, and industrial organizations have developed many Internet-enabled reuse repositories to provide access to software components and related resources. It has necessitated semantic interoperation to allow distributed maintenance and management of these repositories while enabling users to efficiently and conveniently access resources from multiple reuse repositories via a single representation view. In this paper, we have proposed an approach to enhancing the semantic interoperability of reuse repositories, called the improved relevancy matching and ranking (IRMR) method, based on analyzing the correlation of terms in representation methods of the repositories. A prototype system, the Virtual Repository supporting Semantic Interoperation (VRSI), is presented to illustrate the application of this approach to support the semantic interoperation of reuse repositories. Experimental results on real world reuse repositories demonstrated significant improvement in terms of searching effectiveness.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Towards Universal Text Retrieval: Tipster Text Retrieval Research at New Mexico State University</title>
	<abstract>New Mexico State University's Computing Research Lab has participated in research in all three phases of the US Government's Tipster program. Our work on information retrieval has focused on research and development of multilingual and cross-language approaches to automatic retrieval. The work on automatic systems has been supplemented by additional research into the role of the IR system user in interactive retrieval scenarios: monolingual, multilingual and cross-language. The combined efforts suggest that “universal” text retrieval, in which a user can find, access and use documents in the face of language differences and information overload, may be possible.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Multimedia search with pseudo-relevance feedback</title>
	<abstract>We present an algorithm for video retrieval that fuses the decisions of multiple retrieval agents in both text and image modalities. While the normalization and combination of evidence is novel, this paper emphasizes the successful use of negative pseudorelevance feedback to improve image retrieval performance. Although we have not solved all problems in video information retrieval, the results are encouraging, indicating that pseudo-relevance feedback shows great promise for multimedia retrieval with very varied and errorful data.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Using Corpus-Based Approaches in a System for Multilingual Information Retrieval</title>
	<abstract>We present a system for multilingual information retrieval that allows users to formulate queries in their preferred language and retrieve relevant information from a collection containing documents in multiple languages. The system is based on a process of document level alignments, where documents of different languages are paired according to their similarity. The resulting mapping allows us to produce a multilingual comparable corpus. Such a corpus has multiple interesting applications. It allows us to build a data structure for query translation in cross-language information retrieval (CLIR). Moreover, we also perform pseudo relevance feedback on the alignments to improve our retrieval results. And finally, multiple retrieval runs can be merged into one unified result list. The resulting system is inexpensive, adaptable to domain-specific collections and new languages and has performed very well at the TREC-7 conference CLIR system comparison.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Implicit ambiguity resolution using incremental clustering in Korean-to-English cross-language information retrieval</title>
	<abstract>This paper presents a method to implicitly resolve ambiguities using dynamic incremental clustering in Korean-to-English cross-language information retrieval. In the framework we propose, a query in Korean is first translated into English by looking up Korean-English dictionary, then documents are retrieved based on the vector space retrieval for the translated query terms. For the top-ranked retrieved documents, query-oriented document clusters are incrementally created and the weight of each retrieved document is re-calculated by using clusters. In experiment on TREC-6 CLIR test collection, our method achieved 28.29% performance improvement for translated queries without ambiguity resolution for queries. This corresponds to 97.27% of the monolingual performance for original queries. When we combine our method with query ambiguity resolution, our method even outperforms the monolingual retrieval.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Toward cross-language and cross-media image retrieval</title>
	<abstract>This paper describes our approach used in ImageCLEF 2004. Our focus is on image retrieval using text, i.e. Cross-Media IR. To do this, we first determine the strong relationships between keywords and types of visual features such as texture or shape. Then, the subset of images retrieved by text retrieval are used as examples to match other images according to the most important types of features of the query words.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Using Statistical Term Similarity for Sense Disambiguationin Cross-Language Information Retrieval</title>
	<abstract>With the increasing availability of machine-readable bilingual dictionaries, dictionary-based automatic query translation has become a viable approach to Cross-Language Information Retrieval (CLIR). In this approach, resolving term ambiguity is a crucial step. We propose a sense disambiguation technique based on a term-similarity measure for selecting the right translation sense of a query term. In addition, we apply a query expansion technique which is also based on the term similarity measure to improve the effectiveness of the translation queries. The results of our Indonesian to English and English to Indonesian CLIR experiments demonstrate the effectiveness of the sense disambiguation technique. As for the query expansion technique, it is shown to be effective as long as the term ambiguity in the queries has been resolved. In the effort to solve the term ambiguity problem, we discovered that differences in the pattern of word-formation between the two languages render query translations from one language to the other difficult.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Query expansion for hash-based image object retrieval</title>
	<abstract>An efficient indexing method is essential for content-based image retrieval with the exponential growth in large-scale videos and photos. Recently, hash-based methods (e.g., locality sensitive hashing - LSH) have been shown efficient for similarity search. We extend such hash-based methods for retrieving images represented by bags of (high-dimensional) feature points. Though promising, the hash-based image object search suffers from low recall rates. To boost the hash-based search quality, we propose two novel expansion strategies - intra-expansion and inter-expansion. The former expands more target feature points similar to those in the query and the latter mines those feature points that shall co-occur with the search targets but not present in the query. We further exploit variations for the proposed methods. Experimenting in two consumer-photo benchmarks, we will show that the proposed expansion methods are complementary to each other and can collaboratively contribute up to 76.3% (average) relative improvement over the original hash-based method.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Combining wikipedia-based concept models for cross-language retrieval</title>
	<abstract>As a low-cost ressource that is up-to-date, Wikipedia recently gains attention as a means to provide cross-language brigding for information retrieval. Contradictory to a previous study, we show that standard Latent Dirichlet Allocation (LDA) can extract cross-language information that is valuable for IR by simply normalizing the training data. Furthermore, we show that LDA and Explicit Semantic Analysis (ESA) complement each other, yielding significant improvements when combined. Such a combination can significantly contribute to retrieval based on machine translation, especially when query translations contain errors. The experiments were perfomed on the Multext JOC corpus und a CLEF dataset.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Comparing different units for query translation in Chinese cross-language information retrieval</title>
	<abstract>Although both words and n-grams of characters have been used in Chinese IR, they have often been used as two competing methods. For cross-language IR with Chinese, word translation has been used in all previous studies. In this paper, we re-examine the use of n-grams and words for monolingual Chinese IR. We show that both types of indexing unit can be combined within the language modeling framework to produce higher retrieval effectiveness. For CLIR with Chinese, we investigate the possibility of using bigrams and unigrams as translation units. Several translation models from English words to Chinese unigrams, bigrams and words are created based on a parallel corpus. An English query is then translated in several ways, each producing a ranking score. The final ranking score combines all these types of translation. Our experiments on several collections show that Chinese character n-grams are reasonable alternative translation units to words, and they lead to retrieval effectiveness comparable to words. In addition, combinations of both words and n-grams produce higher effectiveness.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>To translate or not to translate?</title>
	<abstract>Query translation is an important task in cross-language information retrieval (CLIR) aiming to translate queries into languages used in documents. The purpose of this paper is to investigate the necessity of translating query terms, which might differ from one term to another. Some untranslated terms cause irreparable performance drop while others do not. We propose an approach to estimate the translation probability of a query term, which helps decide if it should be translated or not. The approach learns regression and classification models based on a rich set of linguistic and statistical properties of the term. Experiments on NTCIR-4 and NTCIR-5 English-Chinese CLIR tasks demonstrate that the proposed approach can significantly improve CLIR performance. An in-depth analysis is also provided for discussing the impact of untranslated out-of-vocabulary (OOV) query terms and translation quality of non-OOV query terms on CLIR performance.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Resource selection for domain-specific cross-lingual IR</title>
	<abstract>An under-explored question in cross-language information retrieval (CLIR) is to what degree the performance of CLIR methods depends on the availability of high-quality translation resources for particular domains. To address this issue, we evaluate several competitive CLIR methods - with different training corpora - on test documents in the medical domain. Our results show severe performance degradation when using a general-purpose training corpus or a commercial machine translation system (SYSTRAN), versus a domain-specific training corpus. A related unexplored question is whether we can improve CLIR performance by systematically analyzing training resources and optimally matching them to target collections. We start exploring this problem by suggesting a simple criterion for automatically matching training resources to target corpora. By using cosine similarity between training and target corpora as resource weights we obtained an average of 5.6% improvement over using all resources with no weights. The same metric yields 99.4% of the performance obtained when an oracle chooses the optimal resource every time.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An empirical study of required dimensionality for large-scale latent semantic indexing applications</title>
	<abstract>The technique of latent semantic indexing is used in a wide variety of commercial applications. In these applications, the processing time and RAM required for SVD computation, and the processing time and RAM required during LSI retrieval operations are all roughly linear in the number of dimensions, k, chosen for the LSI representation space. In large-scale commercial LSI applications, reducing k values could be of significant value in reducing server costs. This paper explores the effects of varying dimensionality.

The approach taken here focuses on term comparisons. Pairs of terms are considered which have strong real-world associations. The proximities of members of these pairs in the LSI space are compared at multiple values of k. The testing is carried out for collections of from one to five million documents. For the five million document collection, a value of k ≈ 400 provides the best performance.

The results suggest that there is something of an 'island of stability' in the k = 300 to 500 range. The results also indicate that there is relatively little room to employ k values outside of this range without incurring significant distortions in at least some term-term correlations.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Co-reranking by mutual reinforcement for image search</title>
	<abstract>Most existing reranking approaches to image search focus solely on mining "visual" cues within the initial search results. However, the visual information cannot always provide enough guidance to the reranking process. For example, different images with similar appearance may not always present the same relevant information to the query. Observing that multi-modality cues carry complementary relevant information, we propose the idea of co-reranking for image search, by jointly exploring the visual and textual information. Co-reranking couples two random walks, while reinforcing the mutual exchange and propagation of information relevancy across different modalities. The mutual reinforcement is iteratively updated to constrain information exchange during random walk. As a result, the visual and textual reranking can take advantage of more reliable information from each other after every iteration. Experiment results on a real-world dataset (MSRA-MM) collected from Bing image search engine shows that co-reranking outperforms several existing approaches which do not or weakly consider multi-modality interaction.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Ontologies and text retrieval</title>
	<abstract>Analogues to much of today's work in ontologies have existed for centuries in text retrieval. The use of controlled vocabularies, or thesauri, has been fundamental to document indexing in library science. Thesauri serve several purposes, including:• Knowledge organisation A thesaurus provides a hierarchy of concepts that organises domain-specific knowledge.• Terminology normalisation By selecting a unique word or phrase to represent each domain concept, then linking synonymous terms to it, a thesaurus enforces terminological consistency.• Query expansion A thesaurus facilitates the addition of terms to a query by providing explicit hierarchical and lateral relationships among terms.These properties serve to mediate the information flow from indexer to user. Thesauri thus serve many of the same functions for people that ontologies are designed to serve for software agents. As automated retrieval has developed over the decades since the inception of computer processing of text, many techniques have been introduced to apply this typically manual work to the automated arena (see Soergel (1985) for an introduction to library information systems, also Anderson and Pélrez-Carballo (2001a, 2001b) for a summary of the intersection of human and machine indexing).</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Video search reranking via information bottleneck principle</title>
	<abstract>We propose a novel and generic video/image reranking algorithm, IB reranking, which reorders results from text-only searches by discovering the salient visual patterns of relevant and irrelevant shots from the approximate relevance provided by text results. The IB reranking method, based on a rigorous Information Bottleneck (IB) principle, finds the optimal clustering of images that preserves the maximal mutual information between the search relevance and the high-dimensional low-level visual features of the images in the text search results. Evaluating the approach on the TRECVID 2003-2005 data sets shows significant improvement upon the text search baseline, with relative increases in average performance of up to 23%. The method requires no image search examples from the user, but is competitive with other state-of-the-art example-based approaches. The method is also highly generic and performs comparably with sophisticated models which are highly tuned for specific classes of queries, such as named-persons. Our experimental analysis has also confirmed the proposed reranking method works well when there exist sufficient recurrent visual patterns in the search results, as often the case in multi-source news videos.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Cross-language query classification using web search for exogenous knowledge</title>
	<abstract>The non-English Web is growing at phenomenal speed, but available language processing tools and resources are predominantly English-based. Taxonomies are a case in point: while there are plenty of commercial and non-commercial taxonomies for the English Web, taxonomies for other languages are either not available or of arguable quality. Given that building comprehensive taxonomies for each language is prohibitively expensive, it is natural to ask whether existing English taxonomies can be leveraged, possibly via machine translation, to enable text processing tasks in other languages. Our experimental results confirm that the answer is affirmative with respect to at least one task. In this study we focus on query classification, which is essential for understanding the user intent both in Web search and in online advertising. We propose a robust method for classifying non-English queries into an English taxonomy, using an existing English text classifier and off-the-shelf machine translation systems. In particular, we show that by considering the Web search results in the query's original language as additional sources of information, we can alleviate the effect of erroneous machine translation. Empirical evaluation on query sets in languages as diverse as Chinese and Russian yields very encouraging results; consequently, we believe that our approach is also applicable to many additional languages.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Building Bilingual Dictionaries from Parallel Web Documents</title>
	<abstract>In this paper we describe a system for automatically constructing a bilingual dictionary for cross-language information retrieval applications. We describe how we automatically target candidate parallel documents, filter the candidate documents and process them to create parallel sentences. The parallel sentences are then automatically translated using an adaptation of the EMIM technique and a dictionary of translation terms is created. We evaluate our dictionary using human experts. The evaluation showed that the system performs well. In addition the results obtained from automatically-created corpora are comparable to those obtained from manually created corpora of parallel documents. Compared to other available techniques, our approach has the advantage of being simple, uniform, and easy-to-implement while providing encouraging results.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The ESA retrieval model revisited</title>
	<abstract>Among the retrieval models that have been proposed in the last years, the ESA model of Gabrilovich and Markovitch received much attention. The authors report on a significant improvement in the retrieval performance, which is explained with the semantic concepts introduced by the document collection underlying ESA. Their explanation appears plausible but our analysis shows that the connections are more involved and that the "concept hypothesis" does not hold. In our contribution we analyze several properties that in fact affect the retrieval performance. Moreover, we introduce a formalization of ESA, which reveals its close connection to existing retrieval models.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Cross-language linking of news stories on the web using interlingual topic modelling</title>
	<abstract>We have studied the problem of linking event information across different languages without the use of translation systems or dictionaries. The linking is based on interlingua information obtained through probabilistic topic models trained on comparable corpora written in two languages (in our case English and Dutch). The achieve this, we expand the Latent Dirichlet Allocation model to process documents in two languages. We demonstrate the validity of the learned interlingual topics in a document clustering task, where the evaluation is performed on Google News.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Video search reranking through random walk over document-level context graph</title>
	<abstract>Multimedia search over distributed sources often result in recurrent images or videos which are manifested beyond the textual modality. To exploit such contextual patterns and keep the simplicity of the keyword-based search, we propose novel reranking methods to leverage the recurrent patterns to improve the initial text search results. The approach, context reranking, is formulated as a random walk problem along the context graph, where video stories are nodes and the edges between them are weighted by multimodal contextual similarities. The random walk is biased with the preference towards stories with higher initial text search scores - a principled way to consider both initial text search results and their implicit contextual relationships. When evaluated on TRECVID 2005 video benchmark, the proposed approach can improve retrieval on the average up to 32% relative to the baseline text search method in terms of story-level Mean Average Precision. In the people-related queries, which usually have recurrent coverage across news sources, we can have up to 40% relative improvement. Most of all, the proposed method does not require any additional input from users (e.g., example images), or complex search models for special queries (e.g., named person search).</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Bayesian video search reranking</title>
	<abstract>Content-based video search reranking can be regarded as a process that uses visual content to recover the "true" ranking list from the noisy one generated based on textual information. This paper explicitly formulates this problem in the Bayesian framework, i.e., maximizing the ranking score consistency among visually similar video shots while minimizing the ranking distance, which represents the disagreement between the objective ranking list and the initial text-based. Different from existing point-wise ranking distance measures, which compute the distance in terms of the individual scores, two new methods are proposed in this paper to measure the ranking distance based on the disagreement in terms of pair-wise orders. Specifically, hinge distance penalizes the pairs with reversed order according to the degree of the reverse, while preference strength distance further considers the preference degree. By incorporating the proposed distances into the optimization objective, two reranking methods are developed which are solved using quadratic programming and matrix computation respectively. Evaluation on TRECVID video search benchmark shows that the performance improvement up to 21% on TRECVID 2006 and 61.11% on TRECVID 2007 are achieved relative to text search baseline.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A reranking approach for context-based concept fusion in video indexing and retrieval</title>
	<abstract>We propose to incorporate hundreds of pre-trained concept detectors to provide contextual information for improving the performance of multimodal video search. The approach takes initial search results from established video search methods (which typically are conservative in usage of concept detectors) and mines these results to discover and leverage co-occurrence patterns with detection results for hundreds of other concepts, thereby refining and reranking the initial video search result. We test the method on TRECVID 2005 and 2006 automatic video search tasks and find improvements in mean average precision (MAP) of 15%-30%. We also find that the method is adept at discovering contextual relationships that are unique to news stories occurring in the search set, which would be difficult or impossible to discover even if external training data were available.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A personalized multilingual web content miner: PMWebMiner</title>
	<abstract>This paper presents the development of a novel personal concept-based multilingual Web content mining system. Multilingual linguistic knowledge required by multilingual Web content mining is made available by encoding all multilingual concept-term relationships within a multilingual concept space using self-organising map. With this linguistic knowledge base, a personal space of interest is generated to reveal the conceptual content of a user’s multiple topics of interest using the user’s bookmark file. To personalise the multilingual Web content mining process, a concept-based Web crawler is developed to automatically gather multilingual web documents that are relevant to the user’s topics of interest As such, user-oriented concept-focused knowledge discovery in the multilingual Web is facilitated.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Robust visual reranking via sparsity and ranking constraints</title>
	<abstract>Visual reranking has become a widely-accepted method to improve traditional text-based image search engines. Its basic principle is that visually similar images should have similar ranking scores. While existing methods are different in specifics, almost all of them are based on explicit or implicit pseudo-relevance feedback (PRF). Explicit PRF-based approaches, including classification-based and clustering-based reranking, suffer from the difficulty of selecting reliable positive and negative samples. Implicit PRF-based approaches, such as graph-based and Bayesian visual reranking, deal with such unreliability by making use of the initial ranking in a soft manner, but have limited capability of promoting relevant images and lowering down irrelevant images.

In this paper, we propose l1 square loss optimization based on sparsity and ranking constraints to detect confident samples which are most likely to be relevant to a query. Based on the discovered confident samples, we present an adaptive kernel-based scheme to rerank the images. The success of our proposed method comes from another important observation that irrelevant images, whether initially positioned at the top or bottom, are usually less-popular and more diverse than relevant images. Therefore, it is robust against outlier images and suitable when relevant images are multi-modally distributed. The experimental results demonstrate significant improvement of our method over several existing reranking approaches on both MSRA-MM V1.0 and Web Queries datasets.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Trading spaces: on the lore and limitations of latent semantic analysis</title>
	<abstract>Two decades after its inception, Latent Semantic Analysis (LSA) has become part and parcel of every modern introduction to IR. For any tool that matures so quickly, it is important to check its lore and limitations, or else stagnation will set in. We focus here on the three main aspects of LSA that are well accepted, and the gist of which can be summarized as follows: (1) that LSA recovers latent semantic factors underlying the document space, (2) that such can be accomplished through lossy compression of the document space by eliminating lexical noise, and (3) that the latter can best be achieved by Singular Value Decomposition.

For each aspect we performed experiments analogous to those reported in the LSA literature and compared the evidence brought to bear in each case. On the negative side, we show that the above claims about LSA are much more limited than commonly believed. Even a simple example may show that LSA does not recover the optimal semantic factors as intended in the pedagogical example used in many LSA publications. Additionally, and remarkably deviating from LSA lore, LSA does not scale up well: the larger the document space, the more unlikely that LSA recovers an optimal set of semantic factors. On the positive side, we describe new algorithms to replace LSA (and more recent alternatives as pLSA, LDA, and kernel methods) by trading its l2 space for an l1 space, thereby guaranteeing an optimal set of semantic factors. These algorithms seem to salvage the spirit of LSA as we think it was initially conceived.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Evaluating cross-language explicit semantic analysis and cross querying</title>
	<abstract>This paper describes our participation in the TEL@CLEF task of the CLEF 2009 ad-hoc track. The task is to retrieve items from various multilingual collections of library catalog records, which are relevant to a user's query. Two different strategies are employed: (i) the Cross-Language Explicit Semantic Analysis, CL-ESA, where the library catalog records and the queries are represented in a multilingual concept space that is spanned by aligned Wikipedia articles, and, (ii) a Cross Querying approach, where a query is translated into all target languages using Google Translate and where the obtained rankings are combined. The evaluation shows that both strategies outperform the monolingual baseline and achieve comparable results.

Furthermore, inspired by the Generalized Vector Space Model we present a formal definition and an alternative interpretation of the CL-ESA model. This interpretation is interesting for real-world retrieval applications since it reveals how the computational effort for CL-ESA can be shifted from the query phase to a preprocessing phase.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Applying CLIR techniques to event tracking</title>
	<abstract>Cross-lingual event tracking from a very large number of information sources (thousands of Web sites, for example) is an open challenge. In this paper we investigate effective and scalable solutions for this problem, focusing on the use of cross-lingual information retrieval techniques to translate a small subset of the training documents, as an alternative to the conventional approach of translating all the multilingual test documents. In addition, we present a new variant of weighted pseudo-relevance feedback for adaptive event tracking. This new method simplifies the assumption and the computation in the best-known approach of this kind, yielding a better result than the latter on benchmark datasets in our evaluations.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Structural annotation of search queries using pseudo-relevance feedback</title>
	<abstract>Marking up queries with annotations such as part-of-speech tags, capitalization, and segmentation, is an important part of many approaches to query processing and understanding. Due to their brevity and idiosyncratic structure, search queries pose a challenge to existing annotation tools that are commonly trained on full-length documents. To address this challenge, we view the query as an explicit representation of a latent information need, which allows us to use pseudo-relevance feedback, and to leverage additional information from the document corpus, in order to improve the quality of query annotation.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Automatic image annotation and retrieval using subspace clustering algorithm</title>
	<abstract>The development of technology generates huge amounts of non-textual information, such as images. An efficient image annotation and retrieval system is highly desired. Clustering algorithms make it possible to represent visual features of images with finite symbols. Based on this, many statistical models, which analyze correspondence between visual features and words and discover hidden semantics, have been published. These models improve the annotation and retrieval of large image databases. However, image data usually have a large number of dimensions. Traditional clustering algorithms assign equal weights to these dimensions, and become confounded in the process of dealing with these dimensions. In this paper, we propose a top-down, subspace clustering algorithm as a solution to this problem. For a given cluster, we determine relevant features based on histogram analysis and assign greater weight to relevant features as compared to less relevant features. We have implemented four different models to link visual tokens with keywords based on the clustering results of our clustering algorithm and K-means algorithm, and evaluated performance using precision, recall and correspondence accuracy using benchmark dataset. The results show that our algorithm is better than traditional ones for automatic image annotation and retrieval.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Discovering parallel text from the World Wide Web</title>
	<abstract>Parallel corpus is a rich linguistic resource for various multilingual text management tasks, including cross-lingual text retrieval, multilingual computational linguistics and multilingual text mining. Constructing a parallel corpus requires effective alignment of parallel documents. In this paper, we develop a parallel page identification system for identifying and aligning parallel documents from the World Wide Web. The system crawls the Web to fetch potentially parallel multilingual Web documents using a Web spider. To determine the parallelism between potential document pairs, two modules are developed. First, a filename comparison module is used to check filename resemblance. Second, a content analysis module is used to measure the semantic similarity. The experiment conducted to a multilingual Web site shows the effectiveness of the system.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Knowledge transfer across multilingual corpora via latent topics</title>
	<abstract>This paper explores bridging the content of two different languages via latent topics. Specifically, we propose a unified probabilistic model to simultaneously model latent topics from bilingual corpora that discuss comparable content and use the topics as features in a cross-lingual, dictionary-less text categorization task. Experimental results on multilingual Wikipedia data show that the proposed topic model effectively discovers the topic information from the bilingual corpora, and the learned topics successfully transfer classification knowledge to other languages, for which no labeled training data are available.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Implementation techniques for large-scale latent semantic indexing applications</title>
	<abstract>The technique of latent semantic indexing (LSI) has wide applicability in information retrieval and data mining tasks. To date, however, most applications of LSI have addressed relatively small collections of data. This has been due partly to hardware and software limitations and partly to overly pessimistic estimates of the processing requirements of the singular value decomposition (SVD) process. In recent years, advances in hardware capabilities and software implementations have enabled much larger LSI applications. Moreover, experience with large LSI indexes has shown that the SVD is not the limitation on scalability that it was long thought to be. This paper describes techniques applicable to creating large-scale (multi-million document) LSI indexes. Detailed data regarding the LSI index creation process is presented for collections of up to 100 million documents. Four key factors are shown to contribute to the scalability of LSI. First, in most situations, the time required for calculation of the singular value decomposition (SVD) of the term-document matrix is not the dominant factor determining the overall time required to build an LSI index. Second, the time required to calculate the SVD in LSI is linear in the number of objects indexed. Third, incremental index creation greatly facilitates use of LSI in dynamic environments. Fourth, distributed query processing can be employed to support large numbers of users. It is shown that LSI is well-suited for implementation in modern distributed computing environments. This paper provides the first measurements of the execution time for large-scale LSI build processes in a cloud environment.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Fuzzy-neuro web-based multilingual knowledge management</title>
	<abstract>This paper presents new methodology towards the automatic development of multilingual Web portal for multilingual knowledge discovery and management. It aims to provide an efficient and effective framework for selecting and organizing knowledge from voluminous linguistically diverse Web contents. To achieve this, a concept-based approach that incorporates text mining and Web content mining using neural network and fuzzy techniques is proposed. First, a concept-based taxonomy of themes, which will act as the hierarchical backbone of the Web portal, is automatically generated. Second, a concept-based multilingual Web crawler is developed to intelligently harvest relevant multilingual documents from the Web. Finally, a concept-based multilingual text categorization technique is proposed to organize multilingual documents by concepts. As such, correlated multilingual Web documents can be gathered/filtered/organised/ based on their semantic content to facilitate high-performance multilingual information access.</abstract>
	<search_task_number>5</search_task_number>
	<query>translingual information retrieval</query>
	<relevance>1</relevance>
  </item>



  <item>
    <title>Continuous Malayalam speech recognition using Hidden Markov Models</title>
    <abstract>Accurate and computationally efficient means of recognizing continuous speech has been a subject of research in recent years. This paper reports the development of a small vocabulary, speaker independent continuous Malayalam speech recognition system based on Hidden Markov Models (HMMs). Continuous density HMM, which is used in this work to model phonemes, represents the general case where the observation probability density functions (pdfs) are continuous. The observation pdf is approximated using a Gaussian mixture density. Mel-frequency Cepstral Coefficients (MFCC) method is used to extract acoustic features from the input signal. To represent temporal variations in the speech signal, the first and second order derivatives of MFCC are added to the set of static parameters. The training and decoding are performed by the Baum-Welch and Viterbi algorithms respectively. The corpus contains naturally and continuously spoken sentences with multiple pronunciations and speaker variations. On evaluation the proposed system has produced promising results with 94.67% word accuracy and 93.33% sentence correct.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Visual speech recognition using motion features and hidden Markov models</title>
    <abstract>This paper presents a novel visual speech recognition approach based on motion segmentation and hidden Markov models (HMM). The proposed method identifies utterances from mouth video, without evaluating voice signals. The facial movements in the video data are represented using 2D spatial-temporal templates (STT). The proposed technique combines discrete stationary wavelet transform (SWT) and Zernike moments to extract rotation invariant features from the STTs. HMMs are used as speech classifier to model English phonemes. The preliminary results demonstrate that the proposed technique is suitable for phoneme classification with a high accuracy.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Specifics of Hidden Markov Model Modifications for Large Vocabulary Continuous Speech Recognition</title>
    <abstract>Specifics of hidden Markov model-based speech recognition are investigated. Influence of modeling simple and context-dependent phones, using simple Gaussian, two and three-component Gaussian mixture probability density functions for modeling feature distribution, and incorporating language model are discussed. Word recognition rates and model complexity criteria are used for evaluating suitability of these modifications for practical applications. Development of large vocabulary continuous speech recognition system using HTK toolkit and WSJCAM0 English speech corpus is described. Results of experimental investigations are presented.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Limited-Vocabulary Estonian Continuous Speech Recognition System using Hidden Markov Models</title>
    <abstract>The article presents a limited-vocabulary speaker independent continuous Estonian speech recognition system based on hidden Markov models. The system is trained using an annotated Estonian speech database of 60 speakers, approximately 4 hours in duration. Words are modelled using clustered triphones with multiple Gaussian mixture components. The system is evaluated using a number recognition task and a simple medium-vocabulary recognition task. The system performance is explored by employing acoustic models of increasing complexity. The number recognizer achieves an accuracy of 97%. The medium-vocabulary system recognizes 82.9% words correctly if operating in real time. The correctness increases to 90.6% if real-time requirement is discarded.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Large vocabulary continuous speech recognition using associative memory and hidden Markov model</title>
    <abstract>We attempted to improve recognition accuracy, avoiding extensive retraining when the vocabulary is changed or extended, by applying a hidden Markov model and neural associative memory based hybrid approach to continuous speech recognition. In this approach hidden Markov models are used for subword-unit recognition such as syllables. For a given subword-unit sequence a network of neural associative memories generates first spoken single words and then the whole sentence. The fault-tolerance property of neural associative memory enables the system to correctly recognize words although they are not perfectly pronounced or run into each other. The approach are evaluated for TIMIT, and for WSJ1 5k and 20k test sets. The obtained results are encouraging.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Hybrid Hidden Markov Model and Artificial Neural Network for Automatic Speech Recognition</title>
    <abstract>Automatic speech recognition (ASR) is an important topic to be performed by a computer system. This paper presents the use of a hybrid Hidden Markov Model (HMM) and Artificial Neural Networks (ANNs) for automatic speech recognition. The proposed hybrid system for ASR is to take advantage from the properties of both HMM and ANN, improving flexibility and recognition performance. The hybrid ANN/HMM assumes that the output of an ANN is sent to the HMM for ASR. The architecture relies on a probabilistic interpretation of the ANN outputs. Each output unit of the ANN is trained to perform a non-parametric estimate of the posterior probability of a continuous density HMM state given the acoustic observations. After a brief review of HMM and ANN, the paper reports the theoretical aspects and the performance of the proposed hybrid model. Experimental results are listed to demonstrate the potential of this hybrid model.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A Genetic Algorithm-aided Hidden Markov Model Topology Estimation for Phoneme Recognition of Thai Continuous Speech</title>
    <abstract>The use of Hidden Markov Models (HMM) in many pattern recognition tasks is now very common. Like other pattern recognitions, most Automatic Speech Recognition systems rely on HMM acoustic models. In such systems, recognition performances are significantly affected by their topologies. In this paper,we propose an HMM topology estimation approach for Thai phoneme recognition tasks whose process is divided into 2 stages. First, a set of suitable topologies are constructed by combinations of different objective functions and topology generation methods. Second, a Genetic Algorithm is deployed as the topology selection algorithm which considers global fitness and selects the most suitable topology from the candidates proposed in the previous stage for each phoneme. As aresult, the well-trained topology yields a maximum of4.36% error reduction over predefined left-to-right models. The estimated topologies still work well when the topology estimation was performed on speech utterances whose recording environments differ from the ones recognized.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Accelerating Speech Recognition Algorithm with Synergic Hidden Markov Model and Genetic Algorithm Based on Cellular Automata</title>
    <abstract>Abstract - One of the best current methods for modeling dynamic speech signal is using of HMM model. The speech recognition systems based on HMM can be able to compute the best likelihood measure between unknown input pattern and reference models by using Viterbi algorithm. Whereas such algorithm is based on dynamic programming, it consists of many computations with increasing number of reference words. In this paper, we will present a new evolutionary methodology based on synergic HMM and GA that will be able to compute likelihood measurement between unknown input pattern and reference patterns in the parallel form and based on cellular automata. We introduce this algorithm as HGC. The HGC algorithm will be compared with the Viterbi algorithm from the“recognition accuracy” and “recognition speed” viewpoints.Obtained results show that the HGC and Viterbi algorithms are close from “recognition accuracy” viewpoint, but HGCisso faster than the Viterbi</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>The application of hidden Markov models in speech recognition</title>
    <abstract>
      Hidden Markov Models (HMMs) provide a simple and effective framework for modelling time-varying spectral vector sequences. As a consequence, almost all present day large vocabulary continuous speech recognition (LVCSR) systems are based on HMMs.

      Whereas the basic principles underlying HMM-based LVCSR are rather straightforward, the approximations and simplifying assumptions involved in a direct implementation of these principles would result in a system which has poor accuracy and unacceptable sensitivity to changes in operating environment. Thus, the practical application of HMMs in modern systems involves considerable sophistication.

      The aim of this review is first to present the core architecture of a HMM-based LVCSR system and then describe the various refinements which are needed to achieve state-of-the-art performance. These refinements include feature projection, improved covariance modelling, discriminative parameter estimation, adaptation and normalisation, noise compensation and multi-pass system combination. The review concludes with a case study of LVCSR for Broadcast News and Conversation transcription in order to illustrate the techniques described.
    </abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Speech Recognition on an FPGA Using Discrete and Continuous Hidden Markov Models</title>
    <abstract>Speech recognition is a computationally demanding task, particularly the stage which uses Viterbi decoding for converting pre-processed speech data into words or sub-word units. Any device that can reduce the load on, for example, a PC's processor, is advantageous. Hence we present FPGA implementations of the decoder based alternately on discrete and continuous hidden Markov models (HMMs) representing monophones, and demonstrate that the discrete version can process speech nearly 5,000 times real time, using just 12% of the slices of a Xilinx Virtex XCV1000, but with a lower recognition rate than the continuous implementation, which is 75 times faster than real time, and occupies 45% of the same device.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A study on high-order hidden markov models and applications to speech recognition</title>
    <abstract>We propose high-order hidden Markov models (HO-HMM) to capture the duration and dynamics of speech signal. In the proposed model, both the state transition probability and the output observation probability depend not only on the current state but also on several previous states. An extended Viterbi algorithm was developed to train model and recognize speech. The performance of the HO-HMM was investigated by conducting experiments on speaker independent Mandarin digits recognition. From the experimental results, we find that as the order of HO-HMM increases, the number of error reduces. We also find that systems with both high-order state transition probability distribution and output observation probability distribution outperform systems with only high-order state transition probability distribution.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A Speech Recognition IC Using Hidden Markov Models with Continuous Observation Densities</title>
    <abstract>This paper presents the design of a speech recognition IC using hidden Markov models (HMMs) with continuous observation densities. Results of offline and live recognition tests are also given. Our design employs a table look-up method to simplify the computation and hence the architecture of the circuit. Currently each state of the HMMs is represented by a double-mixture Gaussian distribution. With minor modifications, the proposed architecture can be extended to implement a recognizer in which models with higher order multi-mixture Gaussian distribution are used for more precise acoustic modeling. The test chip is fabricated with a 0.35 μm CMOS technology. The maximum operating frequency is 62.5 MHz at 3.3 V. For a 50-word vocabulary, the estimated recognition time is about 0.16 s. Using noise-corrupted utterances, the recognition accuracy is 93.8% for isolated English digits. Such a performance is comparable to the software implementation with the same algorithm. Live recognition test was also run for a vocabulary of 11 Chinese words. The accuracy is 91.8% for five male and five female speakers.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Fixed-point implementation of isolated sub-word level speech recognition using hidden Markov models</title>
    <abstract>This paper presents a limited vocabulary isolated-word speech recognition system based on Hidden Markov Model (HMM) that involves two stage classification and is implemented on Texas Instruments' (TI) DaVinci embedded platform for a home infotainment system. A methodology using simple metric has been proposed for segmenting the words into sub-word units and these sub-words are used in the second stage to improve recognition accuracy. Also, a simple and efficient way of handling the out-of-vocabulary words using an additional HMM model is presented. We have achieved recognition accuracy of around 89% for a fixed point implementation on the TI DaVinci platform, demonstrating its suitability for embedded systems.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Generalised Fuzzy Hidden Markov Models for Speech Recognition</title>
    <abstract>A generalised fuzzy approach to statistical modelling techniques for speech recognition is proposed in this paper. Fuzzy C-means (FCM) and fuzzy entropy (FE) techniques are combined into a generalised fuzzy technique and applied to hidden Markov models (HMMs). A more robust version of the above fuzzy technique based on the noise clustering (NC) method is also proposed. Experimental results were performed on the TI46 speech data corpus. A significant result for isolatedword recognition performed on a highly confusable vocabulary consisting of the nine English E-set words is that, a 33.8% recognition error rate for the HMM-based system was reduced to 30.5%, 29.9%, 29.8% and 27.8%, respectively, by using the FCM-HMM, the FE-HMM, the NC-FE-HMM, and the NC-FCM-HMM-based systems.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Speech silicon: an FPGA architecture for real-time hidden Markov-model-based speech recognition</title>
    <abstract>This paper examines the design of an FPGA-based system-on-a-chip capable of performing continuous speech recognition on medium-sized vocabularies in real time. Through the creation of three dedicated pipelines, one for each of the major operations in the system, we were able to maximize the throughput of the system while simultaneously minimizing the number of pipeline stalls in the system. Further, by implementing a token-passing scheme between the later stages of the system, the complexity of the control was greatly reduced and the amount of active data present in the system at any time was minimized. Additionally, through in-depth analysis of the SPHINX 3 large vocabulary continuous speech recognition engine, we were able to design models that could be efficiently benchmarked against a known software platform. These results, combined with the ability to reprogram the system for different recognition tasks, serve to create a system capable of performing real-time speech recognition in a vast array of environments.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Feature extraction based on wavelet domain hidden markov tree model for robust speech recognition</title>
    <abstract>We present a new feature extraction method for robust speech recognition in the presence of additive white Gaussian noise The proposed method is made up of two stages in cascade The first stage is denoising process based on the wavelet domain hidden Markov tree model, and the second one is reduction of the influence of the residual noise in the filter bank analysis To evaluate the performance of the proposed method, recognition experiments were carried out for noisy speech with signal-to-noise ratio from 25 dB to 0 dB Experiment results demonstrate the superiority of the proposed method to the conventional ones.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Recognition of Visual Speech Elements Using Hidden Markov Models</title>
    <abstract>In this paper, a novel subword lip reading system using continuous Hidden Markov Models (HMMs) is presented. The constituent HMMs are configured according to the statistical features of lip motion and trained with the Baum-Welch method. The performance of the proposed system in identifying the fourteen visemes defined in MPEG-4 standards is addressed. Experiment results show that an average accuracy above 80% can be achieved using the proposed system.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Large-vocabulary speaker-independent continuous speech recognition with semi-continuous hidden Markov models</title>
    <abstract>A semi-continuous hidden Markov model based on the multiple vector quantization codebooks is used here for large-vocabulary speaker-independent continuous speech recognition In the techniques employed here, the semi-continuous output probability density function for each codebook is represented by a combination of the corresponding discrete output probabilities of the hidden Markov model and the continuous Gaussian density functions of each individual codebook. Parameters of vector quantization codebook and hidden Markov model are mutually optimized to achieve an optimal model codebook combination under a unified probabilistic framework Another advantages of this approach is the enhanced robustness of the semi-continuous output probability by the combination of multiple codewords and multiple codebooks For a 1000-word speaker-independent continuous speech recognition using a word-pair grammar the recognition error rate of the semi-continuous hidden Markov model was reduced by more than 29% and 41% in comparison to the discrete and continuous mixture hidden Markov model respectively</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A Fully Consistent Hidden Semi-Markov Model-Based Speech Recognition System</title>
    <abstract>In a hidden Markov model (HMM), state duration probabilities decrease exponentially with time, which fails to adequately represent the temporal structure of speech. One of the solutions to this problem is integrating state duration probability distributions explicitly into the HMM. This form is known as a hidden semi-Markov model (HSMM). However, though a number of attempts to use HSMMs in speech recognition systems have been proposed, they are not consistent because various approximations were used in both training and decoding. By avoiding these approximations using a generalized forward-backward algorithm, a context-dependent duration modeling technique and weighted finite-state transducers (WFSTs), we construct a fully consistent HSMM-based speech recognition system. In a speaker-dependent continuous speech recognition experiment, our system achieved about 9.1% relative error reduction over the corresponding HMM-based system.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Hybrid Model of Continuous Hidden Markov Model and Multi-Layer Perceptron in Speech Recognition</title>
    <abstract>In order to overcome shortcomings of basic hidden markov model (HMM), a hybrid model of multi-layer perceptron (MLP) and continuous hidden markov model (CHMM) is presented which bases on basic HMM. In this hybrid mode, MLP calculates each state’s output probability instead of CHMM. The main purpose of this model is to improve the recognition ratio of CHMM by means of the strong of MLP’s nonlinear predictive capability. Speaker independent mandarin digit speech recognition which based on the hybrid models is realized. Experimental results show that the hybrid model is efficiency and has higher recognition ratio than basic CHMM.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Hidden Markov model-based speech emotion recognition</title>
    <abstract>In this contribution we introduce speech emotion recognition by use of continuous hidden Markov models. Two methods are propagated and compared throughout the paper. Within the first method a global statistics framework of an utterance is classified by Gaussian mixture models using derived features of the raw pitch and energy contour of the speech signal. A second method introduces increased temporal complexity applying continuous hidden Markov models considering several states using low-level instantaneous features instead of global statistics. The paper addresses the design of working recognition engines and results achieved with respect to the alluded alternatives. A speech corpus consisting of acted and spontaneous emotion samples in German and English language is described in detail. Both engines have been tested and trained using this equivalent speech corpus. Results in recognition of seven discrete emotions exceeded 86% recognition rate. As a basis of comparison the similar judgment of human deciders classifying the same corpus at 79.8% recognition rate was analyzed.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Hybrid simulated annealing and its application to optimization of hidden Markov models for visual speech recognition</title>
    <abstract>We propose a novel stochastic optimization algorithm, hybrid simulated annealing (SA), to train hidden Markov models (HMMs) for visual speech recognition. In our algorithm, SA is combined with a local optimization. operator that substitutes a better solution for the current one to improve the convergence speed and the quality of solutions. We mathematically prove that the sequence of the objective values converges in probability to the global optimum in the algorithm. The algorithm is applied to train HMMs that are used as visual speech recognizers. While the popular training method of HMMs, the expectation-maximization algorithm, achieves only local optima in the parameter space, the proposed method can perform global optimization of the parameters of HMMs and thereby obtain solutions yielding improved recognition performance. The superiority of the proposed algorithm to the conventional ones is demonstrated via isolated word recognition experiments.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>A parallel implementation of a hidden Markov model with duration modeling for speech recognition</title>
    <abstract>This paper describes a parallel implementation of a Hidden Markov Model (HMM) for spoken language recognition on the MasPar MP-1. By exploiting the massive parallelism of explicit duration HMMs, we can develop more complex models for real-time speech recognition. Implementational issues such as choice of data structures, method of communication, and utilization of parallel functions are explored. The results of our experiments show that the parallelism in HMMs can be effectively exploited by the MP-1.Training that use to take nearly a week can now be completed in about an hour. The system can recognize the phones of a test utterance in a fraction of a second.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Combination of Vector Quantization and Hidden Markov Models for Arabic Speech Recognition</title>
    <abstract>Abstract: In this paper we present experiments we perform in order to recognize Arabic isolated words. Our recognition system is based on the combination of the vector quantization technique at the acoustical level and the Markovian modelling. The Hidden Markov Models (HMMs) are widely used in number of practical applications and especially suitable in speech recognition because of their ability to handle the variability of the speech signal. In our system, a word is analysed and represented as a set of acoustical vectors then transformed into a symbolic sequence, using the vector quantizer. This observation sequence is compared to the reference Markov models. The word associated to the model who obtained the highest score is declared the recognized word.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Partly hidden Markov model and its application to speech recognition</title>
    <abstract>A new pattern matching method, the partly hidden Markov model, is proposed and applied to speech recognition. The hidden Markov model, which is widely used for speech recognition, can deal with only piecewise stationary stochastic process. We solved this problem by introducing the modified second order Markov model, in which the first state is hidden and the second one is observable. In this model, not only the feature parameter observations but also the state transitions are dependent on the previous feature observation. Therefore, even the complicated transient can be modeled precisely. Some simulation experiments showed the high potential of the proposed model. From the results of the word recognition test is was observed that the error rate was reduced by 39% compared with the normal HMM.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Speech silicon AM: an FPGA-based acoustic modeling pipeline for hidden Markov model based speech recognition</title>
    <abstract>This paper presents the design of a FPGA-based hardware co-processor, based on the SPHINX 3 speech recognition engine from CMU; capable of performing Acoustic Modeling (AM) for medium sized vocabularies in real-time. By creating an input-driven pipeline for performing the calculations we were able to maximize the throughput of the system while simultaneously minimizing the number of pipeline stalls. Use advanced placement techniques enabled post place-and-route speeds even greater than those necessary for real-time operation while operating at maximum workload. Further, by using input control vectors all FSMs were removed from the design, greatly increasing the flexibility of the design. These results combined with the ability to reprogram the system for different recognition tasks serve to create a system capable of in a vast array of environments. Synthesis to both Xilinx Virtex 4 and Spartan 3 FPGAs helps to further characterize the flexibility of the architecture.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Noisy speech recognition using hidden Markov model state-based filtering</title>
    <abstract>A system which exploits this property of hidden Markov models (HMM) in order to implement effective noise canceling filters within the speech recognition task is described. The filtering process reduces the sensitivity of the recognition system to input fluctuations caused by noise. The system developed uses continuous probability density function, single Gaussian mixture HMMs trained on filterback output vectors. Using autocorrelation statistics collected for speech and noise during training, noise canceling Wiener filters are designed for each hidden Markov model state. The resulting system outperforms by a significant margin results obtained using clean-speech HMMs on either noisy speech or noisy speech with the noise mean subtracted.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Modeling state durations in hidden Markov models for automatic speech recognition</title>
    <abstract>Hidden Markov modeling (HMM) techniques have been used successfully for connected speech recognition in the last several years. In the traditional HMM algorithms the probability of duration of a state decreases exponentially with time which is not appropriate for representing the temporal structure of speech. Non-parametric modeling of duration using semi-Markov chains does accomplish the task with a large increase in the computational complexity. Applying a post processing state duration penalty after Viterbi decoding adds very little computation but does not affect the forward recognition path. In this paper we present a way of modeling state durations in HMM using time dependent state transitions. This new inhomogeneous HMM (IHMM) does increase the computation by a small amount but reduces recognition error rates by 14-25%. Also, a suboptimal implementation of this scheme that requires no more computation than the traditional HMM is presented which also has reduced errors by 14-22% on a variety of databases.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Non-linear spectral subtraction (NSS) and hidden Markov models for robust speech recognition in car noise environments</title>
    <abstract>
      Achieving reliable performance for a speech recogniser is an important challenge, especially in the context of mobile telephony applications where the user can access telephone functions through voice.

      This paper addresses the problem of speaker-dependent discrete utterance recognition in noise. Special reference is made to the mismatch effects due to the fact that training and testing are made in different environments.

      This contribution extends recently published work [11] where a robust HMM training / recognition framework is proposed. The present contribution introduces several new aspects: use of enhanced NSS schemes, introduction of root-MFCC parameters, use of dynamic features, training of HMMs by a dynamic inference scheme (DIHMM). These enhancements are discussed from tests performed on band limited signals (200-3000 Hz). We show that these various optimisations allow a rise from 20 % to over 99 % in performance. A 93% recognition rate is already achievable on raw data using a weighted modified projection and a root-MFCC dynamic representation.
    </abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Vector equalization in hidden Markov models for noisy speech recognition</title>
    <abstract>Speech recognizers often experience serious performance degradation when deployed in an unknown acoustic (particularly, noise contaminated) environment. To combat this problem, we proposed in a previous study a distortion measure that takes into account the norm shrinkage bias in the noisy cepstrum. In this paper, we incorporate a first order equalization mechanism, specifically aiming at avoiding the norm shrinkage problem, in a hidden Markov model (HMM) framework to model the speech cepstral sequence. Such a modeling technique requires special care as the formulation inevitably involves parameter estimation from a set of data with singular dispersion. We provide solutions to this HMM stochastic modeling problem and give algorithms for estimating the necessary model parameters. We experimentally show that incorporation of the first order norm equalization model makes the HMM-based speech recognizer robust to noise. With respect to a conventional HMM recognizer, this leads to an improvement in recognition performance which is equivalent to about 15-20 dB gain in signal-to-noise ratio.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Convolutional density estimation in hidden Markov models for speech recognition</title>
    <abstract>In continuous density hidden Markov models (HMMs) for speech recognition, the probability density function (PDF) for each state is usually expressed as a mixture of Gaussians. We present a model in which the PDF is expressed as the convolution of two densities. We focus on the special case where one of the convolved densities is a M-Gaussian mixture, and the other is a mixture of N impulses. We present the reestimation formulae for the parameters of the M/spl times/N convolutional model, and suggest two ways for initializing them, the residual K-Means approach, and the deconvolution from a standard HMM with MN Gaussians per state using a genetic algorithm to search for the optimal assignment of Gaussians. Both methods result in a compact representation that requires only /spl Oscr/(M+N) storage space for the model parameters, and O(MN) time for training and decoding. We explain how the decoding time can be reduced to O(M+kN), where k</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Speech recognition using hidden Markov model decomposition and a general background speech model</title>
    <abstract>HMM Decomposition is used for recognising speech in the presence of an interfering background speaker. The foreground speech is modelled by a set of left-to-right isolated word HMM's trained on a small isolated word database, and the background speech is modelled by a parallel ergodic HMM trained on a subset of TIMIT. The standard Output Approximation (OA) method of estimating the output probability distributions is used, and compared with a simple Model Combination technique (MC). Recent work in this area has shown the effectiveness of vocabulary specific background speech models, and hence this is used as a baseline. The results show that the general ergodic background model is as effective as a vocabulary specific model. However, the MC technique is not effective.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Continuously variable duration hidden Markov models for automatic speech recognition</title>
    <abstract>During the past decade, the applicability of hidden Markov models (HMM) to various facets of speech analysis has been demonstrated in several different experiments. These investigations all rest on the assumption that speech is a quasi-stationary process whose stationary intervals can be identified with the occupancy of a single state of an appropriate HMM. In the traditional form of the HMM, the probability of duration of a state decreases exponentially with time. This behavior does not provide an adequate representation of the temporal structure of speech. The solution proposed here is to replace the probability distributions of duration with continuous probability density functions to form a continuously variable duration hidden Markov model (CVDHMM). The gamma distribution is ideally suited to specification of the durational density since it is one-sided and only has two parameters which, together, define both mean and variance. The main result is a derivation and proof of convergence of re-estimation formulae for all the parameters of the CVDHMM. It is interesting to note that if the state durations are gamma-distributed, one of the formulae is non-algebraic but, fortuitously, has properties such that it is easily and rapidly solved numerically to any desired degree of accuracy. Other results are presented including the performance of the formulae on simulated data.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Robust Speech Recognition Using Neural Networks and Hidden Markov Models</title>
    <abstract>This paper proposes a robust, speaker-independent isolated word speech recognition (IWSR) system (SMQ/HMM_SVQ/HMM)/MLP which combines Dual Split Matrix Quantization (SMQ) and Split Vector Quantization (SVQ) pair combined with both the strength of HMM in modeling stochastic sequences and the non-linear classification capability of MLP neural networks (NN). The system efficiently utilizes processing resources and improves speech recognition performance by using neural networks as the classifier of the system. Computer simulation clearly indicates superiority over conventional VQ/HMM and MQ/HMM systems with 98% and 95.8% recognition accuracy at 20 dB and 5 dB SNR levels, respectively in a car noise environment, based on the database TIDIGIT.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Bayesian large margin hidden Markov models for speech recognition</title>
    <abstract>This paper presents a Bayesian learning approach to large margin classifier for hidden Markov model (HMM) based speech recognition. We build the Bayesian large margin HMMs (BLM-HMMs) and improve the model generalization for handling unknown test environments. Using BLM-HMMs, the variational Bayesian HMM parameters are estimated by maximizing lower bound of a marginal likelihood over the uncertainties of HMM parameters. The Bayesian large margin estimation is performed with frame selection mechanism, and is illustrated to meet the objective of support vector machines, i.e. maximal class margin and minimal training errors. The new objective function is not only interpreted as a discriminative criterion, but also feasible to deal with model selection and adaptive training. Experiments on phone recognition show that BLM-HMMs perform better than other generative and discriminative models.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Hidden Markov model/neural network training techniques for connected alphadigit speech recognition</title>
    <abstract>A neural network formulation for an HMM (hidden Markov model) is presented, and training using maximum likelihood, maximum mutual information, minimum mean-squared-error (MMSE), and unconstrained MMSE is described. Recognition results are presented for the variously trained models evaluated on a speaker-independent, connected alphadigit speech recognition task. It is concluded that viewing neural networks as HMMs provides a framework for building temporally dependent neural networks, while viewing HMMs as neural networks broadens the class of natural training methods. Despite several drawbacks, performance results indicate that models trained with error-correcting criteria on sufficient amounts of data may do better at discriminating similar sounds.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Recognition of Vowel Segments in Spanish Esophageal Speech Using Hidden Markov Models</title>
    <abstract>This paper presents a system for recognition of voiced segments in Spanish esophageal speech. It exposes different algorithms for the feature extraction of speech segment like formant analysis, linear prediction coefficients (LPC) and Mel Frequency Cepstral Coefficients (MFCC), as well as, the recognition stage through the Hidden Markov Models. Simulation results are presented for normal and esophageal speech. The system was implemented in a real time processing platform based on a digital signal processor TMS320C5416 of Texas Instruments.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Visual speech recognition using active shape models and hidden Markov models</title>
    <abstract>This paper describes a novel approach for visual speech recognition. The shape of the mouth is modelled by an active shape model which is derived from the statistics of a training set and used to locate, track and parameterise the speaker's lip movements. The extracted parameters representing the lip shape are modelled as continuous probability distributions and their temporal dependencies are modelled by hidden Markov models. We present recognition tests performed on a database of a broad variety of speakers and illumination conditions. The system achieved an accuracy of 85.42% for a speaker independent recognition task of the first four digits using lip shape information only.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Application of a weighted projection measure for robust hidden Markov model based speech recognition</title>
    <abstract>The use of a projection-based cepstral measure for speech recognition in noise is investigated. Interpretations of the measure's spectral and perceptual significance are given, along with its application to cepstral, melcepstral, and delta parameters. It is shown how the projection measure can be incorporated into a continuous density hidden Markov model system in the form of a weighted measure. Both the case where these densities are unimodal Gaussians and mixtures of Gaussians are addressed. In recognition experiments, the weighted projection measure significantly outperformed the standard weighted Euclidean or Gaussian distance measure.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A comparative study of continuous speech recognition using neural networks and hidden Markov models</title>
    <abstract>The recognition performances of two front ends are compared for two continuous speech recognition tasks. First, a neural network model (NNM) front end was used, with frame labeling performed by a radial basis function network and segmentation by a Viterbi algorithm. The second front end was a discrete hidden Markov model (HMM), featuring explicit state duration probability distributions. Two experiments were performed. The first used a speaker-dependent database, with a lexicon of 571 words. Using a low-perplexity grammar, the NNM front end produced a word accuracy of 94% and a sentence accuracy of 86%. This was slightly inferior to the HMM front end, which produced word accuracies of 96% and sentence accuracies of 88%. Without a grammar, word accuracies of 58% (NNM) and 49% (HMM) were recorded. The second set of experiments used the MIT portion of the TIMIT database (415 speakers and 2072 sentences in total). Results were poor for both front ends, with the NNM producing marginally better results.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Modeling improvement of the continuous hidden Markov model for speech recognition</title>
    <abstract>In this paper, we improve the modeling of the conventional continuous HMM for speech recognition in two aspects. One is to use new functions to model the phonetic duration distribution. These functions can well approximate the unsymmetrical distribution and have relatively simple forms for computation. The other aspect is to use proportional coefficient to adjust dimensional effects of the output density functions and the phonetic duration functions in the HMM. Using these new techniques, we got 7.8% improvement of the recognition correct rate in the vowel recognition experiments of the continuous speech.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Implementing a Hidden Markov Model Speech Recognition System in Programmable Logic</title>
    <abstract>Performing Viterbi decoding for continuous real-time speech recognition is a highly computationally-demanding task, but is one which can take good advantage of a parallel processing architecture. To this end, we describe a system which uses an FPGA for the decoding and a PC for pre- and post-processing, taking advantage of the properties of this kind of programmable logic device, specifically its ability to perform in parallel the large number of additions and comparisons required. We compare the performance of the FPGA decoder to a software equivalent, and discuss issues related to this implementation.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Large-Margin Discriminative Training of Hidden Markov Models for Speech Recognition</title>
    <abstract>Discriminative training has been a leading factor for improving automatic speech recognition (ASR) performance over the last decade. The traditional discriminative training, however, has been aimed to minimize empirical error rates on training sets, which may not be well generalized to test sets. Many attempts have been made recently to incorporate the principle of large margin (PLM) into the training of hidden Markov models (HMMs) in ASR to improve the generalization abilities. Significant error rate reduction on the test sets has been observed on both small vocabulary and large vocabulary continuous ASR tasks using large-margin discriminative training (LMDT) techniques. In this paper, we introduce the PLM, define the concept of margin in the HMMs, and survey a number of popular LMDT algorithms proposed and developed recently. Specifically, we review and compare the large-margin minimum classification error (LM-MCE) estimation, soft-margin estimation (SME), large margin estimation (LME), large relative margin estimation (LRME), and large margin training (LMT) with a focus on the insights, the training criteria, the optimization techniques used, and the strengths and weaknesses of these different approaches. We suggest future research directions in our conclusion of this paper.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Speech Recognition Using Hidden Markov Model with MFCC-Subband Technique</title>
    <abstract>This paper presents an approach to the recognition of speech signal using frequency spectral information with mel frequency for the improvement of speech feature representation in a HMM based recognition approach. The mel frequency approach exploits the frequency observation for speech signal in a given resolution which results in resolution feature overlapping resulting in recognition limit. Resolution decomposition with frequency mapping approach for a HMM based speech recognition system. The Simulation results show a improvement in the quality metrics of speech recognition wrt. to computational time, learning accuracy for a speech recognition system.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Hardware for hidden Markov-model-based, large-vocabulary real-time speech recognition</title>
    <abstract>SRI and U.C. Berkeley have begun a cooperative effort to develop a new architecture for real-time implementation of spoken language systems (SLS). Our goal is to develop fast speech recognition algorithms, and supporting hardware capable of recognizing continuous speech from a bigram- or trigram-based 20,000-word vocabulary or a 1,000- to 5,000-word SLS.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Speech recognition using temporally connected kernels in mixture density hidden Markov models</title>
    <abstract>A method is presented for speeding up the performance of the HMM based speech recognition system where the states are modeled by a large number of Gaussian kernels. The emission probabilities of the states are usually dominated by the nearest Gaussians to the input vector. The speedup is gained without deteriorating the recognition accuracy by concentrating on these kernels in the reduced K-best-kernel search. In this work, the time information of the input is encoded to the connections of the kernels. The search for the dominating kernels is then performed along the kernel connections which model the trajectories of the speech in the feature space. In the experiments, speaker-dependent speech recognizers were trained for ten speakers. The number of distance computations between feature vectors and kernel mean vectors was reduced 75% without increasing the average phoneme recognition error, which was 5.7% for the baseline system.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Telephone speech recognition using neural networks and hidden Markov models</title>
    <abstract>The performance of well-trained speech recognizers using high quality full bandwidth speech data is usually degraded when used in real world environments. In particular, telephone speech recognition is extremely difficult due to the limited bandwidth of the transmission channels. In this paper, neural network based adaptation methods are applied to telephone speech recognition and a new unsupervised model adaptation method is proposed. The advantage of the neural network based approach is that the retraining of speech recognizers for telephone speech is avoided. Furthermore, because the multi-layer neural network is able to compute nonlinear functions, it can accommodate for the non-linear mapping between full bandwidth speech and telephone speech. The new unsupervised model adaptation method does not require transcriptions and can be used with the neural networks. Experimental results on TIMIT/NTIMIT corpora show that the performance of the proposed methods is comparable to that of recognizers retained on telephone speech.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Environment-independent continuous speech recognition using neural networks and hidden Markov models</title>
    <abstract>Environment-independent continuous speech recognition is important for the successful development of speech recognizers in real world applications. Linear compensation methods do not work well if the mismatches between training; and testing environments are not linear. In this paper, a neural network compensation technique is explored to mitigate the distortion resulting from additive noise, distant-talking, or telephone channels. The advantage of the neural network compensation method is that retraining of a speech recognizer for each particular application is avoided. Furthermore, since neural networks are trained to transform distorted speech feature vectors to those corresponding to clean speech, it may outperform a retrained speech recognizer trained on distorted speech. Three experiments are conducted to evaluate the capability of the neural network compensation method; recognition of additive noisy speech, distant-talking speech, and telephone speech.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>An Omnifont Open-Vocabulary OCR System for English and Arabic</title>
    <abstract>We present an omnifont, unlimited-vocabulary OCR system for English and Arabic. The system is based on Hidden Markov Models (HMM), an approach that has proven to be very successful in the area of automatic speech recognition. In this paper we focus on two aspects of the OCR system. First, we address the issue of how to perform OCR on omnifont and multi-style data, such as plain and italic, without the need to have a separate model for each style. The amount of training data from each style, which is used to train a single model, becomes an important issue in the face of the conditional independence assumption inherent in the use of HMMs. We demonstrate mathematically and empirically how to allocate training data among the different styles to alleviate this problem. Second, we show how to use a word-based HMM system to perform character recognition with unlimited vocabulary. The method includes the use of a trigram language model on character sequences. Using all these techniques, we have achieved character error rates of 1.1 percent on data from the University of Washington English Document Image Database and 3.3 percent on data from the DARPA Arabic OCR Corpus.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Training Augmented Models Using SVMs</title>
    <abstract>There has been significant interest in developing new forms of acoustic model, in particular models which allow additional dependencies to be represented than those contained within a standard hidden Markov model (HMM). This paper discusses one such class of models, augmented statistical models. Here, a local exponential approximation is made about some point on a base model. This allows additional dependencies within the data to be modelled than are represented in the base distribution. Augmented models based on Gaussian mixture models (GMMs) and HMMs are briefly described. These augmented models are then related to generative kernels, one approach used for allowing support vector machines (SVMs) to be applied to variable length data. The training of augmented statistical models within an SVM, generative kernel, framework is then discussed. This may be viewed as using maximum margin training to estimate statistical models. Augmented Gaussian mixture models are then evaluated using rescoring on a large vocabulary speech recognition task.</abstract>
    <search_task_number>9</search_task_number>
    <query>speech recognition hidden markov model</query>
    <relevance>0</relevance>
  </item>



  <item>
    <title>Evaluation of spam detection and prevention frameworks for email and image spam: a state of art</title>
	<abstract>In recent years, online spam has become a major problem for the sustainability of the Internet. Excessive amounts of spam are not only reducing the quality of information available on the Internet but also creating concern amongst search engines and web users. This paper aims to analyse existing works in two different categories of spam domains - email spam and mage spam to gain a deeper understanding of this problem. Future reserch directions are also presented in these spam domains.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>TrackBack spam: abuse and prevention</title>
	<abstract>Contemporary blogs receive comments and TrackBacks, which result in cross-references between blogs. We conducted a longitudinal study of TrackBack spam, collecting and analyzing almost 10 million samples from a massive spam campaign over a one-year period. Unlike common delivery of email spam, the spammers did not use bots, but took advantage of an official Chinese site as a relay. Based on our analysis of TrackBack misuse found in the wild, we propose an authenticated TrackBack mechanism that defends against TrackBack spam even if attackers use a very large number of different source addresses and generate unique URLs for each TrackBack blog.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Catching spam before it arrives: domain specific dynamic blacklists</title>
	<abstract>The arrival of any piece of unsolicited and unwanted email (spam) into a user's email inbox is a problem. It results in real costs to organisations and possibly an increasing reluctance to use email by some users. Currently most spam prevention techniques rely on methods that examine the whole email message at the mail server. This paper describes research that aims to deny spam entry into the internal network in the first place.Examination of live amalgamated audit logs from a Linux kernel firewall, the PortSentry intrusion detection system and the Sendmail mail transfer agents has shown that it is possible that automated mailing programs send characteristic probes to the network gateway just before launching an avalanche of mail. Similarly it seems possible to detect precursor activity from some potential zombie machines. A real time system that could detect such activity needs to be certain that a particular IP address is about to send spam before blocking all of its packets at the network gateway. The architecture for a system that establishes certainty that a particular IP address is about to or has started sending spam is described in this paper. The eventual aim is to recognise precursor activity from spammers in real time, establish certainty that this IP address is about to send or is currently sending spam packets and to then deny packets from this IP address at a range of communicating gateways</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A social-spam detection framework</title>
	<abstract>Social networks such as Facebook, MySpace, and Twitter have become increasingly important for reaching millions of users. Consequently, spammers are increasing using such networks for propagating spam. Existing filtering techniques such as collaborative filters and behavioral analysis filters are able to significantly reduce spam, each social network needs to build its own independent spam filter and support a spam team to keep spam prevention techniques current. We propose a framework for spam detection which can be used across all social network sites. There are numerous benefits of the framework including: 1) new spam detected on one social network, can quickly be identified across social networks; 2) accuracy of spam detection will improve with a large amount of data from across social networks; 3) other techniques (such as blacklists and message shingling) can be integrated and centralized; 4) new social networks can plug into the system easily, preventing spam at an early stage. We provide an experimental study of real datasets from social networks to demonstrate the flexibility and feasibility of our framework.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Tribler: P2P media search and sharing</title>
	<abstract>Tribler is an open-source software project facilitating search, streaming and sharing content using P2P technology. Over 800,000 people have used Tribler since the project started in 2005. The Tribler P2P core supports BitTorrent-compatible downloading, video on demand and live streaming. Aside from a regular desktop GUI that runs on multiple OSes, it can be installed as a browser plug-in, currently used by Wikipedia. Aditionally, it runs on a 450~MHz processor, showcasing future TV support. We continuously work on extensions and test out novel research ideas within our user base, resulting in sub-second content search, a reputation system for rewarding upload, and channels for content publishing and spam prevention. Presently, 1200 channels have been created, enabling rich multimedia communities without requiring any server.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Incorporating accountability into internet email</title>
	<abstract>Email used to be the "number one killer application" of the Internet. However, misuse and abuse such as spam, phishing, and malware attacks have plagued the email systems. Considering deterrence as important as prevention and protection in countering misuse and abuse, we aim to improve the accountability in the email system beyond identification and non-repudiability. Full accountability should be an intrinsic condition for trust, and it constitutes the basis of deterrence against email misuse and abuse. Therefore, we propose to use a layered trust management framework to help email receivers eliminate their unwitting trust and provide them with accountability support. This helps systems to deter misuses and address wrongdoings. By describing and analyzing how our trust management facilitates email accountability, we also show that it can be used to improve the trustworthiness of the Internet services as a whole.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Small trusted primitives for dependable systems</title>
	<abstract>Secure, fault-tolerant distributed systems are difficult to build, to validate, and to operate. Conservative design for such systems dictates that their security and fault tolerance depend on a very small number of assumptions taken on faith; such assumptions are typically called the "trusted computing base" (TCB) of a system. However, a rich trade-off exists between larger TCBs and more secure, more faulttolerant, or more efficient systems. In our recent work, we have explored this trade-off by defining "small," generic trusted primitives--for example, an attested, monotonically sequenced FIFO buffer of a few hundred machine words guaranteed to hold appended words until eviction and showing how such primitives can improve the performance, fault tolerance, and security of systems using them. In this article, we review our efforts in generating simple trusted primitives such as an attested circular buffer (called Attested Appendonly Memory), and an attested human activity detector. We describe the benefits of using these primitives to increase the fault-tolerance of replicated systems and archival storage, and to improve the security of email SPAM and click-fraud prevention systems. Finally, we share some lessons we have learned from this endeavor.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Statistical analysis of honeypot data and building of Kyoto 2006+ dataset for NIDS evaluation</title>
	<abstract>With the rapid evolution and proliferation of botnets, large-scale cyber attacks such as DDoS, spam emails are also becoming more and more dangerous and serious cyber threats. Because of this, network based security technologies such as Network based Intrusion Detection Systems (NIDSs), Intrusion Prevention Systems (IPSs), firewalls have received remarkable attention to defend our crucial computer systems, networks and sensitive information from attackers on the Internet. In particular, there has been much effort towards high-performance NIDSs based on data mining and machine learning techniques. However, there is a fatal problem in that the existing evaluation dataset, called KDD Cup 99' dataset, cannot reflect current network situations and the latest attack trends. This is because it was generated by simulation over a virtual network more than 10 years ago. To the best of our knowledge, there is no alternative evaluation dataset. In this paper, we present a new evaluation dataset, called Kyoto 2006+, built on the 3 years of real traffic data (Nov. 2006 ~ Aug. 2009) which are obtained from diverse types of honeypots. Kyoto 2006+ dataset will greatly contribute to IDS researchers in obtaining more practical, useful and accurate evaluation results. Furthermore, we provide detailed analysis results of honeypot data and share our experiences so that security researchers are able to get insights into the trends of latest cyber attacks and the Internet situations.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A survey of emerging approaches to spam filtering</title>
	<abstract>From just an annoying characteristic of the electronic mail epoch, spam has evolved into an expensive resource and time-consuming problem. In this survey, we focus on emerging approaches to spam filtering built on recent developments in computing technologies. These include peer-to-peer computing, grid computing, semantic Web, and social networks. We also address a number of perspectives related to personalization and privacy in spam filtering. We conclude that, while important advancements have been made in spam filtering in recent years, high performance approaches remain to be explored due to the large scale of the problem.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Advantages and vulnerabilities of pull-based email-delivery</title>
	<abstract>Over the last decade spam has become a serious problem to email-users all over the world. Most of the daily email-traffic consists of this unwanted spam. There are various methods that have been proposed to fight spam, from IP-based blocking to filtering incoming email-messages. However it seems that it is impossible to overcome this problem as the number of email-messages that are considered spam is increasing. But maybe these techniques target the problem at the wrong side: it is the email-delivery protocol itself that fosters the existence of spam. What once was created to make internet-mail communication as easy and as reliable as possible became abused by modern day spammers. This paper proposes a different approach: instead of accepting all messages unquestioned it introduces a way to empower the receiver by giving him the control to decide if he wants to receive a message or not. By extending SMTP to pull messages instead of receiving them an attempt to stem the flood of spam is made. The pull-based approach works without involvement of the end-users. However this new system does not come without a price: it opens the possibility of a distributed denial of service (DDOS)-attacks against legitimate mail-transfer agents. This vulnerability and possible ways to overcome it are also discussed in this paper.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Protection through multimedia CAPTCHAs</title>
	<abstract>CAPTCHAS which are well known as complete automatic public Turing test to tell computers and humans apart are a modern implementation of the Turing test, which ask a series of questions of two players: a human and computer. But both of the players pretend to be human. On the bases of the answers the judge has to decide which one is human and which one is computer, but the judge itself is a computer. In this article, we review current CAPTCHAs. After analysis of all the current CAPTCHAS we propose a new 3-D AI CATCHA which has all the strengths of existing CAPTCHAS to provide a better security alternative for ecommerce.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Polynomial factorization: a success story</title>
	<abstract>The problem of factoring a polynomial in a single or several variables over a finite field, the rational numbers or the complex numbers is one of the success stories in the discipline of symbolic computation. In the early 1960s implementors investigated the constructive methods known from classical algebra books, but--with the exception of Gauss's distinct degree factorization algorithm--found the algorithms quite inefficient in practice [16]. The contributions in algorithmic techniques that have been made over the next 40 years are truly a hallmark of symbolic computation research.

The early pioneers, Berlekamp, Musser, Wang, Weinberger, Zassenhaus and others applied new ideas like randomization, that even before the now famous algorithms for primality testing by Rabin and Strassen, and like generic programming with coefficient domains as abstract data classes, and they introduced the powerful Hensel lifting lemma to computer algebra. We note that while de-randomization for integer primality testing has been accomplished recently [1], the same remains open for the problem of computing a root of a polynomial modulo a large prime [12, Research Problem 14.48].

Polynomial-time complexity for rational coefficients was established in the early 1980s by the now-famous lattice basis reduction algorithm of A. Lenstra, H. W. Lenstra, Jr., and L. Lovász. The case of many variables first became an application of the DeMillo and Lipton/Schwartz/Zippel lemma [30] and then triggered a fundamental generalization from the standard sparse (distributed) representation of polynomials to the one by straight line and black box programs [11, 17, 19]. Effective versions of the Hilbert irreducibility theorem are needed for the probabilistic analysis, which serendipitously later have also played a role in the PCP characterization of <i>NP</i> [2]. Unlike many other problems in commutative algebra and algebraic geometry, such as algebraic system solving, the polynomial factoring problem is of probabilistic polynomial-time complexity in the number of variables.

Complex coefficients in multivariate factors can be represented either by exact algebraic numbers or by imprecise floating point numbers. The latter formulation is a cornerstone in the new computer algebra subject of SNAP (Symbolic-Numeric Algorthms for Polynomials) (see, e.g., [4]). The approaches for both exact and imprecise coefficients are manifold, including Ruppert's partial differential equations [26, 27, 6, 10] and Gao's and Lauder's far-reaching generalization of Eisenstein's criterion in the multivariate case to Newton polytope decomposition [8, 9]. The currently best algorithms were all discovered recently within the past ten years.

The baby steps/giant steps technique and fast distinct and equal degree factorization implementations have, at last, yielded in the mid 1990s theoretical and practical improvements over the original univariate Berlekamp algorithm for coefficients in finite fields [13, 29, 18, 3]. The average time analysis for selected algorithms is also completed [5]. For bivariate polynomials over finite fields, surprisingly Gröbner basis techniques are useful in practice [23].

New polynomial-time complexity results are the computation of low degree factors of very high degree sparse (lacunary) polynomials by H. W. Lenstra, Jr. [20, 21], and the deterministic distinct degree factorization for multivariate polynomials over large finite fields [7]. However, many problems with high degree polynomials over large finite fields in sparse or straight line program representations, such as computing a root modulo a large prime, are not known to be in random polynomial time or NP-hard (cf. [24, 25, 15]).

Finally, in 2000 Mark van Hoeij [14] reintroduced lattice basis reduction, now in the Berlekamp-Zassenhaus algorithm, to conquer the hard-to-factor Swinnerton-Dyer polynomials in practice. Sasaki in 1993 had already hinted of the used approach [28].

In my talk I will discuss a selection of the highlights, state remaining open problems, and give some applications including an unusual one due to Moni Naor [22].</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Comment spam detection by sequence mining</title>
	<abstract>Comments are supported by several web sites to increase user participation. Users can usually comment on a variety of media types - photos, videos, news articles, blogs, etc. Comment spam is one of the biggest challenges facing this feature. The traditional approach to combat spam is to train classifiers using various machine learning techniques. Since the commonly used classifiers work on the entire comment text, it is easy to mislead them by embedding spam content in good content.

In this paper, we make several contributions towards comment spam detection. (1) We propose a new framework for spam detection that is immune to embed attacks. We characterize spam by a set of frequently occurring sequential patterns. (2) We introduce a variant (called min-closed) of the frequent closed sequence mining problem that succinctly captures all the frequently occurring patterns. We prove as well as experimentally show that the set of min-closed sequences is an order of magnitude smaller than the set of closed sequences and yet has exactly the same coverage. (3) We describe MCPRISM, extension of the recently published PRISM algorithm that effectively mines min-closed sequences, using prime encoding. In the process, we solve the open problem of using the prime-encoding technique to speed up traditional closed sequence mining. (4) We finally need to whittle down the set of frequent subsequences to a small set without sacrificing coverage. This problem is NP-Hard but we show that the coverage function is submodular and hence the greedy heuristic gives a fast algorithm that is close to optimal. We then describe the experiments that were carried out on a large real world comment data and the publicly available Gazelle dataset. (1) We show that nearly 80% of spam on real world data can be effectively captured by the mined sequences at very low false positive rates. (2) The sequences mined are highly discriminative. (3) On Gazelle data, the proposed algorithmic enhancements are faster by at least by a factor and by an order of magnitude on the larger comment dataset.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The anti-social tagger: detecting spam in social bookmarking systems</title>
	<abstract>The annotation of web sites in social bookmarking systems has become a popular way to manage and find information on the web. The community structure of such systems attracts spammers: recent post pages, popular pages or specific tag pages can be manipulated easily. As a result, searching or tracking recent posts does not deliver quality results annotated in the community, but rather unsolicited, often commercial, web sites. To retain the benefits of sharing one's web content, spam-fighting mechanisms that can face the flexible strategies of spammers need to be developed.

A classical approach in machine learning is to determine relevant features that describe the system's users, train different classifiers with the selected features and choose the one with the most promising evaluation results. In this paper we will transfer this approach to a social bookmarking setting to identify spammers. We will present features considering the topological, semantic and profile-based information which people make public when using the system. The dataset used is a snapshot of the social bookmarking system BibSonomy and was built over the course of several months when cleaning the system from spam. Based on our features, we will learn a large set of different classification models and compare their performance. Our results represent the groundwork for a first application in BibSonomy and for the building of more elaborate spam detection mechanisms.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Fighting the spam wars: A remailer approach with restrictive aliasing</title>
	<abstract>We present an effective method of eliminating unsolicited electronic mail (so-called spam) and discuss its publicly accessible prototype implementation. A subscriber to our system is able to obtain an unlimited number of aliases of his/her permanent (protected) E-Mail address to be handed out to parties willing to communicate with the subscriber. It is also possible to set up publishable aliases, which can be used by human correspondents to contact the subscriber, while being useless to harvesting robots and spammers. The validity of an alias can be easily restricted to a specific duration in time, a specific number of received messages, a specific population of senders, and/or in other ways. The system is fully compatible with the existing E-Mail infrastructure and can be immediately accessed via any standard E-Mail client software (MUA). It can be easily deployed at any institution or organization running its private E-Mail server (MTA) with a trivial modification to that server. Our system offers a simple method to salvage the existing population of E-Mail addresses while eliminating all spam aimed at them.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Email filters can adversely affect free and open flow of communication</title>
	<abstract>With the unprecedented growth and widespread nature of computer viruses and worms spreading through electronic mail systems, network administrators have developed and installed filters to stem the flow of unsolicited and dangerous email. However, in the vigilant atmosphere of today due to malicious software such as SoBig.F, email filters can have unintended consequences. Examples of such consequences range from blocking of legitimate email to blacklisting senders accused of spam or unsolicited, mass-mailing. In this paper, we will provide a brief overview of the topic in order to depict the adverse, albeit inadvertent, consequences of email filters on the free and open flow of communication among network users. We will also provide several suggestions on resolving the problems.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Using cross-media relations to identify important communication requests: testing the concept and implementation</title>
	<abstract>Important calls that originate from persons or organizations connected to the callee with weak social ties are often mistakenly labeled as unwanted ("spam", "SPIT") since their contact address is not found in the callee's address book. We have focused on the fact that the weak social ties are usually established through other communication means such as a web transaction for online purchase, travel reservation, or social media. We hypothesize that prior contact is a helpful distinguishing feature between important (or non-spam) calls and unwanted ones, and have proposed two mechanisms using cross-media relations such as web-then-call and email-then-call. One proposed mechanism is to help the callee collect addresses of potential callers in order to determine whether or not to answer a call using a caller ID. Another is to help the callee identify important calls using a proof of prior contact. We introduce an implementation of our proposed mechanisms called CURE (Controlling Unwanted REquests) system as proof of concept. We also conducted an initial survey of email messages as call detail records substitutes in order to help prove our hypothesis. This survey shows that 52% of all incoming email messages are triggered by web-then-email relations, but carried unknown sender IDs. This result demonstrates that using cross-media relations can be potentially effective in identifying important calls and would be useful as an additional component of a spam filtering system.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The social bookmark and publication management system bibsonomy</title>
	<abstract>Social resource sharing systems are central elements of the Web 2.0 and use the same kind of lightweight knowledge representation, called folksonomy. Their large user communities and ever-growing networks of user-generated content have made them an attractive object of investigation for researchers from different disciplines like Social Network Analysis, Data Mining, Information Retrieval or Knowledge Discovery. In this paper, we summarize and extend our work on different aspects of this branch of Web 2.0 research, demonstrated and evaluated within our own social bookmark and publication sharing system BibSonomy, which is currently among the three most popular systems of its kind. We structure this presentation along the different interaction phases of a user with our system, coupling the relevant research questions of each phase with the corresponding implementation issues. This approach reveals in a systematic fashion important aspects and results of the broad bandwidth of folksonomy research like capturing of emergent semantics, spam detection, ranking algorithms, analogies to search engine log data, personalized tag recommendations and information extraction techniques. We conclude that when integrating a real-life application like BibSonomy into research, certain constraints have to be considered; but in general, the tight interplay between our scientific work and the running system has made BibSonomy a valuable platform for demonstrating and evaluating Web 2.0 research.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Secure distributed human computation</title>
	<abstract>This paper is a preliminary exploration of secure distributed human computation. We consider the general paradigm of using large-scale distributed computation to solve difficult problems, but where humans can act as agents and provide candidate solutions. We are especially motivated by problem classes that appear to be difficult for computers to solve effectively, but are easier for humans; e.g., image analysis, speech recognition, and natural language processing. This paradigm already seems to be employed in several real-world scenarios, but we are unaware of any formal and unified attempt to study it. Nonetheless, this concept spawns interesting research questions in algorithm design, human computer interfaces, and programming language / API design, distributed systems, and cryptography, among other fields. There are also interesting implications for Internet commerce and the B24b model. We describe this research area and suggest a basic framework for the design of such systems. We analyze security and reliability against malicious parties using standard probability theory tools. We then derive design principles using standard decision-theory concepts. Finally, we list extensions and open problems.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A Conversation with Phil Smoot</title>
	<abstract>In the landscape of today’s megaservices, Hotmail just might be Mount Everest. One of the oldest free Web e-mail services, Hotmail relies on more than 10,000 servers spread around the globe to process billions of e-mail transactions per day. What’s interesting is that despite this enormous amount of traffic, Hotmail relies on less than 100 system administrators to manage it all.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>SELS: a secure e-mail list service</title>
	<abstract>Exchange of private information content among a large number of users via E-mail List Services is becoming increasingly common. In this paper we address security requirements in that setting and develop a new protocol, SELS (a Secure E-mail List Service) that provides confidentiality, integrity, and authentication for e-mails exchanged via lists. In addition, SELS also protects against the use of lists for e-mail spamming. We have developed a prototype of SELS in Java, and integrated it with the Eudora e-mail client.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Preventing Spams and Relays</title>
	<abstract>The smtpd package is a useful mail demon for stopping spam, thereby saving money and resources</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Business email: the killer impact</title>
	<abstract>Workplace email is quickly evolving to keep up with those who use it---and perhaps to make way for the next killer app.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A pact with the devil</title>
	<abstract>We study malware propagation strategies which exploit not the incompetence or naivety of users, but instead their own greed, malice and short-sightedness. We demonstrate that interactive propagation strategies, for example bribery and blackmail of computer users, are effective mechanisms for malware to survive and entrench, and present an example employing these techniques. We argue that in terms of propagation, there exists a continuum between legitimate applications and pure malware, rather than a quantised scale.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Understanding block-level address usage in the visible internet</title>
	<abstract>Although the Internet is widely used today, we have little information about the edge of the network. Decentralized management, firewalls, and sensitivity to probing prevent easy answers and make measurement difficult. Building on frequent ICMP probing of 1% of the Internet address space, we develop clustering and analysis methods to estimate how Internet addresses are used. We show that adjacent addresses often have similar characteristics and are used for similar purposes (61% of addresses we probe are consistent blocks of 64 neighbours or more). We then apply this block-level clustering to provide data to explore several open questions in how networks are managed. First, we provide information about how effectively network address blocks appear to be used, finding that a significant number of blocks are only lightly used (most addresses in about one-fifth of 24 blocks are in use less than 10% of the time), an important issue as the IPv4 address space nears full allocation. Second, we provide new measurements about dynamically managed address space, showing nearly 40% of 24 blocks appear to be dynamically allocated, and dynamic addressing is most widely used in countries more recent to the Internet (more than 80% in China, while less than 30% in the U.S.). Third, we distinguish blocks with low-bitrate last-hops and show that such blocks are often underutilized.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Better, Faster, More Secure</title>
	<abstract>Since I started a stint as chair of the IETF (Internet Engineering Task Force) in March 2005, I have frequently been asked, “What’s coming next?” but I have usually declined to answer. Nobody is in charge of the Internet, which is a good thing, but it makes predictions difficult (and explains why this article starts with a disclaimer: It represents my views alone and not those of my colleagues at either IBM or the IETF).</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Efficient Tag Recommendation for Real-Life Data</title>
	<abstract>Despite all of the advantages of tags as an easy and flexible information management approach, tagging is a cumbersome task. A set of descriptive tags has to be manually entered by users whenever they post a resource. This process can be simplified by the use of tag recommendation systems. Their objective is to suggest potentially useful tags to the user. We present a hybrid tag recommendation system together with a scalable, highly efficient system architecture. The system is able to utilize user feedback to tune its parameters to specific characteristics of the underlying tagging system and adapt the recommendation models to newly added content. The evaluation of the system on six real-life datasets demonstrated the system’s ability to combine tags from various sources (e.g., resource content or tags previously used by the user) to achieve the best quality of recommended tags. It also confirmed the importance of parameter tuning and content adaptation. A series of additional experiments allowed us to better understand the characteristics of the system and tagging datasets and to determine the potential areas for further system development.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A view of cloud computing</title>
	<abstract>Clearing the clouds away from the true potential and obstacles posed by this computing capability.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Towards a readiness model for health 2.0</title>
	<abstract>Enterprise 2.0 applications are changing the way organizations run their business, especially in terms of communication, information sharing and integration capabilities. With Enterprise 2.0, employees become knowledge workers equipped with social tools to better understand the business processes, the services and the customers they are involved with,. This paper explores potential areas for using Enterprise 2.0 in healthcare with the aim of meeting the specific process and collaborative needs of healthcare services. We provide a mapping model which illustrates how Enterprise 2.0 can support specific healthcare processes and future research and recommendations for healthcare organizations based on our model. This research provides the starting point for a health 2.0 readiness model to assess the readiness of a healthcare organization for adopting Enterprise 2.0 technologies.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Will recommenders kill search?: recommender systems - an industry perspective</title>
	<abstract>At the 2010 annual ACM Conference on Recommender Systems (RecSys 2010) a panel addressed emerging topics regarding recommender systems as a whole and specifically their role in industry. This report summarizes answers from a distinguished group of industry leaders representing different industries in which recommender systems are highly relevant. Panel members discuss questions regarding the role of recommender systems in their own industry area, killer applications, opportunities, and future directions.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Mechanism design on trust networks</title>
	<abstract>We introduce the concept of a trust network--a decentralized payment infrastructure in which payments are routed as IOUs between trusted entities. The trust network has directed links between pairs of agents, with capacities that are related to the credit an agent is willing to extend another; payments may be routed between any two agents that are connected by a path in the network. The network structure introduces group budget constraints on the payments from a subset of agents to another on the trust network: this generalizes the notion of individually budget constrained bidders.

We consider a multi-unit auction of identical items among bidders with unit demand, when the auctioneer and bidders are all nodes on a trust network. We define a generalized notion of social welfare for such budget-constrained bidders, and show that the winner determination problem under this notion of social welfare is NP-hard; however the flow structure in a trust network can be exploited to approximate the solution with a factor of 1 -1/e. We then present a pricing scheme that leads to an incentive compatible, individually rational mechanism with feasible payments that respect the trust network's payment constraints and that maximizes the modified social welfare to within a factor 1-1/e.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Spam filter optimality based on signal detection theory</title>
	<abstract>Unsolicited bulk email, commonly known as spam, represents a significant problem on the Internet. The seriousness of the situation is reflected by the fact that approximately 97% of the total e-mail traffic currently (2009) is spam. To fight this problem, various anti-spam methods have been proposed and are implemented to filter out spam before it gets delivered to recipients, but none of these methods are entirely satisfactory. In this paper we analyze the properties of spam filters from the viewpoint of Signal Detection Theory (SDT). The Bayesian approach of Signal Detection Theory provides a basis for determining the optimality of spam filters, i.e. whether they provide positive utility to users. In the process of decision making by a spam filter various tradeoff's are considered as a function of the costs of incorrect decisions and the benefits of correct decisions.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A rule-based system for end-user e-mail annotations</title>
	<abstract>A new system for spam e-mail annotation by end-users is presented. It is based on the recursive application of handwritten annotation rules by means of an inferential engine based on Logic Programming. Annotation rules allow the user to express nuanced considerations that depend on deobfuscation, word (non-)occurrence and structure of the message in a straightforward, human-readable syntax. We show that a sample collection of annotation rules are effective on a relevant corpus that we have assembled by collecting emails that have escaped detection by the industry-standard SpamAssassin filter. The system presented here is intended as a personal tool enforcing personalized annotation rules that would not be suitable for the general e-mail traffic.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A survey of mobile malware in the wild</title>
	<abstract>Mobile malware is rapidly becoming a serious threat. In this paper, we survey the current state of mobile malware in the wild. We analyze the incentives behind 46 pieces of iOS, Android, and Symbian malware that spread in the wild from 2009 to 2011. We also use this data set to evaluate the effectiveness of techniques for preventing and identifying mobile malware. After observing that 4 pieces of malware use root exploits to mount sophisticated attacks on Android phones, we also examine the incentives that cause non-malicious smartphone tinkerers to publish root exploits and survey the availability of root exploits.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Rethinking the design of the Internet: the end-to-end arguments vs. the brave new world</title>
	<abstract>This article looks at the Internet and the changing set of requirements for the Internet as it becomes more commercial, more oriented toward the consumer, and used for a wider set of purposes. We discuss a set of principles that have guided the design of the Internet, called the end-to-end arguments, and we conclude that there is a risk that the range of new requirements now emerging could have the consequence of compromising the Internet's original design principles. Were this to happen, the Internet might lose some of its key features, in particular its ability to support new and unanticipated applications. We link this possible outcome to a number of trends: the rise of new stakeholders in the Internet, in particular Internet service providers; new government interests; the changing motivations of a growing user base; and the tension between the demand for trustworthy overall operation and the inability to trust the behavior of individual users.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Using decision trees for generating adaptive SPIT signatures</title>
	<abstract>With the spread of new and innovative Internet services such as SIP-based communications, the challenge of protecting and defending these critical applications has been raised. In particular, SIP firewalls attempt to filter the signaling unwanted activities and attacks based on the knowledge of the SIP protocol. Optimizing the SIP firewall configuration at real-time by selecting the best filtering rules is problematic because it depends on both natures of the legal traffic and the unwanted activities. More precisely, we do not know exactly how the unwanted activities are reflected in the SIP messages and in what they differ from the legal ones. In this paper, we address the case of Spam over Internet Telephony (SPIT) mitigation.

We propose an adaptive solution based on extracting signatures from learnt decision trees. Our simulations show that quickly learning the optimal configuration for a SIP firewall leads to reduce at lowest the unsolicited calls as reported by the users under protection. Our results promote the application of machine learning algorithms for supporting network and service resilience against such new challenges.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>VoIP defender: highly scalable SIP-based security architecture</title>
	<abstract>VoIP services are becoming increasingly a big competition to existing telephony services (POTS / ISDN). The increasing number of customers using VoIP makes VoIP services a valuable target for attackers that want to bring down the service, take it over or simply abuse it to distribute their own content, like SPAM. Hence, the need arises to protect VoIP services from all kinds of attacks that target network bandwidth, server capacity or server architectural constrains. In this article we present VoIP Defender, a generic security architecture, called VoIP-Defender, to monitor, detect, analyze and counter attacks relevant for a SIP-based VoIP infrastructure. The VoIP-Defender is highly scalable and can be easily extended with new detection algorithms. Analysis and traffic control can be performed from the SIP layer down to the transport-, network- and MAC layer. VoIP Defender is designed to work fully transparent to clients and SIP servers, and can analyze and filter traffic in real time, which we demonstrate with measurements with our implementation.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Socio-technical defense against voice spamming</title>
	<abstract>Voice over IP (VoIP) is a key enabling technology for migration of circuit-switched PSTN (Public Switched Telephone Network) architectures to packet-based networks. One problem of the present VoIP networks is filtering spam calls referred to as SPIT (Spam over Internet Telephony). Unlike spam in e-mail systems, VoIP spam calls have to be identified in real time. Many of the techniques devised for e-mail spam detection rely upon content analysis, and in the case of VoIP, it is too late to analyze the content (voice) as the user would have already attended the call. Therefore, the real challenge is to block a spam call before the telephone rings. In addition, we believe it is imperative that spam filters integrate human behavioral aspects to gauge the legitimacy of voice calls. We know that, when it comes to receiving or rejecting a voice call, people use the social meaning of trust, reputation, friendship of the calling party and their own mood. In this article, we describe a multi-stage, adaptive spam filter based on presence (location, mood, time), trust, and reputation to detect spam in voice calls. In particular, we describe a closed-loop feedback control between different stages to decide whether an incoming call is spam. We further propose formalism for voice-specific trust and reputation analysis. We base this formal model on a human intuitive behavior for detecting spam based on the called party's direct and indirect relationships with the calling party. No VoIP corpus is available for testing the detection mechanism. Therefore, for verifying the detection accuracy, we used a laboratory setup of several soft-phones, real IP phones and a commercial-grade proxy server that receives and processes incoming calls. We experimentally validated the proposed filtering mechanisms by simulating spam calls and measured the filter's accuracy by applying the trust and reputation formalism. We observed that, while the filter blocks a second spam call from a spammer calling from the same end IP host and domain, the filter needs only a maximum of three calls---even in the case when spammer moves to a new host and domain. Finally, we present a detailed sensitivity analysis for examining the influence of parameters such as spam volume and network size on the filter's accuracy.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Botnet spam campaigns can be long lasting: evidence, implications, and analysis</title>
	<abstract>Accurately identifying spam campaigns launched by a large number of bots in a botnet allows for accurate spam campaign signature generation and hence is critical to defeating spamming botnets. The straight-forward approach of clustering all spam containing the same label such as an URL into a campaign can be easily defeated by techniques such as simple obfuscations of URLs. In this paper, we perform a comprehensive study of content-agnostic characteristics of spam campaigns, e.g. duration and source-network distribution of spammers, in order to ascertain whether and how they can assist the simple label-based clustering methods in identifying campaigns and generating campaign signatures. In particular, from a five-month trace collected by a relay sinkhole, we manually identified and then analyzed seven URL-based botnet spam campaigns consisting of 52 million spam messages sent over 2.09 million SMTP connections originated from over 150,000 non-proxy spamming hosts and destined to about 200,000 end domains. Our analysis shows that the spam campaigns, when observed from large destination domains, exhibit durations far longer than the five-day period as reported in a recent study. We analyze the implications of this finding on spam campaign signature generation. We further study other characteristics of these long-lasting campaigns. Our analysis reveals several new findings regarding workload distribution, sending patterns, and coordination among the spamming machines.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Characterizing a spam traffic</title>
	<abstract>The rapid increase in the volume of unsolicited commercial e-mails, also known as spam, is beginning to take its toll in system administrators, business corporations and end-users. Widely varying estimates of the cost associated with spam are available in the literature. However, a quantitative analysis of the determinant characteristics of spam traffic is still an open problem. This work fills this gap and presents what we believe to be the first extensive characterization of a spam traffic.

As basis for our characterization, standard spam detection techniques are used to classify over 360 thousand incoming e-mails to a large university into two categories, namely spam and non-spam. For each of the two resulting workloads, as well as for the aggregate workload, we analyze a set of parameters, aiming at identifying the characteristics that significantly distinguish spam from non-spam traffic, assessing the qualitative impact of spam on the aggregate traffic and, possibly, drawing insights into the design of more effective spam detection techniques.

Our characterization reveals significant differences in the spam and non-spam traffic patterns. E-mail arrival process, size distribution as well as the distributions of popularity and temporal locality of e-mail recipients are key workload aspects which distinguish spam from traditional e-mail traffic. We conjecture that these differences are consequence of the inherently different mode of operation of spam and non-spam senders. Whereas non-spam e-mail transmissions are typically driven by social bilateral relationships, spam transmission is usually a unilateral action, based solely on the senders's will to reach as many users as possible.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Spamming botnets: signatures and characteristics</title>
	<abstract>In this paper, we focus on characterizing spamming botnets by leveraging both spam payload and spam server traffic properties. Towards this goal, we developed a spam signature generation framework called AutoRE to detect botnet-based spam emails and botnet membership. AutoRE does not require pre-classified training data or white lists. Moreover, it outputs high quality regular expression signatures that can detect botnet spam with a low false positive rate. Using a three-month sample of emails from Hotmail, AutoRE successfully identified 7,721 botnet-based spam campaigns together with 340,050 unique botnet host IP addresses.

Our in-depth analysis of the identified botnets revealed several interesting findings regarding the degree of email obfuscation, properties of botnet IP addresses, sending patterns, and their correlation with network scanning traffic. We believe these observations are useful information in the design of botnet detection schemes.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Filtering spam with behavioral blacklisting</title>
	<abstract>Spam filters often use the reputation of an IP address (or IP address range) to classify email senders. This approach worked well when most spam originated from senders with fixed IP addresses, but spam today is also sent from IP addresses for which blacklist maintainers have outdated or inaccurate information (or no information at all). Spam campaigns also involve many senders, reducing the amount of spam any particular IP address sends to a single domain; this method allows spammers to stay "under the radar". The dynamism of any particular IP address begs for blacklisting techniques that automatically adapt as the senders of spam change.

This paper presents SpamTracker, a spam filtering system that uses a new technique called behavioral blacklisting to classify email senders based on their sending behavior rather than their identity. Spammers cannot evade SpamTracker merely by using "fresh" IP addresses because blacklisting decisions are based on sending patterns, which tend to remain more invariant. SpamTracker uses fast clustering algorithms that react quickly to changes in sending patterns. We evaluate SpamTracker's ability to classify spammers using email logs for over 115 email domains; we find that SpamTracker can correctly classify many spammers missed by current filtering techniques. Although our current datasets prevent us from confirming SpamTracker's ability to completely distinguish spammers from legitimate senders, our evaluation shows that SpamTracker can identify a significant fraction of spammers that current IP-based blacklists miss. SpamTracker's ability to identify spammers before existing blacklists suggests that it can be used in conjunction with existing techniques (e.g., as an input to greylisting). SpamTracker is inherently distributed and can be easily replicated; incorporating it into existing email filtering infrastructures requires only small modifications to mail server configurations.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Link spamming Wikipedia for profit</title>
	<abstract>Collaborative functionality is an increasingly prevalent web technology. To encourage participation, these systems usually have low barriers-to-entry and permissive privileges. Unsurprisingly, ill-intentioned users try to leverage these characteristics for nefarious purposes. In this work, a particular abuse is examined -- link spamming -- the addition of promotional or otherwise inappropriate hyperlinks.

Our analysis focuses on the wiki model and the collaborative encyclopedia, Wikipedia, in particular. A principal goal of spammers is to maximize exposure, the quantity of people who view a link. Creating and analyzing the first Wikipedia link spam corpus, we find that existing spam strategies perform quite poorly in this regard. The status quo spamming model relies on link persistence to accumulate exposures, a strategy that fails given the diligence of the Wikipedia community. Instead, we propose a model that exploits the latency inherent in human anti-spam enforcement.

Statistical estimation suggests our novel model would produce significantly more link exposures than status quo techniques. More critically, the strategy could prove economically viable for perpetrators, incentivizing its exploitation. To this end, we address mitigation strategies.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Fighting unicode-obfuscated spam</title>
	<abstract>In the last few years, obfuscation has been used more and more by spammers to make spam emails bypass filters. The standard method is to use images that look like text, since typical spam filters are unable to parse such messages; this is what is used in so-called "rock phishing". To fight image-based spam, many spam filters use heuristic rules in which emails containing images are flagged, and since not many legit emails are composed mainly of a big image, this aids in detecting image-based spam. The spammers are thus interested in circumventing these methods. Unicode transliteration is a convenient tool for spammers, since it allows a spammer to create a large number of homomorphic clones of the same looking message; since Unicode contains many characters that are unique but appear very similar, spammers can translate a message's characters at random to hide black-listed words in an effort to bypass filters. In order to defend against these unicode-obfuscated spam emails, we developed a prototype tool that can be used with Spam Assassin to block spam obfuscated in this way by mapping polymorphic messages to a common, more homogeneous representation. This representation can then be filtered using traditional methods. We demonstrate the ease with which Unicode polymorphism can be used to circumvent spam filters such as SpamAssassin, and then describe a de-obfuscation technique that can be used to catch messages that have been obfuscated in this fashion.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Stopping outgoing spam</title>
	<abstract>We analyze the problem of preventing outgoing spam. We show that some conventional techniques for limiting outgoing spam are likely to be ineffective. We show that while imposing per message costs would work, less annoying techniques also work. In particular, it is only necessary that the average cost to the spammer over the lifetime of an account exceed his profits, meaning that not every message need be challenged. We develop three techniques, one based on additional HIP challenges, one based on computational challenges, and one based on paid subscriptions. Each system is designed to impose minimal costs on legitimate users, while being too costly for spammers. We also show that maximizing complaint rates is a key factor, and suggest new standards to encourage high complaint rates.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Social spam detection</title>
	<abstract>The popularity of social bookmarking sites has made them prime targets for spammers. Many of these systems require an administrator's time and energy to manually filter or remove spam. Here we discuss the motivations of social spam, and present a study of automatic detection of spammers in a social tagging system. We identify and analyze six distinct features that address various properties of social spam, finding that each of these features provides for a helpful signal to discriminate spammers from legitimate users. These features are then used in various machine learning algorithms for classification, achieving over 98% accuracy in detecting social spammers with 2% false positives. These promising results provide a new baseline for future efforts on social spam. We make our dataset publicly available to the research community.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>How dynamic are IP addresses?</title>
	<abstract>This paper introduces a novel algorithm, UDmap, to identify dynamically assigned IP addresses and analyze their dynamics pattern. UDmap is fully automatic, and relies only on application-level server logs. We applied UDmap to a month-long Hotmail user-login trace and identified a significant number of dynamic IP addresses - more than 102 million. This suggests that the fraction of IP addresses that are dynamic is by no means negligible. Using this information in combination with a three-month Hotmail email server log, we were able to establish that 95.6% of mail servers setup on the dynamic IP addresses in our trace sent out solely spam emails. Moreover, these mail servers sent out a large amount of spam - amounting to 42.2% of all spam emails received by Hotmail. These results highlight the importance of being able to accurately identify dynamic IP addresses for spam filtering. We expect similar benefits to arise for phishing site identification and botnet detection. To our knowledge, this is the first successful attempt to automatically identify and understand IP address dynamics.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Filtering spam from bad neighborhoods</title>
	<abstract>One of the most annoying problems on the Internet is spam. To fight spam, many approaches have been proposed over the years. Most of these approaches involve scanning the entire contents of e-mail messages in an attempt to detect suspicious keywords and patterns. Although such approaches are relatively effective, they also show some disadvantages. Therefore an interesting question is whether it would be possible to effectively detect spam without analyzing the entire contents of e-mail messages. The contribution of this paper is to present an alternative spam detection approach, which relies solely on analyzing the origin (IP address) of e-mail messages, as well as possible links within the e-mail messages to websites (URIs). Compared to analyzing suspicious keywords and patterns, detection and analysis of URIs is relatively simple. The IP addresses and URIs are compared to various kinds of blacklists; a hit increases the probability of the message being spam. Although the idea of using blacklists is well known, the novel idea proposed within this paper is to introduce the concept of 'bad neighborhoods'. To validate our approach, a prototype has been developed and tested on our university's mail server. The outcome was compared to SpamAssassin and mail server log files. The result of that comparison was that our prototype showed remarkably good detection capabilities (comparable to SpamAssassin), but puts only a small load on the mail server.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Detection of near-duplicate user generated contents: the SMS spam collection</title>
	<abstract>Today, the number of spam text messages has grown in number, mainly because companies are looking for free advertising. For the users is very important to filter these kinds of spam messages that can be viewed as near-duplicate texts because mostly created from templates. The identification of spam text messages is a very hard and time-consuming task and it involves to carefully scanning hundreds of text messages. Therefore, since the task of near-duplicate detection can be seen as a specific case of plagiarism detection, we investigated whether plagiarism detection tools could be used as filters for spam text messages. Moreover we solve the near-duplicate detection problem on the basis of a clustering approach using CLUTO framework. We carried out some preliminary experiments on the SMS Spam Collection that recently was made available for research purposes. The results were compared with the ones obtained with the CLUTO. Althought plagiarism detection tools detect a good number of near-duplicate SMS spam messages even better results are obtained with the CLUTO clustering tool.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Cyber security: a national effort to improve</title>
	<abstract>New and advanced combinations of cyber security threats are being introduced into the online world. Malware authors are forced to create more efficient software with the advancement of network security systems. Recent governmental legislation offers support to companies that operate through the use of sensitive consumer information. Governmental agencies also promote consumer education about law enforcement actions under way. The reporting mechanism for information security incidents needs to be centralized to eliminate confusion amount agencies.</abstract>
	<search_task_number>12</search_task_number>
	<query>spam prevention</query>
	<relevance>0</relevance>
  </item>


  <item>
    <title>An extensible application platform for heterogeneous smart home appliances and mobile devices</title>
	<abstract>Nowadays, various kinds of smart home appliances are widely deployed in our daily life. These embedded, networked, and programmable appliances could be accessed through various methods, but may not be integrated together due to the different kinds of home network protocols on which these appliances are deployed. On the other hand, with the penetration of mobile devices, people could leverage these appliances by fingers from anywhere around the world. However, the mobile accesses to these appliances are usually proprietary and require complicated development efforts. The paper aims at developing an application platform prototype named HomeBox for home appliances and mobile devices. The platform provides connectivity for heterogeneous smart home appliances, hosts an application runtime environment for smart home applications, and provides user interface transparency for mobile-devices to reduce the development efforts. The prototype has been explored in various applications to manage smart home appliances from mobile devices and demonstrate the extensibility for future devices and architectures.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Controlling Home and Office Appliances with Smart Phones</title>
	<abstract>Most home and office appliances contain microprocessors. All these appliances have some user interface, but many users become frustrated with their appliances' difficult, complex functions. However, a new framework, the personal universal controller (PUC), lets users interact with appliances through a separate user interface device that they carry. Smart phones are good candidates for providing interfaces because they're common, their communication capabilities connect users to appliances, and they're already being used for a wide range of applications. The framework includes an abstract specification language for describing appliances, a two-way communication protocol, and automatic interface generation software that customizes user interfaces. This article overviews the PUC system and describes in detail the design and implementation of automatic interface generation for Microsoft's Smartphone platform.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A user interface for a Jini-based appliances in ubiquitous smart homes</title>
	<abstract>The goal in this paper is to provide the uniform appliances interface. One of the problems with many of existing appliances may be not used the same interface for different devices, because the respective device have their heterogeneous characteristics and limitations, even among different versions made by the same manufacturer. Therefore, if end-users can have the remote controller meaning any device that is installed UI which provide the interaction between different providers and various versions, he or she is able to more efficiently control a device. In this paper, we present a system, called REMOCON, which approaches this problem by building natural language interfaces, and by using the instant messenger. In Ubiquitous smart homes environment, this mechanism can make various services to be used among service agents. Based on the mobility of service agent on the Jini platform and instant messaging system (IMS), the mobile device can receive the rapid notify from its federation leader in order to download the necessary service, to remove the unnecessary service, or to update the newly service without reconfiguring the system. Also, using the natural language processing, the client is possible to express smoothly the order. Therefore, our proposed agent-based management system for ubiquitous smart homes provides a user-friendly interface, and can make use of add-on functionalities of the IMS such as voice chatting, SMS, and multimedia.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>iSCSI protocol parameter optimization for mobile appliance remote storage system at smart home environment approach</title>
	<abstract>In Mobile appliance, users have a limited amount of storage availability to them due to their limited size and weight. To relieve problem we developed iSCSI remote storage system, which is an excellent solution for smart home automation too. User can store or access their valuable data to the home server from anywhere, anytime and also get facility to use mass storage space. The iSCSI protocol has emerged as a transport for carrying SCSI block-level access protocol over the ubiquitous TCP protocol. It enables a client's block-level access to the data on remote storage over an existing IP infrastructure. However, the performance of the iSCSI based remote storage system for mobile appliances were sharply dropped in wireless networks; especially when we adapt default parameters value suggested in standard for our remote storage system in wireless networks. This paper focuses our experiments, which are performed to investigate the best performance values of iSCSI parameters for iSCSI-based remote storage system, are taken out in CDMA networks in order to realize the access to a remote storage system anytime and anywhere. And after the experiment, we suggest the optimal value of parameters. The experiment results from several test cases show us the best values are not the default values specified in the iSCSI standard.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A remote monitoring and control of home appliances on ubiquitous smart homes</title>
	<abstract>The goal in this paper is to design and implementation the service agent society mechanism of home network community by using Jini platform. On Ubiquitous smart homes environment, this mechanism can make various services to be used among service agents. Based on the mobility of service agent on the Jini platform and instant messaging system (IMS), the mobile device can receive the real time notify from its federation leader in order to download the necessary service, to remove the unnecessary service, or to update the newly service without reconfiguring the system. Moreover, using the Natural Language Processing, the client is possible to express smoothly the order. Therefore, our proposed agent-based management system for ubiquitous smart homes provides a user-friendly interface, and can make use of add-on functionalities of the IMS such as voice chatting, SMS, and multimedia.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Effective appliance selection by complementary context feeding in smart home system</title>
	<abstract>Smart Home System (SHS) is one of popular applications in ubiquitous computing, which provides convenient services for a user with userfriendly intelligent system interfaces. Among them, voice recognition is a popular interface. However, voice command statements given by users are often too unclear and incomplete for the devices in SHS to understand the original user intention. So, the devices become complicated and have no idea about whether to work or not. Therefore, we should make sure the proximate selection for the devices which will be eventually targeted and operated following user intention. In this paper, we propose an effective method to make a decision in electing a promising target device among candidates by taking advantage of complementary context feeding around user environment in SHS even with initial incomplete interface information. The proposed method is based on Bayes theorem using the way of empirical statistic inference.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A User Interface for Controlling Information Appliances in Smart Homes</title>
	<abstract>The web user interface offers many advantages; however, a fixed IP address is required to manage each server, it does not have a push function for completion or error messages, it suffers from problems associated with firewalls, and it is slow. To address these issues, we propose an efficient method for controlling information appliances in smart homes that uses a natural language processing technique and an instant messaging system (IMS). The proposed agent-based management system for smart homes provides a user-friendly interface, and can make use of add-on functionalities of the IMS such as voice chatting, SMS, and multimedia. Moreover, the proposed method can be applied using middleware technologies such as Jini [1] UPnP [2], OSGi [4], and HAVi [5].</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Home appliances get smart</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Echonet: A Home Network Standard</title>
	<abstract>Echonet is a de jure home network standard ratified by the IEC and the ISO for control and monitor sensors, household appliances, and equipment. Its small footprint offers many unique features such as media-independent control systems, interoperability in a multiple vendor environment, compatibility with long-term ecosystem construction, and secure communication to protect personal information.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Intelligent Oven in Smart Home Environment</title>
	<abstract>Smart homes are gradually becoming one of the main applications in the high technology area. The intelligent appliance is a fundamental component of a smart home environment. People tend to invest in the kitchen appliances as they will be used throughout their lifetime, therefore many manufacturers focus on making these appliance more interesting. However, majority of manufacturers focus on how to make them cook faster. Very few manufacturers pay attention on the ability to cook healthier food. In this paper we introduce an innovative intelligent oven for the healthier food choice that is woven inside smart home environment. The intelligent oven is designed for manipulating recommended healthy recipe choices to suit each family member health concerns. Moreover, its ability to interact with other smart appliances such as smart fridge, mobile phone, and smart fire alarm are beneficial. We believe that the features mentioned above will make the intelligent oven an essential component in the smart home environment.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A Remote Surveillance Model for Appliance Control</title>
	<abstract>Remote appliance control is an important vision for the smart home. Even in the next few decades, it is inevitable that conversional appliances and smart appliance will coexist together to help people live better. For this reason, this research proposed a remote control model for the conversional appliances to integrate with smart home environment. In this model, a smart home portal combined with video surveillance system was used to realize the appliance control. Total control cycle is base on what you see is what you have done. The main benefits of this model come from the mature technology, simple design, light-weight architecture, and practicable for implementation.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Smart Home Sensor Networks Pose Goal-Driven Solutions to Wireless Vacuum Systems</title>
	<abstract>Home sensor nodes are devices embedded in home appliances and are designed to sense environments, to process collected information, to perform a specific task, and to cooperate with other units. The advances of VLSI technologies and wireless sensor networks (WSN) turn the inspirational idea of intelligent home appliances into reality. In this paper, we focus on a category of home appliances, Smart Home Vacuum (SHV), where mobility and battery are critical design concerns. The emphasis of this paper is on the design of SHV system and the development of a goal driven task planning (GDTP) engine which can be implemented in the wireless vacuum systems to maximize the network lifetime as well as the cleaning efficiency. Unlike LEACH, proposed GDTP engine is a goal-driven approach to select the cluster heads to satisfy the design goals. Simulations are conducted by the network simulator (ns-2) and the experiment results indicate that GDTP performs better than other algorithms in terms of the network lifetime and the cleaning area coverage.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Service-Oriented Actuator for Ubiquitous Smart Space</title>
	<abstract>In an ubiquitous smart space (USS), a lot of contexts exist based on the abundant sensing infrastructure and smart devices are needed to understand them. If the legacy devices for being adapted to USS are all replaced to new and smart ones, they have to be re-produced with a new standard form. To connect and execute them effectively in this environment involves compatibility and cost problems. Even if smart devices are operating in the USS, they just understand not user’s abstract request such as ‘manage temperature’ but the function-level operations such as ‘turn on/off’. In this paper, we propose a service-oriented actuating method to provide the service with user-level abstraction in the USS. We first deploy service ontology, metaservice and then develop an embedded system, USA (Universal Service Actuator) which helps to make connecting devices by three interfaces such as Serial, Infra-Red and Digital I/O to solve the compatibility and the cost problem. As experimental results, some home appliances with USA can understand and execute user’s abstract request from the home server and report the running state.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Research on Key Technologies of Building Home Control Based on the Wireless Network</title>
	<abstract>Relatively higher cost, worse extensibility of the monitoring points and weaker mobility are those disadvantages that widely exist in the wire data collection system of the traditional household facilities. This paper introduces a scheme for building a wireless sensor network in a larger scope, utilizing the advantages of ZigBee technology in lower power, lower cost, easier extensibility and convenient installation, to monitor various kinds of information at home. For example, it collects the information of the appliances to the control center (PC) for data processing, and by the 3G communication between PC and smart mobile phones it manages to monitor the appliances by remote control.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Service Oriented Framework for Modern Home Appliances</title>
	<abstract>User demands and technological advances are moving us closer to the pervasive computing vision. The home of the future will include networked appliances that publish the functions they offer as intelligent middleware services. In the conventional home network systems, a powerful centralized server controls all electric home appliances connected to provide value-added integrated services. However, when the number of the appliances increases and the appliances become more sophisticated, the conventional architecture would suffer from problems in superfluous resources, flexibility, scalability and reliability. This paper presents Service Oriented Architecture as a new architecture to solve the problems. We propose a two-layered design of an appliance, including device and service layers. The device layer corresponds to the physical device of the appliance controlled by vendor-specific interfaces. The service layer exhibits features of the device as self-contained services accessible via device-independent interfaces. Thus, the appliances are loosely coupled at the service layer without any centralized server. This enables more flexible, robust and load-balanced integrated services.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A Web Based Approach to Virtual Appliance Creation, Programming and Management</title>
	<abstract>The Internet and Web technology is advancing at a frantic pace, expanding into almost every aspect of our everyday life. One of the latest scientific activities for the Internet and the Web is the so-called pervasive or ubiquitous computing where networking plays a vital role in its core computational framework. In this, people are able to use the Internet and Web to manage the operation of embedded network devices, services and to coordinate their services in ways that create applications such as smart-homes, smart-offices, smart-cars etc, collectively referred to as intelligent environments. For ordinary people (non technologists) to be able to use this technology, it is required that the interaction between the users and the environment must be as transparent and simple as possible, employing intuitive and user-friendly interfaces wherever possible. A popular approach to empowering users to customise the functionality of their environments is via end-user programming. In this work-in-progress paper we describe an approach based on using a web based GUI to augment earlier work of ours concerning an end user programming paradigm known as Pervasive interactive Programming (PiP), in a way that makes it more flexible and easy to use. By doing this, we present a conceptual model and discuss the issues in developing and using this model.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>S2A: secure smart household appliances</title>
	<abstract>Security protection is an integral component for smart homes; however, smart appliances security has received little attention in the research community. Household appliances become very vulnerable if we introduce smart functions without proper security protection. In particular, smart access functions enable users to operate devices remotely. Meanwhile, smart devices are are also designed to support residential demand response, i.e. postpone non-urgent tasks to non-peak hours. However, remote adversaries could utilize such functions to manipulate smart appliances' operations without physically touching them. Such interferences, if not properly handled, could damage the smart devices, disturb owners' life or even harm the households' physical security.

In this paper, we present S2A, a security protection solution to be embedded in smart appliances. First, a SUP model is developed to quantify penalties from device security, usability and electricity price. We employ multi-criteria reinforcement learning to integrate the three factors to determine an optimal operation strategy. Next, to leverage the risk of forged control commands or pricing data, we present a realtime assessment mechanism based on Bayesian inference. Risk indices are further integrated into the SUP model to serve as weighting factors of corresponding decision criteria. Evaluation shows that S2A ensures appliances security while providing good usability and economical efficiency.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>User-configurable semantic home automation</title>
	<abstract>The ideas of smart home and home automation have been proposed for many years. However, when discussing homes of the future, related studies have usually focused on deploying various smart appliances (or devices) within a home environment and employing those appliances automatically by pre-defined procedures. The difficulties of supporting user-configurable automation are due to the complexity of various dynamic home environments. Moreover, within their home domains, users usually think semantically; for example, ''I want to turn off all the lights on the second floor''. This paper proposes a semantic home automation system, USHAS (User-configurable Semantic Home Automation System), which adopts Web Service and WSBPEL for executing automated process; OWL and OWL-S for defining home environments and service ontology; and a self-defined markup language, SHPL (Semantic Home Process Language), for describing semantic processes.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Robust one-time password authentication scheme using smart card for home network environment</title>
	<abstract>Due to the exponential growth of the Internet users and wireless devices, interests on home networks have been enormously increased in recent days. In digital home networks, home services including remote access and control to home appliances as well as services offered by service providers are alluring. However, the remote control services cause digital home networks to have various security threats. Hence, for digital home networks, robust security services, especially remote user authentication, should be considered. This paper presents a robust and efficient authentication scheme based on strong-password approach to provide secure remote access in digital home network environments. The proposed scheme uses lightweight computation modules including hashed one-time password and hash-chaining technique along with low-cost smart card technology. It aims to satisfy several security requirements including stolen smart card attack and forward secrecy with lost smart card as well as functional requirements including no verification table and no time synchronization. Comparing with the existing representative schemes, it can be validated that the proposed scheme is more robust authentication mechanism having better security properties. We have conducted formal verification of the proposed scheme.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>An Intercommunication Home Energy Management System with Appliance Recognition in Home Network</title>
	<abstract>In present days there are wide varieties of household electric appliances along with different power consumption habits of consumers, making identifying electric appliances without presetting difficulty. This paper introduces smart appliance management system to recognize electric appliances in home networks, which uses sensing devices that measure current to calculate the power consumption of the appliances. The system will set the characteristics and categories of each electric appliance, and then uses the classifications of the electronic energy features in order to recognize different appliances. The system searches the cluster data while eliminating noise for recognition functionality and error detection mechanism or the electric appliances using the current clustering algorithm. Afterwards the recognition are used to build a control list of appliances on the platform to provide appliance intercommunication. Simultaneously, the household appliance automatic control services are integrated by the system to control appliances based on userspower consumption plans to realize a bidirectional monitoring services. In actual experiments, the proposed system achieves a recognition rate or 95% as well as successfully controls general household electric appliances in home network.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Location-aware communications in smart environments</title>
	<abstract>We present a location-aware communication approach for smart home environments that is based on a symbolic location model that represents the containment relationships between physical objects, computing devices, and places. The approach enables people and devices to discover and connect with communication partners based on their co-locations. It also provides non-smart objects, including home appliances, everyday objects, and places, with virtual counterparts, which both represent them and work as proxies for their targets. We present the design for this approach and describe its implementation and applications.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Control smart homes easily with simple touch</title>
	<abstract>Ambient Intelligence (AmI) environments contain many smart artifacts and appliances that users can control automatically or manually. In the manual mode, user interacts with intelligent controllers, touch screens or mobile-phones to control electric and electronic appliances in the house. In the past few years, various research efforts have focuses on the creation of an abstract application that can control every appliance in the environment through smart controllers. This paper describes the Control Home Easily application that auto-generates user interfaces and allows users to manage easily home appliances of every room from inside or outside the home with various interactive units through simple touch. A web application has also been developed that can be run through mobile phones. These approaches help simple users to accept AmI technologies in the home environment and reduce the development time of a new smart artifact.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Trust based security auto-configuration for smart assisted living environments</title>
	<abstract>Recent progresses in wireless sensor networking and pervasive computing have created huge opportunities for providing elderly people with technological facilities. For elderly people, conceiving technologies for increasing their autonomy, so as to enable them to self-manage their life is of utmost importance. However, when it comes to smart home, once all appliances in a home are automated and connected through internet, it becomes essential to consider issues of security, especially security configuration. In the smart home, security has to be configured and managed by technology-unaware elderly people. One mechanism of auto security configuration in such environment can be achieved by observing the trustworthiness of smart devices. Trust-based security mechanisms allow access rights to evolve among previously unknown devices, thus minimizing security configuration. In this paper, we present a security configuration model which takes critical security decisions by determining the trustworthiness of an entity based on the sources of trust: Direct interaction and Recommendation trust.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Operating appliances with mobile phones: strengths and limits of a universal interaction device</title>
	<abstract>Mobile phones are increasingly becoming ubiquitous computational devices that are almost always available, individually adaptable, and nearly universally connectable (using both wide area and short range communication capabilities). Until Star Trek-like speech interfaces are fully developed, mobile phones seem thus poised to become our main devices for interacting with intelligent spaces and smart appliances, such as buying train passes, operating vending machines, or controlling smart homes (e.g., TVs, stereos, and dishwashers, as well as heating and light). But how much can a mobile phone simplify our everyday interactions, before it itself becomes a usability burden? What are the capabilities and limitations of using mobile phones to control smart appliances, i.e., operating things like ATMs or coffee makers that typically do not benefit from remote control? This paper presents a user study investigating the use of a prototypical, mobile phone based interaction system to operate a range of appliances in a number of different task settings. Our results show that mobile devices can greatly simplify appliance operation in exceptional situations, but that the idea of a universal interaction device is less suited for general, everyday appliance control.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>u-Texture: self-organizable universal panels for creating smart surroundings</title>
	<abstract>This paper introduces a novel way to allow non-expert users to create smart surroundings. Non-smart everyday objects such as furniture and appliances found in homes and offices can be converted to smart ones by attaching computers, sensors, and devices. In this way, non-smart components that form non-smart objects are made smart in advance. For our first prototype, we have developed u-Texture, a self-organizable universal panel that works as a building block. The u-Texture can change its own behavior autonomously through recognition of its location, its inclination, and surrounding environment by assembling these factors physically. We have demonstrated several applications to confirm that u-Textures can create smart surroundings easily without expert users.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The COSE ontology: bringing the semantic web to smart environments</title>
	<abstract>The number of smart appliances and devices in the home and office has grown dramatically in recent years. Unfortunately, these devices rarely interact with each other or the environment. In order to move from environments filled with smart devices to smart environments, there must be a framework for devices to communicate with each other and with the environment. This enables reasoners and automated decision makers to understand the environment and the data collected from it. Semantic web technologies provide this framework in a well-documented and flexible package. In this paper we present the Casas Ontology for Smart Environments (COSE) and accompanying data from a test smart environment and discuss the current and future challenges associated with a Smart Environment on the Semantic Web.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>BlueS: semantic caching approaches for data reuse in smart environments data management in healthcare scenarios</title>
	<abstract>Pervasive technologies are essential at any field of healthcare issues. Especially the growing number elderly necessitates the development of mesures to support care-dependent people at their private homes. One important research field are so called Smart-Rooms, local agglomerations of smart appliances, whose composition is prone to frequent, unforeseeable, and substantial changes. These compositions are able to react on the user's needs by interaction of individual appliances. Normally, these environments are equipped with smart sensors to infer the activities of the user. In case the environments know the current activity they can try to infer the next possible actions the user will take, thus assist the user pro-active. On the other side, these environments should be able to manage information or data the user 1 is interested in. Therefore it is necessary to offer solutions for data access and information exchange in ubiquitous environments with respect to the users needs and skills. Therefore, we present BlueS (Bluetooth-Services), our service based solution for data access and information exchange in spontaneous linked smart environments, e.g. Bluetooth networks. In this paper we want to focus on smart caching approaches for data reuse. A full descritpion of BlueS can be found in 10].</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The Essential Guide to Home Networking</title>
	<abstract>Many consumers access the Internet, e-commerce stores, and e-mail accounts from the comfort of their homes. The home networking business is now only beginning and is expected to soar in the next couple of years as more and more households around the world connect to the Internet. Many large IT and telecommunication companies are planning to offer a range of new products that will allow entertainment devices and PCs scattered around your household to "talk to each other." Allied Business Intelligence estimates that the home networking equipment marketplace will grow dramatically to reach $2.4 billion by 2005.

Simply put, a digital home network is a cluster of audio/visual (A/V) devices, including set-top boxes, TVs, VCRs, DVD players, and general-purpose computing devices such as personal computers. Companies that are involved in the home networking industry need to convince consumers that the new technology can help them save time, make organizing activities more convenient, and can even entertain them. With the steady rise in high-speed access to the Web and the proliferation of households with multiple PCs, the need for home networking solutions has increased dramatically in recent months. Several companies and organizations have responded to this need by developing standards and affordable solutions for consumers. The creation of open standards is an important catalyst for creating high-growth consumer markets. Adopting such a strategy will allow the home networking market to grow faster, without interruption, and will keep consumers confident that the products they buy today will continue to be viable solutions for thefuture.

Because no single technology fulfills all of the application requirements of the home network, multiple technologies will be deployed at different times, each addressing the needs of unique market segments. Several technology development efforts are currently underway to support the application requirements of the home network. Organizations like HomePNA and HomeRF are primarily focused on the networking of PCs and peripheral devices together. In parallel to these developments, groups, companies, and technologies such as HAVi, UPnP, HomePnP, LonWorks, Digital Harmony, and Jini are actively promoting software systems for networking PCs, home control, and entertainment systems together. In parallel to these in-home technology developments, an industry group called OSGi is working to define and promote an open software standard for connecting the coming generation of smart appliances with commercial network service providers. This book unravels the benefits, technical details, and features of all of these.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Towards Dependable Home Networking: An Experience Report</title>
	<abstract>As the success of the Web increasingly brings us towards a fully connected world, home networking systems that connect and manage home appliances become the natural next step to complete the connectivity. Although there has been fast-growing interest in the design of smart appliances and environments, there has been little study on the dependability issues, which is essential to making home networking part of our daily lives. The heterogeneity of various in-home networks, the undependable nature of consumer devices, and the lack of knowledgeable system administrators in the home environment introduce both opportunities and challenges for dependability research. In this paper, we report the dependability problems we encountered and the solutions we adopted in the deployment of the Aladdin home networking system. We propose the use of a soft-state store as a shared heartbeat infrastructure for monitoring the health of diverse hardware and software entities. We also describe a system architecture for connecting powerline devices to enhance dependability, and a monitoring tool for detecting unusual powerline activities potentially generated by intruders, interferences, or ill-behaved devices.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Programming Open Service Gateways with Java Embedded Server Technology</title>
	<abstract>Technology is invented and advanced by, well, technical people. However, a truly successful technology is marked by its adoption by people in their daily lives. Few ponder radio frequency modulation when they turn on the TV, or the internal combustion engine when they drive around. The technology has disappeared behind the utility.
The last decade saw two new technologies begin to blend into our lives: the computer and the Internet. We only need to launch a browser and the resources of the World Wide Web are at our fingertips, and we are hard pressed to tell the difference between a computer and a game console, a personal digital assistant (such as PalmPilot), or a cell phone. It is now entirely feasible to bring services to smart consumer devices at home and to small businesses through the Internet. Utility providers and network, computer, wireless, consumer electronics, and home appliance companies recognize the tremendous potential and have started to tap into this market. As a result, new horizons are open for application developers.

The Open Services Gateway Initiative (OSGi) was formed to explore these exciting opportunities, and its membership includes such diverse companies as Bell South, Echelon, Electricite de France, IBM, Sun, Ericsson, Nokia, Sony, Maytag, and Whirlpool, to name just a few from a roster of more than 80 organizations. With these combined resources, OSGi stands a good chance to turn this vision into reality.

The OSGi Service Gateway Specification 1.0 defines a Java™ technology-based software architecture for developing and deploying services, which is the topic of this book.

What compels us to write this book, inaddition to our enthusiasm for the emerging new applications, is the unique software model involved. We stumbled through a lot of unfamiliar territory ourselves when we worked on the Java Embedded Server™ product, the predecessor to the OSGi Service Gateway Specification, only to find our fellow developers encountering and struggling with the same class of problems. It is our hope to be able to elucidate the model and capture the hard-won solutions in one place.

This book is primarily for programmers interested in writing services for residential gateways in the Java programming language. It should also be useful to anyone who wants to learn about residential gateway technology and the efforts made by the OSGi consortium.

This book may be of interest to those who are involved with component-based software construction in general. Interestingly, nothing in the underlying programming model limits the kinds of applications that can be written. It aims at residential gateway applications at the "small" end of the spectrum in terms of code size and resource consumption, but it is just as viable for developing applications for desktop and enterprise environments. Indeed, the task will be made easier and the end result will be more powerful when fewer constraints on computing resources are imposed.
We assume the readers are well versed in the Java programming language and experienced in software development on the Java platform. However, no experience is needed in embedded systems at the hardware and operating system levels.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The Management of Device Group for Home Automation Network</title>
	<abstract>In the past years, the service of digital home almost focuses on the automatic surveillance, automatic controlling and health-care. Currently, how to provide an automatic interconnection and control for different devices on the heterogeneous home network is a very pressing problem. In this paper, we implement a systemic design to connect the heterogeneous networks and devices in the home automation network (HAN). The all devices of remote family can be easy controlled, found, and communicated without manual setting by local family. Simulation results show that the proposed system is a better mechanism for digital home and smart appliances.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Remote Access to Universal Plug and Play (UPnP) Devices Utilizing the Atom Publishing Protocol</title>
	<abstract>The Universal Plug and Play (UPnP) Forum has specified protocols which enable consumer electronics devices, computers and smart appliances to discover and use each other's services. This is possible when devices are connected to the same network, usually that being a home network. Currently, UPnP does not specify any method for remotely accessing the services of home devices, from devices that are connected on another, external, network. In this paper we propose a solution that utilizes web syndication protocols, such as the IETF Atom, for enabling remote usage of UPnP devices, without major modifications to the end-user UPnP applications.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Activity-aware computing: modeling of human activity and behavior</title>
	<abstract>With our society becoming increasingly mobile and devices that are small, inexpensive and wireless, we are transitioning from an age of desktop computing to an age where computers are used in all aspects of life and leisure. Ubiquitous Computing is largely concerned with the progression of computers from stationary desktop environments to environments where computers and sensors are integrated with objects and every aspect of our daily life, often in an invisible way.
This dissertation investigates an important problem in Ubiquitous Computing: detecting domestic activities using ubiquitously deployed sensors from data sets of limited size. The dissertation assumes that home environments in the next 20 years will support a wide range of sensing technologies that are built in smart appliances and the surrounding environment (e.g. RFID tags and readers, accelerometers, temperature sensors etc.). The dissertation also assumes that there will be an abundance of embedded CPU power in the environment that will enable fast and efficient spectral analysis and feature extraction from sensor signals. Using efficient wireless technologies such as the new Bluetooth Wibree protocol, these devices will be able to communicate their sensed data in an efficient way.

Two approaches are presented for domestic activity recognition from wireless sensors. The first approach is rule-based and logical in nature and is suitable when sensor data is not present for training. Importantly, fuzzy distributions model the uncertainty and variability in expert knowledge. The second approach is probabilistic in nature and learns by observation without human intervention. This approach uses Bayesian Learning and is optimized to deal with sparse data sets (with hundreds of sensor readings and few instances of activities). Further, a case study is presented in which activity recognition optimizes energy consumption for wireless PC cards that results in significant energy savings. This dissertation concludes by highlighting major and minor results. A summary of the author's future and current research efforts is presented including the application of activity recognition in medical interventions and resource allocation problems.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Research and implementation of the context-aware middleware based on neural network</title>
	<abstract>Smart homes integrated with sensors, actuators, wireless networks and context-aware middleware will soon become part of our daily life. This paper describes a context-aware middleware providing an automatic home service based on a user's preference inside a smart home. The context-aware middleware utilizes 6 basic data for learning and predicting the user's preference on the home appliances: the pulse, the body temperature, the facial expression, the room temperature, the time, and the location. The six data sets construct the context model and are used by the context manager module. The user profile manager maintains history information for home appliances chosen by the user. The user-pattern learning and predicting module based on a neural network predicts the proper home service for the user. The testing results show that the pattern of an individual's preferences can be effectively evaluated and predicted by adopting the proposed context model.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Evolution towards smart home environments: empirical evaluation of three user interfaces</title>
	<abstract>Smart home environments have evolved to the point where everyday objects and devices at home can be networked to give the inhabitants new means to control them. Familiar information appliances can be used as user interfaces (UIs) to home functions to achieve a more convenient user experience. This paper reports an ethnographic study of smart home usability and living experience. The purpose of the research was to evaluate three UIs—a PC, a media terminal, and a mobile phone—for smart home environments. The results show two main types of activity patterns, pattern control and instant control, which require different UI solutions. The results suggest that a PC can act as a central unit to control functions for activity patterns that can be planned and determined in advance. The mobile phone, on the other hand, is well suited for instant control. The mobile phone turned out to be the primary and most frequently used UI during the 6-month trial period in the smart apartment.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Organic smart home: architecture for energy management in intelligent buildings</title>
	<abstract>In this paper, we focus on a real world scenario of energy management of a smart-home. External signals, reflecting the low voltage grid's state, are used to address the challenge of balancing energy demand and generation. A flexible energy management framework for smart buildings is presented, to control the intelligent appliances, decentralized power plants and electrical storages.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Robotic smart house to assist people with movement disabilities</title>
	<abstract>This paper introduces a new robotic smart house, Intelligent Sweet Home, developed at KAIST in Korea, which is based on several robotic agents and aims at testing advanced concepts for independent living of the elderly and people with disabilities. The work focuses on technical solutions for human-friendly assistance in motion/mobility and advanced human-machine interfaces that provide simple control of all assistive robotic systems and home-installed appliances. The smart house concept includes an intelligent bed, intelligent wheelchair, and robotic hoist for effortless transfer of the user between bed and wheelchair. The design solutions comply with most of the users' requirements and suggestions collected by a special questionnaire survey of people with disabilities. The smart house responds to the user's commands as well as to the recognized intentions of the user. Various interfaces, based on hand gestures, voice, body movement, and posture, have been studied and tested. The paper describes the overall system structure and explains the design and functionality of some main system components.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Service-oriented smart home applications: composition, code generation, deployment, and execution</title>
	<abstract>A smart home usually has a variety of devices or home appliance, instead of designing software for a specific home, this paper proposes a service-oriented framework with a set of ontology systems to support service and device publishing, discovery of devices and their services, composition of control software using existing control services that wrap devices, deployment, and execution of the composed service in an computing environment, monitoring the execution, and recovery from device failure. The ontology systems specify semantic information about devices, services, and workflows used in various smart home, and users can compose and recompose services for their specific needs. New devices, workflows, and services can be added into ontology. Most of the steps in this process can be automated including code generation. For example, service composition will be carried out in three steps: abstract workflow design, function construction, and device discovery, and different codes can be generated for different computing platforms such as Java and Open Services Gateway initiative environments. In this way, a variety of smart home can be constructed rapidly using the framework by discovery and composition using existing services and workflows. This paper illustrates this framework using a media control example to illustrate the ontology, discovery, composition, deployment, execution, monitoring, and recovery.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Multiple method switching system for electrical appliances using programmable logic controller</title>
	<abstract>Smart switching system is designed for smart home control applications. The system uses real time programmable logic controller (PLC) for switching the electrical appliances. Electrical appliance is a part of our daily needs for mankind. When such needs are in a very high demand, the user's requirement is also increasing to access their electrical appliances in a more flexible way and also safer to use. The system is capable of switching any electrical appliances manually, preprogrammed and remotely. The most common practice in our daily life's is manual switching. Manual switching is done with human access according to instant needs. The second method in this system is preprogramming method which allows the users to program the usage of certain electrical appliances in advance or repetitively. Such function can be used on daily, weekly, monthly and even yearly basis based on the requirements of the user. The third method in this system is remote switching where the users are capable of sending Short Messaging System (SMS) to switch ON and OFF their electrical appliances using mobile phones. Global System for Mobile Communication Modulator and Demodulator (GSM Modem) enables SMS commands to be received to the PLC. The applications of technology have seen many new intelligent systems overwriting the conventional method to ease human life in many ways. Many systems are designed to be flexible for the users and enable them to communicate for the latest status of their electrical appliances at any time. Such applications will be practical and benefits for both the domestic and industrial users from various level of electrical applications without boundaries.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Protecting consumer privacy from electric load monitoring</title>
	<abstract>The smart grid introduces concerns for the loss of consumer privacy; recently deployed smart meters retain and distribute highly accurate profiles of home energy use. These profiles can be mined by Non Intrusive Load Monitors (NILMs) to expose much of the human activity within the served site. This paper introduces a new class of algorithms and systems, called Non Intrusive Load Leveling (NILL) to combat potential invasions of privacy. NILL uses an in-residence battery to mask variance in load on the grid, thus eliminating exposure of the appliance-driven information used to compromise consumer privacy. We use real residential energy use profiles to drive four simulated deployments of NILL. The simulations show that NILL exposes only 1.1 to 5.9 useful energy events per day hidden amongst hundreds or thousands of similar battery-suppressed events. Thus, the energy profiles exhibited by NILL are largely useless for current NILM algorithms. Surprisingly, such privacy gains can be achieved using battery systems whose storage capacity is far lower than the residence's aggregate load average. We conclude by discussing how the costs of NILL can be offset by energy savings under tiered energy schedules.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Real-Time human tracker based location and motion recognition for the ubiquitous smart home</title>
	<abstract>The ubiquitous smart home is the home of the future that takes advantage of context information from the human and the home environment and provides an automatic home service for the human. Human location and motion are the most important contexts in the ubiquitous smart home. We present a real-time human tracker that predicts human location and motion for the ubiquitous smart home. We used four network cameras for real-time human tracking. This paper explains the real-time human tracker’s architecture, and presents an algorithm with the details of two functions (prediction of human location and motion) in the real-time human tracker. The human location uses three kinds of background images (IMAGE1: empty room image, IMAGE2: image with furniture and home appliances in the home, IMAGE3: image with IMAGE2 and the human). The real-time human tracker decides whether the human is included with which furniture (or home appliance) through an analysis of three images, and predicts human motion using a support vector machine. A performance experiment of the human’s location, which uses three images, took an average of 0.037 seconds. The SVM’s feature of human’s motion recognition is decided from pixel number by array line of the moving object. We evaluated each motion 1000 times. The average accuracy of all the motions was found to be 86.5%.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Smart home – digitally engineered domestic life</title>
	<abstract>The purpose of the Smart Home Project is to devise a set of intelligent home appliances that can provide an awareness of the users' needs, providing them with a better home life experience without overpowering them with complex technologies and intuitive user interfaces. Our main aim for the project is to improve day-to-day home life with smart computer technologies while still keeping the home life as normal as possible – we refer to this as “digitally engineering analogue home life”.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The smart home landscape: a qualitative meta-analysis</title>
	<abstract>Technological innovations, varying from ubiquitous computing, intelligent appliances, telecommunication, robotics, to wearable sensors, enable new Smart Home (SH) applications. More and more academic publications reporting on experiments on SH can be found. A comprehensive clustering of concepts and approaches is largely missing. Based on an extensive review of SH literature, this paper proposes a framework that decomposes the SH research into four domains and 15 sub-domains. The framework is applied to visualize the state of the art of SH research, and to outline future challenges. The framework helps researchers to identify gaps in SH research.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A JXTA-based system for smart home</title>
	<abstract>In this paper, we present a JXTA-based system for a smart home environment where various types of computing devices communicate with each other to provide inhabitants with contextual services. The JXTA-based smart home network consists of peers which are appliances like PC, PDA, cell phone, TV, refrigerator, etc. Appliances share information in a JXTA network to understand different context and recommend the information suited to the context. Each appliance has a context interpreter that determines how to provide inhabitants with contextual service and resource. Since JXTA packages are simple to install and configure, average users can easily construct a smart home environment.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Research on smart multi-agent middleware for RFID-Based ubiquitous computing environment</title>
	<abstract>Previous RFID (Radio Frequency IDentification) middleware did not include intelligent capabilities such as automatically controlling home appliances and users. Therefore, we add USN (Ubiquitous Sensor Network) technology, which recognizes and manages an appliance’s state-information (temperature, humidity, pollution and so on) by connecting RFID tags, and propose the Smart Multi-Agent Middleware architecture for intelligently managing the RFID-based ubiquitous computing environment. We also present the proposed architecture’s execution procedure for controlling ubiquitous appliances.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The Smart Home meets the Web of Things</title>
	<abstract>In recent years, the merging of computing with physical things, enabled the transformation of everyday objects into information appliances. We propose to reuse the central principles of the modern Web architecture to fully integrate physical objects to the Web and build an interoperable Smart Home. We present an application framework that offers support for multiple home residents concurrently. We show that by using the Web as application layer we can build flexible applications on top of heterogeneous embedded devices with only a few lines of code, transforming home automation into a trivial task. We address many issues related to Web-enabling these devices, from their discovery and service description to the uniform interaction with them. Our evaluation efforts indicate that our framework offers acceptable performance and reliability.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Research on design and implementation of the artificial intelligence agent for smart home based on support vector machine</title>
	<abstract>In this paper, we provide information an artificial intelligence agent for a smart home and discuss a context model for implementation in an efficient smart home. An artificial intelligence agent in a smart home learns about the occupants and the smart environment, and predicts the appliance service that they will want. We propose the SVM (Support Vector Machine) for the learning and prediction aspects of the artificial intelligence agent. The experiment was done using three methods. Each of these three methods applies a higher importance to a different set of context data, out of the data related to the occupant, home environment, and the characteristics of the home appliances. Excellent results were seen when the experiment applied a higher importance to the data related to the characteristics of the home appliances.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Personalised acquisition of user preferences in Smart Homes</title>
	<abstract>Smart Home is a context-aware home environment which is constantly attentive to the activities of its inhabitants and provides proactive support to their goals -- that is, not only responds to the users' requests, but also initiates interactions with the users. Smart Home can interact with its inhabitants and guests via diverse home computers, augmented home appliances and personal devices, and act upon user's context and preferences. Apart from learning user preferences in different contexts from interaction history, Smart Home shall provide users with means to configure most important settings whenever they want it in a convenient and up-to-date way (so that the users can find all options fast, and do not wonder why they see broken devices in the list instead of recently bought ones). This work presents a summary of configuration requirements, based on discussions with users during system development; and implementation of GUI for acquisition of user preferences for Smart Home personalisation, which adapts to different users and system capabilities.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Multiagent system for home automation</title>
	<abstract>Smart-home conception has emerged in recent years and played a very important part in the formation of future houses. Since the beginning of the smart-home era, home automation benefits have never overshadowed the cost of such systems. One of these costs is that there is always the need for home inhabitants to program the system to perform daily tasks. In this paper we present prototype of a system that overcomes this problem by giving the home enough intelligence to adapt to its inhabitants life style without the need for the inhabitants to exercise authority. The system makes use of multi-agent and prediction techniques to provide intelligent smart-home appliances automation. Results indicate that the technique applied in this research not only makes the system very fast and accurate but also makes it portable and cost effective.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Context aware approach for smart homes</title>
	<abstract>Smart Homes is a classic example of ubiquitous computing. It includes several of intelligent home appliances capable of sensing the home's occupants and their current state, and providing appropriate services to them. The term context-aware systems are defined as one that can adapt according to its location of use, the collection of nearby people and objects, as well as the changes to those objects over time over the course of the day. There are various approaches to context-aware systems for smart homes as described in the paper below. It is an emerging new topic in the smart home arena.</abstract>
	<search_task_number>11</search_task_number>
	<query>smart appliance home</query>
	<relevance>1</relevance>
  </item>


  <item>
    <title>Analysis of selected industrial decision rules in a dynamic environment by means of computer simulation</title>
	<abstract>An abstract is not available. </abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
   <item>
    <title>HPNS: a hybrid process net simulation environment executing online dynamic models of industrial manufacturing systems</title>
	<abstract>Modelling technical systems nowadays is a great challenge of automation technology. Particular complex manufacturing processes, like the industrial paper production, consists of discrete and continuous signals and dynamic response times. Inspired by Petri nets, this paper proposes a new modelling approach is presented to describe those technical systems on different abstraction layer. The hybrid process net simulator (HPNS) framework allows to specify complex models, typically represented as a system of differential equations mixed with continuous and discrete-event subsystems, based on a bipartite graph structure. In addition, the HPNS execute model specifications with dynamic delays.

This paper focuses on the concept and the modelling approach. We give a short review of this new unified representation model for hybrid technical systems, the presentation of the model formalism is out of scope of this paper. A short summary about capabilities and restrictions of HPNS is presented. Moreover, examples of hybrid systems are presented.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Numerical health check of industrial simulation codes from HPC environments to new hardware technologies</title>
	<abstract>The numerical health check of industrial codes is crucial to give confidence about the computed results performed by studying the round-off error propagation. This problem is exacerbated in a supercomputing environment where trillions of floating-point operations may be performed every second. A parallel program based on domain decomposition as shown in this paper could compute slightly different results depending on the number of processors. This numerical health check is also needed to verify if a numerical code (or some parts of the numerical code) could still have an acceptable accuracy when using single precision instead of double precision which is useful to run numerical codes on new hardware technologies like GPU where the double precision is unavailable or expensive. The round-off error propagation is measured with the MPFI (interval arithmetic approach) and CADNA (probabilistic approach) libraries.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Industrial Strength Formal Verification Techniques for Hardware Designs</title>
	<abstract>The past decade has seen tremendous progress in the application of formal methods for hardware design and verification. While a number of different techniques based on BDDs, symbolic simulation, special-purpose decision procedures, model checking, and theorem proving have been applied with varying degrees of success, no one technique by itself has proven to be effective enough to verify a complex register-transfer level design, such as a state-of-the-art microprocessor. To scale up formal verification to industrial-scale designs it is necessary to combine these complimentary techniques within a general logical environment that can support appropriate abstraction mechanisms. The Prototype Verification System (PVS) is an environment to support the exploration of such a combined approach to verification. PVS is designed to exploit the synergies between language and deduction, automation and interaction, and theorem proving and model checking. This paper gives an overview of PVS and describes some of the major applications of PVS.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Architectural design of a situated multiagent system for controlling automatic guided vehicles</title>
	<abstract>Automatic Guided Vehicles (AGVs) are fully automated vehicles that are able to transport goods in an industrial environment. To cope with new and future system requirements such as flexibility and openness, we have applied a situated Multiagent System (MAS) to develop a decentralised control architecture for AGV transportation systems. In this paper, we give an overview of the software architecture of the system and we zoom in on two specific concerns: transport assignment and collision avoidance. We discuss the evaluation of the software architecture and the test results obtained from realistic simulations and a demonstrator system that we have developed. The architectural design and development of this real-world application teaches us that the primary use of a situated MAS comes from the way in which it structures the software. In particular, the set of adaptive agents that coordinate through the environment allows us to shape the software architecture of the transport application to provide the required functionalities of the system and achieve the important quality goals of flexibility and openness.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Simulating evolution in model-based product line engineering</title>
	<abstract>Context: Numerous approaches are available for modeling product lines and their variability. However, the long-term impacts of model-based development on maintenance effort and model complexity can hardly be investigated due to a lack of empirical data. Conducting empirical research in product line engineering is difficult as companies are typically reluctant to provide access to data from their product lines. Also, many benefits of product lines can be measured only in longitudinal studies, which are difficult to perform in most environments. Objective: In this paper, we thus aim to explore the benefit of simulation to investigate the evolution of model-based product lines. Method: We present a simulation approach for exploring the effects of product line evolution on model complexity and maintenance effort. Our simulation considers characteristics of product lines (e.g., size, dependencies in models) and we experiment with different evolution profiles (e.g., technical refactoring vs. placement of new products). Results: We apply the approach in a simulation experiment that uses data from real-world product lines from the domain of industrial automation systems to demonstrate its feasibility. Conclusion: Our results demonstrate that simulation contributes to understanding the effects of maintenance and evolution in model-based product lines. </abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Analysis and enhancement of planning and scheduling applications in a distributed simulation testbed</title>
	<abstract>Planning and scheduling applications and operations simulation models jointly represent the manufacturing activities of an enterprise. This paper relates to a framework that enables integration of both into a unified model and allows improvement of their performance with discrete event simulation (DES) technology. The High Level Architecture, which is the IEEE standard for interoperability of simulations, forms the backbone of this framework in which business applications can be re-used with operations simulation models to generate an integrated simulation model. This enables a company to optimise not only operational processes such as shop floor or warehouse operations but also business processes such as planning, order management and scheduling through simulation using the same software infrastructure. A case study to demonstrate the feasibility of this framework is included and ongoing work on implementation of this framework in an industrial environment is presented.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Rapid Modeling and Discovery of Priority Dispatching Rules: An Autonomous Learning Approach</title>
	<abstract>Priority-dispatching rules have been studied for many decades, and they form the backbone of much industrial scheduling practice. Developing new dispatching rules for a given environment, however, is usually a tedious process involving implementing different rules in a simulation model of the facility under study and evaluating the rule through extensive simulation experiments. In this research, an innovative approach is presented, which is capable of automatically discovering effective dispatching rules. This is a significant step beyond current applications of artificial intelligence to production scheduling, which are mainly based on learning to select a given rule from among a number of candidates rather than identifying new and potentially more effective rules. The proposed approach is evaluated in a variety of single machine environments, and discovers rules that are competitive with those in the literature, which are the results of decades of research.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Simulation based scheduling system in a semiconductor backend facility</title>
	<abstract>The semiconductor manufacturing process is usually divided in two parts: frontend and backend. In contrast to the frontend, where the manufacturing process is dominated by cluster-tools and cyclic routes, the backend has a predominant linear structure. In contrast to the frontend flow which is mostly controlled by dispatch rules, the backend process is suitable for real scheduling. A scheduling system for the backend of Infineon Technologies Dresden based on a Discrete Event Simulation (DES) system was developed and tested in the real industrial environment. The simulation model is automatically generated from the databases of the manufacturer. The system is used for short term scheduling - from one shift up to one week. The paper will focus on the aspect of optimizing the process flow and calculating exact release dates for lots. The basic principles are applicable not only in the semiconductor industry but also in other industrial sectors.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Constrained motion planning for open-chain industrial robots</title>
	<abstract>In the industrial environment, several constraints affect the robot motion planning. These are imposed by manufacturing considerations, such as, e.g., to strictly follow a given path, or by physical constraints, such as, e.g., to avoid torque saturation. Among the others, limitation on the velocity, acceleration, and jerk at the joints is often required by the robot manufacturers. In this paper, a motion planning algorithm for open-chain robot manipulators that takes into account several constraints simultaneously is presented. The algorithm developed approaches the motion planning algorithm from a wide perspective, solving systematically the joint as well as the Cartesian motion, both for the point-to-point and the fly movements. The validation has been performed first by numerical simulations and then by experiments on two different industrial manipulators, with different size, with and without the presence of a payload, by imposing demanding trajectories where all the constraints have been excited.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A CAN/IEEE 802.11b wireless Lan local bridge design</title>
	<abstract>Employing CAN in distributed real-time control applications sometimes critically requires increasing the size of distributed area, and communication with both other LANs and independent CAN segments. An interworking device with wireless support to extend CAN segments, utilizing an IEEE 802.11b WLAN is a practical solution for such an industrial environment. A key objective of this research work is to design and implement an interworking device called Wireless Interworking Unit (WIU) enabling remote CAN2.0A nodes to communicate over IEEE 802.11b WLAN using encapsulation method. Computer modeling and simulations of the proposed WIU are carried out using OPNET Modeler. The SAE Benchmark has been utilized in the networking models to evaluate the simulation results obtained. Considering the total end-to-end delay results of especially remote CAN messages for above 40 kbit/s bus rates, effect of the designed WIU is proved not causing to exceed the required arrival time deadline set by the SAE Benchmark. </abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Simulating Realistic Bridging and Crosstalk Faults in an Industrial Setting</title>
	<abstract>Three different techniques for simulating realistic faults generated from IC layout are discussed. Two of them deal with bridging faults, and the third one handles crosstalk faults. The simulation is performed on top of a commercial simulator and thus is very well applicable in an industrial environment. No change of the design database and only minimal changes of the test shell are required. Experimental results are reported for a library cell and a block from a full-custom design.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Risk assessment and management for supply chain networks: A case study</title>
	<abstract>The aim of this study is to show how a timed Petri nets framework can be used to model and analyze a supply chain (SC) network which is subject to various risks. The method is illustrated by an industrial case study. We first investigate the disruption factors of the SC network by a failure mode, effects and criticality analysis (FMECA) technique. We then integrate the risk management procedures into design, planning, and performance evaluation process of supply chain networks through Petri net (PN) based simulation. The developed PN model provides an efficient environment for defining uncertainties in the system and evaluating the added value of the risk mitigation actions. The findings of the case study shows that the system performance can be improved using risk management actions and the overall system costs can be reduced by mitigation scenarios. </abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Software products for risk assessment</title>
	<abstract>For assessing risks in all environments it is often necessary to use a systematic approach of modeling and simulation with the aim to simplify this process in the framework of designing new technological lines, machines, equipment and processes. Also an experiment on a real technological device can be used as the basic scientific method of research when assessing and evaluating risks in the industrial processes. However, experimenting with technological equipment which uses hazardous substances is very risky and can be the source of an accident, or disaster, e.g. the disaster at the nuclear power plant in Chernobyl in 1986.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Performance analysis of Ethernet Powerlink networks for distributed control and automation systems</title>
	<abstract>Industrial communication networks are a key element for developing advanced distributed control and automation systems. On the one hand, high performance and low costs are generally required to cope with more and more demanding application requirements, while, on the other hand, real-time capabilities are often needed in an increasing number of automation scenarios. Communication solutions based on Ethernet technologies are becoming popular in many industrial and factory environments and a number of soft/hard real-time competing products have been proposed in the last few years. This paper focuses on the Ethernet Powerlink standard, in general, and on some of its real-time characteristics, in particular. In fact, we present some basic results concerning both cyclic and acyclic real-time traffic in Ethernet Powerlink networks, obtained by extensive simulation, which can be used as a basic step for a better understanding of the real-time behavior of this protocol. </abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>High Performance Computing in Multibody System Design</title>
	<abstract>This paper presents recent developments of high performance computing and networking techniques in the field of computer-aided multibody analysis and design. The authors describe the main achievements obtained in the development of a tool to aid in the design of new industrial mechanical systems by performing parallel parametric multibody simulations. The parallel software is composed of four main modules: two user-friendly interfaces to input the data and visualize the results, a simulation module, and a parallel manager module that is the main focus of this paper. The authors show that the implementation, using PVM, of simple and well-known ideas leads to efficient and flexible parallel software targeted for heterogeneous networks of nondedicated workstations, which is the parallel platform available in most mechanical design departments. The authors also describe the main features of this module that implements, for the sake of efficiency and robustness, a load-balancing strategy and fault tolerance capabilities. One of the main results of this development has been to demonstrate that high performance computing can sometimes be easily and efficiently introduced into industrial companies without requiring major investments. Pictures of the user interfaces are depicted to illustrate the usability of the parallel software. Performance observed on single and multiprocessor workstation networks is given to assess the relevance of using high performance computing techniques in an industrial environment for the design of mechanical systems.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Alleviating the collision states and fleet optimization by introducing a new generation of automated guided vehicle systems</title>
	<abstract>The aim of the current research is to propose a new generation of automated guided vehicle systems for alleviating the collision states in material handling systems where the automated guided vehicles movements are allowed to be both unidirectional and bidirectional. The objective function is to maximize the average annual profit in an FMS system using a simulation method. Despite several researches done in this field, this criterion has been studied rarely. The current study includes some new changes in AGV design for preventing some common problems such as congestions and deadlocks based on real profits/costs analysis in a flexible manufacturing system. For this reason, some experiments have been carried out to study the effects of several empty vehicle dispatching rules on average annual profit. The results show that the proposed framework is efficient and robust enough for industrial environments.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Shop floor scheduling in a mobile robotic environment</title>
	<abstract>Nowadays,it is far more common to see mobile robotics working in the industrial sphere due to the mandatory need to achieve a new level of productivity and increase profits by reducing production costs. Management scheduling and task scheduling are crucial for companies that incessantly seek to improve their processes, increase their efficiency, reduce their production time and capitalize on their infrastructure by increasing and improving production.

However, when faced with the constant decrease in production cycles, management algorithms can no longer solely focus on the mere management of the resources available, they must attempt to optimize every interaction between them, to achieve maximum efficiency for each production resource.

In this paper we focus on the presentation of the new competition called Robot@Factory, its environment and its main objectives, paying special attention to the scheduling algorithm developed for this specific case study. The findings from the simulation approach have allowed us to conclude that mobile robotic path planning and the scheduling of the associated tasks represent a complex problem that has a strong impact on the efficiency of the entire production process.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Formal Specification and Analysis of Domain Specific Models Using Maude</title>
	<abstract>Modeling languages play a cornerstone role in model-driven software development for representing models and metamodels. Modeling languages are usually defined in terms of their abstract and concrete syntax. This allows the rapid development of languages and some associated tools (e.g. editors), but does not allow the representation of their behavioral semantics, something especially important in certain industrial environments in which simulation and verification are critical issues. In this paper we explore the use of Maude as a formal notation for describing models, metamodels, and their dynamic behavior, making models amenable to formal analysis, reasoning, and simulation.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A novel real-time MAC protocol exploiting spatial and temporal channel diversity in wireless industrial networks</title>
	<abstract>Wireless technology is increasingly finding its way into industrial communication because of the tremendous advantages it is capable of offering. However, the high bit error rate characteristics of wireless channel due to conditions like attenuation, noise, channel fading and interference seriously impact the timeliness and guarantee that need to be provided for real-time traffic. Existing wired protocols including the popular PROFIBUS perform unfavorably when extended or adapted to the wireless context. Other wireless protocols proposed either do not adapt well to erroneous channel conditions or do not provide real-time guarantees. In this paper, we present a novel real-time MAC (Medium Access Control) protocol that is specifically tailored to the message characteristics and requirements of the industrial environments. The protocol exploits both the spatial and temporal diversity of the wireless channel to effectively schedule real-time messages in the presence of bursty channel error conditions. Simulation results show that the proposed protocol achieves much better loss rate compared to baseline protocols under bursty channel conditions.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Requirements for Safe Robots: Measurements, Analysis and New Insights</title>
	<abstract>Physical human-robot interaction and cooperation has become a topic of increasing importance and of major focus in robotics research. An essential requirement of a robot designed for high mobility and direct interaction with human users or uncertain environments is that it must in no case pose a threat to the human. Until recently, quite a few attempts were made to investigate real-world threats via collision tests and use the outcome to considerably improve safety during physical human-robot interaction. In this paper, we give an overview of our systematic evaluation of safety in human-robot interaction, covering various aspects of the most significant injury mechanisms. In order to quantify the potential injury risk emanating from such a manipulator, impact tests with the DLR-Lightweight Robot III were carried out using standard automobile crash test facilities at the German Automobile Club (ADAC). Based on these tests, several industrial robots of different weight have been evaluated and the influence of the robot mass and velocity have been investigated. The evaluated non-constrained impacts would only partially capture the nature of human-robot safety. A possibly constrained environment and its effect on the resulting human injuries are discussed and evaluated from different perspectives. As well as such impact tests and simulations, we have analyzed the problem of the quasi-static constrained impact, which could pose a serious threat to the human even for low-inertia robots under certain circumstances. Finally, possible injuries relevant in robotics are summarized and systematically classified.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The Impact of Delay Spread on Irreducible Errors for Wideband Channels on Industrial Sites</title>
	<abstract>To allow development of highly reliable wide-band mobile communications on industrial sites it is imperative to be able to characterise the multi-path performance of the propagation. This performance can be statistically characterised with channel impulse response (CIR) results. Such results from a wide variety of industrial environments are used with bit error ratio (BER) measurements to predict irreducible-BER performance as a function of RMS delay spread (RDS). These are compared to Chuang's simulation results and are first experimental results to challenge and to some extent confirm those results. The way these results can be used in developing communications systems for industrial sites is discussed.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems: demo papers</title>
	<abstract>The AAMAS conference is the premier international forum for research in autonomous agents and multi-agent systems. Since its first edition in 2002, it has featured a demonstration session where live interactive demos are given. The goal of the AAMAS demonstration session is to give participants, from industry and academia alike, an opportunity to present their latest developments on software and robotic systems.

In 2006, the demonstration track featured 17 demos, followed in 2007 by 11 demos coming primarily from academic (software) contributors, as well as a some research labs and industry.

In the 2008 edition, demonstrations were invited for three different categories: academic software, industrial software, and robotics. Another change we introduced this year is that the winners of the "best demo" awards will be chosen by public vote, and there will separate awards for each of the demo categories. Also, the demonstrations will be presented in a plenary session for the first time this year.

We were delighted to receive an unusually high number of excellent submissions, each one of them attesting to the clear maturity of current Agents technology. Due to space constraints, we have been able to accept only 34 demos from the many strong submissions we received. However, the vast majority of them are still academic, although some with apparent industrial links. In particular, the accepted demos were 23 in the academic software category, 5 in industrial software, and 6 in robotics.

Examples of demo topics of interest included, but were not limited to:

• Multi-agent software systems

• Industrial and military applications (including prototypes)

• Agent-based games

• Agent platforms and development environments

• Open-source software tools

• Robotic systems (single and multi-agent)

• Virtual agents and interactive virtual environments

• Simulation environments

This section of the proceedings includes 2-page papers describing each demo presentation at AAMAS'08. Taken together, we believe these articles showcase the amazing recent progress in the area of Autonomous Agents and Multi-Agent Systems, and we look forward for a very exciting demo session at AAMAS'08. We hope that these articles will promote discussion and future research, as well as encourage the AAMAS community to get involved in implementing, deploying, and popularising novel Agents technology.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Design and implementation of efficient intelligent robotic gripper</title>
	<abstract>The gripper is a critical component of the industrial robot. It is greatly useful in industrial environments for object grasping during handling process. In this paper, a gripper is designed and implemented to grasp unknown objects with different masses, dimensions, and coefficients of friction. The design and control of the gripper system takes into consideration simplicity in the mechanical system, large variety of grasped objects and low cost. The proposed grasping process during object lifting and handling is mainly based on the slip reflex principle, as applying insufficient force leads to object slipping, and dropping may occur. On the other hand, applying extra force during grasping may lead to object crushing. A new system controller using fuzzy logic based on empirical investigation of the human hand skills is proposed. The target is to control the applied force on the object to avoid object crushing or dropping. Adaptive neuro-fuzzy inference system ANFIS is used to model the assigned robotic gripper based on input/output variables measurements. The performance of the gripper system and the control algorithm was tested first by simulation. Reduction of the distance of the slippage and process time was confirmed experimentally. Simulation and experimental results are presented and discussed.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Fuzzy volumes, principles and applications</title>
	<abstract>The 3D simulation of objects in an evolutionist environment is hard to implement when we consider the amount of parameters and the high cost of computer resources. These facts bring us to introduce the notion of associated fuzzy volumes. This concept is based on the fuzzy geometry principles, which can be used in production with robot assistance. The technique is the associating to complex objects and their environment some fuzzy volumes, constitute of peaks network (points positioned in 3D) in which these points are situated around an object or on its surface in order to approach a target object. With the algorithm developed in this paper, which is based on a genetic algorithm, it is possible to detect interference during the simulation time. The industrial manipulator is used as first application of this technology, the second application is the NC machining.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Fractional order adaptive control for manipulator systems</title>
	<abstract>The relatively slow progress in the subject of sensor-based robot intelligence and autonomy naturally made the interest in the development of haptic devices and master-slave systems increase. Such systems can roughly be categorized as devices of high precision and limited work-load, and instruments developed for dealing with great work-load and limited precision in harsh industrial environment like in the case of deburring iron casts. In this latter class hydraulic actuation shows crucial advantages in comparison with electric and pneumatic systems. Aiming at this potential field of application in the labs of Budapest Tech a 6 Degree Of Freedom (DOF) hydraulic joystick-like master arm has been developed in which reflecting and sensing the necessary forces is solved by a hydraulic subsystem, too. In the first part of this paper a brief overview of the master-slave development is presented. In the rest potential improvement of the already achieved results is summarized on the basis of a detailed mathematical model of the electric servo valve controlled hydraulic differential cylinder with especial emphasis on the adhesion between the cylinder and the packing ring of the piston. Adhesion is roughly nonlinear effect causing abrupt change in the direction of the friction forces in the zero-transmission of the piston-cylinder relative velocity. Traditional control applying PID-type feedback yields poor results near zero velocity where the behavior of the friction forces is roughly nonlinear, and almost singular. The operation of a proposed adaptive control using fractional order derivatives is discussed and demonstrated by simulation results in the second part of the paper.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Distributed Simulation in Manufacturing and Logistics</title>
	<abstract>Distributed simulation technology was originally developed for application in the military domain. Subsequently, the availability of synchronisation middleware such as the Runtime Infrastructure of the High Level Architecture has also inspired research looking at application of distributed simulation for modelling and analysis of other large-scale, heterogeneous systems such as supply networks. In this keynote talk, research accomplishments that have been made in Singapore in the area of distributed simulation in the context of manufacturing and logistics will be reviewed. With a dozen wafer fabrication plants and more than 20 assembly &amp; test facilities currently being operated in Singapore, potential application scenarios have naturally been looking at the semiconductor manufacturing domain. Challenges in view of implementation of distributed simulation in an industrial environment and application for the resolution of real-world problems will be discussed as well.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Demo abstract: Application of WINTeR industrial testbed to the analysis of closed-loop control systems in wireless sensor networks</title>
	<abstract>WINTeR is an open access, multi-user experimental facility which supports implementation and evaluation of wireless sensor networks for industrial applications in radio-harsh environments. An important new characteristic of the testbed is its ability to incorporate built-in and user generated dynamical models of industrial processes. This allows researchers to inexpensively and realistically evaluate wireless technologies, topologies and other variables in the context of direrent closed-loop dynamical systems. The present demo gives an overview of the testbed and introduces its real-time simulation abilities by utilizing three dynamical models of known processes with direrent response times. As two of a myriad of variables, the demonstration will show the effects of electromagnetic interference and topology in the response of such closed loop control systems.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Modifications of High Level VHDL Descriptions for Fault Detection or Tolerance</title>
	<abstract>The need for integrated mechanisms providing on-lineerror detection or fault tolerance is becoming a majorconcern due to the increasing sensitivity of the circuits totheir environment. This paper reports on a toolautomating the implementation of such mechanisms bymodifying high-level VHDL descriptions. Themodifications are compatible with industrial design flowsbased on commercial synthesis and simulation tools. Theresults demonstrate the feasibility and the efficiency of theapproach.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Real time nonlinear indoor positioning with trilateration based on microwave backscatter</title>
	<abstract>We present two techniques for estimating accurately the position of a mobile base station in industrial environment with Trilateration. By eliminating the need to measure angles, trilateration uses only distance measurements to estimate position, which facilitate the implementation of real-time positioning systems like the Global Positioning System (GPS). Iterative linear least squares and nonlinear least squares estimators are implemented through simulation. We consider the minimization of the squares of errors, where the main difference depends on the linearity of the measurement equation which relates the coordinates to be estimated to the measured distance. The results obtained show that the nonlinear squares technique results in more accurate position estimates than linear least squares. Error distribution functions and histograms in both cases are also derived and analysed.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Research Article: The metabolic pH response in Lactococcus lactis: An integrative experimental and modelling approach</title>
	<abstract>Lactococcus lactis is characterised by its ability to convert sugar almost exclusively into lactic acid. This organic acid lowers extracellular pH, thus inhibiting growth of competing bacteria. Although L. lactis is able to survive at low pH, glycolysis is strongly affected at pH values below 5, showing reduced rate of glucose consumption. Therefore, in order to deepen our knowledge on central metabolism of L. lactis in natural or industrial environments, an existing full scale kinetic model of glucose metabolism was extended to simulate the impact of lowering extracellular pH in non-growing cells of L. lactis MG1363. Validation of the model was performed using ^1^3C NMR, ^3^1P NMR, and nicotinamide adenine dinucleotide hydride auto-fluorescence data of living cells metabolizing glucose at different pH values. The changes in the rate of glycolysis as well as in the dynamics of intracellular metabolites (NADH, nucleotide triphosphates and fructose-1,6-bisphosphate) observed during glucose pulse experiments were reproduced by model simulations. The model allowed investigation of key enzymes at sub-optimum extracellular pH, simulating their response to changing conditions in the complex network, as opposed to in vitro enzyme studies. The model predicts that a major cause of the decrease in the glycolytic rate, upon lowering the extracellular pH, is the lower pool of phosphoenolpyruvate available to fuel glucose uptake via the phosphoenolpyruvate-dependent transport system. </abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>WATIS2: Design and Application of an Environment Simulation System for Test Improvement of Control Software for Automatic Logistic Systems</title>
	<abstract>Control computer systems for automatic logistic systems contain reactive modules, which are usually tested on-site, in conjunction with the real environment. This leads to high-test costs and unsatisfactory test coverage. Environment simulation models offer an execution environment for customizable, in-house testing. Architecture and process model of WATIS2, a flexible, efficient and expansible environment simulation system for automatic logistic systems are presented. Experiments in an industrial environment show a high model reuse rate and a significant improvement of baseline software development regarding software quality and project cost, in this way justifying a systematic application of WATIS2. </abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Dynamic priority allocation for conflict free coordinated manipulation of multiple agents</title>
	<abstract>This article presents a decoupled approach for conflict free coordinated manipulation of multiple industrial robot agents operating in a typical industrial environment. Decoupled method proposed in this work plans the path for the robots in two steps. In the first step, for each participating robot agent a path collision free with respect to stationary obstacle is obtained. The coordination among agents is achieved in the second step of the approach by use of off-line strategy for conflict resolution. The key objective of the conflict resolution is dynamic assignment of priorities for path modification of the robot agents. Thus, the agents involved in a local conflict situation coordinate their movement by adopting the lowest movement cost motion paths. Results obtained from realistic simulation of multi-robot agent environment with three industrial robots demonstrate that the proposed approach assures the rapid, concurrent and conflict free coordinated motion of the robot agents.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Study on the evolutionary optimisation of the topology of network control systems</title>
	<abstract>Computer networks have been very popular in enterprise applications. However, optimisation of network designs that allows networks to be used more efficiently in industrial environment and enterprise applications remains an interesting research topic. This article mainly discusses the topology optimisation theory and methods of the network control system based on switched Ethernet in an industrial context. Factors that affect the real-time performance of the industrial control network are presented in detail, and optimisation criteria with their internal relations are analysed. After the definition of performance parameters, the normalised indices for the evaluation of the topology optimisation are proposed. The topology optimisation problem is formulated as a multi-objective optimisation problem and the evolutionary algorithm is applied to solve it. Special communication characteristics of the industrial control network are considered in the optimisation process. In respect to the evolutionary algorithm design, an improved arena algorithm is proposed for the construction of the non-dominated set of the population. In addition, for the evaluation of individuals, the integrated use of the dominative relation method and the objective function combination method, for reducing the computational cost of the algorithm, are given. Simulation tests show that the performance of the proposed algorithm is preferable and superior compared to other algorithms. The final solution greatly improves the following indices: traffic localisation, traffic balance and utilisation rate balance of switches. In addition, a new performance index with its estimation process is proposed.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An intelligent modeling system for simulation manufacturing processes</title>
	<abstract>This paper presents the development of an intelligent system for simulating manufacturing processes. The emphasis of this paper is placed on the following elements of the intelligent system: the natural language interface, especially the parser, the simulation analyzer, and the simulation writer. An application of the intelligent manufacturing simulation system in an industrial environment is also presented.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Building Robust Wireless LAN for Industrial Control with DSSS-CDMA Cellphone Network Paradigm</title>
	<abstract>Deploying Wireless LAN for Industrial Control (IC-WLAN) has many benefits, such as mobility, low deployment cost and ease of reconfiguration. However, the top concern is robustness of wireless communications. Wireless control loops must be maintained under persistent adverse channel conditions, such as noise, large-scale path loss and fading. Many electro-magnetic interference sources in industrial environments, e.g. electric motor and welding, make wireless communication more challenging. The conventional IEEE 802.11 WLANs, which are designed for providing high bandwidth instead of high robustness, are therefore inappropriate for IC-WLAN. On the other hand, if the low data rate feature of industrial control is fully exploited by the state-of-the-art Direct Sequence Spread Spectrum (DSSS) technology, much higher robustness can be achieved. We hereby propose using DSSS-CDMA to build IC-WLAN, and exploiting the low data rate feature of industrial control loops for enhanced robustness. We carried out fine-grained physical layer simulations and Monte Carlo comparisons. The results show that DSSS-CDMA IC-WLAN provides much higher robustness than IEEE 802.11 WLAN, so that reliable wireless industrial control loops are made feasible. The DSSS-CDMA IC-WLAN scheme also opens up a new problem space for interdisciplinary study, involving real-time scheduling and resource management, communication, networking and control. In this paper, we study the resource management problems on maximizing robustness and minimizing control utility loss. Analytical resource optimization solutions are given.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Simulation pour l'aide a l'optimisation et fabrication intelligente des composites par injection sous renfort</title>
	<abstract>The goal of this thesis is to develop modeling tools designed towards engineering needs of the composites manufacturing industry. Most of the discussion will be focused on the resin transfer moulding process (RTM) although most of it applies also to the liquid composite moulding processes in general.' In this thesis, the scientific notions are reviewed, explained and simplified to help engineers in deeply understanding the capability of composite modeling.

The first chapter starts by defining the vocabulary needed to understand the RTM process. The physical models found in literature are introduced one at a time. When possible some analytical solutions and graphics are proposed to give a first understanding of the phenomenon. Then, scientific characterisation tools for quantifying the physical parameters applied to models are presented followed by a description of the sensing devices available for the molds. This chapter finishes by introducing the optimization techniques that are widely used in the literature.

The second chapter gives an exhaustive review of the numerical filling algorithms dedicated to the RTM process. Each algorithm is explained and tested for its precision and performance. This section includes the proposition a new filling algorithm based on finite element and Monte-Carlo methods. This algorithm shows an average speedup of four times the speed of the equivalent implicit algorithm. It can use meshes that are poorly adapted to filling simulation as those created for structural analysis.

The third chapter present the details of a new software algorithm. This software was created to vulgarise the use of numerical simulations. A methodology is proposed using analytical, semi-analytical and numerical solution to delimit the functional space of a specific process and to compare different processing scenarios in real time. By combining physical constraints with numerical results, the software is able to plot the moldability diagram. This diagram can be plotted on different forms exposing visually different optimal solution of a given problem. One novelty of the software is to include the intelligence and experience of the user in the optimisation loop. Also, it should be noted that this software possesses a 3D game interface that enable to use it as would children do without reading the instruction manual.

Engineers often ask about the applicability of scientific models in an industrial environment. The fourth chapter described the design of a new measurement apparatus half way between numerical modeling and physical manufacturing of real part. It is a heated small scale mold instrumented with heat flux sensor and thermocouples. This mini-mould was conceived to be used as an industrial differential scanning calorimeter. This chapter reviews the work done during this thesis from the first prototype to the actual version describing the computer, electronic and mechanical design. Software was developed for this tool that enable network users to control the experiments. This will allows engineers to reach specialists from over the world to get their feedback in real time.

Finally, the last chapter gives a short discussion of the overall work and propose some future works. Some annexes were also appended to this thesis.

To conclude, this document proposed a series of scientific modeling tools addressed toward industrial needs. From finite elements and analytical model up to small scale simulator, this thesis has given different levels of modeling. The author hopes that it will put forward the use of modeling in composite manufacturing less prone to trials and errors and more toward intelligence and knowledge.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Automating the Device Interface Board Modeling for Virtual Test</title>
	<abstract>Virtual Test (VT), allowing to verify test programs and associated test hardware in a simulation environment before silicon and without a real Automatic Test Equipment (ATE), has proven to be an effective technique to cut time-to-market and costs for mixed-signal System on Chip (SoC), even in an industrial environment. Mixed-signal VT requires behavioral models both for the Device Under Test (DUT) and the Device Interface Board (DIB), interfacing the DUT with the ATE. Test Engineers are used to design the DIB circuitry in detail. The resulting schematic is used for the layout and manufacturing of the DIB. In order to develop a behavioral model of the DIB, the Test Engineer has (1) to abstract this low-level schematic, by identifying therelevant functions and signal flows, and (2) to write the model manually. This bottom-up procedure is very time consuming and may lead to inconsistencies and errors. This paper introduces a new top-down flow for DIB development, allowing the automatic generation of the DIB behavioral model using VHDL as the modeling language. Test Engineers enter the design of the DIB by a schematic entry tool at a very abstract level (i.e. as a block diagram). Using specific libraries, Test Engineers can then generate the real circuit schematic or VHDLmodel by dedicated netlisters. </abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Stereo vision based navigation for automated vehicles in industry</title>
	<abstract>This paper proposes a stereo vision based localization and mapping strategy for vehicular navigation within industrial environments using natural landmarks. The work proposed is strictly related to factory automation, since focus is on industrial vehicle autonomous navigation for material handling, in order to increase the operating efficiency with reduced risk for accidents. The stereovision system, proposed as the main sensor, provides the necessary feedback to navigate and simultaneously calibrate the stereocamera parameters (like the camera separation, focal length, camera placement with respect to the robot, etc.). It uses the natural landmarks already present in the environment without additional infrastructures. Some simulation and experimental results are presented in order to explain the proposed method and current status.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Performance of UWB Systems with Suboptimal Receivers under IEEE 802.15.4a Industrial Environments</title>
	<abstract>In this paper, under the IEEE 802.15.4a line-of-sight (LOS) and non-line-of-sight (NLOS) industrial multipath environments, the performances of the selective Rake receiver and the differential detection receiver are simulated for impulse radio ultra-wideband (UWB) systems. The simulation results reveal that in the LOS industrial environments the selective Rake receiver has better performance. In the NLOS industrial environments, the differential detection receiver is a more viable choice with the low pulse repeat rate when the inter-symbol interference (ISI) effect can be ignored; however, the selective Rake receiver is a better choice with the high pulse repeat rate when the ISI has to be taken into account. </abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A system simulation framework</title>
	<abstract>A generic framework is described that supports the design and simulation of complex systems. The simulation framework incorporates a CAD Framework that has been extended with high-level services for visualizing and exploring system performance. The simulation framework is used in a number of industrial environments for the development and optimization of VLSI designs, electro-optical systems, Cathode Ray Tubes, and a manufacturing process.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Memory Testing Under Different Stress Conditions: An Industrial Evaluation</title>
	<abstract>This paper presents the effectiveness of various stress conditions (mainly voltage and frequency) on detecting the resistive shorts and open defects in deep sub-micron embedded memories in an industrial environment. Simulation studies on very-low voltage, high voltage and at-speed testing show the need of the stress conditions for high quality products; i.e., low defect-per-million (DPM) level, which is driving the semiconductor market today.The above test conditions have been validated to screen out bad devices on real silicon (a test-chip) built on CMOS 0.18 um technology.IFA (inductive fault analysis) based simulation technique leads to an efficient fault coverage and DPM estimator, which helps the customers upfront to make decisions on test algorithm implementations under different stress conditions in order to reduce the number of test escapes.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Building Robust Wireless LAN for Industrial Control with the DSSS-CDMA Cell Phone Network Paradigm</title>
	<abstract>Wireless LAN for Industrial Control (IC-WLAN) provides many benefits, such as mobility, low deployment cost, and ease of reconfiguration. However, the top concern is robustness of wireless communications. Wireless control loops must be maintained under persistent adverse channel conditions, such as noise, large-scale path loss, fading, and many electromagnetic interference sources in industrial environments. The conventional IEEE 802.11 WLANs, originally designed for high bandwidth instead of high robustness, are therefore inappropriate for IC-WLAN. A solution lies in the Direct Sequence Spread Spectrum (DSSS) technology: By deploying the largest possible processing gain (slowest bit rate) that fully exploits the low data rate feature of industrial control, much higher robustness can be achieved. We hereby propose using DSSS-CDMA to build IC-WLAN. We carry out fine-grained physical layer simulations and Monte Carlo comparisons. The results show that DSSS-CDMA IC-WLAN provides much higher robustness than IEEE 802.11/802.15.4 WLAN, so that reliable wireless industrial control loops become feasible. We also show that deploying larger processing gain is preferable to deploying more intensive convolutional coding. The DSSS-CDMA IC-WLAN scheme also opens up a new problem space for interdisciplinary study, involving real-time scheduling, resource management, communication, networking, and control.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>High performance air pollution modeling for a power plant environment</title>
	<abstract>The aim of this work is to provide a high performance air quality simulation using the STEM-II (Sulphur Transport Eulerian Model 2) program, a large-scale pollution modeling application. First, we optimize the sequential program with the aim of increasing data locality. Then, we parallelized the program using OpenMP directives for shared memory systems, and the MPI library for distributed memory machines. Performance results are presented for a SGI O2000 multiproccessor, a Fujitsu AP3000 multicomputer and a Cluster of PCs. Experimental results show that the parallel versions of the code achieve important reductions in the CPU time needed by each simulation. This will allow us to obtain results with adequate speed and reliability for the industrial environment where it is intended to be applied.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Physically-based sampling for motion planning</title>
	<abstract>Motion planning is a fundamental problem with applications in a wide variety of areas including robotics, computer graphics, animation, virtual prototyping, medical simulations, industrial simulations, and traffic planning. Despite being an active area of research for nearly four decades, prior motion planning algorithms are unable to provide adequate solutions that satisfy the constraints that arise in these applications. We present a novel approach based on physics-based sampling for motion planning that can compute collision-free paths while also satisfying many physical constraints. Our planning algorithms use constrained simulation to generate samples which are biased in the direction of the final goal positions of the agent or agents. The underlying simulation core implicitly incorporates kinematics and dynamics of the robot or agent as constraints or as part of the motion model itself. Thus, the resulting motion is smooth and physically-plausible for both single robot and multi-robot planning.

We apply our approach to planning of deformable soft-body agents via the use of graphics hardware accelerated interference queries. We highlight the approach with a case study on pre-operative planning for liver chemoembolization. Next, we apply it to the case of highly articulated serial chains. Through dynamic dimensionality reduction and optimized collision response, we can successfully plan the motion of "snake-like" robots in a practical amount of time despite the high number of degrees of freedom in the problem. Finally, we show the use of the approach for a large number of bodies in dynamic environments. By applying our approach to both global and local interactions between agents, we can successfully plan for thousands of simple robots in real-world scenarios. We demonstrate their application to large crowd simulations.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Analyzing Rule-Based Behavioral Semantics of Visual Modeling Languages with Maude</title>
	<abstract>There is a growing need to explicitly represent the behavioral semantics of Modeling Languages in a precise way, something especially important in industrial environments in which simulation and verification are critical issues. Graph transformation provides one way to specify the semantics of Domain Specific Visual Languages (DSVLs), with the advantage of being intuitive and easy to use for the system designer. Even though its theory has been extensively developed during the last 30 years, it has some limitations concerning specific analysis capabilities. On the contrary, Maude is a rewriting logic-based language with very good formal analysis support, but which requires specialized knowledge. In this paper we show how a mapping between graph transformation-based specifications of DSVL semantics and Maude is possible. This allows performing simulation, reachability and model-checking analysis on the models, using the tools and techniques that Maude provides.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Extending CAN segments with IEEE 802.11 WLAN</title>
	<abstract>Summary form only given. Controller area networks (CAN) are currently employed in many distributed real-time control applications in industrial environments. The CAN protocol based on a CSMA/CD+CR access mechanism with the use of priorities is a serial communication protocol and is used to support distributed real-time control and multiplexing. CAN-based distributed control systems have two main problems. They are the size of distributed area and the need for communication with other LANs and with remote CAN segments. A straightforward solution is to use interworking devices with wireless support to extend CAN segments, exploiting an IEEE 802.11 WLAN that is low cost technology with high data rates. This research study aims at designing and implementing such an interworking device called a wireless internetworking unit (WIU) that is capable of connecting remote CAN 2.0A nodes over IEEE 802.11b WLAN using an encapsulation method. Computer modeling and simulation of the proposed approach realized using OPNET Modeler and analysis of the simulation results obtained are also presented. </abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>On Identifying Indistinguishable Path Delay Faults and Improving Diagnosis</title>
	<abstract>Work in path delay fault testing and fault simulation has gained importance in the industry since supplementing stuck-at fault coverage with delay fault coverage may result in better defect coverage than that is possible with stuck-at fault coverage alone. Timing related failures must be diagnosed to determine which nodes or paths in the circuit have excessive delays and techniques are needed to simplify the process of attributing the delays to a minimal set of nodes or paths. In this work, we propose a method for determining path delay faults that are tested simultaneously by the same test and do not have separate tests either because ofcircuit functionality or restrictions imposed by a given test set. We call such faults indistinguishable path delay faults. We further propose a method for identifying scanout nodesin the circuit that reduce the number of indistinguishable faults and improve diagnosticresolution. The method for identifying indistinguishable faults can be used before test application to determine the extent of diagnostic resolution possible for a given test set. It also enables a designer to make the design changes needed for improving diagnostic resolution. This has a large impact on the turnaround time for the diagnosis and debug processes. We perform experiments on the partitions of a picoJava¿ processor core to show the effectiveness of the proposed method. Results show that it requires only a reasonable amount of time for identifying indistinguishable faults and that the proposed methods are practical to be deployed in an industrial environment.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>High performance air pollution simulation on shared memory systems</title>
	<abstract>The aim of this work is to provide a high performance air quality simulation using the STEM-II (Sulphur Transport Eulerian Model 2) program, a large pollution modelling application. The execution of the sequential program on a workstation requires significant CPU time and memory resources. Thus, in order to reduce the computational cost, we have applied parallelization techniques to the most costly parts of the original source code. Experimental results on a 32-processor SGI Origin 2000 show that the parallel program achieves important reductions in the execution times. This will allow us to obtain results in valid response times and with the adequate reliability for the industrial environment where it is intended to be applied.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Analysis of machine learning models and prediction tools for paper machine systems</title>
	<abstract>Organizations within the paper industry have dedicated significant time and resources into the development and application of modeling and simulation techniques and prediction tools. This research looks to apply some of these software tools to make the best use of real world data from various sensor locations in a secondary fiber recovery mill.

This research explored two separate problem areas in the mill and analyzed machine learning models and prediction tools which were best suited for the challenges presented. The first part of this research explores data analysis techniques to optimize a neural network model and a subsequent sensitivity analysis to predict and analyze the sources of variability in moisture content on the paper machine wet end.

The second part of this work explores some tools which could help a paper mill operator or engineer better assess when a paper break is likely to occur. The model developed will show how to make better use of information from operator logs by incorporating this information into a predictive model. This problem was especially challenging because of the great deal of variability in the industrial environment and because of the influence of many outside factors. The methods explored in this research proved useful in identifying clusters which represent various modes of good and poor operation on the paper machine.

A data set was provided for this research by the RockTenn Paper Mill in Solvay, NY. This mill is a 100% recycled containerboard paper mill. As with other secondary fiber recovery systems, this mill has a great deal of variability in its incoming raw material which makes it even more of a challenge to develop predictive models.

Keywords. Artificial Neural Networks, K-Means, Clustering, Machine-Learning, Fault Detection, Supervised Learning, Unsupervised Learning.</abstract>
	<search_task_number>8</search_task_number>
	<query>simulation industrial environment</query>
	<relevance>0</relevance>
  </item>  



  <item>
    <title>Exploring many-to-one speech-to-text correlation for web-based language learning</title>
	<abstract>This article investigates the correlations between multimedia objects (particularly speech and text) involved in language lectures in order to design an effective presentation mechanism for web-based learning. The cross-media correlations are classified into implicit relations (retrieved by computing) and explicit relations (recorded during the preprocessing stage). The implicit temporal correlation between speech and text is primarily to help to negotiate supplementary lecture navigations like tele-pointer movement, lips-sync movement, and content scrolling. We propose a speech-text alignment framework, using an iterative algorithm based on local alignment, to probe many-to-one temporal correlations, and not the one-to-one only. The proposed framework is a more practical method for analyzing general language lectures, and the algorithm's time complexity conforms to the best-possible computation cost, O(nm), without introducing additional computation. In addition, we have shown the feasibility of creating vivid presentations by exploiting implicit relations and artificially simulating some explicit media. To facilitate the navigation of integrated multimedia documents, we develop several visualization techniques for describing media correlations, including guidelines for speech-text correlations, visible-automatic scrolling, and levels of detail of timeline, to provide intuitive and easy-to-use random access mechanisms. We evaluated the performance of the analysis method and human perceptions of the synchronized presentation. The overall performance of the analysis method is that about 99.5% of the words analyzed are of a temporal error within 0.5 sec and the subjective evaluation result shows that the synchronized presentation is highly acceptable to human beings.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Automatic Speech-to-Text Transcription in Arabic</title>
	<abstract>The Arabic language presents a number of challenges for speech recognition, arising in part from the significant differences in the spoken and written forms, in particular the conventional form of texts being non-vowelized. Being a highly inflected language, the Arabic language has a very large lexical variety and typically with several possible (generally semantically linked) vowelizations for each written form. This article summarizes research carried out over the last few years on speech-to-text transcription of broadcast data in Arabic. The initial research was oriented toward processing of broadcast news data in Modern Standard Arabic, and has since been extended to address a larger variety of broadcast data, which as a consequence results in the need to also be able to handle dialectal speech. While standard techniques in speech recognition have been shown to apply well to the Arabic language, taking into account language specificities help to significantly improve system performance.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Mixed-source multi-document speech-to-text summarization</title>
	<abstract>Speech-to-text summarization systems usually take as input the output of an automatic speech recognition (ASR) system that is affected by issues like speech recognition errors, disfluencies, or difficulties in the accurate identification of sentence boundaries. We propose the inclusion of related, solid background information to cope with the difficulties of summarizing spoken language and the use of multi-document summarization techniques in single document speech-to-text summarization. In this work, we explore the possibilities offered by phonetic information to select the background information and conduct a perceptual evaluation to better assess the relevance of the inclusion of that information. Results show that summaries generated using this approach are considerably better than those produced by an up-to-date latent semantic analysis (LSA) summarization method and suggest that humans prefer summaries restricted to the information conveyed in the input source.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>The IBM rich transcription spring 2006 speech-to-text system for lecture meetings</title>
	<abstract>We describe the IBM systems submitted to the NIST RT06s Speech-to-Text (STT) evaluation campaign on the CHIL lecture meeting data for three conditions: Multiple distant microphone (MDM), single distant microphone (SDM), and individual headset microphone (IHM). The system building process is similar to the IBM conversational telephone speech recognition system. However, the best models for the far-field conditions (SDM and MDM) proved to be the ones that use neither variance normalization nor vocal tract length normalization. Instead, feature-space minimum-phone error discriminative training yielded the best results. Due to the relatively small amount of CHIL-domain data, the acoustic models of our systems are built on publicly available meeting corpora, with maximum a-posteriori adaptation applied twice on CHIL data during training: First, at the initial speaker-independent model, and subsequently at the minimum phone error model. For language modeling, we utilized meeting transcripts, text from scientific conference proceedings, and spontaneous telephone conversations. On development data, chosen in our work to be the 2005 CHIL-internal STT evaluation test set, the resulting language model provided a 4% absolute gain in word error rate (WER), compared to the model used in last year's CHIL evaluation. Furthermore, the developed STT system significantly outperformed our last year's results, by reducing close-talking microphone data WER from 36.9% to 25.4% on our development set. In the NIST RT06s evaluation campaign, both MDM and SDM systems scored well, however the IHM system did poorly due to unsuccessful cross-talk removal.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Large-scale language modeling with random forests for mandarin Chinese speech-to-text</title>
	<abstract>In this work the random forest language modeling approach is applied with the aim of improving the performance of the LIMSI, highly competitive, Mandarin Chinese speech-to-text system. The experimental setup is that of the GALE Phase 4 evaluation. This setup is characterized by a large amount of available language model training data (over 3.2 billion segmented words). A conventional unpruned 4-gram language model with a vocabulary of 56K words serves as a baseline that is challenging to improve upon. However moderate perplexity and CER improvements over this model were obtained with a random forest language model. Different random forest training strategies were explored so as to attain the maximal gain in performance and Forest of Random Forest language modeling scheme is introduced.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Parse decoration of the word sequence in the speech-to-text machine-translation pipeline</title>
	<abstract>Parsing, or the extraction of syntactic structure from text, is appealing to natural language processing (NLP) engineers and researchers. Parsing provides an opportunity to consider information about word sequence and relatedness beyond simple adjacency. This dissertation uses automatically-derived syntactic structure (parse decoration) to improve the performance and evaluation of large-scale NLP systems that have (in general) used only word-sequence level measures to quantify success. In particular, this work focuses on parse structure in the context of large-vocabulary automatic speech recognition (ASR) and statistical machine translation (SMT) in English and (in translation) Mandarin Chinese. The research here explores three characteristics of statistical syntactic parsing: dependency structure, constituent structure, and parse-uncertainty — making use of the parser's ability to generate an M-best list of parse hypotheses.
Parse structure predictions are applied to ASR to improve word-error rate over a baseline non-syntactic (sequence-only) language model (achieving 6-13% of possible error reduction). Critical to this success is the joint reranking of an N × M-best list of N ASR hypothesis transcripts and M-best parse hypotheses (for each transcript). Jointly reranking the N × M lists is also demonstrated to be useful in choosing a high-quality parse from these transcriptions.

In SMT, this work demonstrates expected dependency pair match (EDPM), a new mechanism for evaluating the quality of SMT translation hypotheses by comparing them to reference translations. EDPM, which makes direct use of parse dependency structure directly in its measurement, is demonstrated to be superior in correlation with human measurements of translation quality to the competitor (and widely-used) evaluation metrics BLEU4 and translation edit rate.

Finally, this work explores how syntactic constituents may predict or improve the behavior of unsupervised word-aligners, a core component of SMT systems, over a collection of Chinese-English parallel text with reference alignment labels. Statistical word-alignment is improved over several machine-generated alignments by exploiting the coherence of certain parse constituent structures to identify source-language regions where a high-recall aligner may be trusted.

These diverse results across ASR and SMT point together to the utility of including parse information into large-scale (and generally word-sequence oriented) NLP systems and demonstrate several approaches for doing so.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>RNN-based prosodic modeling for mandarin speech and its application to speech-to-text conversion</title>
	<abstract>In this paper, a recurrent neural network (RNN) based prosodic modeling method for Mandarin speech-to-text conversion is proposed. The prosodic modeling is performed in the post-processing stage of acoustic decoding and aims at detecting word-boundary cues to assist in linguistic decoding. It employs a simple three-layer RNN to learn the relationship between input prosodic features, extracted from the input utterance with syllable boundaries pre-determined by the preceding acoustic decoder, and output word-boundary information of the associated text. After the RNN prosodic model is properly trained, it can be used to generate word-boundary cues to help the linguistic decoder solving the problem of word-boundary ambiguity. Two schemes of using these word-boundary cues are proposed. Scheme 1 modifies the baseline scheme of the conventional linguistic decoding search by directly taking the RNN outputs as additional scores and adding them to all word-sequence hypotheses to assist in selecting the best recognized word sequence. Scheme 2 is an extended version of Scheme 1 by further using the RNN outputs to drive a finite state machine (FSM) for setting path constraints to restrict the linguistic decoding search. Character accuracy rates of 73.6%, 74.6% and 74.7% were obtained for the systems using the baseline scheme, Schemes 1 and 2, respectively. Besides, a gain of 17% reduction in the computational complexity of the linguistic decoding search was also obtained for Scheme 2. So the proposed prosodic modeling method is promising for Mandarin speech recognition.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Speech-to-text transcription in support of pervasive computing</title>
	<abstract>Speech recognition technology can help transcribe discussions, interviews, meetings, and conversations.This paper describes a concept demonstrator of an automatic speech-to-text transcriber that uses speech recognition. It is defined in terms of motivation for the product, how users operate it, and its similarities and differences with other work being carried out in other research and commercial bodies.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>The ISL RT-06S speech-to-text system</title>
	<abstract>This paper describes the 2006 lecture and conference meeting speech-to-text system developed at the Interactive Systems Laboratories (ISL), for the individual head-mounted microphone (IHM), single distant microphone (SDM), and multiple distant microphone (MDM) conditions, which was evaluated in the RT-06S Rich Transcription Meeting Evaluation sponsored by the US National Institute of Standards and Technologies (NIST). We describe the principal differences between our current system and those submitted in previous years, namely improved acoustic and language models, cross adaptation between systems with different front-ends and phoneme sets, and the use of various automatic speech segmentation algorithms.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Jawi Character Speech-to-Text Engine Using Linear Predictive and Neural Network for Effective Reading</title>
	<abstract>Jawi is an old version of Malay Language Writing that need to be preserved. Therefore, it is important to develop tools for teaching kids about Jawi characters and Speech-To-Text (STT) application can serve this purpose well. Unlike English, Jawi uses special characters similar to Arabic Characters. However, its pronunciations are in Malay Language. This uniqueness makes STT development a challenging task. In this paper, we investigate the applicability of Linear Predictive Coding to extract important features from voice signal and Neural Network with Backpropagation to classify and recognize spoken words into Jawi Characters. A total of 225 samples of words in Jawi Characters are recorded from speakers with over 95% accuracy. Jawi Characters Speech-To-Text Engine aims to help students to read Jawi document accurately and independently without the need for close monitoring from parents or teachers.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>1</relevance>
  </item>
  
    <item>
    <title>The IBM Rich Transcription 2007 Speech-to-Text Systems for Lecture Meetings</title>
	<abstract>The paper describes the IBM systems submitted to the NIST Rich Transcription 2007 (RT07) evaluation campaign for the speech-to-text (STT) and speaker-attributed speech-to-text (SASTT) tasks on the lecture meeting domain. Three testing conditions are considered, namely the multiple distant microphone (MDM), single distant microphone (SDM), and individual headset microphone (IHM) ones --- the latter for the STT task only. The IBM system building process is similar to that employed last year for the STT Rich Transcription Spring 2006 evaluation (RT06s). However, a few technical advances have been introduced for RT07: (a) better speaker segmentation; (b) system combination via the ROVER approach applied over an ensemble of systems, some of which are built by randomized decision tree state-tying; and (c) development of a very large language model consisting of 152M n-grams, incorporating, among other sources, 525M words of web data, and used in conjunction with a dynamic decoder. These advances reduce STT word error rate (WER) in the MDM condition by 16% relative (8% absolute) over the IBM RT06s system, as measured on 17 lecture meeting segments of the RT06s evaluation test set, selected in this work as development data. In the RT07 evaluation campaign, both MDM and SDM systems perform competitively for the STT and SASTT tasks. For example, at the MDM condition, a 44.3% STT WER is achieved on the RT07 evaluation test set, excluding scoring of overlapped speech. When the STT transcripts are combined with speaker labels from speaker diarization, SASTT WER becomes 52.0%. For the STT IHM condition, the newly developed large language model is employed, but in conjunction with the RT06s IHM acoustic models. The latter are reused, due to lack of time to train new models to utilize additional close-talking microphone data available in RT07. Therefore, the resulting system achieves modest WERs of 31.7% and 33.4%, when using manual or automatic segmentation, respectively.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Jurilinguistic engineering in Cantonese Chinese: an N-gram-based speech to text transcription system</title>
	<abstract>A Cantonese Chinese transcription system to automatically convert stenograph code to Chinese characters is reported. The major challenge in developing such a system is the critical homocode problem because of homonymy. The statistical N-gram model is used to compute the best combination of characters. Supplemented with a 0.85 million character corpus of domain-specific training data and enhancement measures, the bigram and trigram implementations achieve 95% and 96% accuracy respectively, as compared with 78% accuracy in the baseline model. The system performance is comparable with other advanced Chinese Speech-to-Text input applications under development. The system meets an urgent need of the Judiciary of post-1997 Hong Kong.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Further progress in meeting recognition: the ICSI-SRI spring 2005 speech-to-text evaluation system</title>
	<abstract>We describe the development of our speech recognition system for the National Institute of Standards and Technology (NIST) Spring 2005 Meeting Rich Transcription (RT-05S) evaluation, highlighting improvements made since last year [1]. The system is based on the SRI-ICSI-UW RT-04F conversational telephone speech (CTS) recognition system, with meeting-adapted models and various audio preprocessing steps. This year's system features better delay-sum processing of distant microphone channels and energy-based crosstalk suppression for close-talking microphones. Acoustic modeling is improved by virtue of various enhancements to the background (CTS) models, including added training data, decision-tree based state tying, and the inclusion of discriminatively trained phone posterior features estimated by multilayer perceptrons. In particular, we make use of adaptation of both acoustic models and MLP features to the meeting domain. For distant microphone recognition we obtained considerable gains by combining and cross-adapting narrow-band (telephone) acoustic models with broadband (broadcast news) models. Language models (LMs) were improved with the inclusion of new meeting and web data. In spite of a lack of training data, we created effective LMs for the CHIL lecture domain. Results are reported on RT-04S and RT-05S meeting data. Measured on RT-04S conference data, we achieved an overall improvement of 17% relative in both MDM and IHM conditions compared to last year's evaluation system. Results on lecture data are comparable to the best reported results for that task.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>The ISL RT-07 Speech-to-Text System</title>
	<abstract>This paper describes the 2007 meeting speech-to-text system for <em>lecture rooms</em>developed at the Interactive Systems Laboratories (ISL), for the multiple distant microphone condition, which has been evaluated in the RT-07 Rich Transcription Meeting Evaluation sponsored by the US National Institute of Standards and Technologies (NIST). We describe the principal differences between our current system and those submitted in previous years, namely the use of a signal adaptive front-end (realized by warped-twice warped minimum variance distortionless response spectral estimation), improved acoustic (including maximum mutual information estimation) and language models, cross adaptation between systems which differ in the front-end as well as the phoneme set, the use of a discriminative criteria instead of the signal-to-noise ratio for the selection of the channel to be used and the use of decoder based speech segmentation.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>
Analysis and modeling of F0 contours for cantonese text-to-speech</title>
	<abstract>For the generation of highly natural synthetic speech, the control of prosody is of primary importance. The fundamental frequency (F0) is one of the most important components of speech prosody. This research investigates the variation of F0 in continuous Cantonese speech, with the goal of establishing an effective mechanism of prosody control in Cantonese text-to-speech (TTS) applications. Cantonese is a commonly used Chinese dialect that is well known for being rich in tones. This article describes a simple yet effective approach to the analysis and modeling of F0. The surface F0 contour of a continuous Cantonese utterance is considered to be the combination of a global component--phrase-level intonation curve, and local components--syllable-level tone contoursA novel method of F0 normalization is proposed to separate the local components from the global one. As a result, the variation in tone contours is greatly reduced. Statistical analysis is performed for the phrase curves and context-dependent tone contours that are extracted from a large corpus of 1,200 utterances. Specifically, the analysis is focused on co-articulated tone contours for disyllabic words, cross-word contours, and phrase-initial tone contours. Based on the results of the analysis, a template-based model for F0 generation is established and integrated with a Cantonese TTS system. Subjective listening tests show that the proposed model significantly improves the naturalness of the output speech.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Development of syllable-based text to speech synthesis system in Bengali</title>
	<abstract>This paper presents the design and development of unrestricted text to speech synthesis (TTS) system in Bengali language. Unrestricted TTS system is capable to synthesize good quality of speech in different domains. In this work, syllables are used as basic units for synthesis. Festival framework has been used for building the TTS system. Speech collected from a female artist is used as speech corpus. Initially five speakers' speech is collected and a prototype TTS is built from each of the five speakers. Best speaker among the five is selected through subjective and objective evaluation of natural and synthesized waveforms. Then development of unrestricted TTS is carried out by addressing the issues involved at each stage to produce good quality synthesizer. Evaluation is carried out in four stages by conducting objective and subjective listening tests on synthesized speech. At the first stage, TTS system is built with basic festival framework. In the following stages, additional features are incorporated into the system and quality of synthesis is evaluated. The subjective and objective measures indicate that the proposed features and methods have improved the quality of the synthesized speech from stage-2 to stage-4.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>An investigation into the effects of Text-To-Speech voice and 3D avatars on the perception of presence and flow of live help in electronic commerce</title>
	<abstract>Expansion and growth of online shopping has led many companies to provide real-time communications on their Web sites to facilitate human-to-human interaction between service representatives and customers. The current study analyzes the interface design of such Live Help functions. More specifically, it attempts to understand whether or not the implementation of Text-To-Speech (TTS) voice communication and 3D avatars in the user interface of Live Help affects consumers' views of their own interactions with a service person.A laboratory experiment was designed to empirically test the hypotheses that TTS voice communication and 3D avatars can significantly affect consumers' perceptions of presence and flow. A 2*3 full factorial design was used (assessing three options for the use of voice and text, and two options for the use of avatars). The results of the experiment demonstrate that the presence of TTS voice significantly increases consumers' perceptions of flow (a construct depicting a user's interaction with a computer as playful and exploratory), while 3D avatars enhance consumers' feelings of telepresence (a user's experience of seeming to be present in a remote environment by means of a communication medium). These findings offer practitioners guidelines on how to improve interface designs for real-time human-to-human communications on electronic commerce Web sites.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Efficient and reliable perceptual weight tuning for unit-selection text-to-speech synthesis based on active interactive genetic algorithms: A proof-of-concept</title>
	<abstract>Unit-selection speech synthesis is one of the current corpus-based text-to-speech synthesis techniques. The quality of the generated speech depends on the accuracy of the unit selection process, which in turn relies on the cost function definition. This function should map the user perceptual preferences when selecting synthesis units, which is still an open research issue. This paper proposes a complete methodology for the tuning of the cost function weights by fusing the human judgments with the cost function, through efficient and reliable interactive weight tuning. To that effect, active interactive genetic algorithms (aiGAs) are used to guide the subjective weight adjustments. The application of aiGAs to this process allows mitigating user fatigue and frustration by improving user consistency. However, it is still unfeasible to subjectively adjust the weights of the whole corpus units (diphones and triphones in this work). This makes it mandatory to perform unit clustering before conducting the tuning process. The aiGA-based weight tuning proposal is evaluated in a small speech corpus as a proof-of-concept and results in more natural synthetic speech when compared to previous objective and subjective-based approaches.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Towards including prosody in a text-to-speech system for modern standard Arabic</title>
	<abstract>Most attempts to provide text-to-speech for modern standard Arabic (MSA) have concentrated on solving the problem of diacritic assignment (i.e. of recovering phonetically relevant information, such as choice of short vowels, which is not explicitly provided in the surface form of MSA). This is clearly a crucial issue: you can hardly produce intelligible spoken output if you do not know what the vowels are. We describe an approach to the task of generating speech from MSA text which not only solves this initial problem, but also provides the information required for imposing an appropriate intonation contour.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Enrich web applications with voice internet persona text-to-speech for anyone, anywhere</title>
	<abstract>To embrace the coming age of rich Internet applications and to enrich applications with voice, we propose a Voice Internet Persona (VIP) service. Unlike current text-to-speech (TTS) applications, in which users need to painstakingly install TTS engines in their own machines and do all customizations by themselves, our VIP service consists of a simple, easy-touse platform that enables users to voice-empower their content, such as podcasts or voice greeting cards. We offer three user interfaces for users to create and tune new VIPs with built-in tools, share their VIPs via this new platform, and generate expressive speech content with selected VIPs. The goal of this work is to popularize TTS features to additional scenarios such as entertainment and gaming with the easy-to-access VIP platform.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Soft-computing methods for text-to-speech driven avatars</title>
	<abstract>This paper presents a new approach for driving avatars with text-to-speech synthesis that uses pure text as an information source. The goal is to move lips and face muscles on the basis of the phonetic nature of the utterance and the related expression. Several methods came together to define this solution. Rule-based text-to-speech synthesis generates phonetic and expression transcription of the text to be uttered by the avatar. Phonetic transcription is used to train two artificial neural networks, one for text-to-phone transcription and the other for phone-to-viseme mapping. Then two fuzzylogic engines were tuned for smoothed control of lip and face movements.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Diphone Databases for Lithuanian Text-to-Speech Synthesis</title>
	<abstract>One of the components of the text-to-speech synthesis system is the database of sounds. Two Lithuanian diphone databases in the MBROLA format are presented in this paper. The list of phonemes and the list of diphones necessary for Lithuanian text-to-speech synthesis are described. The problem of phoneme combinations that are not used in the Lithuanian language is dealt with in the work. Also, the article is concerned with transcribing a Lithuanian text.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>The framework of the Turkish syllable-based concatenative text-to-speech system with exceptional case handling</title>
	<abstract>This paper describes the TTTS (Turkish Text-To-Speech) synthesis system, developed at Fatih University for Turkish language. The framework of the Turkish syllable-based concatenative text-to-speech system with exceptional case handling is introduced. TTTS is a concatenative TTS system aiming to advance the process of developing natural and human sounding Turkish voices. The resulting system is implemented by concatenating pieces of pre-recorded limited number of speech units that are stored in a database. Systems differ in the size of the stored speech units affecting the output range, the quality and the clarity, therefore the number of the concatenation units, synthetic units obtained and the computational power required should be kept in balance. The letters of the Turkish alphabet and the syllables that consist of two letters at most are used as the smallest phonemes in the context of this study. The syllables that have more than two letters are derived from these smallest units. The words, which are generally borrowed from other languages throughout cultural interactions, present exceptional behaviors and should be handled specifically. The results are evaluated by using the Degradation Mean Opinion Score (DMOS) method.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>TTTS: Turkish text-to-speech system</title>
	<abstract>This paper describes the TTTS (Turkish Text-To-Speech) synthesis system, developed at Fatih University for Turkish language. TTTS is a concatenative TTS system aiming to advance the process of developing natural and human sounding Turkish voices. The resulting system can be implemented by concatenating pieces of recorded speech that are stored in a database. Systems differ in the size of the stored speech units effecting the output range, the quality and the clarity. The letters of the Turkish alphabet and the syllables that consist of two letters at most are used as the smallest phonemes in the context of this study. The syllables that have more than two letters are derived from these smallest units. The results are evaluated by using the Degradation Mean Opinion Score (DMOS) method.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Is text-to-speech synthesis ready for use in computer-assisted language learning?</title>
	<abstract>Text-to-speech (TTS) synthesis, the generation of speech from text input, offers another means of providing spoken language input to learners in Computer-Assisted Language Learning (CALL) environments. Indeed, many potential benefits (ease of creation and editing of speech models, generation of speech models and feedback on demand, etc.) and uses (talking dictionaries, talking texts, dictation, pronunciation training, dialogue partner, etc.) of TTS synthesis in CALL have been put forward. Yet, the use of TTS synthesis in CALL is not widely accepted and only a few applications have found their way onto the market. One potential reason for this is that TTS synthesis has not been adequately evaluated for this purpose. Previous evaluations of TTS synthesis for use in CALL, have only addressed the comprehensibility of TTS synthesis. Yet, CALL places demands on the comprehensibility, naturalness, accuracy, register and expressiveness of the output of TTS synthesis. In this paper, the aforementioned aspects of the quality of the output of four state-of-the-art French TTS synthesis systems are evaluated with respect to their use in the three different roles that TTS synthesis systems may assume within CALL applications, namely: (1) reading machine, (2) pronunciation model and (3) conversational partner [Handley, Z., Hamel, M.-J., 2005. Establishing a methodology for benchmarking speech synthesis for computer-assisted language learning (CALL). Language Learning and Technology Journal 9(3), 99-119. Retrieved from: http://llt.msu.edu/vol9num3/handley/default.html.]. The results of this evaluation suggest that the best TTS synthesis systems are ready for use in applications in which they 'add value' to CALL, i.e. exploit the unique capacity of TTS synthesis to generate speech models on demand. An example of such an application is a dialogue partner. In order to fully meet the requirements of CALL, further attention needs to be paid to accuracy and naturalness, in particular at the prosodic level, and expressiveness.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Text analysis and language identification for polyglot text-to-speech synthesis</title>
	<abstract>In multilingual countries, text-to-speech synthesis systems often have to deal with texts containing inclusions of multiple other languages in form of phrases, words, or even parts of words. In such multilingual cultural settings, listeners expect a high-quality text-to-speech synthesis system to read such texts in a way that the origin of the inclusions is heard, i.e., with correct language-specific pronunciation and prosody. The challenge for a text analysis component of a text-to-speech synthesis system is to derive from mixed-lingual sentences the correct polyglot phone sequence and all information necessary to generate natural sounding polyglot prosody. This article presents a new approach to analyze mixed-lingual sentences. This approach centers around a modular, mixed-lingual morphological and syntactic analyzer, which additionally provides accurate language identification on morpheme level and word and sentence boundary identification in mixed-lingual texts. This approach can also be applied to word identification in languages without a designated word boundary symbol like Chinese or Japanese. To date, this mixed-lingual text analysis supports any mixture of English, French, German, Italian, and Spanish. Because of its modular design it is easily extensible to additional languages.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>A complemented Greek text to speech system</title>
	<abstract>This paper tries to give a comprehensive insight of a complemented Greek Text to Speech system by highlighting its basic Digital Signal Processing (DSP) and Natural Language Processing (NLP) modules. The main focus will be the development of such a system by taking into account syntactic, grammatical, phonological and lexical knowledge of Greek language. The ultimate goal is to boost up academic research on speech synthesis, particularly on graphemes to phonemes transcription and on prosody generation, known as two of the most important challenges in Text to Speech Synthesis for the years to come. Furthermore, we introduce a conversational application which uses the system and present an evaluation on the results of this application.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Remote-based text-to-speech modules' evaluation framework: the RES framework</title>
	<abstract>The ECESS consortium (European Center of Excellence in Speech Synthesis) aims to speed up progress in speech synthesis technology, by providing an appropriate evaluation framework. The key element of the evaluation framework is based on the partition of a text-to-speech synthesis system into distributed TTS modules. A text processing, prosody generation, and an acoustic synthesis module have been specified currently. A split into various modules has the advantage that the developers of an institution active in ECESS, can concentrate its efforts on a single module, and test its performance in a complete system using missing modules from the developers of other institutions. In this way, complete TTS systems can be built using high performance modules from different institutions. In order to evaluate the modules and to connect modules efficiently, a remote evaluation platform--the Remote Evaluation System (RES) based on the existing internet infrastructure--has been developed within ECESS. The RES is based on client---server architecture. It consists of RES module servers, which encapsulate the modules of the developers, a RES client, which sends data to and receives data from the RES module servers, and a RES server, which connects the RES module servers, and organizes the flow of information. RES can be used by developers for selecting RES module from the internet, which contains a missing TTS module needed to test and improve the performances of their own modules. Finally, the RES allows for the evaluation of TTS modules running at different institutions worldwide. When using the RES client, the institution performing the evaluation is able to set-up and performs various evaluation tasks by sending test data via the RES client and receiving results from the RES module servers. Currently ELDA www.elda.org is setting-up an evaluation using the RES client, which will then be extended to an evaluation client specializing in the envisaged evaluation tasks.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>An enhanced pitch modeling supporting a Greek text to speech system</title>
	<abstract>This paper tries to describe as accurately as possible an enhanced procedure for predicting the appropriate prosodic structure of speech and apply it to a Greek Text to Speech system. The main focus will be a particular linear stylization of the fundamental frequency function F0 contour (pitch), by using the most important syntactic, grammatical and lexical features of the Greek language. Finally, we present an evaluation on the results of plugging the pitch model into the system.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>An overview of text-to-speech synthesis techniques</title>
	<abstract>The goal of this paper is to provide a short but comprehensive overview of text-to-speech synthesis by highlighting its natural language processing (NLP) and digital signal processing (DSP) components. First, the front-end or the NLP component comprised of text analysis, phonetic analysis, and prosodic analysis is introduced then two rule-based synthesis techniques (formant synthesis and articulatory synthesis) are explained. After that concatenative synthesis is explored. Compared to rule-based synthesis, concatenative synthesis is simpler since there is no need to determine speech production rules. However, concatenative synthesis introduces the challenges of prosodic modification to speech units and resolving discontinuities at unit boundaries. Prosodic modification results in artifacts in the speech that make the speech sound unnatural. Unit selection synthesis, which is a kind of concatenative synthesis, solves this problem by storing numerous instances of each unit with varying prosodies. The unit that best matches the target prosody is selected and concatenated. Finally, hidden Markov model (HMM) synthesis is introduced.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>The paradigm for creating multi-lingual text-to-speech voice databases</title>
	<abstract>Voice database is one of the most important parts in TTS systems. However, creating a high quality new TTS voice is not an easy task even for a professional team. The whole process is rather complicated and contains plenty minutiae that should be handled carefully. In fact, in many stages, human interference such as manually checking or labeling is necessary. In multi-lingual situations, it is more challenge to find qualified people to do this kind of interference. That’s why most state-of-the-art TTS systems can provide only a few voices. In this paper, we outline a uniform paradigm for creating multi-lingual TTS voice databases. It focuses on technologies that can either improve the scalability of data collection or reduce human interference such as manually checking or labeling. With this paradigm, we decrease the complexity and work load of the task.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>A tool for creating speaking web pages: text-to-speech integrated development environment (TTS-IDE)</title>
	<abstract>This paper describes an integrated development environment (IDE) which web authors may use to add synthesized speech to their browser-based presentations. Web authors use the IDE to input the text to be spoken. The IDE embeds that text within the HTML file for playback by a speech-enabled browser. Browsers currently supporting text-to-speech (TTS) are Microsoft Internet Explorer and Opera. The IDE works with either HTML files or Powerpoint Web Presentation files. Playback is controlled by a set of controls that "float" innocuously in the upper right hand corner of each page. Speaking pages can be used by online courses, training programs, or certification processes.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>
Multilingual text analysis for text-to-speech synthesis</title>
	<abstract>We present a model of text analysis for text-to-speech (TTS) synthesis based on (weighted) finite state transducers, which serves as the text analysis module of the multilingual Bell Labs TTS system. The transducers are constructed using a lexical toolkit that allows declarative descriptions of lexicons, morphological rules, numeral-expansion rules, and phonological rules, inter alia. To date, the model has been applied to eight languages: Spanish, Italian, Romanian, French, German, Russian, Mandarin and Japanese.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>On modelling glottal stop in czech text-to-speech synthesis</title>
	<abstract>This paper deals with the modelling of glottal stop for the purposes of Czech text-to-speech synthesis. Phonetic features of glottal stop are discussed here and a phonetic transcription rule for inserting glottal stop into the sequences of Czech phones is proposed. Two approaches to glottal stop modelling are introduced in the paper. The first one uses glottal stop as a stand-alone phone. The second one models glottal stop as an allophone of a vowel. Both approaches are evaluated from the point of view of both the automatic segmentation of speech and the quality of the resulting synthetic speech. Better results are obtained when glottal stop is modelled as a stand-alone phone.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Text-to-speech technology-based programming tool</title>
	<abstract>This paper presents an audio programming tool based on text-to-speech technology for blind and vision impaired people to learn programming. The tool can help users edit a program then compile, debug and run it. All of these stages are voice enabled. The programming language for evaluation is C# and the tool is developed in Visual Studio .NET. Evaluations have shown that the programming tool can help blind and vision impaired people implement software applications and achieve equality of access and opportunity in information technology education.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Letter-to-sound conversion for Urdu text-to-speech system</title>
	<abstract>Urdu is spoken by more than 100 million people across a score countries and is the national language of Pakistan (http://www.ethnologue.com). There is a great need for developing a text-to-speech system for Urdu because this population has low literacy rate and therefore speech interface would greatly assist in providing them access to information. One of the significant parts of a text-to-speech system is a natural language processor which takes textual input and converts it into an annotated phonetic string. To enable this, it is necessary to develop models which map textual input onto phonetic content. These models may be very complex for various languages having unpredictable behaviour (e.g. English), but Urdu shows a relatively regular behaviour and thus Urdu pronunciation may be modelled from Urdu text by defining fairly regular rules. These rules have been identified and explained in this paper.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Prosodic reading style simulation for text-to-speech synthesis</title>
	<abstract>The simulation of different reading styles (mainly by adapting prosodic parameters) can improve the naturalness of synthetic speech and supports a more intelligent human machine interaction. The article exemplarily investigates the reading styles News and Tale. For comparison, all examined texts contained the same genre-neutral paragraphs which have been read without a specific style instruction: Normal but also faster, slower, rather monotone or more emotional which led to corresponding artificial styles.

The measured original intonation and durations style patterns control a diphone synthesizer (mapped contours). Additionally, the patterns are used to train a neural network (NN) model.

Within two separate listening tests, different stimuli presented as original signal/style, respectively, with mapped or NN generated prosodic contours have been evaluated. The results show that both, original utterances and artificial styles are basically perceived in their intended reading styles. Some reciprocal confusions indicate the similarities between different styles like News and Fast, Tale and Slow as well as Tale and Expressive. The confusions are more likely for synthetic speech. To produce e. g. the complex style Tale, different features of the prosodic variations Slow and Expressive are combined. The training method for the synthetic styles requires a further improvement.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>An Introduction to Text-to-Speech Synthesis</title>
	<abstract>An Introduction to Text-to-Speech Synthesis is a comprehensive introduction; Dutoit treats two areas of speech synthesis: Part I of the book concerns natural language processing and the inherent problems it presents for speech synthesis; Part II focuses on digital signal processing, with an emphasis on the concatenative approach. Both parts of the text guide the reader through the material in a step-by-step easy to follow way. This is the book to treat the topic of speech synthesis from the perspective of two different engineering approaches. The book will be of interest to researchers and students in phonetics and speech communication, in both academia and industry.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Generating Arabic text in multilingual speech-to-speech machine translation framework</title>
	<abstract>The interlingual approach to machine translation (MT) is used successfully in multilingual translation. It aims to achieve the translation task in two independent steps. First, meanings of the source-language sentences are represented in an intermediate language-independent (Interlingua) representation. Then, sentences of the target language are generated from those meaning representations. Arabic natural language processing in general is still underdeveloped and Arabic natural language generation (NLG) is even less developed. In particular, Arabic NLG from Interlinguas was only investigated using template-based approaches. Moreover, tools used for other languages are not easily adaptable to Arabic due to the language complexity at both the morphological and syntactic levels. In this paper, we describe a rule-based generation approach for task-oriented Interlingua-based spoken dialogue that transforms a relatively shallow semantic interlingual representation, called interchange format (IF), into Arabic text that corresponds to the intentions underlying the speaker's utterances. This approach addresses the handling of the problems of Arabic syntactic structure determination, and Arabic morphological and syntactic generation within the Interlingual MT approach. The generation approach is developed primarily within the framework of the NESPOLE! (NEgotiating through SPOken Language in E-commerce) multilingual speech-to-speech MT project. The IF-to-Arabic generator is implemented in SICStus Prolog. We conducted evaluation experiments using the input and output from the English analyzer that was developed by the NESPOLE! team at Carnegie Mellon University. The results of these experiments were promising and confirmed the ability of the rule-based approach in generating Arabic translation from the Interlingua taken from the travel and tourism domain.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Designing help topics for use with text-to-speech</title>
	<abstract>Speech technology can be used to provide online help to users in situations where visual display of online help is not possible, or has some display-related limitations. Presenting help material in this manner can also complement traditional online help systems. To date, most online help material has been developed with the assumption that the material will be read. This paper proposes a number of guidelines to assist in the creation and testing of help material that may be presented to users via speech synthesis engines. The paper also provides a brief overview of an on-going project that provides online help using speech technolog</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>A dynamic cost weighting framework for unit selection text-to-speech synthesis</title>
	<abstract>Unit selection text-to-speech synthesis relies on multiple cost criteria, each encapsulating a different aspect of acoustic and prosodic context at any given concatenation point. Constraints are normally invoked on diverse characteristics such as inter-unit discontinuity overall pitch contour, local duration profile, etc., leading to costs often too heterogeneous for a direct quantitative comparison. In order to rank available candidate uints, this complexity must be reduced to a single number, and the relative importance of each information stream becomes highly critical. Yet this influence is typically determined in an empirical manner (e.g., based on a limited amount of synthesized data), yielding global weights that are thus applied to broad classes of concatenations indiscriminately. This paper proposes an alternative approach, dynamic cost weighting, based on a data-driven framework separately optimized for each concatenation considered. Specifically, the cost distribution in every stream is dynamically leveraged on a per concatenation basis to locally shift weight towards those characteristics that offer a high discrimination between candidate units, and away from those characteristics that are intrinsically less discriminative. An illustrative case study demonstrates the potential benefits of this solution, and listening evidence suggests that it does indeed entail higher perceived TTS quality.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Streaming speech: a framework for generating and streaming 3D text-to-speech and audio presentations to wireless PDAs as specified using extensions to SMIL</title>
	<abstract>While monochrome unformatted text and richly colored graphical content are both capable of conveying a message, well designed graphical content has the potential for better engaging the human sensory system. It is our contention that the author of an audio presentation should be afforded the benefit of judiciously exploiting the human aural perceptual ability to deliver content in a more compelling, concise and realistic manner. While contemporary streaming media players and voice browsers share the ability to render content non-textually, neither technology is currently capable of rendering three dimensional media. The contributions described in this paper are proposed 3D audio extensions to SMIL and a server-based framework able to receive a request and, on-demand, process such a SMIL file and dynamically create the multiple simultaneous audio objects, spatialize them in 3D space, multiplex them into a single stereo audio and prepare it for transmission over an audio stream to a mobile device. To the knowledge of the authors, this is the first reported solution for delivering and rendering on a commercially available wireless handheld device a rich 3D audio listening experience as described by a markup language. Naturally, in addition to mobile devices this solution also works with desktop streaming media players.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Linguistic contributions to text-to-speech computer programs for French</title>
	<abstract>This paper presents the development of computer programs used for transcribing French text into phonetic speech. Based on an earlier program (Maggs &amp; Trescases, 1980) made of a set of some 500 text-to-phonetics rules and of a compact set of prosodic rules for synthetic speech, the present research was primarily aiming at developing the best possible algorithm to account for practically any word in the French general lexicon, as opposed to most frequently used words only. In order to considerably enhance the original rules, these were systematically tested against a 50, 000 word French pronunciation dictionary and an equally important corpus of texts which were all entered in an IBM PC-XT. At the same time, a set of syntactic rules was developed for most liaisons, mandatory and forbidden, and homographs to be found in French speech. The result is a set of some 4, 800 conversion rules. Tested against the 50, 000 words of the pronunciation dictionary, they yield a low percentage of error. Errors in text are similarly minimal, and due mainly to foreign (English) words. To allow faster programing, while accounting for most commonly used words in French, a compact set of 2, 000 rules has been developed. It is essentially based on a statistical analysis yielding most frequently used general rules. The aim of both algorithms has been to make possible better text-to-speech software for French.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Robust speaker-adaptive HMM-based text-to-speech synthesis</title>
	<abstract>This paper describes a speaker-adaptive HMM-based speech synthesis system. The new system, called "HTS-2007," employs speaker adaptation (CSMAPLR+MAP), feature-space adaptive training, mixed-gender modeling, and full-covariance modeling using CSMAPLRtransforms, in addition to several other techniques that have proved effective in our previous systems. Subjective evaluation results show that the new system generates significantly better quality synthetic speech than speaker-dependent approaches with realistic amounts of speech data, and that it bears comparison with speaker-dependent approaches even when large amounts of speech data are available. In addition, a comparison study with several speech synthesis techniques shows the new system is very robust: It is able to build voices from less-than-ideal speech data and synthesize good-quality speech even for out-of-domain sentences.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Refining Unit Boundaries for Mandarin Text-to-Speech Database</title>
	<abstract>In unit selection based Text-to-Speech (TTS) synthesis, the accurate position of the unit boundaries in the unit selection database is one of the factors that determine the quality of the synthesized speech. To ensure the accuracy of the boundary positions, developers often have to manually verify the speech boundaries that are generated by automatic speech recognition techniques. In order to reduce the manual workload, it is necessary to use automatic methods of refining the position of the unit boundaries. This paper proposes a frame-shift method to find the globally optimal joint position for unit concatenation between any two matching units. Experiment results show that this method can improve the boundary accuracy compared to manual labeling.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Usability assessment of text-to-speech synthesis for additional detail in an automated telephone banking system</title>
	<abstract>This paper describes a comprehensive usability evaluation of an automated telephone banking system which employs text-to-speech (TTS) synthesis in offering additional detail on customers' account transactions. The paper describes a series of four experiments in which TTS was employed to offer an extra level of detail to recent transactions listings within an established banking service which otherwise uses recorded speech from a professional recording artist. Results from the experiments show that participants welcome the added value of TTS in being able to provide additional detail on their account transactions, but that TTS should be used minimally in the service.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Toward language-independent text-to-speech synthesis</title>
	<abstract>Text-to-speech (TTS) synthesis is becoming a fundamental part of any embedded system that has to interact with humans. Language-independence in speech synthesis is a primary requirement for systems that are not practical to update, as is the case for most embedded systems. Because current text-to-speech synthesis usually refers to a single language and to a single speaker (or at most a limited set of voices), a framework for language-independent, text-to-speech synthesis is proposed to overcome these limitations in implementing speech synthesis on embedded systems. The proposed text-to-speech synthesis framework was designed to embed phonetic and prosodic information in a set of rules. To complete this language-independent speech-synthesis solution, a universal set of phones has been defined so that the appropriate speech sounds for every language are available at run time. Synthesis of more than one language can easily be carried out by switching from one rule set to another while keeping a common phone-data set. Using a vocal-track-based speech synthesizer, the system does not depend on phone sets recorded from an actual specific human voice, so voice types can be chosen at run time.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Time and space-efficient architecture for a corpus-based text-to-speech synthesis system</title>
	<abstract>This paper proposes a time and space-efficient architecture for a text-to-speech synthesis system (TTS). The proposed architecture can be efficiently used in those applications with unlimited domain, requiring multilingual or polyglot functionality. The integration of a queuing mechanism, heterogeneous graphs and finite-state machines gives a powerful, reliable and easily maintainable architecture for the TTS system. Flexible and language-independent framework efficiently integrates all those algorithms used within the scope of the TTS system. Heterogeneous relation graphs are used for linguistic information representation and feature construction. Finite-state machines are used for time and space-efficient representation of language resources, for time and space-efficient lookup processes, and the separation of language-dependent resources from a language-independent TTS engine. Its queuing mechanism consists of several dequeue data structures and is responsible for the activation of all those TTS engine modules having to process the input text. In the proposed architecture, all modules use the same data structure for gathering linguistic information about input text. All input and output formats are compatible, the structure is modular and interchangeable, it is easily maintainable and object oriented. The proposed architecture was successfully used when implementing the Slovenian PLATTOS corpus-based TTS system, as presented in this paper.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Non-linear frequency scale mapping for voice conversion in text-to-speech system with cepstral description</title>
	<abstract>Voice conversion, i.e. modification of a speech signal to sound as if spoken by a different speaker, finds its use in speech synthesis with a new voice without necessity of a new database. This paper introduces two new simple non-linear methods of frequency scale mapping for transformation of voice characteristics between male and female or childish. The frequency scale mapping methods were developed primarily for use in the Czech and Slovak text-to-speech (TTS) system designed for the blind and based on the Pocket PC device platform. It uses cepstral description of the diphone speech inventory of the male speaker using the source-filter speech model or the harmonic speech model. Three new diphone speech inventories corresponding to female, childish and young male voices are created from the original male speech inventory. Listening tests are used for evaluation of voice transformation and quality of synthetic speech.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>A framework for mixed-language text-to-speech synthesis</title>
	<abstract>The task of text-to-speech (TTS) synthesis usually refers to a single language and to a single speaker, concatenating short parametrically controlled speech segments by means of a rule-based algorithm. The main disadvantage of this solution is its strong language and speaker dependency. We propose a framework designed to overcame this limitation, employing a multi-language text-to-speech synthesis system. The text-to-speech synthesis framework was designed to embed phonetic and prosodic information in a set of rules. Synthesis of more than one language can easily be carried out by switching from one rule set to another. The system does not depend on phone sets recorded from an actual specific human voice. Rather, it relies on a human-like, speech-synthesis model that can generate the units needed to produce the desired utterance for a specific test string in any kind of voice (male, female, child).</abstract>
	<search_task_number>9</search_task_number>
	<query>speech to text</query>
	<relevance>0</relevance>
  </item>



  <item>
    <title>'Every group carries the flavour of the admins': leadership on Flickr</title>
	<abstract>Although leadership in many types of online community has been recognised as important, relatively little has been written about its nature in informal communities, particularly in the context of Web 2.0. This study explores the role played by admins on the photo sharing site Flickr. Semi-structured interviews with six admins investigated roles and motivation in some depth. A content analysis was conducted to explore the character of admins' activities on discussion boards in four types of group. The interviews revealed the dedication that admins gave to the role. They saw their activities as one of the main critical success factors for any group. Groups were heavily shaped by admins. The content analysis revealed that admins play a wide range of roles in groups, with markedly varying emphasis between different group types.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Proceedings of the 13th International MindTrek Conference: Everyday Life in the Ubiquitous Era</title>
	<abstract>It is our great pleasure to welcome you to the 13th MindTrek Conference and the International Academic Conference MindTrek from September 30th - October 2nd 2009: Everyday Life in the Ubiquitous Era.

Academic MindTrek brings together a cross-disciplinary crowd of people in order to investigate current and emerging topics within media in the ubiquitous arena. Business, social, technical and content-related media topics will be discussed this year.

The MindTrek Association hosts MindTrek as an annual conference, where the Academic MindTrek conference has been a part this unique set of events comprising competitions, world-famous keynote speakers, plenary sessions, media festivals and workshops, since 1997.

The scientific part of the conference is organized in co-operation with ACM SIGMM, and ACM SIGCHI. Conference proceedings will be published in the ACM Digital Library. Selected high quality papers will be selected and published in international journals, as book chapters, or via open access journals.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Networked Graphics: Building Networked Games and Virtual Environments</title>
	<abstract>This broad-ranging book equips programmers and designers with a thorough grounding in the techniques used to create truly network-enabled computer graphics and games. Written for graphics/game/VE developers and students, it assumes no prior knowledge of networking. The text offers a broad view of what types of different architectural patterns can be found in current systems, and readers will learn the tradeoffs in achieving system requirements on the Internet.

The book explains the foundations of networked graphics, then explores real systems in depth, and finally considers standards and extensions. Numerous case studies and examples with working code are featured throughout the text, covering groundbreaking academic research and military simulation systems, as well as industry-leading game designs.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Wii all play: the console game as a computational meeting place</title>
	<abstract>In this paper, we present results from a qualitative study of collocated group console gaming. We focus on motivations for, perceptions of, and practices surrounding the shared use of console games by a variety of established groups of gamers. These groups include both intragenerational groups of youth, adults, and elders as well as intergenerational families. Our analysis highlights the numerous ways that console games serve as a computational meeting place for a diverse population of gamers.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>The individual and the group in console gaming</title>
	<abstract>In this paper, we present results from a study of collocated group console gaming. We focus, in particular, on observed gaming practices that emphasized the individual gamer within a gaming group as well as practices that emphasized the gaming group as a whole. We relate each of these practices, where possible, to specific elements of the game design including game mechanics, interaction design, and special effects design. We argue that the classic distinction between competitive and cooperative modes of gameplay does not fully transfer to account for the interpersonal dynamics within collocated gaming groups.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>What makes people experience flow? Social characteristics of online games</title>
	<abstract>Online games are games in which users play simultaneously with each other in a virtual environment. The success of online games relies on the repetitive visits of players, and thus on the very personal experiences of players. Drawing on the theory of flow, this paper empirically explores how the characteristics of online games affect the individual flow experience. Three antecedents to flow (skills, challenges and focused attention) have positive influence on the flow experience. Interactivity, whether it is human or machine, is found to influence the antecedents of the flow experience, and system performance is shown to have no or marginally significant effects. The results show that social characteristics of online games are more crucial to online game success than technological ones.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>The moderating role of locus of control on the links between experiential motives and intention to play online games</title>
	<abstract>Online games represent a burgeoning market sector with growth potential. The distinctive entertainment-oriented features of such games provide experiential motives for users. However, most previous studies have focused on the single dimensional affective motive of online game use. The multi-dimensional affective motivational aspects of entertainment technologies have been relatively neglected. In addition, previous studies report inconsistent relationships between motives and online game usage. To fill these gaps in the literature, the present study proposes five experiential motives such as concentration, enjoyment, escape, epistemic curiosity, and social affiliation as predictors of intention to play online games. External locus of control is also introduced as a moderator in the links between experiential motives and intention. The model was tested with the data from 576 current online game players who answered online survey. Structural equation model analysis confirmed that three experiential motives with the exception of concentration and epistemic curiosity have a positive effect on intention. @g^2 difference test confirmed that the coefficients linking experiential motives such as concentration, enjoyment, and escape to intention are higher for people with external locus of control than for people with internal locus of control. Implications are discussed in conclusion.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>AVIEN Malware Defense Guide for the Enterprise</title>
	<abstract>Members of AVIEN (the Anti-Virus Information Exchange Network) have been setting agendas in malware management for several years: they led the way on generic filtering at the gateway, and in the sharing of information about new threats at a speed that even anti-virus companies were hard-pressed to match. AVIEN members represent the best-protected large organizations in the world, and millions of users. When they talk, security vendors listen: so should you. 

AVIEN's sister organization AVIEWS is an invaluable meeting ground between the security vendors and researchers who know most about malicious code and anti-malware technology, and the top security administrators of AVIEN who use those technologies in real life. This new book uniquely combines the knowledge of these two groups of experts. Anyone who is responsible for the security of business information systems should be aware of this major addition to security literature.

* "Customer Power" takes up the theme of the sometimes stormy relationship between the antivirus industry and its customers, and tries to dispel some common myths. It then considers the roles of the independent researcher, the vendor-employed specialist, and the corporate security specialist.
* "Stalkers on Your Desktop" considers the thorny issue of malware nomenclature and then takes a brief historical look at how we got here, before expanding on some of the malware-related problems we face today.
* "A Tangled Web" discusses threats and countermeasures in the context of the World Wide Web. 
* "Big Bad Bots" tackles bots and botnets, arguably Public Cyber-Enemy Number One.
* "Crème de la CyberCrime" takes readers into the underworld of old-school virus writing, criminal business models, and predicting future malware hotspots.
* "Defense in Depth" takes a broad look at DiD in the enterprise, and looks at some specific tools and technologies.
* "Perilous Outsorcery" offers sound advice on how to avoid the perils and pitfalls of outsourcing, incorporating a few horrible examples of how not to do it.
* "Education in Education" offers some insights into user education from an educationalist's perspective, and looks at various aspects of security in schools and other educational establishments.
* "DIY Malware Analysis" is a hands-on, hands-dirty approach to security management, considering malware analysis and forensics techniques and tools.
* "Antivirus Evaluation &amp; Testing" continues the D-I-Y theme, discussing at length some of the thorny issues around the evaluation and testing of antimalware software.
* "AVIEN &amp; AVIEWS: the Future" looks at future developments in AVIEN and AVIEWS.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Demand Estimation with Social Interactions and the Implications for Targeted Marketing</title>
	<abstract>This paper develops a model for the estimation and analysis of demand in the context of social interactions. Decisions made by a group of customers are modeled to be an equilibrium outcome of an empirical discrete game, such that all group members must be satisfied with chosen outcomes. The game-theoretic approach assists estimation by allowing us to account for the endogeneity of group members' decisions while also serving as a managerial tool that can simulate equilibrium outcomes for the group when the firm alters the marketing mix to the group. The model builds upon the existing literature on empirical models of discrete games by introducing a random coefficients heterogeneity distribution. Monte Carlo simulations reveal that including the heterogeneity resolves the endogenous group formation bias commonly noted in the social interactions literature. By estimating the heterogeneous equilibrium model using Bayesian hierarchical Markov chain Monte Carlo, we can also recover some parameters at the individual level to evaluate group-specific characteristics and targeted marketing strategies. To validate the model and illustrate its implications, we apply it to a data set of groups of golfers. We find significant social interaction effects, such that 65% of the median customer value is attributable to the customer and the other 35% is attributable to the customer's affect on members of his group. We also consider targeted marketing strategies and show that group-level targeting increases profit by 1%, whereas targeting within groups can increase profitability by 20%. We recognize that customer backlashes to targeting could be greater when group members receive different offers, so we suggest some alternatives that could retain some of the profitability of within group targeting while avoiding customer backlashes.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Effects of anonymity, invisibility, and lack of eye-contact on toxic online disinhibition</title>
	<abstract>The present research studied the impact of three typical online communication factors on inducing the toxic online disinhibition effect: anonymity, invisibility, and lack of eye-contact. Using an experimental design with 142 participants, we examined the extent to which these factors lead to flaming behaviors, the typical products of online disinhibition. Random pairs of participants were presented with a dilemma for discussion and a common solution through online chat. The effects were measured using participants' self-reports, expert judges' ratings of chat transcripts, and textual analyses of participants' conversations. A 2x2x2 (anonymity/non-anonymityxvisibility/invisibilityxeye-contact/lack of eye-contact) MANOVA was employed to analyze the findings. The results suggested that of the three independent variables, lack of eye-contact was the chief contributor to the negative effects of online disinhibition. Consequently, it appears that previous studies might have defined the concept of anonymity too broadly by not addressing other online communication factors, especially lack of eye-contact, that impact disinhibition. The findings are explained in the context of an onlinesense of unidentifiability, which apparently requires a more refined view of the components that create a personal sense of anonymity.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>CHI '07 extended abstracts on Human factors in computing systems</title>
	<abstract>Welcome to the CHI 2007 proceedings. We believe the technical papers and notes herein present some of the best current work in the diverse and dynamic field of human-computer interaction (HCI).

CHI is the leading HCI conference. Creating the technical program requires a huge investment of time and effort from members of the research community. 840 submissions were processed (571 papers, 269 notes), requiring over 3000 reviews. We thank all the reviewers for the dedication with which they undertook this task. We are particularly indebted to the papers and notes program committee members, also known as the Associate Chairs (ACs). Balancing areas of expertise, ACs were selected from the field's leading researchers. The AC role included recruiting all reviewers, moderating and supervising the review process to ensure a high-quality set of reviews was obtained, initiating and organizing author rebuttal and reviewer discussions and approving final submissions. The estimated time expenditure to serve as an AC was 11 days of full-time work; many committee members spent more time than that. Papers ACs came to San Jose in December 2006 from around the world for two intense days of review, debates, and deliberation; Notes ACs who could not attend the parallel notes meeting in San Jose engaged in a virtual conference. The committee was extremely serious and careful in making CHI paper and note decisions, with many submissions receiving multiple discussions, before and during the program committee meetings. No review process can guarantee perfect decisions, but we are confident that every possible effort was made to ensure fair process and high quality decision-making. This year's program committee certainly has our respect and gratitude, and deserves the sincere appreciation of the entire HCI community. We would also like to thank the ACs and their organizations for underwriting the travel expenses for meeting.

CHI is both a journal-quality archival forum and a community-building conference. To encourage quality in the written presentation of accepted work, all of the 142 full paper and 40 note acceptances were provisional. As a result, authors actively responded and incorporated feedback from the reviews into the final versions of the papers that appear here.

Twenty-eight accepted papers and four accepted notes (5% of submissions) deemed to make an especially noteworthy contribution to human-computer interaction research were nominated by the program committee for Best Paper and Best Note Awards; these nominated papers and notes are identified in the Final Program. At the conference, up to six of these will be announced as winners of a CHI Best Paper Award (1% of submissions), and one note will be selected as an exemplary note. While all papers accepted into the CHI technical papers program have passed a rigorous examination of their quality, the Best Paper and Best Notes Awards signal and reward particularly outstanding contributions in each year.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Cross-validation of reliability, convergent and discriminant validity for the problematic online game use scale</title>
	<abstract>The main purpose of the present study is to develop a measure of problematic online game use by identifying underlying factors and testing external validities of the scale. The authors tested the scale with the three age groups: 5th, 8th, and 11th graders. Through a series of exploratory and confirmatory factor analyses, the present study confirmed that the POGU scale produced reliable and consistent factorial structures across the independent samples. The results supported convergent validity of the scale: POGU showed significant correlations with academic self-efficacy, anxiety, loneliness, and satisfaction with daily life. The results also supported the discriminant validity. The POGU scale did not redundantly measure any of individual difference constructs and was statistically distinguishable from the closely correlated constructs.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Developments in Business Gaming</title>
	<abstract>This article examines developments in business simulation gaming during the past 40 years. Covered in this article are a brief history of business games, the changing technology employed in the development and use of business games, changes in why business games are adopted and used, changes in how business games are administered, and the current state of business gaming. Readers interested in developments in other areas of simulation gaming (urban planning, social studies, ecology, economics, geography, health, etc.) are encouraged to look at other articles appearing during the 40th anniversary year of Simulation &amp; Gaming and at the many fine articles that appeared in the silver anniversary issue of Simulation &amp; Gaming (December 1995).</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>The effects of a player's network centrality on resource accessibility, game enjoyment, and continuance intention: A study on online gaming communities</title>
	<abstract>This study applies social capital theory to investigate how a player's network centrality in an online gaming community (i.e., a guild) affects his/her attitude and continuance intention toward a Massive Multiplayer Online Game (MMOG). Analysis of 347 usable responses shows that players' network centrality has a negative impact on their ties to players who belong to other guilds (i.e., non-guild interaction), but a positive effect on players' access to resources. However, players' network centrality fails to increase their perceived game enjoyment directly. Players' resource accessibility and perceived game enjoyment play mediating roles in the relationship between network centrality and attitude toward playing an MMOG, which in turn influences game continuance intention. The results also show that although players' non-guild interaction is negatively related to their resource accessibility from the networks, it is positively associated with perceived game enjoyment. The article concludes with implications and limitations of the study.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>How computer gamers experience the game situation: a behavioral study</title>
	<abstract>Very little is known about computer gamers' playing experience. Most social scientific research has treated gaming as an undifferentiated activity associated with various factors outside the gaming context. This article considers computer games as behavior settings worthy of social scientific investigation in their own right and contributes to a better understanding of computer gaming as a complex, context-dependent, goal-directed activity. The results of an exploratory interview-based study of computer gaming within the "first-person shooter" (FPS) game genre are reported. FPS gaming is a fast-paced form of goal-directed activity that takes place in complex, dynamic behavioral environments where players must quickly make sense of changes in their immediate situation and respond with appropriate actions. Gamers' perceptions and evaluations of various aspects of the FPS gaming situation are documented, including positive and negative aspects of game interfaces, map environments, weapons, computer-generated game characters (bots), multiplayer gaming on local area networks (LANs) or the internet, and single player gaming. The results provide insights into the structure of gamers' mental models of the FPS genre by identifying salient categories of their FPS gaming experience. It is proposed that aspects of FPS games most salient to gamers were those perceived to be most behaviorally relevant to goal attainment, and that the evaluation of various situational stimuli depended on the extent to which they were perceived either to support or to hinder goal attainment. Implications for the design of FPS games that players experience as challenging, interesting, and fun are discussed.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>


  <item>
    <title>Exploring advergaming and its online advertising implications</title>
	<abstract>Traditional methods of Internet advertising, such as banners and pop-ups, have lost their support by marketers in recent years. Good businesses recognise the value of ongoing customer relationship management and the potential profits that come from cultivating and strengthening that relationship over time. The end of the Information Technology (IT) boom has brought managers' attention back to the essence of Customer Relationship Management (CRM). When IT companies were no longer able to sustain their billion dollar businesses, they began focusing their attention to the importance of retaining loyal customers. Advergaming is potentially a dramatic way, because advertising online is cheaper than companies can through more traditional media and more effectively than using banners and pop-ups. Advergames may be able to take advantage of the recent trend of online gaming by targeting many different and specific demographic groups of consumers. Many companies that utilise advergaming have seen a great response from consumers. The Diffusion of Innovations Theory may assist in explaining the basic evolution as some companies are adopting the usage of advergaming in their marketing strategies.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>This is not a one-horse race: understanding player types in multiplayer pervasive health games for youth</title>
	<abstract>Technology-based interventions for promoting health behavior-change frequently leverage multiplayer game mechanics such as group-based competitions. However, health interventions successful for groups writ large may not always translate to successful behavior change at the individual level. In this paper, we explore the tension between group and individual success, based on an empirical study on a long-term real-world deployment of a pervasive health game for youth. We report five distinctive player types along the dimensions of motivation, behavior, and influence on others. Based on the findings, we provide design suggestions to help game designers integrate group-based mechanisms that maximize intervention effectiveness.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Unleashing Web 2.0: From Concepts to Creativity</title>
	<abstract>The emergence of Web 2.0 is provoking challenging questions for developers: What products and services can our company provide to customers and employees using Rich Internet Applications, mash-ups, Web feeds or Ajax? Which business models are appropriate and how do we implement them? What are best practices and how do we apply them? 

If you need answers to these and related questions, you need this book--a comprehensive and reliable resource that guides you into the emerging and unstructured landscape that is Web 2.0.

* Presents a complete view of Web 2.0 including services and technologies
* Discusses potential new products and services and the technology and programming ability needed to realize them
* Offers `how to' basics presenting development frameworks and best practices
* Compares and contrasts Web 2.0 with the Semantic Web

Gottfried Vossen is a professor of Information Systems and Computer Science at the University of Muenster in Germany. He is the European Editor-in-Chief of Elsevier's Information Systems--An International Journal. Stephan Hagemann is a PhD. Student in Gottfried's research group focused on Web technologies.

* Presents a complete view of Web 2.0 including services and technologies
* Discusses potential new products and services and the technology and programming ability needed to realize them
* Offers `how to' basics presenting development frameworks and best practices
* Compares and contrasts Web 2.0 with the Semantic Web</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Vlogging: A survey of videoblogging technology on the web</title>
	<abstract>In recent years, blogging has become an exploding passion among Internet communities. By combining the grassroots blogging with the richness of expression available in video, videoblogs (vlogs for short) will be a powerful new media adjunct to our existing televised news sources. Vlogs have gained much attention worldwide, especially with Google's acquisition of YouTube. This article presents a comprehensive survey of videoblogging (vlogging for short) as a new technological trend. We first summarize the technological challenges for vlogging as four key issues that need to be answered. Along with their respective possibilities, we give a review of the currently available techniques and tools supporting vlogging, and envision emerging technological directions for future vlogging. Several multimedia technologies are introduced to empower vlogging technology with better scalability, interactivity, searchability, and accessability, and to potentially reduce the legal, economic, and moral risks of vlogging applications. We also make an in-depth investigation of various vlog mining topics from a research perspective and present several incentive applications such as user-targeted video advertising and collective intelligence gaming. We believe that vlogging and its applications will bring new opportunities and drives to the research in related fields.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Exploring user-producer interaction in an online community: the case of Habbo Hotel</title>
	<abstract>This article attempts to explore the user-producer interaction in the online community of Habbo Hotel. Based on desk research, interviews, an online survey among more than 3000 Habbo Hotel users in The Netherlands and online discussion groups with 45 Habbos, three specific issues that illustrate the interactions between the users and producers are highlighted. First, different levels of user-generated content in Habbo Hotel will be discussed. Second, the communication between the users and producers will be analysed and, finally, the matter of online safety will be reviewed.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

<item>
    <title>Technical, Commercial and Regulatory Challenges of QoS: An Internet Service Model Perspective</title>
	<abstract>This book provides a comprehensive examination of Internet QoS theory, standards, vendor implementation and network deployment from the practitioner's point of view, including extensive discussion of related economic and regulatory issues. Written in a technology-light way so that a variety of professionals and researchers in the information and networking industries can easily grasp the material. Includes case studies based on real-world experiences from industry. 

The author starts by discussing the economic, regulatory and technical challenges of the existing QoS model. Key coverage includes defining a clear business model for selling and buying QoS in relation to current and future direction of government regulation and QoS interoperability (or lack thereof) between carriers and networking devices. The author then demonstrates how to improve the current QoS model to create a clear selling point, less regulation uncertainty, and higher chance of deployment success. This includes discussion of QoS re-packaging to end-users; economic and regulatory benefits of the re-packaging; and the overall benefits of an improved technical approach. Finally, the author discusses the future evolution of QoS from an Internet philosophy perspective and lets the reader draw the conclusions.

This book is the first QoS book to provide in depth coverage on the commercial and regulatory aspects of QoS, in addition to the technical aspect. From that, readers can grasp the commercial and regulatory issues of QoS and their implications on the overall QoS business model. This book is also the first QoS book to provide case studies of real world QoS deployments, contributed by the people who did the actual deployments. From that, readers can grasp the practical issues of QoS in real world. This book is also the first QoS book to cover both wireline QoS and wireless QoS. Readers can grasp the QoS issues in the wireless world. The book was reviewed and endorsed by a long list of prominent industrial and academic figures.

* The only book to discuss QoS technology in relation to economic and regulatory issues 
* Includes case studies based on real-world examples from industry practitioners.
* Provides unique insight into how to improve the current QoS model to create a clear selling point, less regulatory uncertainty, and higher chance of deployment success.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Identification of professional players in an online community: a case study</title>
	<abstract>In this paper we analyze different kinds of World of Warcraft players -- normal, professional and high ELO player - and evaluate their behavior in online gaming communities in an experimental environment. The experiment points out a linkage between the group of professional players and the group of high ELO players. We created an online community for the massive multiplayer online game World of Warcraft. The main findings of the experiment are a lower login rate of the high ELO players, higher count of posts for the online community by high ELO players and an overall higher game experience by these players. Some of these results are counter-intuitive to the picture of a professional player. The gathered data during the experiment does not support the hypotheses that high ELO users have a higher number of logins in the community. This implies that high ELO players tend rather to visit less and be more active in the community in this time or that they be active over a extend period of time within one login at the community site.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Sociable killers: understanding social relationships in an online first-person shooter game</title>
	<abstract>Online video games can be seen as medium for the formation and maintenance of social relationships. In this paper, we explore what social relationships mean under the context of online First-Person Shooter (FPS) games, how these relationships influence game experience, and how players manage them. We combine qualitative interview and quantitative game log data, and find that despite the gap between the non-persistent game world and potentially persistent social relationships, a diversity of social relationships emerge and they play a central role in the enjoyment of online FPS games. We report the forms, development, and impact of such relationships, and discuss our findings in light of design implications and comparison with other game genres.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Business models and operational issues in the Chinese online game industry</title>
	<abstract>The rapid growth of Internet usage has enabled many new online communities to develop. A particularly interesting phenomenon that has arisen through Internet communities is the virtual world (VW) style of online game. This paper identifies the challenges that developers of VWs will face in their efforts to find viable business models. This is a single case study of China as an exploratory project to determine the issues surrounding business models for virtual world developers and users. The paper discusses the feedback effects between broadband adoption and online games as well as issues such as culture, history, Waigua, private servers, virtual property trade, developer control, governance, and regulation. In spite of the profitability of major Chinese VW operators, close observation of the Chinese case suggests that even the most successful VW operators are still in the early stages of their business model development.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Digital Game-Based Learning in high school Computer Science education: Impact on educational effectiveness and student motivation</title>
	<abstract>The aim of this study was to assess the learning effectiveness and motivational appeal of a computer game for learning computer memory concepts, which was designed according to the curricular objectives and the subject matter of the Greek high school Computer Science (CS) curriculum, as compared to a similar application, encompassing identical learning objectives and content but lacking the gaming aspect. The study also investigated potential gender differences in the game's learning effectiveness and motivational appeal. The sample was 88 students, who were randomly assigned to two groups, one of which used the gaming application (Group A, N=47) and the other one the non-gaming one (Group B, N=41). A Computer Memory Knowledge Test (CMKT) was used as the pretest and posttest. Students were also observed during the interventions. Furthermore, after the interventions, students' views on the application they had used were elicited through a feedback questionnaire. Data analyses showed that the gaming approach was both more effective in promoting students' knowledge of computer memory concepts and more motivational than the non-gaming approach. Despite boys' greater involvement with, liking of and experience in computer gaming, and their greater initial computer memory knowledge, the learning gains that boys and girls achieved through the use of the game did not differ significantly, and the game was found to be equally motivational for boys and girls. The results suggest that within high school CS, educational computer games can be exploited as effective and motivational learning environments, regardless of students' gender.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>


  <item>
    <title>Learning from playing with microworlds in policy making: An experimental evaluation in fisheries management</title>
	<abstract>This study determines whether stakeholders learn from playing with microworlds. This is investigated through a case study of Belgian fisheries management. Policymakers, scientists and fishermen participated in a ''before-after with control group''-experiment in which they played with a microworld that aims at gaining insight into the long-term effect of policy instruments on the Belgian fisheries system. The outcome of this experiment indicates that using the microworld did not result in learning outcomes (i.e., changes in participants' subjective knowledge, attitude and behavioural intention towards policy instruments). This however, contradicts all stakeholders' reports that they had learned from the microworld and that they had confidence in the microworld and perceived the microworld to be valid. Hence, three alternative explanations for these results are discussed: (1) methodological issues blocked the detection of learning outcomes, (2) the way in which the microworld was administered did not result in learning outcomes, or (3) participants have not ''learned'' anything new from the microworld. Finally, the paper ends with discussing guidelines and further steps in evaluating learning from microworlds.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Segmenting online gamers by motivation</title>
	<abstract>Online games are becoming one of the most profitable online entertainment businesses. Nevertheless, online game players may have diverse motivations for playing online games, which in turn influence their behaviors and preferences in playing online games. Through factor analysis for dimensionality reduction, this study identifies two underlying motivational factors: (1) the need for exploration, and (2) the need for aggression. From these two factors, this study clusters online gamers into three segments: (1) the aggressive gamers, (2) the social gamers, and (3) the inactive gamers. The aggressive gamers score highest on both factors. The social gamers score high on the first factor, but score lowest on the second factor. The inactive gamers score lowest on the first factor, and score in the middle on the second factor. Significant differences exist in consumer behaviors across different segments of online gamers. Theoretical supports for the two motivational factors are discussed, and managerial implications for online game service firms are provided.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Multinational web uses and gratifications: Measuring the social impact of online community participation across national boundaries</title>
	<abstract>This paper describes the rationale and findings from a multinational study of online uses and gratifications conducted in the United States, Korea, and the Netherlands in spring 2003. Survey questions developed in three languages by native speaking researchers was presented to approximately 400 respondents in each country via the Web. Web uses and gratifications were analyzed cross-nationally in a comparative fashion focusing on involvement in different types of on-line communities. Findings indicate that demographic characteristics, cultural values, and Internet connection type emerged as critical factors that explain why the same technology is adopted differently.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Problematic Internet use and psychosocial well-being among MMO players</title>
	<abstract>The current study examined problematic Internet use (PIU) among people who play MMO games and sought to determine whether aspects of the MMO experience are useful predictors of PIU. The study sought to determine whether game-related variables could predict PIU scores after accounting for their relationships with psychosocial well-being. Novel methods allowed us, for the first time, to connect in-game behaviors with survey results of over 4000 MMO players. The results revealed that MMO gaming variables contributed a substantively small, but statistically significant amount of explained variance to PIU scores.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Unpacking the potential of educational gaming: A new tool for gaming research</title>
	<abstract>The article begins by reviewing the theoretical bases for the contention that advanced computer-based educational gaming can provide powerful learning experiences, and overviews the limited research on the use of such games. Although studies to date have generally supported their value, most of the published investigations have methodological limitations. Critical process data are typically not collected, and unreliable student and teacher self-reports are heavily relied on in evaluating the educational efficacy of many games. To address these and other limitations, the authors have developed research software that can remotely and unobtrusively record screen activity during game play in classroom settings together with synchronized audio of player discussion. A field trial of this data collection system in which 42 college students were studied as they played a coursework-related Web-based learning game is described, and the article discusses how the trial outcomes concretely demonstrate the methodological advantages the tool offers researchers.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

<item>
    <title>Mobile gaming: Industry challenges and policy implications</title>
	<abstract>Mobile games are a prime example of a successful mobile application and demonstrate the increasing range of platforms for the media and entertainment industries. Against this convergent background, this paper introduces the basic features of the mobile gaming market and its industrial ecosystem together with its main actors and activities. The focus of the paper lies in the challenges ahead for the evolution of mobile applications into a potentially dominant game platform and the possible disruptions along this road. The deep personal relationships between users and their mobile devices are considered to further explore the link between mobile games, players' strategies and pending techno-economic developments. The paper concludes with a brief discussion of some policy options to assist with the development of this domain.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Seriously fun: exploring how to combine promoting health awareness and engaging gameplay</title>
	<abstract>Combining engaging gameplay and educational aspects promoting health awareness gives an interesting challenge to game designers. This case study explores adolescents' (aged 13--16) technology usage, gaming habits and gaming motivations, as well as the elements affecting the user experience in serious games. Findings emphasize the importance of social aspects in gaming in form of presence of friends while gaming, or in offline or online multiplayer gaming. According to the findings games should offer challenges matching the player's competence and enable improvement, advancement and developing skills. Modifying characters, exploring the game world and physical activity were also important. The causes and consequences of selections in game and their relation to real-life were wished to be shown clearly in games. Design implications for designing games that promote health awareness are presented.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Online community building techniques used by video game developers</title>
	<abstract>Online fan communities are an important element in the market success of a videogame, and game developers have begun to recognize the importance of fostering online communities associated with their games. In this paper we report on a study that investigated the techniques used by game developers to maintain and promote online communities within and around their games. We found that game developers consider online communities to be important to the success of both single-player and online multiplayer games, and that they actively support and nourish these communities. Online community building techniques identified in the study are categorized and discussed. The results represent a snapshot of current developer thinking and practice with regards to game-based online communities. The study augments existing research concerning the relationship between design features, online community and customer loyalty in new media, Internet and game-related industries.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Virtual worlds - past, present, and future: New directions in social computing</title>
	<abstract>Virtual worlds, where thousands of people can interact simultaneously within the same three-dimensional environment, represent a frontier in social computing with critical implications for business, education, social sciences, and our society at large. In this paper, we first trace the history of virtual worlds back to its antecedents in electronic gaming and on-line social networking. We then provide an overview of extant virtual worlds, including education-focused, theme-based, community-specific, children-focused, and self-determined worlds - and we analyze the relationship among these worlds according to an initial taxonomy for the area. Recognizing the apparent leadership of Second Life among today's self-determined virtual worlds, we present a detailed case study of this environment, including surveys of 138 residents regarding how they perceive and utilize the environment. Lastly, we provide a literature review of existing virtual world research, with a focus on business research, and a condensed summary of research issues in education, social sciences, and humanities.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>'Migrating to a new virtual world': Exploring MMORPG switching through human migration theory</title>
	<abstract>Online gaming has become a popular leisure-time activity. In this study, we enlisted and adapted the Push-Pull-Mooring model, which analyzes human migratory behavior based on the Demographic Migration Theory, to study the game switching of gamers. Data was obtained via an empirical survey of 654 online gamers and then was analyzed using the Structural Equation Modeling (SEM) technique. The results indicate that the Push-Pull-Mooring model can be extended to explain the switching intentions of online gamers. The ''mooring effect'' appears to have a stronger influence on the player's switching intention than the ''pull effect'', while the ''push effect'' appears to have no influence at all. We discuss the implications of our findings and offer possible avenues of exploration for managers of online game providers in order to help them understand their customers better.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>


  <item>
    <title>Massively multiplayer online role-playing games: the past, present, and future</title>
	<abstract>Massively multiplayer online role-playing games (MMORPGs) are emerging in the computer game industry as a very popular genre. These games have existed since the late 1990s, but in the last few years the market has become increasingly strong. This relatively new genre is attracting a widespread audience, bringing together those who previously enjoyed both pen and paper and computer role-playing games, as well as those who enjoy socializing with other players in a virtual environment. Game developers see MMORPGs as a potentially profitable business due to its widespread appeal, but the reality is that only a small percentage of MMORPGs that are released become a success [Kosak 2006].

This article attempts to determine the many aspects that make a successful MMORPG; it also attempts to ascertain what new and innovative features are expected by the users from the next generation of MMORPGs. This is achieved by looking at and discussing past literature and surveying the MMORPG community's perception of previous and current MMORPGs, as well as their expectations of the next generation.

An online survey attracted 122 participants to provide their perceptions of current and past MMORPGs. This article determines and outlines the respondents' preferences in the MMORPG genre, discussing what implications these could have on its future. The survey also gave insight into the respondents' expectations of the future of MMORPGs. We conclude this article with a discussion of aspects of current MMORPGs that the participants would like improved, as well as new features they would like incorporated into the next generation of games.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Online gaming and web-based communities: serious games for community development</title>
	<abstract>Serious games take the compelling and creative aspects of traditional computer games and apply them for non-entertainment purposes. The development of serious online games could contribute to many web-located community developments. This paper explores the potential for serious online games for inclusion in web-based community developments and outlines a participatory design project which utilises online gaming technologies. We conclude that serious online games are a useful component in such developments, but that their design and development should take into account their potential users and that as people become more experienced and used to interacting with gaming technologies, the expectations for these types of interaction will rise.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Internet inequality: The relationship between high school students' Internet use in different locations and their Internet self-efficacy</title>
	<abstract>This research study utilized the framework of digital inequality proposed by DiMaggio and Hargittai (2001) to examine the relationships among the subdimensions of Internet inequality and their outcomes. We firstly investigated the relationships between constructs of technical apparatus, autonomy of use, availability of social support, variation of use at different locations of Internet access (school, home, Internet cafe, and combinations of these locations) and Internet self-efficacy (ISE). Then the relationships between ISE and high school students' exploratory behavior and academic were also investigated. The survey was developed from reliable instruments used in previous research to measure the following variables: Internet Self-Efficacy, Internet accessibility at home and school, exploratory behaviors, academic performance, study use, leisure use, parents influence, superior influence, and training support. Internet access at the Internet cafe, gender, and self-reported academic achievement were added to the student survey. Bivariate correlation and regression statistical analyses were conducted to find significant relationships among these variables. ANOVA statistical analysis was used to find significant differences among groups. Significant findings indicated that digital inequality in Internet existed in school, home and Internet cafe and students with Internet access at home had the highest level of ISE. Our study also showed that different dimension of Internet inequality had different relationships with ISE. Home Internet accessibility positively related to ISE. Availability of social support from school had a greater effect than that from home as parents influence did not associate with ISE. And last, the variation of use was also related to ISE. Leisure use at Internet cafe, leisure use at home and study use at home positively associated with ISE. In addition, at home and Internet cafe, the relationship between leisure use and ISE was stronger than that between study use and ISE. As to the outcome of ISE, high levels of ISE were positively related to exploratory behaviors, and for those students who used the Internet at school and home, higher ISE related to better academic performance.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Distribution of online hardcore player behavior: (how hardcore are you?)</title>
	<abstract>Within the last few years the game market has seen a tremendous growth. On pair with that, the way games are played has also evolved. Not only the community and variety of gaming has grown in numbers; the way games are understood has fundamentally changed. In fact terms like "hardcore" and "casual" became well known in the player scene. Even so called pro-gaming (professional gaming for money - like sportsmen) has led to tournaments and world championships.

The underlying social deterministic and the changing player behavior are responsible for upcoming game design. First of all one must understand the user's perspective to find effective solutions for open problem fields.

This paper introduces a large user survey about hardcore player behavior and gaming. It will show correlations between game types, deterministic factors, game related behavior and different ways of approaching games. Furthermore it features a detailed statistic analysis with surprising results.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Factors influencing users' decisions to adopt voice communication in online console games</title>
	<abstract>We used a Model of Technology Appropriation to understand users' initial reaction to, and use of, the voice communication channel provided by Xbox Live. We found that while users expected voice to be an advance over text-based communication, in practice they found it difficult to use, leading some to reject it. There appear to be usability and sociability problems with the way the voice channel is currently configured in some games. We argue that game developers will need to address these problems in order to realise the potential of voice in online multiplayer videogames.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

<item>
    <title>Social networking and digital gaming media convergence: Classification and its consequences for appropriation</title>
	<abstract>Within the field of Information Systems, a good proportion of research is concerned with the work organisation and this has, to some extent, restricted the kind of application areas given consideration. Yet, it is clear that information and communication technology deployments beyond the work organisation are acquiring increased importance in our lives. With this in mind, we offer a field study of the appropriation of an online play space known as Habbo Hotel. Habbo Hotel, as a site of media convergence, incorporates social networking and digital gaming functionality. Our research highlights the ethical problems such a dual classification of technology may bring. We focus upon a particular set of activities undertaken within and facilitated by the space--scamming. Scammers dupe members with respect to their `Furni', virtual objects that have online and offline economic value. Through our analysis we show that sometimes, online activities are bracketed off from those defined as offline and that this can be related to how the technology is classified by members--as a social networking site and/or a digital game. In turn, this may affect members' beliefs about rights and wrongs. We conclude that given increasing media convergence, the way forward is to continue the project of educating people regarding the difficulties of determining rights and wrongs, and how rights and wrongs may be acted out with respect to new technologies of play online and offline.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Street fighter IV: braggadocio off and on-line</title>
	<abstract>In its heyday, the video arcade was a social scene to prove one's video gaming prowess. The introduction of a revolutionary head-to-head fighting game called "Street Fighter II" in 1991 ushered in an era of competitive video gaming with unparalleled complexity. An influx of copy-cat games and the arrival of consoles with capabilities rivaling coin-ops led to the arcade's demise. However, the release of "Street Fighter IV" (SF4) has brought about a revival. I report on the cultural practices of hardcore gaming that have revolved around SF4. SF4's release on both the console (which enables fighting others online) and the arcade has engendered a new set of challenges in constructing what it means to be competitive and legitimate in the world of head-to-head fighting games. I observe that the "enrolment" of an ecology of technological artifacts allows players to translate braggadocio from the arcade, a central phenomenon in competitive gaming.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Business Models for Monetizing Internet Applications and Web Sites: Experience, Theory, and Predictions</title>
	<abstract>Almost all attempts to date to monetize Internet applications targeted at individuals have focused on natural extensions of traditional media or traditional retailing. Most are either some form of consumer-focused advertising or of consumer-focused e-commerce. And yet the Net is far more powerful than traditional media on one hand, and far more liberating and thus inappropriate as an alternative to traditional media on the other. There are numerous potential online business models that are not based on advertising. This paper explores several such areas and divides them into two basic categories, those that sell some product, experience, content, or service and earn revenues from the sale, and those that provide access to consumers and charge for access. It further divides each of these major categories into subcategories, and for each subcategory reviews current experience, places it in the context of the relevant literature in competitive strategy, and uses that theory to make predictions. The paper concludes with strategic recommendations for corporations as well as with suggestions for further research.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>"It is always a lot of fun!": exploring dimensions of digital game experience using focus group methodology</title>
	<abstract>This paper focuses on digital game experience: the feelings and experiences people have when they play digital games. Digital game experience is not a one-dimensional concept. Great variety exists in game genres and game players, and game experiences will differ accordingly. To date, game experience is studied in a rather fragmented way. As such, the field still lacks a common vocabulary and a shared taxonomy of the different dimensions of game experience. In this paper we describe a focus group study and present a tentative, but comprehensive categorisation of game experience. Focus groups with various types of gamers were organised to capture a full first-hand account of game experiences and second, findings were discussed in an expert meeting in which empirical findings were consolidated with existing theoretical findings. The categorisation bears relevance for both game theorists and game developers wanting to get to the heart of digital game experience.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Using Gaming Literacies to Cultivate New Literacies</title>
	<abstract>The use of games in educational contexts has recently received growing attention; however, many teachers struggle with finding a right context to adopt games in the classroom. To strengthen teachers' beliefs about the educational value of games, this article explains the similarities and differences between new literacies and gaming literacy and defines gaming literacy from the "play" and "design" perspectives. This article presents examples of game use that will help teachers cultivate learners' new literacies through gaming, understand how the game debriefing process can help scaffold students' learning, and identify factors affecting the adoption of games in educational contexts. The audience for this article includes teachers and teacher educators who would like greater clarification regarding gaming literacy and its positive effects on the learning and development of new literacies.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>


  <item>
    <title>Multiplayer networked gaming with the session initiation protocol</title>
	<abstract>With a strong push from commercial ventures like Microsoft Xbox^(R) and Sony Playstation^(R), the multiplayer networked gaming industry continues to grow steadily. Multiplayer games allow geographically dispersed players to participate in a single game and in order to provide interaction amongst players in such environments, text messaging and real-time voice through VoIP is used. However, such interactions are out-of-band (not based on game contexts), user-initiated and severely limited in operability and functionality. In this paper, we present mechanisms and design of a prototype that uses the lightweight Session Initiation Protocol (SIP) to provide context-aware gaming. In addition to allowing players to talk to each other to coordinate teammates and activities (through a static team-based audio conference) as in current systems, it supports real-time communication among players based on shared contexts like the same physical location or room within the gaming environment. This is provided through seamless management of audio sessions (conferences) based on player movements/behaviors which change the shared game context. We extend our earlier work by providing a dynamic form of shared context, using a modified SIP session negotiation mechanism. In addition, through the use of SIP as the game communication protocol, we propose to make VoIP a first class member of the game state. This allows a unified architecture for context-aware communication and gaming. We also present a sophisticated gaming scenario, in which VoIP is used to relay information about another player's distance and location with respect to the recipient, e.g. players farther away sound farther away. ion mechanism. In addition, through the use of SIP as the game communication protocol, we propose to make VoIP a first class member of the game state. This allows a unified architecture for context-aware communication and gaming. We also present a sophisticated gaming scenario, in which VoIP is used to relay information about another player's distance and location with respect to the recipient, e.g. players farther away sound farther away.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Deploying QoS for Cisco IP and Next Generation Networks: The Definitive Guide</title>
	<abstract>Deploying QoS for IP Next Generation Networks: The Definitive Guide provides network architects and planners with insight into the various aspects that drive QoS deployment for the various network types. It serves as a single source of reference for businesses that plan to deploy a QoS framework for voice, video, mobility and data applications creating a converged infrastructure. It further provides detailed design and implementation details for various service deployments across the various Cisco platforms such as the CRS-1, 12000, 7600 &amp; 7200 series routers that are widely deployed in most Carrier Networks. 

The book covers architectural and implementation specific information plus recommendations for almost all the popular line cards across the various hardware platforms widely used in the market. It also addresses QoS architecture and deployment on the Cisco CRS-1 platform and is considered as a unique selling point of this book. 

In short the books serve as an "On the Job Manual" which can also be used as a study guide for Cisco specialist certification programs (CCNA, CCIP, CCIE)

This book will includes detailed illustration and configurations. In addition, it provides detailed case studies along with platform specific tests and measurement results. A link to a detailed tutorial on QoS metrics and associated test results will be available at the book's companion website in order to ensure that the reader is able to understand QoS functionality from a deployment standpoint.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Game content creation and it proficiency: An exploratory study</title>
	<abstract>Computer and video gaming are often considered to be potential routes to the development of aptitude and interest in using other forms of information technology (IT). The purpose of this exploratory study was to determine the extent to which young people who play games engage in related IT practices, such as creating and sharing content or creating fan sites. Additional goals were to identify differences in such practices according to grade level, gender, and access to IT-related resources in the home, as well as to explore relationships between engagement in game-related practices and perceived proficiency in general computer-related skills.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Why would online gamers share their innovation-conducive knowledge in the online game user community? Integrating individual motivations and social capital perspectives</title>
	<abstract>The user community has been an important external source of a firm's product or service innovation. Users' innovation-conducive knowledge sharing enables the community to work as a vital source of innovation. But, traditional economic theories of innovation seem to provide few explanations about why such knowledge sharing takes place for free in the user community. Therefore, this study investigates what drives community users to freely share their innovation-conducive knowledge, using the theory of planned behavior. Based on an empirical analysis of the data from 1244 members of a South Korean online game user community, it reveals that intrinsic motivation, shared goals, and social trust are salient factors in promoting users' innovation-conducive knowledge sharing. Extrinsic motivation and social tie, however, were found to affect such sharing adversely, contingent upon whether a user is an innovator or a non-innovator. The study illustrates how social capital, in addition to individual motivations, forms and influences users' innovation-conducive knowledge sharing in the online gaming context.</abstract>	
	<search_task_number>18</search_task_number>
	<query>online gaming group competition</query>
	<relevance>1</relevance>
  </item>




  <item>
    <title>Gestures without libraries, toolkits or training: a $1 recognizer for user interface prototypes</title>
	<abstract>Although mobile, tablet, large display, and tabletop computers increasingly present opportunities for using pen, finger, and wand gestures in user interfaces, implementing gesture recognition largely has been the privilege of pattern matching experts, not user interface prototypers. Although some user interface libraries and toolkits offer gesture recognizers, such infrastructure is often unavailable in design-oriented environments like Flash, scripting environments like JavaScript, or brand new off-desktop prototyping environments. To enable novice programmers to incorporate gestures into their UI prototypes, we present a "$1 recognizer" that is easy, cheap, and usable almost anywhere in about 100 lines of code. In a study comparing our $1 recognizer, Dynamic Time Warping, and the Rubine classifier on user-supplied gestures, we found that $1 obtains over 97% accuracy with only 1 loaded template and 99% accuracy with 3+ loaded templates. These results were nearly identical to DTW and superior to Rubine. In addition, we found that medium-speed gestures, in which users balanced speed and accuracy, were recognized better than slow or fast gestures for all three recognizers. We also discuss the effect that the number of templates or training examples has on recognition, the score falloff along recognizers' N-best lists, and results for individual gestures. We include detailed pseudocode of the $1 recognizer to aid development, inspection, extension, and testing.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Human gesture recognition using 3.5-dimensional trajectory features for hands-free user interface</title>
	<abstract>We present a new human motion recognition technique for a hands-free user interface. Although many motion recognition technologies for video sequences have been reported, no man-machine interface that recognizes enough variety of motions has been developed. The difficulty was the lack of spatial information that could be acquired from video sequences captured by a normal camera. The proposed system uses a depth image in addition to a normal grayscale image from a time-of-flight camera that measures the depth to objects, so various motions are accurately recognized. The main functions of this system are gesture recognition and posture measurement. The former is performed using the bag-of-words approach. The trajectories of tracked key points around the human body are used as features in this approach. The main technical contribution of the proposed method is the use of 3.5D spatiotemporal trajectory features, which contain horizontal, vertical, time, and depth information. The latter is obtained through face detection and object tracking technology. The proposed user interface is useful and natural because it does not require any contact-type devices, such as a motion sensor controller. The effectiveness of the proposed 3.5D spatiotemporal features was confirmed through a comparative experiment with conventional 3.0D spatiotemporal features. The generality of the system was proven by an experiment with multiple people. The usefulness of the system as a pointing device was also proven by a practical simulation.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Online gesture recognition for user interface on accelerometer built-in mobile phones</title>
	<abstract>Recently, several smart phones are equipped with a 3D-accelerometer that can be used for gesture-based user interface (UI). In order to utilize the gesture UI for the real-time systems with various users, the diversity robust algorithm, yet having low training/recognition complexity, is required. Meantime, dynamic time warping (DTW) has shown good performance on the simple time-series pattern recognition problems. Since DTW is based on the template matching, its processing time and accuracy depend on the number of templates and their quality, respectively. In this paper, an optimized method for online gesture UI of mobile devices is proposed which is based on the DTW and modified k-means clustering algorithm. The templates, estimated by using the modified clustering algorithm, can preserve the time varying attribute while contain diversities of the given training patterns. The proposed method was validated on 20 types of gestures which are designed for the mobile contents browsing. The experimental results showed that the proposed method is suitable to the online mobile UI.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Cultural similarities and differences in user-defined gestures for touchscreen user interfaces</title>
	<abstract>As the first phase of a two-phase project, the International Usability Partners (IUP; http://www.international-usability-partners.com/) conducted a study in nine different countries to identify cultural similarities and differences in the use of gestures on small, handheld, touchscreen user interfaces. A total of 340 participants in the study were asked to define their own gestures for 28 common actions like "zoom" and "copy" on a custom-constructed gesture recorder that simulated a handheld touchscreen device. Actions were described pictorially by showing participants a "before" screen and an "after" screen to clarify the exact context for each action.Initial analysis suggests four primary findings. The first is that there is generally a high level of agreement across cultures. One exception, however, is the use of symbolic gestures; Chinese participants created significantly (p  .01) more symbolic gestures (e.g. letters, question mark, check mark) than participants from other countries. The second finding is that experience with gesture-enabled devices influenced the gestures that participants created for the following actions: back, forward, scroll up, and scroll down. The third finding is that when a gesture to elicit an action was not immediately identifiable, participants generally tapped on the screen to bring up a menu. The final finding is that there is higher agreement on actions that can be performed through direct manipulation and lower agreement scores on actions that are more symbolic in nature.
Phase two of this research effort will be to present the most common three to five user-defined gestures for each action to a large number of participants and ask them to select the gesture that they believe to be the most intuitive gesture for that action.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Towards context-aware gesture enabled user interfaces</title>
	<abstract>Conventional graphical user interface techniques appear to be ill-suited for the kinds of interactive platforms that are required for future generations of computing devices. 3D graphics and immersive virtual reality applications require interactive 3D object manipulation and navigation. Perceptual user interfaces using speech and gestures are in high demand to provide a more natural human-computer interaction modality. The major challenge facing Perceptual user interfaces is the lack of a standard application programming interfaces capable of handling ambiguity and providing the means to include domain-specific knowledge about the context in which the user interface is used.

In this dissertation, we study dynamic hand gestures, which are defined as a sequence of hand postures. We emphasize the generality of our dynamic gesture model, which is capable of recognizing essentially any dynamic hand gesture confined in a sequence of postures. Hand postures are static poses and are defined by an array of posture attributes. We use a generic definition hand postures capable of covering the space of hand postures at different levels of granularity and abstraction; and we timely monitor the posture variation as it unfolds within the dynamic gesture.

We also study the role of context in gesture interpretation without making assumptions about a specific application. We view the hand-tracking and gesture-recognition subsystems as integral parts of a larger distributed and multi-user multi-service application, where gesture interpretation plays the role of resolving ambiguity of the recognized gesture. We identify the relevant aspects to hand gesture interpretation and we propose agent-based system architecture for gesture interpretation.

We finally propose a framework for gesture-enabled system design, where context is placed in a middleware layer that interfaces with all sub modules in the system and plays a dialectic role and keeping the overall system stable.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Brave NUI World: Designing Natural User Interfaces for Touch and Gesture</title>
	<abstract>Touch and gesturaldeviceshave been hailed as next evolutionary step in human-computer interaction. As software companies struggle to catch up with one another in terms of developing the next great touch-based interface, designers are charged with the daunting task of keeping up with the advances innew technology and this new aspect to user experience design. Product and interaction designers, developers and managers are already well versed in UI design, but touch-based interfaces have added a new level of complexity. They need quick references and real-world examples in order to make informed decisions when designing for these particular interfaces. Brave NUI World is the first practical book for product and interaction developers and designing touch and gesture interfaces. Written by developers of industry-first, multi-touch, multi-user products, this book gives you the necessary tools and information to integrate touch and gesture practices into your daily work, presenting scenarios, problem solving, metaphors, and techniques intended to avoid making mistakes. *Provides easy-to-apply design guidance for the unique challenge of creating touch- and gesture-based user interfaces *Considers diverse user needs and context, real world successes and failures, and a look into the future of NUI *Presents thirty scenarios, giving practitioners a multitude of considerations for making informed design decisions and helping to ensure that missteps are never made again</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>New User Interfaces for Mobile Devices Using Augmented Reality: Expanding the Interaction by Intuitive Gesture Recognition</title>
	<abstract>This work started 10 years ago when the first generation of smarter mobile phones was introduced. At that time the author anticipated the limitations in interaction. The dilemma is that mobility requires the devices to get smaller whereas the users demand better interaction. One possible solution would be to create a virtual interface which would not only be easily portable but also provide virtually unlimited display. This book presents the background literature, how other experts addressed the issue and a simple solution to the problem. The system is based on a combination of intuitive gestures, a processing unit with a display and a small video camera. The technologies are ready and quite mature today. Similar systems can be used in various areas. It is therefore very possible that in the next few years something similar will become the next keyboard or mouse in everyones hands.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Gesture avatar: a technique for operating mobile user interfaces using gestures</title>
	<abstract>Finger-based touch input has become a major interaction modality for mobile user interfaces. However, due to the low precision of finger input, small user interface components are often difficult to acquire and operate on a mobile device. It is even harder when the user is on the go and unable to pay close attention to the interface. In this paper, we present Gesture Avatar, a novel interaction technique that allows users to operate existing arbitrary user interfaces using gestures. It leverages the visibility of graphical user interfaces and the casual interaction of gestures. Gesture Avatar can be used to enhance a range of mobile interactions. A user study we conducted showed that compared to Shift (an alternative technique for target acquisition tasks), Gesture Avatar performed at a much lower error rate on various target sizes and significantly faster on small targets (1mm). It also showed that using Gesture Avatar while walking did not significantly impact its performance, which makes it suitable for mobile uses.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Gesture-based user interfaces for public spaces</title>
	<abstract>Gesture-based approaches are becoming an increasingly popular technique in Human Computer Interaction. Recent developments in the hardware field have made it more affordable and more reliable to use gesture-based interfaces and they are becoming more of a standard way for human users to interact with computers. Most of the research has been investigating the usage of gestures in personal and limited access situations. But gesture interfaces are promising great benefits to usage scenarios in public spaces or general access environments. This paper will summarize and evaluate the particular aspects of using gesture-based interfaces in application contexts in public and semipublic spaces.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>A trajectory-based approach for device independent gesture recognition in multimodal user interfaces</title>
	<abstract>With the rise of technology in all areas of life new interaction techniques are required. With gestures and voice being the most natural ways to interact, it is a goal to also support this in human-computer interaction. In this paper, we introduce our approach to multimodal interaction in smart home environments and illustrate how device independent gesture recognition can be of great support in this area. We describe a trajectory-based approach that is applied to support device independent dynamic hand gesture recognition from vision systems, accelerometers or pen devices. The recorded data from the different devices is transformed to a common basis (2D-space) and the feature extraction and recognition is done on this basis. In a comprehensive case study we show the feasibility of the recognition and the integration with a multimodal and adaptive home operating system.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>A perceptual user interface for recognizing head gesture acknowledgements</title>
	<abstract>We present the design and implementation of a perceptual user interface for a responsive dialog-box agent that employs real-time computer vision to recognize user acknowledgements from head gestures (e.g., nod = yes). IBM Pupil-Cam technology together with anthropometric head and face measures are used to first detect the location of the user's face. Salient facial features are then identi ed and tracked to compute the global 2-D motion direction of the head. For recognition, timings of natural gesture motion are incorporated into a state-space model. The interface is presented in the context of an enhanced text editor employing a perceptual dialog-box agent.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Gesture-Based user interfaces for handheld devices using accelerometer</title>
	<abstract>This paper is about how to treat the signals of accelerometers to recognize user gestures from detected signals from accelerometers after applying small accelerometers to handheld devices, and about how to precisely recognize gestures to detect user gestures. To use handheld devices in recognizing gestures, overheads arising from the process of recognizing gestures should be little and gestures should be effectively recognized in real operational environments. Therefore, signals detected from accelerometers were treated after classifying them "static acceleration" and "dynamic acceleration", and signal patterns of accelerometers about simple gestures were analyzed. In addition, a device control module was created and evaluated the usability of gestures recognition. The result was that because gesture-based control is easy to use and can reduce preparation process to control for rapid system reaction, it is a proper user interface for handheld devices primarily used in mobile environments.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Evoking Gestures in SmartKom - Design of the Graphical User Interface</title>
	<abstract>The aim of the SmartKom project is to develop an intelligent, multimodal computer-user interface which can deal with various kinds of input and allows a quasi-natural communication between user and machine. This contribution is concerned with the design of the Graphical User Interface (GUI) of the Wizard-of-Oz recordings. Our special interest was to create a display different from known internet applications which could motivate the users to communicate with the machine also via gestures. The following sections give a short overview about the system itself followed by a detailed description of the used methods and the results in the first phase. To conclude, a short overwiew of the methods we implemented in the second phase is given.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>3D user interface combining gaze and hand gestures for large-scale display</title>
	<abstract>In this paper, we present a novel attentive and immersive user interface based on gaze and hand gestures for interactive large-scale displays. The combination of gaze and hand gestures provide more interesting and immersive ways to manipulate 3D information.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Gestures all around us: user differences in social acceptability perceptions of gesture based interfaces</title>
	<abstract>Gesture based interfaces provide a new way for us to interact with mobile devices, but also require us to make new decisions about how we feel about this new technology and which gestures we decide are usable and appropriate. These decisions are based on the social and public settings where these devices are used on a daily basis. Our ideas about which gestures are socially acceptable or not are an important factor in whether or not these gestures will be adopted. The ways in which users evaluate social acceptability is not only highly variable, but with drastically different results amongst different users. These differences are not dependant on factors such as age, gender, occupation, geographic location, or previous technology usage. Future work into the social acceptability perceptions of users will focus on personality traits as a new way of understanding how social acceptability is determined.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Users' views of facial expressions and body gestures in e-learning interfaces: an empirical evaluation</title>
	<abstract>This paper introduces an experimental study performed to investigate the usability aspects of e-learning interfaces that incorporate the use of avatar as a virtual lecturer. A within-subject experiment has been conducted using three different e-learning interfaces which were developed from scratch and tested by a group of 48 users. Each of these three interfaces involved the use of a human-like avatar as a virtual lecturer to present one of three different lessons about class diagram notation usually used in the software engineering process. The scope of this paper is to report and discuss the experimental results related to users' satisfaction and views in regard to a set of facial expressions and body gestures when used by a virtual lecturer in the presence and absence of interactive context in e-learning interfaces. These results highlighted that some facial expressions as well as some body gestures were perceived by the users more positively than other expressions and gestures. Consequently, it could be used to improve the attractiveness of virtual lecturers which in turn will be reflected in increasing users' motivation and interest about the presented learning material.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Designing the user interface for multimodal speech and pen-based gesture applications: state-of-the-art systems and future research directions</title>
	<abstract>The growing interest in multimodal interface design is inspired in large part by the goals of supporting more transparent, flexible, efficient, and powerfully expressive means of human-computer interaction than in the past. Multimodal interfaces are expected to support a wider range of diverse applications, be usable by a broader spectrum of the average population, and function more reliably under realistic and challenging usage conditions. In this article, we summarize the emerging architectural approaches for interpreting speech and pen-based gestural input in a robust manner-including early and late fusion approaches, and the new hybrid symbolic-statistical approach. We also describe a diverse collection of state-of-the-art multimodal systems that process users' spoken and gestural input. These applications range from map-based and virtual reality systems for engaging in simulations and training, to field medic systems for mobile use in noisy environments, to web-based transactions and standard text-editing applications that will reshape daily computing and have a significant commercial impact. To realize successful multimodal systems of the future, many key research challenges remain to be addressed. Among these challenges are the development of cognitive theories to guide multimodal system design, and the development of effective natural language processing, dialogue processing, and error-handling techniques. In addition, new multimodal systems will be needed that can function more robustly and adaptively, and with support for collaborative multiperson use. Before this new class of systems can proliferate, toolkits also will be needed to promote software development for both simulated and functioning systems.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>A quantitative quality model for gesture based user interfaces</title>
	<abstract>The technological advancement of computers and cameras over the past few years has given us the ability to control objects without touching them. There have already been a number of attempts at producing gesture based applications, but many of them have usability issues. This paper proposes a model that reflects the usability of a gesture based interface, in order to evaluate and improve a gesture-controlled system. The model defines four levels of abstraction, with the higher levels based on the lower ones. The levels of the model allow us to propose quantitative notions for 1) the parameters affecting the quality of individual gestures, 2) the overall quality of a gesture, 3) the quality of particular functionalities, or use cases, in a system, and 4) the overall quality of a system. The model was evaluated using an existing gesture-based interface for a popular media center application.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Designing user interfaces for multi-touch and surface-gesture devices</title>
	<abstract>Initially Designers only had a keyboard and lines of text to design. Then, the mouse enabled a richer design ecosystem with two dimensional plains of UI. Now the Design and Research communities have access to multi-touch and gestural interfaces which have been released on a mass market scale. This allows them to design and develop new, unique, and richer design patterns and approaches. These methods are no longer confined to research projects or innovation labs, but are now offered on a large scale to millions of consumers. With these new interface behaviors, in combination with multiple types of hardware devices that can affect the interface, there are new problems and patterns that have increased the complexity of designing interfaces.

The aim of this SIG is to provide a forum for Designers, Researchers, and Usability Professionals to discuss this new and emerging technology trends for multi-touch and gesture interfaces, as well as discuss current design patterns within these interfaces. Our goal is to cross pollinate ideas and current solutions from practitioners and researchers across communities to help drive awareness of this new field for those interested in, just starting in, or currently involved in the design of these systems.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Designing user interfaces for multi-touch and gesture devices</title>
	<abstract>Initially Designers only had a keyboard and lines of text to design. Then, the mouse enabled a richer design ecosystem with two dimensional plains of UI. Now the Design and Research communities have access to multi-touch and gestural interfaces which have been released on a mass market scale. This allows them to design and develop new, unique, and richer design patterns and approaches. These methods are no longer confined to research projects or innovation labs, but are now offered on a large scale to millions of consumers. With these new interface behaviors, in combination with multiple types of hardware devices that can affect the interface, there are new problems and patterns that have increased the complexity of designing interfaces.

The aim of this SIG is to provide a forum for Designers, Researchers, and Usability Professionals to discuss this new and emerging technology trends for multi-touch and gesture interfaces, as well as discuss current design patterns within these interfaces. Our goal is to cross pollinate ideas and current solutions from practitioners and researchers across communities to help drive awareness of this new field for those interested in, just starting in, or currently involved in the design of these systems.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>An HMM based gesture recognition for perceptual user interface</title>
	<abstract>This paper proposes a novel hidden Markov model (HMM)-based gesture recognition method and applies it to the HCI to control a computer game. The novelty of the proposed method is two-folds. First one, the proposed method uses a continuous sequence of human motion as an input of HMM, instead of isolated data sequences or pre-segmented sequences of the data. The other one, it performs both segmentation and recognition of the human gesture automatically. To assess the validity of the proposed method, we applied the proposed system to a real game, Quake II, and then the results demonstrate that the proposed HMM can provide very useful information to enhance the discrimination between the different classes and reduce the computational cost.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Issues limiting the acceptance of user interfaces using gesture input and handwriting character recognition (panel)</title>
	<abstract>Recently there has been increasing attention to character recognition/graphical user interfaces under the name of "gesture input". This technique actually has a long history: "sketch recognition" interfaces of 15 or more years ago were highly praised [Applicon 73], and user interfaces using handwriting input before the wide use of text keyboards were one of the first research goals in computer science [Bledsoe 59]. The underlying character and symbol recognition technologies have been a major research area in their own right since the early 1950s [Suen 80].The last two years have seen an upsurge in the number of developments in this area, both from commercial companies attempting to exploit new character and symbol recognition technologies, [CIC 85] [Pencept 84] [Cooper 82] and from researchers starting from fundamental questions in user interactions [Buxton 86] [Wolf 86]. However, one question still remains: "Why has this set of techniques had so little impact on user interface design practice, despite its long history and promise?" This panel discussion should give many answers to this question.Panelists include the leading commercial developers of handwriting input products, well-known researchers in the psychological aspects of graphical user interactions, and representatives of the research community for character recognition.The issue of supporting this type of interface is very timely: recent standardization efforts such as PHIGS and GKS for graphics interactions are known to have the unfortunate side effects of excluding some of the current user interface designs using this class of technology [10].</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>A gesture based user interface prototyping system</title>
	<abstract>GID, for Gestural Interface Designer, is an experimental system for prototyping gesture-based user interfaces. GID structures an interface as a collection of "controls": objects that maintain an image on the display and respond to input from pointing and gesture-sensing devices. GID includes an editor for arranging controls on the screen and saving screen layouts to a file. Once an interface is created, GID provides mechanisms for routing input to the appropriate destination objects even when input arrives in parallel from several devices. GID also provides low level feature extraction and gesture representation primitives to assist in parsing gestures.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Toward the use of gesture in traditional user interfaces</title>
	<abstract>This work describes the design of a functioning user interface based on visual recognition of hand gestures, and details its performance. In the interface, gesture replaces the mouse for many actions including selecting, moving and resizing windows. A camera below the screen observes the user. The hand is segmented from the background using color. Features of the hand's motion are extracted from the sequence of segmented images, and when needed the hand's pose is classified using a neural net. This information is parsed by a task specific grammar. The system runs in real time on standard PC hardware. It has demonstrated its abilities with various users in several different office environments. Having experimented with a functioning gestural interface, the authors discuss the practicality and best applications of this technology. </abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>From human movement analysis to interface design: applications to writing and gesture based user interface</title>
	<abstract>A number of authors have studied the possibility of using handwriting or gesture as inputs. In most cases, the focus was either on building robust algorithms for recognition or identifying a set of assumed "natural" movements. Few authors have taken the analysis of actual movements as a starting point to build a set of precise and stable movements. This paper presents some arguments in favor of using results from human movement science to improve the elaboration of robust interfaces.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Hand Gesture Recognition: Self-Organising Maps as a Graphical User Interface for the Partitioning of Large Training Data Sets</title>
	<abstract>Gesture recognition is a difficult task in computer vision due to the numerous degrees of freedom of a human hand. Fortunately, human gesture covers only a small part of the theoretical "configuration space" of a hand, so an appearance based representation of human gesture becomes tractable. A major problem, however, is the acquisition of appropriate labelled image data from which an appearance based representation can be built. In this paper we apply self-organising maps for a visualisation of large amounts of segmented hands performing pointing gestures. Using a graphical interface, an easy labelling of the data set is facilitated. The labelled set is used to train a neural classification system, which is itself embedded in a larger architecture for the recognition of gestural reference to objects. </abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>An Approach for Interpretation of MEMS Data for Gesture Based Man Machine Interaction in Advanced Set-top Box</title>
	<abstract>Currently different type of interface between man and machine is a reach research area. One such method for human machine interaction is gesture based interface. This paper presents a novel approach for interpreting MEMS data for Human Machine Interaction (HMI) that can be used to give user input to an interactive Set top box(STB) in terms of gesture. Moreover recent STBs are coming with a internet browser integrated into it. In this paper we have proposed a method where users can use the (MEMS) as a mouse pointer or a method to give some gesture based user input (like channel/volume up/down) or as an alternative to keyboard (by writing the English alphabet in capital letter) for web browsing. We get a recall rate of 0.994 and precision rate as 0.998.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Visual Gesture Interfaces for Virtual Environments</title>
	<abstract>Virtual environments provide a whole new way of viewing and manipulating 3D data. Current technology moves the images out of desktop monitors and into the space immediately surrounding the user. Users can literally put their hands on the virtual objects. Unfortunately techniques for interacting with such environments have yet to mature. Gloves and sensor based trackers are unwieldy, constraining and uncomfortable to use. A natural, more intuitive method of interaction would be to allow the user to grasp objects with their hands and manipulate them as if they were real objects.We are investigating the use of computer vision in implementing a natural interface based on hand gestures. A framework for a gesture recognition system is introduced along with results of experiments in color segmentation, feature extraction and template matching for finger and hand tracking and hand pose recognition. Progress in the implementation of a gesture interface for navigation and object manipulation in virtual environments is discussed.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>RealNav: Exploring natural user interfaces for locomotion in video games</title>
	<abstract>We present a reality-based locomotion study directly applicable to video game interfaces; specifically, locomotion control of the quarterback in American football. Focusing on American football drives requirements and ecologically grounds the interface tasks of: running down the field, maneuvering in a small area, and evasive gestures such as spinning, jumping, and the "juke". The locomotion interface is constructed by exploring data interpretation methods on two commodity hardware configurations. The choices represent a comparison between hardware available to video game designers, trading off traditional 3D interface data for greater hardware availability. Configuration one matches traditional 3D interface data, with a commodity head tracker and leg accelerometers for running in place. Configuration two uses a spatially convenient device with a single accelerometer and infrared camera. Data interpretation methods on configuration two use two elementary approaches and a third hybrid approach, making use of the disparate and intermittent input data combined with a Kalman filter. Methods incorporating gyroscopic data are used to further improve the interpretation. Our results show spatially convenient hardware, currently in many gamers' homes, when properly interpreted can lead to more robust interfaces. We support this by a user evaluation on the metrics of position and orientation accuracy, range and gesture recognition.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Design of the 3D Input Method Based on Touch Device for Mobile</title>
	<abstract>This thesis describes the technology to express 3D gesture as an input from common 2-dimension touch input panel withoutadditional hardware development or change to provide near real 3D UI and algorithm to implement such technology. It introduces how 3D user interface has become the major issues among the major mobile vendors and briefly describes existing touch devices which can be divided into resistive and capacitive touch panels. It also introduces Open GLES which takes a major role in graphical presentation of 3D user interface. Algorithm to achieve the 3D gesture in 2D space is presented at last.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Keyboardless Visual Programming Using Voice, Handwriting, and Gesture</title>
	<abstract>Visual programming languages have facilitated the application development process, improving our ability to express programs, as well as our ability to view, edit, and interact with them. Yet even in visual programming environments, productivity is often restricted by the primary input sources: the mouse and the keyboard. As an alternative, we investigate a program development interface which responds to the most natural human communication technologies: voice, handwriting, and gesture. Speech- and pen-based systems have yet to find broad acceptance in everyday life because they are insufficiently advantageous to overcome problems with reliability. However, we believe that a visual programming environment with a multimodal user interface properly constrained so as not to exceed the limits of the current technology has the potential to increase programming productivity for not only those people who are manually or visually impaired, but for the general population as well. In this paper we report on such a system. </abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Gesture-Based Interaction: Concept Map and Application Scenarios</title>
	<abstract>Nowadays new interaction forms are not limited by Graphical User Interfaces (GUIs) making Human Computer Interaction (HCI) more natural. This paper presents: a novel interaction approach within the scope of gesture-based interfaces, and some practical consequences of adopting this approach. Thus, we redefine the concept of gesture based on the sum of three interrelated concepts (the Para-language, the Body language and the Sound language). The relationships among these concepts are expressed in a conceptual map through parameterized links. Finally, the article shows how the combination of the link parameters yields to the definition of a specific application scenario. Thus, the result of the identification of these scenarios opens new horizons towards the identification of new ways of gesture-based interaction.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Cross-Dimensional Gestural Interaction Techniques for Hybrid Imrnersive Environments</title>
	<abstract>We present a set of cross-dimensional interaction techniques for a hybrid user interface that integrates existing 2D and 3D visualization and interaction devices. Our approach is built around one- and two-handed gestures that support the seamless transition of data between co-located 2D and 3D contexts. Our testbed environment combines a 2D multi-user, multi-touch, projection surface with 3D head-tracked, see-through, head-worn displays and 3D tracked gloves to form a multi-display augmented reality. We address some of the ways in which we can interact with private data in a collaborative, heterogeneous workspace. We also report on a pilot usability study to evaluate the effectiveness and ease of use of the cross-dimensional interactions.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Interaction with On-Screen Objects using Visual Gesture Recognition</title>
	<abstract>This paper will review the design of a working system that visually recognizes hand gestures for the control of a window based user interface. After an overview of the system, it will explore one aspect of gestural interaction in depth, hand tracking, and what is needed for the user to be able to interact comfortably with on-screen objects. We describe how the location of the hand is mapped to a location on the screen, and how it is both necessary and possible to smooth the camera input using a non-linear physical model of the cursor. The performance of the system is examined, especially with respect to object selection. We show how a standard HCI model of object selection (Fitts' Law) can be extended to model the selection performance of free-hand pointing.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Real-Time Natural Hand Gestures</title>
	<abstract>The authors introduces the methods and implementations of a hand visualization system they created that can generate natural hand gestures in real time. This system can be used to provide ground truth data for research on human hand gesture analysis and recognition. First, a lifelike hand model is constructed. Then, based on the biomechanical features of the hand, constraints are applied to the constructed model to allownatural hand gestures and animations. After that, graphical user interfaces and built-in gesture and animation data structures are created and designed to facilitate theoperation process.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>A multi-touch enabled human-transporter metaphor for virtual 3D traveling</title>
	<abstract>In this tech-note we demonstrate how multi-touch hand gestures in combination with foot gestures can be used to perform navigation tasks in interactive 3D environments. Geographic Information Systems (GIS) are well suited as a complex testbed for evaluation of user interfaces based on multi-modal input. Recent developments in the area of interactive surfaces enable the construction of low-cost multi-touch displays and relatively inexpensive sensor technology to detect foot gestures, which allows to explore these input modalities for virtual reality environments. In this tech-note, we describe an intuitive 3D user interface metaphor and corresponding hardware, which combine multi-touch hand and foot gestures for interaction with spatial data.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Rock 'n' Scroll Is Here to Stay</title>
	<abstract>A novel user input method for digital appliances is introduced using the example of an electronic photo album. Based on tilting and gesturing with the device, the Rock 'n' Scroll input method handles scrolling, selection, and commanding an application without resorting to buttons, touch screens, spoken commands, or other input methods. User experiments with a prototype system showed that users could quickly learn how to operate such devices. The experiments also offered some insight into user expectations. These positive results, combined with improvements in the sensor technology, encouraged us to incorporate this input method into an experimental handheld system.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Gesture-Based Chemical Formula Editing System</title>
	<abstract>In this paper, a gesture-based editing system is proposed for the recognition of on-line handwritten chemical expressions. We put forward a set of operations, including eleven independent gestures, which are easy to recognize and quick to learn. In gesture recognition section, we propose a two-tier classification, which deals with the global as well as the local features of gestures. The results show that these gestures are well-designed and easy to recognize. We have applied these gestures to the chemical formula editing system.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>A Fuzzy Based Approach for the Real-Time Capture and Dissemination of Visual Interest Information in Shared Environments</title>
	<abstract>Participants in collaborative sessions, whether in reality or virtuality, often experience difficulties interacting if all they can perceive about peer activities are actions without context. Contextual information has a significant influence on the evolution of collaborative interactions by allowing opportunities for timely interruptions and for peer intents to be identified. Computer Supported Cooperative Work (CSCW) researchers have demonstrated collaborative systems that allow geographically dispersed users to work together synchronously. Evaluations of this class of systems have confirmed the need for transmitting at a distance, contextual cues, some socially relevant and others task relevant, in addition to the actions of the participants. For example, in a real world collaborative drawing task, cues that help to establish context can take the form of peer gestures, e.g. hand movements, gaze pursuits, gaze fixations, and head shifts. Unfortunately, in real-time groupware systems, resources for context building often cannot be made available. System specific constraints restrict user representations to artifacts that do not have sufficient expressive power to describe gestures. Thus, we propose a strategy to deal with the lack of contextual linkage between views of a collaborative information space that do not lend themselves easily to effective peer user representations.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>gRmobile: A Framework for Touch and Accelerometer Gesture Recognition for Mobile Games</title>
	<abstract>Mobile phone games are usually design to be able to play using the traditional number pads of the handsets. This is stressfully difficult for the user interaction and consequently for the game design. Because of that, one of the most desired features of a mobile games is the usage of few buttons as possible. Nowadays, with the evolution of the mobile phones, more types of user interaction are appearing, like touch and accelerometer input. With these features, game developers have new forms of exploring the user input, being necessary to adapt or create new kinds of game play. With mobile phones equipped with 3D accelerometers, developers can use the simple motion of the device to control the game or use complex accelerated gestures. And with mobile phones equipped with the touch feature, they can use a simple touch or a complex touch gesture recognitions. For the gesture to be recognized one can use different methods like simple brute force gestures, that only works well on simple gestures, or more complex pattern recognition techniques like hidden Markov fields, fuzzy logic and neural networks. This work presents a novel framework for touch/accelerometer gesture recognition that uses hidden Markov model for recognition of the gestures. This framework can also be used for the development of mobile application with the use of gestures.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Detection and Estimation of Pointing Gestures in Dense Disparity Maps</title>
	<abstract>In this paper we describe a real-time system for detecting pointing gestures and estimating the direction of pointing using stereo cameras. Previously, similar systems were implemented using color-based blob trackers, which relied on effective skin color detection; this approach is sensitive to lighting changes and the clothing worn by the user. In contrast, we used a stereo system that produces dense disparity maps in real-time. Disparity maps are considerably less sensitive to lighting changes. Our system subtracts the background, analyzes the foreground pixels to break the body into parts using a robust mixture model, and estimates the direction of pointing. We have tested the system on both coarse and fine pointing by selecting the targets in a room and controlling the cursor on a wall screen, respectively.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Tailoring Model-Based Techniques to Facial Expression Interpretation</title>
	<abstract>Computers have been widely deployed to our daily lives, but human-computer interaction still lacks intuition. Researchers intend to resolve these shortcomings by augmenting traditional systems with human-like interaction capabilities. Knowledge about human emotion, behavior, and intention is necessary to construct convenient interaction mechanisms. Today, dedicated hardware often infers the emotional state from human body measures. Similar to humans interpreting facial expressions, our approach acquires video information using standard hardware that does not interfere with people to accomplish this task. It exploits model-based techniques that accurately localize facial features, seamlessly track them through image sequences, and finally interpret the visible information. We make use of state-of-the-art techniques and specifically adapt most of the components involved to this scenario, which provides high accuracy and real-time capability. We base our experimental evaluation on publicly available databases and compare its results to related approaches. Our proof-of-concept demonstrates the feasibility of our approach and shows promising for integration into various applications.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>In the News</title>
	<abstract>AI-empowered speech, gesture, and emotion recognition will make communicating with electronic devices a lot more intuitive.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Online Handwritten Shape Recognition Using Segmental Hidden Markov Models</title>
	<abstract>We investigate a new approach for online handwritten shape recognition. Interesting features of this approach include learning without manual tuning, learning from very few training samples, incremental learning of characters, and adaptation to the user-specific needs. The proposed system can deal with two-dimensional graphical shapes such as Latin and Asian characters, command gestures, symbols, small drawings, and geometric shapes. It can be used as a building block for a series of recognition tasks with many applications.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>A lightweight multistroke recognizer for user interface prototypes</title>
	<abstract>With the expansion of pen- and touch-based computing, new user interface prototypes may incorporate stroke gestures. Many gestures comprise multiple strokes, but building state-of-the-art multistroke gesture recognizers is nontrivial and time-consuming. Luckily, user interface prototypes often do not require state-of-the-art recognizers that are general and maintainable, due to the simpler nature of most user interface gestures. To enable easy incorporation of multistroke recognition in user interface prototypes, we present $N, a lightweight, concise multistroke recognizer that uses only simple geometry and trigonometry. A full pseudocode listing is given as an appendix.

$N is a significant extension to the $1 unistroke recognizer, which has seen quick uptake in prototypes but has key limitations. $N goes further by (1) recognizing gestures comprising multiple strokes, (2) automatically generalizing from one multistroke to all possible multistrokes using alternative stroke orders and directions, (3) recognizing one-dimensional gestures such as lines, and (4) providing bounded rotation invariance. In addition, $N uses two speed optimizations, one with start angles that saves 79.1% of comparisons and increases accuracy 1.3%. The other, which is optional, compares multistroke templates and candidates only if they have the same number of strokes, reducing comparisons further to 89.5% and increasing accuracy another 1.7%. These results are taken from our study of algebra symbols entered in situ by middle and high schoolers using a math tutor prototype, on which $N was 96.6% accurate with 15 templates.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Incorporating tilt-based interaction in multimodal user interfaces for mobile devices</title>
	<abstract>Emerging ubiquitous environments raise the need to support multiple interaction modalities in diverse types of devices. Designing multimodal interfaces for ubiquitous environments using development tools creates challenges since target platforms support different resources and interfaces. Model-based approaches have been recognized as useful for managing the increasing complexity consequent to the many available interaction platforms. However, they have usually focused on graphical and/or vocal modalities. This paper presents a solution for enabling the development of tilt-based hand gesture and graphical modalities for mobile devices in a multimodal user interface development tool. The challenges related to developing gesture-based applications for various types of devices involving mobile devices are discussed in detail. The possible solution presented is based on a logical description language for hand-gesture user interfaces. Such language allows us to obtain a user interface implementation on the target mobile platform. The solution is illustrated with an example application that can be accessed from both the desktop and mobile device supporting tilt-based gesture interaction.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>A model-based approach for gesture interfaces</title>
	<abstract>The interaction technologies had substantial enhancements in later years, with the introduction of devices whose capabilities changed the way people interact with games and mobile devices. However, this change did not really affected desktop systems. Indeed, few applications are able to exploit such new interaction modalities in an effective manner. This work envisions the application of model-based approaches for the engineering of gesture user interfaces, in order to provide the designer with a comprehensive theoretical framework for usage centred application design. The differences between existing gesture-enabling devices will be tackled applying more general solutions for multi-device user interfaces.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>1</relevance>
  </item>
  
  <item>
    <title>Gestural attributions as semantics in user interface sound design</title>
	<abstract>This paper proposes a gesture-based approach to user interface sound design, which utilises projections of body movements in sounds as meaningful attributions. The approach is founded on embodied conceptualisation of human cognition and it is justified through a literature review on the subject of interpersonal action understanding. According to the resulting hypothesis, stereotypical gestural cues, which correlate with, e.g., a certain communicative intention, represent specific non-linguistic meanings. Based on this theoretical framework, a model of a process is also outlined where stereotypical gestural cues are implemented in sound design.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>Interactive video mirrors for sports training</title>
	<abstract>This paper studies gesture and speech controlled video for sports training. The goal is to combine the benefits of recording your performance with video equipment and training with a mirror. For example, a delayed camera view projected on a screen can be used to repeatedly perform and evaluate a spin kick, a move that is difficult to practice with a mirror.

A video mirror can also be augmented with speech or gesture control for playback, recording and inspecting of individual frames. Three different interface design approaches are evaluated, based on testing with eight users that practice martial arts and acrobatics. The results suggest that an interactive video mirror can be highly useful in martial arts and other sports. The paper also introduces new kind of graphical controls that float around the user so that they can be manipulated with gestures regardless of the user's position.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>
  
  <item>
    <title>TapBeats: accessible and mobile casual gaming</title>
	<abstract>Conventional video games today rely on visual cues to drive user interaction, and as a result, there are few games for blind and low-vision people. To address this gap, we created an accessible and mobile casual game for Android called TapBeats, a musical rhythm game based on audio cues. In addition, we developed a gesture system that utilizes text-to-speech and haptic feedback to allow blind and low-vision users to interact with the game's menu screens using a mobile phone touchscreen. A graphical user interface is also included to encourage sighted users to play as well. Through this game, we aimed to explore how both blind and sighted users can share a common game experience.</abstract>
	<search_task_number>1</search_task_number>
	<query>gesture user interface</query>
	<relevance>0</relevance>
  </item>



  <item>
    <title>Measurement of LowPAN Network Coexistence with Home Microwave Appliances in Laboratory and Home Environments</title>
	<abstract>Due to a low installation cost of communication infrastructure, Low Power Wireless Area Networks (low PAN) presents an efficient solution for home and building automation. The low PANs operate in a microwave radio band that is often shared by several home appliances. Laptops, wireless routers, cordless phones and microwave ovens occupy the 2.4 GHz band without any channel allocation and thus interoperability with the low PAN systems presents a significant challenge. Therefore, we have performed several laboratory experiments to study the coexistence of low PAN devices with WiFi equipped laptop, WiFi router and microwave oven. The performance evaluation was based on a measurement of Packet Delivery Ratio (PDR) metric. Results shows that presence of the home appliances in the low PAN environment significantly affects the low PAN communication efficiency. Furthermore, we have deployed Zigbee based wireless network in the home environment to study the communication performance in area perturbed by common home radio interferences and obstacles. The lessons learned from network deployment are also presented within the paper.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An application of radial basis function networks in operation of home appliances</title>
	<abstract>Neural networks (NN) have made a great impact in modeling and synthesizing non-linear mapping of input-output space. In this paper, we describe the design and testing of a particular class of NN, radial basis function networks, for dryness prediction in a clothes dryer. Our objective is to design an improved, robust, accurate, and adaptive system for dryness prediction, leading to a low-cost, energy-efficient, electronically controlled clothes dryer. We synthesize a stepwise radial basis function network to predict clothes moisture content and optimize the number of required sensors, while providing learning capabilities to account for external disturbances. In addition, we quantize clothes moisture content into ``degrees of dryness'', thus enhancing the prediction accuracy. We show that this approach results in a predictor that is superior to non-linear regression and multi-layer perceptron tools.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Remote physiotherapy treatments using wireless body sensor networks</title>
	<abstract>Technology plays an important role in both primary and secondary healthcare. With widespread use of mobile devices and ubiquitous communications, new and novel platforms are emerging to administer care. Ordinary and everyday appliances used in the home are becoming integral components within these platforms and this could potentially revolutionise how health related information is monitored, accessed and used to administer better treatments. Despite the many challenges that exist, such platforms will allow for better exploitation of networked devices to provide benefits to patients with conditions, such as arthritis and back pain. Currently these conditions are treated through physiotherapy sessions in the community, which are often costly and difficult to resource. Physiotherapists alternate between patients. This means that assessments are sporadic and subjective. This paper aims to address these limitations using a system to implement body area and sensor networks within the home with data management functions for collecting and storing motion data. This data can be accessed via the home or remotely in one or more medical facilities. Using this data, quantitative assessments are performed and used to measure the patient's progress. A case study is presented that successfully illustrates tour approach.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Exploiting eye gaze information for operating services in home network system</title>
	<abstract>This paper presents a system which extensively exploits user's eye gaze information for operating services and appliances in the emerging home network system (HNS). We design and implement the system called AXELLA, which captures user's gaze, then invokes a service operation, and finally announces the response via voice. AXELLA interprets the gaze information together with supplementary information as a gaze context, and triggers a service module associated by a service rule. Thus, a simple gazing activity can be used for various service operations. Service developers (or even home users) can easily develop context-aware HNS services with the eye-gaze-based UI. We demonstrate a practical service called “See and Know” implemented using AXELLA, where a user can acquire the current status information of every appliance just by looking at the appliance. It was shown that the proposed system can reduce the artificial dependency significantly with respect to ease-of-learning and system scalability.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Joint admission and rate control for multimedia sharing in wireless home networks</title>
	<abstract>With the increasing popularity of broadband digital TV and digitized residential entertainment, home networking is attracting increasing attentions in the industry, the academia and the standardization organizations. Multimedia sharing is a promising service in home networks. Effective Quality-of-Service (QoS) provision for various multimedia services is a very significant challenge in home networks due to real-time transmission demand, multimedia heterogeneous characteristics and limited resources over radio interface (e.g. power, bandwidth). In this paper, we propose a Joint Admission and Rate Control (JARC) scheme for the QoS requirements of multimedia sharing in wireless home networks. JARC is designed to support QoS at both the user level and the network level. In particular, JARC ensures QoS provision at the user level using rate control at the application layer by considering the heterogenous nature of in-home digital appliances and exploiting the flexible multimedia processing. In addition, JARC is able to provide parameterized QoS guarantee at the network level using admission control of the Medium Access Control (MAC) layer by accurately predicting the QoS parameters with respect to packet delay and the bandwidth configuration of multimedia sessions. Simulation results demonstrate that JARC is able to flexibly allocate the wireless bandwidth to various multimedia services and provide QoS guarantee for multimedia applications in home networks.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  
  
  <item>
    <title>UbiREMOTE: framework for remotely controlling networked appliances through interaction with 3D virtual space</title>
	<abstract>In this paper, we propose a framework named ``UbiREMOTE'' for controlling information appliances connected to a home network with a unified and intuitive user interface from a remote place.The UbiREMOTE framework provides users with a way to control appliances in a home through a virtual space drawn on a mobile terminal screen which reflects the latest conditions of the real appliances and the rooms in the home. With UbiREMOTE, a user controls appliances by (1) moving to the front of an appliance, (2) choosing the appliance to control and (3) pushing buttons on the virtual remote controller which imitates the real remote controller for the appliance or the real console.In this paper, we propose a method to improve the drawing speed of 3D virtual space on mobile terminals and a method for automatically reflecting condition changes of the real space in the virtual space.We implemented the methods and evaluated the performance. The results showed that the proposed methods can be practically used on small mobile terminals.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An Intercommunication Home Energy Management System with Appliance Recognition in Home Network</title>
	<abstract>In present days there are wide varieties of household electric appliances along with different power consumption habits of consumers, making identifying electric appliances without presetting difficulty. This paper introduces smart appliance management system to recognize electric appliances in home networks, which uses sensing devices that measure current to calculate the power consumption of the appliances. The system will set the characteristics and categories of each electric appliance, and then uses the classifications of the electronic energy features in order to recognize different appliances. The system searches the cluster data while eliminating noise for recognition functionality and error detection mechanism or the electric appliances using the current clustering algorithm. Afterwards the recognition are used to build a control list of appliances on the platform to provide appliance intercommunication. Simultaneously, the household appliance automatic control services are integrated by the system to control appliances based on userspower consumption plans to realize a bidirectional monitoring services. In actual experiments, the proposed system achieves a recognition rate or 95% as well as successfully controls general household electric appliances in home network.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Networked appliances in home entertainment</title>
	<abstract>In recent years, advances in multimedia applications, services and intelligent networked appliances have been making ubiquitous home environments a reality. As broadband becomes part of our household infrastructure, in the same way we receive water, gas and electricity, it is envisaged that every device will have a network interface that allows it to be accessed and controlled from anywhere in the world. This idea is generating a great deal of interest where sound business models are being developed to realise such applications, based on market and user needs, that will map the future direction of Internet and home technologies. In this paper we present the current European research projects and initiatives within the area of networked appliances and home networking. These initiatives address a number of key requirements, which include on-demand multimedia services, home automation and home healthcare monitoring using sensors and global communications. We also present our framework on self-adaptive networked appliances and illustrate how services can be described semantically, and automatically composed using high level descriptions, which relate to the "what" part of the service composition rather than the "how".</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>OSGi-based services architecture for Cyber-Physical Home Control Systems</title>
	<abstract>With the development of digital home appliances, the vision of a digital home can be realized, and users can control home appliances by remote control through the home network. However, current home networks have a variety of transmission control information media, such as WLAN, Zigbee, ECHONET, and LonWorks, resulting in an issue of interoperability of different network protocols. To solve this problem, this study proposes a OSGi-based service architecture for Cyber-Physical Home Control Systems, which supports service-oriented control methods. Users can control appliances in the physical environment by intuitive operation through a virtual home on the network. When the state or location of any appliance in the physical environment changes, the virtual context can enact timely changes, accordingly.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Development of Internet Home Server to Control Information Appliances Remotely</title>
	<abstract>As the Internet and home networking technologies are developing rapidly, more users want to control their home appliances from a remote location, i.e. from outside the home via the Internet. However, some problems hinder them from doing so. First, an IP address is allocated dynamically to a subscriber to utilize the limited resources by the network service providers. As a result, it is difficult for users to make connections from outside the home to the home network. Secondly, for security, firewalls or proxies may be installed on the home network. This causes similar problem, too. In this paper, we propose a new approach to solve these problems and present our implementation. We built an Internet Home Server (IHS) outside the home, not inside of it, and it provides similar functions to those of the home server. Through IHS, users can be guaranteed to connect to home appliances from a remote location.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  
  
  <item>
    <title>A one-time password authentication scheme for secure remote access in intelligent home networks</title>
	<abstract>One of the important services that intelligent home networks provide is to remotely control home appliances in home network. However, the remote control service causes intelligent home networks to have various security threats. Thus, for the service, intelligent home networks should provide strong security services, especially user authentication. In this paper, we provide a public key based one-time password authentication scheme for secure remote access in intelligent home networks. To provide 2-factor strong authentication conveniently and cost effectively, we adopt and enhance YEH-SHEN-HWANG’s authentication scheme. Since our scheme uses a server-side public key to addresses the vulnerabilities of YEH-SHEN-HWANG’s scheme, it can securely perform user authentication, server authentication and session key distribution without any pre-shared secret, while defending against server compromise.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>HOTP-Based User Authentication Scheme in Home Networks</title>
	<abstract>Home networks are one of the focused areas of research these days. One of the important services that home networks provide is to remotely control home appliances in home network. However, the remote control service causes home networks to have various security threats. Hence, home networks should provide strong security services, especially remote user authentication. In this paper, we provide an efficient solution for authentication scheme to provide secure remote access in home network environments. Our proposed scheme uses HMAC-based one-time password algorithm for the user authentication in home networks. The proposed scheme is not only has several advantage features but also is quite satisfactory in terms of the security requirements of home networks.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Embedded web server-based home appliance networks</title>
	<abstract>Powerful microcontrollers are used as parts of most home and office appliances of today. Integrating web servers to these intelligent devices will aid in controlling them over the Internet and also in creating effective user interfaces in the form of web pages. Assigning multiple functionalities to a single button on an appliance help manufacturers economize user interfaces, but, this can easily create confusion for the users. Since the cost of web-based interfaces is considerably low, they can be used to provide the infrastructure for the design of simple and more user-friendly interfaces for household appliances. Also, a web page based interface is much easier to change, when needed, as compared to a hardware interface. This paper presents a novel approach to control devices with embedded web servers over the Internet and to form device networks such that their components can make use of one another's services and functions while improving the user interfaces. The main benefits of this approach include its lightweight design, automatic configuration, and, utilization of widely available and tested network protocols of TCP/IP and HTTP. The validity of the approach has been verified through a prototype system working with real appliances.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Robust one-time password authentication scheme using smart card for home network environment</title>
	<abstract>Due to the exponential growth of the Internet users and wireless devices, interests on home networks have been enormously increased in recent days. In digital home networks, home services including remote access and control to home appliances as well as services offered by service providers are alluring. However, the remote control services cause digital home networks to have various security threats. Hence, for digital home networks, robust security services, especially remote user authentication, should be considered. This paper presents a robust and efficient authentication scheme based on strong-password approach to provide secure remote access in digital home network environments. The proposed scheme uses lightweight computation modules including hashed one-time password and hash-chaining technique along with low-cost smart card technology. It aims to satisfy several security requirements including stolen smart card attack and forward secrecy with lost smart card as well as functional requirements including no verification table and no time synchronization. Comparing with the existing representative schemes, it can be validated that the proposed scheme is more robust authentication mechanism having better security properties. We have conducted formal verification of the proposed scheme.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item><item>
    <title>Web-Based Middleware for Home Entertainment</title>
	<abstract>In future domestic environments, we will have various home appliances that are connected to home networks. Connecting these appliances requires middleware for providing high level functionalities that hides details of complex issues about respective home appliances. The middleware should be simple and extensible. Also, the middleware needs to access various services on the Internet in an easy way because it is very important to provide future home entertainments that access Internet services.In this paper, we show our Web-based middleware for home computing environments. Recently, many companies have announced to sell Web-based home appliances. Our middleware enables us to integrate these appliances without their modifications. Also, our middleware allows us to build attractive future home applications that are connected to the Internet. We also show some experiences with our current prototypes and describe current ongoing work to improve the current system.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  
  
  <item>
    <title>Characterizing safety of integrated services in home network system</title>
	<abstract>This paper formalizes three kinds of safety to be satisfied by networked appliances and services in the emerging home network system (HNS). The local safety is defined by safety instructions of individual networked appliances. The global safety is specified as required properties of HNS services, which use multiple appliances simultaneously. The environment safety is derived from residential rules in home and surrounding environments. Based on the safety defined, we propose a modeling/validation framework for the safety. Specifically, we first introduce an object-oriented modeling technique to clarify the relationships among the appliances, the services and the home (environment) objects. We then employ the technique of Design by Contract with JML (Java Modeling Language), which achieves systematic safety validation through testing.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Operating networked appliances using gaze information and voice recognition</title>
	<abstract>In the emerging home network systems (HNS), household appliances and sensors are connected to the network to provide various value-added services. However, the cost taken for users to operate the HNS would increase significantly, due to the diversification of appliances and services. Thus, intuitive and simple human interfaces are strongly required. This paper presents a method that operates the networked appliances using user's voice and gaze information together. The conventional voice command interface cannot achieve high precision, when the number of target operations is large. Therefore, the gaze information is used to reduce the number of operations, so that the system handles only appliances that the user is currently gazing at. We have implemented the proposed method and evaluated its effectiveness.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Determinants of self-report and system-captured measures of mobile Internet use intensity</title>
	<abstract>Most research on the first adoption and subsequent use (= acceptance) of Internet access through cellular networks and portable appliances (= mobile Internet) has followed a similar pattern. It has employed survey responses of mobile network operator [MNO] customers to explain consumers' stated future use (continuance) intentions or claimed use intensities related to mobile Internet [MI] access by various beliefs about MI (e.g., perceived relative advantage, usefulness, ease of use). However, there is ample evidence suggesting that MI use intentions and self-reported use intensities are only weakly correlated with actual MI use. Therefore, the present paper develops hypotheses on how the ability of different types of variables to account for variance in MI use intensity may vary depending on whether subjectively estimated or objectively captured use serves as the criterion variable. The hypotheses are tested by analyzing actual MI use behaviors of 300 adopters in Germany, whose mobile IP traffic was extracted from an MNO's billing engine. This "system-captured" criterion measure is integrated with MI adopter responses collected by means of a standardized telephone survey. Results show that the predictors are more strongly correlated with self-rated than with system-captured MI use intensity. Up to 38% of the variance explained in self-rated use may be attributed to artifactual covariance between variables caused by common measurement methods. Factual MI use case features (MI tariff type and appliance class, fixed Internet home access availability) are better able to account for variance in both self-rated and actual MI use intensity than MI related beliefs. The findings imply that variable relationships observed in earlier MI and information system (IS) acceptance studies are likely to have been inflated by common method biases and thus may have provided spurious support for the conceptual frameworks tested. Implications of the results for future MI and IS acceptance research and for MNO seeking to forecast and to influence the MI use intensity of their customers are discussed.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Are our homes ready for services? A domotic infrastructure based on the Web service stack</title>
	<abstract>The increase in computational power and the networking abilities of home appliances are revolutionizing the way we interact with our homes. This trend is growing stronger and opening a number of technological challenges. From the point of view of distributed systems, there is a need to design architectures for enhancing the comfort and safety of the home, which deals with issues of heterogeneity, scalability and openness. By considering the evolution of domotic research and projects, we advocate a role for Web services in the domestic network. We ground our claim by proposing a concrete architecture for a home in which the health of an elder is monitored. The architecture is implemented on a heterogeneous set of devices, which allows us to evaluate it and draw conclusions on the feasibility of using service-oriented approaches in ubiquitous computing.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Virtual Organizations, Pervasive Computing, and an Infrastructure for Networking at the Edge</title>
	<abstract>That the Internet is undergoing explosive growth is not news. However, how that growth is happening might come as a surprise to many. We are quickly reaching the point (if we have not already) when the majority of nodes on the Internet are not computers running browsers or web servers that are directly used by humans. Instead, the Internet is becoming the communication fabric for devices to talk to services, which in turn talk to other services. Humans are quickly becoming a minority on the Internet, and the majority stakeholders are computational entities that are interacting with other computational entities without human intervention. Such change brings significant challenges and opportunities to those who wish to use or aid in the use of this connecting fabric. In a world of humans browsing the net, we could design our network-available services assuming that the use and applicability of those services could be recognized by something with human intelligence. When services must be recognized and used by other computational entities, no such assumption can be made. The intrinsically human abilities of understanding the meanings of descriptions and being able to figure out how to interact with a service are not currently within the scope of computational entities, and may never be. With a human in the loop, we have been able to go far by using the well-understood client-server model of distributed computing. But as the human moves out of the loop, we enter into the realm of peer-to-peer distribution, an area in which our understanding is far more limited. At the same time that we are switching to this new model of services, we are also expanding the set of client devices that are used to access services over the network. We can no longer assume a pc-class device running a full browser on a megapixel bitmap screen. Instead, the services on the network will be accessed by cell phones, palm devices, home appliances, and embedded systems in our automobiles. Some of these devices will be stationary, but many of them will be mobile. The set of devices will be constantly changing, and the protocols needed to talk to these devices will be constantly evolving. Our current models for system administration are insufficient for this environment, and new models will need to be adopted that allow dynamic evolution and rapid change. This paper will center on the new problems that will be encountered as we go through this change in the Internet, and discuss some of the more promising approaches to the problems. In particular, we will see how a combination of traditional techniques in distributed systems and some of the technologies traditionally associated with agents can lead to a network that is self-administering and allows the kinds of rapid change and evolution that will be required if the Internet is to continue to grow and thrive.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>0</relevance>
  </item>
    
  
  <item>
    <title>Do we need quality of service in the customer environment?</title>
	<abstract>The paper introduces the concept of quality of service (QoS) in the customer environment, as related to broadband customer premises equipment. The paper describes the BT vision for the customer environment, with a QoS-enabled `customer gateway' and `home networks' delivering a range of broadband services to the end user. The reasons behind the need for QoS are discussed and the differing requirements of the downstream (network to end user) as distinct from the upstream (end user to network) data paths are described.

Incorporating downstream QoS protocols in the customer gateway is less likely to give an effective perceived performance benefit due to the simple fact that the traffic priority is essentially predetermined by the network/service provider and only marginal benefits can be seen by its application. Extending downstream QoS across the home network will be essential to provide an effective customer experience. The paper looks at the future strategy for implementing QoS, not just within the customer gateway, but also up to the end terminal, as it should be remembered that all elements of the service such as end user appliances, home networks, customer gateways, the network provider and service provider all have their own distinct parts to play to guarantee the best quality of service and customer experience.

The paper concludes that the predominant area for QoS implementation in the customer environment, is in the upstream direction where even a basic QoS implementation such as simple queuing is likely to give noticeable improvements in the customer experience. New real-time services such as voice over IP (VoIP) will require upstream QoS.

Thus it should be possible to specify QoS enhancements that are straightforward and likely to be simple (and therefore cost effective) to implement, in low-cost CPE. The paper concludes that it is essential that the next generation of customer gateways and home networks, support appropriate QoS functionality to support new services delivered over BT's network.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>ICEbox: toward easy-to-use home networking</title>
	<abstract>Home networking is becoming an essential part of everyday life. However, empirical studies and consumer reports indicate that the complexities of configuring and maintaining the home network impose a high barrier for most householders. In this paper, we explore the sources of the complexity of the home network, and describe a solution we have built to address this complexity. We have developed a prototype network appliance that acts as a centralized point of control for the home network, providing device provisioning and reprovisioning, security, discovery, and monitoring. Our solution provides a simple physical UI for network control, using pointing to introduce new devices onto the network, and a physical lock to secure network access. Results of our user studies indicate that users found this appliance both useful and usable as a network configuration and management tool.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A longitudinal study of pressure sensing to infer real-world water usage events in the home</title>
	<abstract>We present the first longitudinal study of pressure sensing to infer real-world water usage events in the home (e.g., dishwasher, upstairs bathroom sink, downstairs toilet). In order to study the pressure-based approach out in the wild, we deployed a ground truth sensor network for five weeks in three homes and two apartments that directly monitored valve-level water usage by fixtures and appliances. We use this data to, first, demonstrate the practical challenges in constructing water usage activity inference algorithms and, second, to inform the design of a new probabilistic-based classification approach. Inspired by algorithms in speech recognition, our novel Bayesian approach incorporates template matching, a language model, grammar, and prior probabilities. We show that with a single pressure sensor, our probabilistic algorithm can classify real-world water usage at the fixture level with 90% accuracy and at the fixture-category level with 96% accuracy. With two pressure sensors, these accuracies increase to 94% and 98%. Finally, we show how our new approach can be trained with fewer examples than a strict template-matching approach alone.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Prioritization and operations NPD mix in a network with strategic partners under uncertainty</title>
	<abstract>In response to dynamic and individual customer needs and increasing complexity in product design along with rapidly changing technologies, new product development (NPD) is always a focus to companies. NPD process contains a tremendous degree of complexity and uncertainty, and multiple NPD are usually selected to increase the possibility of having more successful products. This, however, leads to an even more complicated problem of designing an appropriate portfolio of NPD in a program. Additionally, since a firm is difficult to survive independently in industry, flexible strategies to collaborate with or to compete with firms when selecting and operating NPD inside the network are essential. This topic, which has not been discussed a lot, is becoming more and more important in the near future. This paper first discusses the critical success factors (CSF) of NPD based on technological conglomerate networks (TCN), and then simplifies 37 CSFs into three criteria, 10 sub-criteria and their detailed judgment factors by factor analysis. Analytic hierarchy process (AHP) is employed to prioritize the relative importance of multiple evaluation criteria and the preferences of NPD mixes by generalizing experts' opinions. To compensate the uncertainty and vagueness in the environment and human judgment, a fuzzy AHP model considering of confidence level (@a) and risk index (@b) is proposed. A home appliances firm in China is used as an example, and the results show that the firm can lead to a more operational excellence in NPD mix practice and core competence by applying the proposed model.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A pluggable service-to-service communication mechanism for home multimedia networks</title>
	<abstract>This paper proposes a pluggable service-to-service (S2S) communication mechanism in a middleware for home networks, called Virtual Networked Appliance (VNA) architecture. In the architecture, service description method and the plug-gable S2S communication mechanism are separated in an orthogonal way. Through the separation, VNA architecture solved problems of home networks on which users have to operate multiple heterogeneous middleware technologies simultaneously: middleware fragmentation problem, due to complexity of realizing heterogeneous services on one middle-ware technology: aspect realization violation problem. The pluggable S2S communication mechanism provides service programmers with a simple aspect representation method to define a service-specific protocol concern apart from the service's implementation. It also provides off-the-shelf protocol modules of such well-known communication protocols as RTP, RTSP, HTTP, and SMTP for an inter-service communication, and dynamically loads them based on the aspects defined by the programmer. This reduces the complexity of implementing heterogeneous services on the VNA architecture, thereby addressing the problems. In this paper, we first clarify the two problems. Then, we describe the proposed mechanism with an overview of the middleware architecture referring to a composite service: "Follow-You-and-Me Video."</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
    
  
  <item>
    <title>Home energy saving through a user profiling system based on wireless sensors</title>
	<abstract>The high energy required by home appliances (like white goods, audio/video devices and communication equipments) and air conditioning systems (heating and cooling), makes our homes one of the most critical areas for the impact of energy consumption on natural environment. In this paper we present a work in progress within the European project AIM for the design of a system that can minimize energy waste in home environments efficiently managing devices operation modes. In our architecture we use a wireless sensor network to monitor physical parameters (like light and temperature) as well as the presence of users at home and in each of its rooms. With gathered data our system creates profiles of the behavior of house inhabitants and through a prediction algorithm is able to automatically set system parameters in order to optimize energy consumption and cost while guaranteeing the required comfort level. When users change their habits due to unpredictable events, the system is able to detect wrong predictions analyzing in real time information from sensors and to modify system behavior accordingly. By the automatic control of energy management system it is possible to avoid complex manual settings of system parameters that would prevent the introduction of home automation systems for energy saving into the mass market.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A novel agent-based concept of household appliances</title>
	<abstract>Mass production, such as white goods manufacturing, is traditionally bound to hierarchical factory-floor procedures and accepts only gradual changes in technology and product architecture. This paper introduces an idea on how to upgrade from classical to network-connected reconfigurable devices. A generic multi-agent architecture was created, derived from belief-desire-and-intention (BDI) agents. It covers all types of white goods in the form of rational home assistant, and enables reconfiguration of agent-based household appliances during the design, production and implementation phases. The introduced concept involves a multi-agent architecture which utilizes distributed processing power at different levels: higher-level agents run on more powerful devices than embedded appliance's controllers, personal assistant (PDAs), or Windows or Linux based personal computers (PCs). PDAs can run a single agent, for example a GUI agent, whereas the embedded controllers execute lower-level device (embedded) agents. In this way, all the appliance's basic functionality, such as its hardware units (e.g., electrical motors, valves, heaters, etc.), are initially simulated by auxiliary agents running together with higher-level agents on a PC or PDA. Using this simulator in the design phase, all vital functions and capabilities of the agent-based appliance under development are thoroughly tested first. Afterwards, the agents that simulate the device's hardware units and environment are simply replaced by the communication to the corresponding device units. In such way, any new functionality or device's behaviour can be upgraded any time just by adapting the core of the multi-agent architecture on the PC and individual agents on the PDA or the embedded agents in appliances. A thorough design and implementation cycle of the proposed solution using two freeware development tools is also described, i.e., the Prometheus agent design methodology and the agent simulation/execution environment called Jadex. The approach is exemplified by building a simulator of an agent-based household appliance, namely a Multi-agent Washing Assistant as a special instance of rational home assistant.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A context-aware multi-model remote controller for electronic home devices</title>
	<abstract>In the past few years, household remote control products have continuously emerged, and more equipment could be controlled at remote distance. With continuous growth in the number of home remote control devices, the number of remote controllers at home is increasing. In addition, with the increase of PC and notebook computer demands, technologies for controlling computer devices or digital home appliances within the home networks have been launched, such as UPnP (Universal Plug and Play), OSGI (Open Service Gateway Initiative), HAVI (Home Audio/Video Interoperability), and Jini. Because there are too many standards of home network, it is difficult to define a universal standard conformed to others. Many researches have focused on linking the various protocols of these devices, but most of them used the Web or PDAs to control the devices. In such situation, users must rely on computer equipment and face the battery problem of handheld devices.

This study aims to develop a cross-heterogeneous network remote control system. In order to relieve the problem of excessive remote controllers, we built a context-aware multi-model remote controller for electronic home devices to relieve the user of a nuisance of enormous amount of remote controllers.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>iSCSI protocol parameter optimization for mobile appliance remote storage system at smart home environment approach</title>
	<abstract>In Mobile appliance, users have a limited amount of storage availability to them due to their limited size and weight. To relieve problem we developed iSCSI remote storage system, which is an excellent solution for smart home automation too. User can store or access their valuable data to the home server from anywhere, anytime and also get facility to use mass storage space. The iSCSI protocol has emerged as a transport for carrying SCSI block-level access protocol over the ubiquitous TCP protocol. It enables a client's block-level access to the data on remote storage over an existing IP infrastructure. However, the performance of the iSCSI based remote storage system for mobile appliances were sharply dropped in wireless networks; especially when we adapt default parameters value suggested in standard for our remote storage system in wireless networks. This paper focuses our experiments, which are performed to investigate the best performance values of iSCSI parameters for iSCSI-based remote storage system, are taken out in CDMA networks in order to realize the access to a remote storage system anytime and anywhere. And after the experiment, we suggest the optimal value of parameters. The experiment results from several test cases show us the best values are not the default values specified in the iSCSI standard.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An intelligent agent for RFID-Based home network system</title>
	<abstract>An intelligent agent which is a software component for the efficient home network system is proposed in this paper. The agent consists of six modules such as the Agent Manager, the Data Collector, the Execution Controller, the Data Storage, the Data Queue and the User Interface. The Agent Manager manages the tasks of modules, and the Data Collector collects the data from home appliances through the RFID readers. The Execution Controller determines the operations of home appliances according to the conditions of the home environment and transfers the operations to the appliances through the RFID readers. Moreover, the Data Storage keeps the data which is necessary for the operations of the agent, and the Data Queue temporarily stores the data which is collected from home appliances. Lastly, the User Interface provides the graphical user interface in which an individual can directly control and monitor the home network. The proposed intelligent agent autonomously learns the circumstances of a home network by analyzing the data about the state of home appliances, and provides the best suited environment to the user. Therefore, the user can live in an optimal home environment without effort if he/she performs home networking through the agent.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
    
  
  <item>
    <title>Component-Based development of web-enabled ehome services</title>
	<abstract>The scope of Web-enabled eHome services covers both automated homes and automated industry facilities. Web-enabled eHome services provide a fully integrated view onto distributed systems comprising automated homes, back-end systems of providers, communication protocols, and services which make distribution aspects transparent. Future platforms should make the development and deployment as easy as achievable. Access should be possible by the way of all communication devices (e.g. desktop computers, PDAs, mobile phones) and all communication networks. Also, all appliances, ubiquitous devices, and their networking protocols have to be supported.

Generations of Web-enabled eHome services have been developed based on proprietary hard- and software. Today, an extensible and modular platform is required for forward-looking design and implementation of such services. One of the main requirements is, that the developed system is maintenance-free and the system brings itself in an operable condition. For setup tasks, both the end-user and a remote operator should be able to execute necessary steps.

It can be observed that services build up hierarchies. We propose a 3-layer system structure, which can be taken to account in system design. Software components grouped by service layers can then be realized in order to implement concrete services. Based on the OSGi platform, we have developed sample services. Gained experience is used for verification of our assumptions. Summarizing, we propose a cookbook for convenient development and deployment of services of the described nature.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Flexible authorization in home network environments</title>
	<abstract>In home network environments, OSGi platform plays a major role as the service gateway to access into home appliances. It is important to provide appropriate services as well as security mechanisms to protect confidential or sensitive information and devices. Authorization is especially important when controlling the access of different users. OSGi platform supports the role based access control but it does not support various facilities in the RBAC model. To address such shortcomings, several works have proposed the enhanced access control mechanisms for the OSGi service platform. However, these are still limited to applying the traditional RBAC conventions to OSGi platform. This paper extends the existing authorization mechanism of OSGi platform to address its limitations for dynamic deployments. By adding the relative role concept and activating the access control using the delegation model, proposed mechanism enables a diverse and outstanding access control. We implement the proposed mechanism using aspectJ and illustrate how to develop a bundle including access control logic.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Proposal for an automatic cleaning robot operating in conjunction with the usage status of electrical appliances</title>
	<abstract>In this paper, we propose a human-friendly cleaning robot system for the domestic ubiquitous environment.

Though conventional automatic cleaning robots already exist, these robots do not work in sync with humans. These robots' cleaning operations often obstruct a person in the home. Therefore, a robot that does not interfere with human activity in the home is required.

What indicates human activity most in the home is the usage status of home appliances such as electrical appliances, doors, and lights. In our system, a robot can identify where humans are active by receiving the usage status of home appliances via a network. The usage status of home appliances changes the robot's running pattern and consequently the robot avoids humans and cleans without causing obstruction.

We conduct an operational experiment of our prototype system in a room in which some sensors are installed to create an experimental environment. We then measure the execution time of the prototype system.

From the results of experiments, it is found that a cleaning robot system which works through interaction with equipment in the home and does not disturb humans can become a reality.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Scriptable sensor network based home-automation</title>
	<abstract>Today, proprietary home automation targets very specific applications which operate mostly on a cable based infrastructure. In contrast to that, our implementation builds a wireless ad-hoc multi-hop network based on the ESB sensor node platform from the FU-Berlin.

The nodes gather sensor readings in a home and transmit them to a central automation server. There, the readings are matched against a list of script statements. In case of a match, a specific action is performed. In this work we will show how the user can implement complex home automation applications optimized for his specific needs by defining very simple script statements. An important property of the system is also that the control of all home appliances is done by means of IR communication and Ethernet enabled multiple plugs. This way, the cooperation between manufacturers is no necessity in order to connect devices to the home automation network.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Soft-appliances: A vision for user created networked appliances in digital homes</title>
	<abstract>In this paper we introduce a vision for a new type of domestic appliance, a soft-appliance, constructed from aggregations of elementary network services. The vision is based on the possibility of ‘deconstructing’, logically, conventional home appliances such as TVs into their elemental functions which may then be combined in novel ways with other deconstructed services to generate soft-appliance of a person's own choosing. Additionally our aim is to describe the computer science challenges involved in fulfilling this vision. An essential component of this vision is a concept called a MAp (meta-appliance/application); a semantic data template that describes the soft or virtual-appliance that can be instantiated by manufacturers and end-users in a way that redefines the nature of an appliance and which can be created, owned and traded. We also present a socio-technical framework to motivate the discussion of this research agenda, especially the use of the agent technology that would be needed to realise this vision.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
    
  
  <item>
    <title>An extensible application platform for heterogeneous smart home appliances and mobile devices</title>
	<abstract>Nowadays, various kinds of smart home appliances are widely deployed in our daily life. These embedded, networked, and programmable appliances could be accessed through various methods, but may not be integrated together due to the different kinds of home network protocols on which these appliances are deployed. On the other hand, with the penetration of mobile devices, people could leverage these appliances by fingers from anywhere around the world. However, the mobile accesses to these appliances are usually proprietary and require complicated development efforts. The paper aims at developing an application platform prototype named HomeBox for home appliances and mobile devices. The platform provides connectivity for heterogeneous smart home appliances, hosts an application runtime environment for smart home applications, and provides user interface transparency for mobile-devices to reduce the development efforts. The prototype has been explored in various applications to manage smart home appliances from mobile devices and demonstrate the extensibility for future devices and architectures.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Functional architecture of mobile gateway and home server for virtual home services</title>
	<abstract>With the progress of portable appliances such as cell phone and handheld PC, it can be foreseen the popularization of Personal Area Network (PAN) and the diversification of services that are based on Personal Mobile Gateway (PMG). Although inter-operability among home appliances reached a service stage, researches about virtual home services and middlewares for a new small scale network such as PAN are at an early stage. Network service/connection methods, terminal control schemes, and middlewares which are used in the traditional home networks must be enhanced to accommodate PMG-based PAN. In this paper, we propose an integrated virtual home network platform that guarantees seamless connections between home network and PAN. We also analyze the proposed indispensable functions and presents functions that should be added to the existent home gateway or the home server.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Automating energy management in green homes</title>
	<abstract>Homes powered fully or partially by renewable sources such as solar are becoming more widely adopted, however energy management strategies in these environments are lacking. This paper presents the first results of a study that explores home automation techniques for achieving better utilization of energy generated by renewable technologies. First, using a network of off-the-shelf sensing devices, we observe that energy generation and consumption in an off-grid home is both variable and predictable. Moreover, we find that reactive energy management techniques are insufficient to prevent critical battery situations. We then present a recommendation based system for helping users to achieve better utilization of resources. Our study demonstrates the feasibility of three recommendation components: an early warning system that allows users of renewable technologies to make more conservative decisions when energy harvested is predicted to be low; a task rescheduling system that advises users when high-power appliances such as clothes dryers should be run to optimize overall energy utilization; and an energy conservation system that identifies sources of energy waste and recommends more conservative usage.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>NAT traversing solutions for SIP applications</title>
	<abstract>Session Initiation Protocol (SIP) has been proposed for multimedia services and wide-area connectivity in smart home environments (SHEs). An important issue for SIP deployment in SHEs is network address translator (NAT) traversing. SIP and Real-time Transport Protocol (RTP) packets are delivered between an SHE (i.e., private IP network) and Internet (i.e., a public IP network) through an NAT function of a home gateway, and the NAT translates the IP/transport layer address and port number but leaves the application layer content unchanged. This results in inconsistency between the IP addresses/port numbers in the IP/transport layers and those in the SIP layer. To resolve this issue, we describe six solutions including static route, UPnP, STUN, ICE, ALG, and SBC. Then we compare these solutions in terms of smart home appliance (SHA) modification, scope of NATs supported, multilayer NAT traversal, ease of configuration, security issue, and time complexities.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A scenario-based user-oriented integrated architecture for supporting interoperability among heterogeneous home network middlewares</title>
	<abstract>There exist many home network middlewares such as Havi, Jini, LonWorks, UPnP, and SLP for the purpose of the information appliance control. As home networks evolve, new middlewares specialized for diverse information appliances will appear continuously. In this paper, we examine an integrated architecture for supporting interoperability among heterogeneous home network middlewares and present a scenario-based user-oriented integrated architecture for home automation, which controls and interoperates information appliances by integrating heterogeneous home network middlewares with an ability of reflecting flexible properties of home network middlewares.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
    
  
  <item>
    <title>NetBem: business equipment energy monitoring through network auditing</title>
	<abstract>Modern office buildings are fully equipped and furnished spaces with arrangements including networked business equipment, such as PC-class machines, copiers, wireless routers and fax machines, and other electrical equipment such as home appliances e.g. coffee machines, and appliances for environmental comfort e.g. electric heaters. The unique characteristics of networked business equipment are well-defined usage pattern, low-power current draw, and connectivity to the local area network (LAN). Business equipment is generally used over working hours adding up to important costs, motivating the need for a system capable of tracking equipment usage and associated energy expenditure, as well as identifying cost saving opportunities. Techniques for monitoring power loads are generally based on power step edge detection, and cannot be applied to business equipment due to the low power consumption of individual devices. This paper presents NetBem, a novel energy monitoring technique ad hoc to office buildings, capturing the contribution of networked business equipment to a power load via side-band detection of the equipment's operating state through the LAN. The technique is presented, and results from experiments within the School of Computer Science and Informatics at University College Dublin in Ireland are given.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Reliable communication methods for mutual complementary networks</title>
	<abstract>In the future home network, information appliances will be controlled and managed by the use of PLC (Power Line Communication) that enables the No New Wire and ZigBee concepts to be implemented in a ubiquitous sensor network. However, the arrival rate of PLC and the communication quality deterioration of ZigBee are significant problems because the control information for the appliances has to be transmitted reliably. To solve these problems, we examined communication methods that increase the arrival rate in a mutual complementary network environment. These methods improve reliability by mutually complementing, through PLC and ZigBee, the places where nodes can't communicate through only one interface. A comparison of these methods through ns-2 simulations shows that the Table Creation method is more reliable than the other methods we examined.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Development of a Residential Gateway and a Service Server for Home Automation</title>
	<abstract>We have developed a residential gateway that centralizes device interfacing between the external Internet and internal devices as well as appliance networks. We have also proposed the concept of a service server to be implemented for a large scale of apartment complex or a wide area house. The primary components of the residential gateway to be implemented in this paper include a processor(Motorola MPC8240), persistent storage(flash RAM, extend storage device), networking modules (such as TCP/IP for Ethernet, ADSL), home networking(HomePNA, IEEE1394, PLC), device interfaces(serial or PCI), home automation, and telecommunication system (PSTN/SLT, VoIP, Video Communication), which are typically powered by a certain RTOS. Finally, we have test results validating the effectiveness of both the residential gateway and the service server.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A multi-dimensional model for task representation and allocation in intelligent environments</title>
	<abstract>In the future, homes will have numerous intelligent communicating devices, and the user would like to configure and coordinate their actions. Appliances and people in intelligent environments will have some degree of mobility. If the user wants to go from one place to another, using the same community, the agent should be able to generalize the service, trying to build an equivalent collection of coordinating services. This ‘work in progress’ paper addresses this issue by proposing a multi-dimensional model that allows visualistation of devices, temporal relationships, mutual interdependencies and the environment dynamics. The model both offers a simplified means of visualising the task space and the interdependencies together with a means of reasoning about algorithmic solutions to task processing. The work is aimed at supporting research into Pervasive Home Environment Networks (PHEN) which is funded by the UK’s Department of Trade and Industry Next Wave Technologies and Markets programme.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A zeroconf approach to secure and easy-to-use remote access to networked appliances</title>
	<abstract>In this paper, we propose a new approach to secure and easy-to-use remote access to networked appliances (NAs). Based on the proposed approach, we develop a system in which servers in an Internet service provider network and a residential gateway (RGW) in a home network play an important role, including secure communication. The system allows us to access NAs, using mobile phone and PC anytime and anywhere. Our emphasis is that the system is based on zeroconf protocols including UPnP, and only by connecting an NA to a RGW enables secure remote access. Another emphasis is deployability. A RGW that has been widely deployed in home networks and always operates for VoIP call serves the zeroconf protocols. Interoperability with both UPnP compliant and noncompliant NAs, together available in the marketplace, is also considered. We deployed the system and conducted a field trial in which approximately 1700 users took part in for four months. Empirical results in terms of the zeroconf protocols will be shown with future directions.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
    
  
  <item>
    <title>UBI-AMI: real-time metering of energy consumption at homes using multi-hop IP-based wireless sensor networks</title>
	<abstract>We present a system for real-time metering of aggregate power consumption and appliance level loads in homes. The power consumption data is collected by sensors in a multi-hop IP-based wireless sensor network. The data is stored at a central server which prepares various representations of the data to be viewed with a web browser. The findings of a longitudinal user evaluation of our system by seven households testify for a successful design and implementation.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Control of information appliances using instant messaging</title>
	<abstract>Many of the systems developed to date for controlling home appliances remotely from outside have used a web server as the interface between the home network and the users. Although the user interface (UI) that uses a web server offers many advantages, this approach requires a fixed IP (Internet Protocol) address and does not have a push function. Moreover, until the user gets reconnected to the web server, he or she cannot know the result of a submitted order. In particular, internet-based applications using web browsers still require considerable time to execute the initialization process for acquiring state information on home appliances from the home server. To solve these problems, we propose an application for home automation that can efficiently control and monitor home appliances using an instant messaging system (IMS) with real time communication. The proposed system has not only the advantages of the web server method, but also with additional advantage that the homeowner does not need to continually reconnect to the home server, and that the home server does not need to have a fixed IP address. Our application can be applied using middleware technologies</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Domotic technologies incompatibility becomes user transparent</title>
	<abstract>Introduction

The potential of current technologies in smart automation has been largely unexploited. Pervasive computing vision is still far from being achieved, especially with regard to Domotics and home applications. In fact, even though many implementations have started to appear in several contexts, few applications have been made available for the home environment and the general public. This is mainly due to the segmentation of standards and proprietary solutions, which are currently confusing the market with a sparse offer of uninteroperable devices and systems.

Although modern houses are equipped with smart technological appliances, still very few of these appliances can be seamlessly connected to each other.

Moreover, inter-working capabilities are required beyond house boundaries, towards external services and towards other houses as nodes of a global network.

Therefore, the main goal of this research is to find solutions to the problem of interoperability that will be in line with open and widely recognized standards.

The result is a computing framework based on open communication standards, capable of abstracting the peculiarities of underlying heterogeneous technologies, and letting them co-exist and interwork, without eliminating their differences. Interoperability can thus be made potentially feasible between any domotic technology, both currently existing, and still to be defined.

Currently, domotic technology vendors concentrate on building closed relationships with their customers, and leveraging their economic investments by establishing barriers against new manufacturers entering the market.

Examples of current domotic protocols are X10, Konnex, LonWorks, UPnP, HAVi, and Jini supporting various communication standards (Ethernet, FireWire, Bluetooth, ZigBee, IrDA and proprietary buses). We believe that no domotic technology currently has the potential to actually play a leading role. Within this wide and heterogeneous framework, the market logic is to tie consumers to a particular domotic protocol, which then forces them to only purchase conforming devices in order to keep a consistent level of interoperability.

In recent years several interesting and innovative solutions have emerged, with a reasonable level of scalability and dependability, providing interoperability among heterogeneous home systems.

Twente University has proposed a solution that aims at supporting heterogeneous technologies (including legacy ones) with a "cluster cultures" approach. The architecture outlines a "touch and play" system which, at device registration time, enables a zero-configuration environment for the exchange of credentials among its gateways and to register device services in a hierarchical structure. The architecture provides a high level of security by using cryptographic algorithms.

Waseda University have proposed a framework designed to easily enable the integration of legacy middleware and legacy services and clients, with a predefined path for the inclusion of new, future, middleware. This is accomplished mainly through the use of a Virtual Service Gateway. This connects one piece of middleware to another by exploiting a Protocol Conversion Manager, whose task is to convert the different middleware protocols into the specific internal protocol used by the Virtual Service Gateway. Information about the location and functions of services is provided by a Virtual Service Repository.

Another interesting project is the "Domotic House Gateway." It implements an event-based mechanism which is used to exchange messages between the single device and the system. These events are internally converted into logical events so as to clearly separate the actual physical issues from the semantics that goes beyond the devices and their role within the house. One level of the architecture implements a rule-based core that can be dynamically adapted either by the system itself or manually through external interfaces. Each device needs a device driver, which is responsible for translating its low level or hardware states and activities into events that can be managed by the system.

Another promising approach, in line with our research, is proposed by the Open Building Information Exchange group who are working to create standard XML and Web Services guidelines to facilitate information exchange among mechanical and electrical systems in building automation.

One such important European project in this context is Amigo. This project was aimed at Ambient Intelligence features for the networked home environment and the usability of the system was among its main goals and included three major guidelines: user-friendly interfaces, interoperability, and automatic discovery of devices and services.

All these projects resolved the interoperability problem with several approaches, all of which are different from what we consider, in our vision, as the optimal solutions.

Lastly, we enlist a prototype previously created by our research laboratory. This solution had the limitation of abstracting each device typology with a Web service implementing their specific functionalities. The implementation of a new ad hoc Web service was needed whenever a new category of device needed to be included in the network. In addition, this prototype solved the problem of cooperation by virtualizing devices belonging to each domotic system onto the others. This solution, however, had a drawback: the same device appeared virtually replicated on every single domotic system, thus creating data replications and possible consistency problems.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Context-Aware home network environment on grid (CAHE-G)</title>
	<abstract>In this paper, we present context-aware home network environment on grid that controls the room appliances providing user mobility. The current home network technology (such as Havi, Jini and UPnP) is limited to a static local area. However, we propose ubiquitous concepts of home network system of user mobility in context-aware computing. It is called as CAHE-G which is not limited to static local area, but provides ubiquitous concepts of home network. CAHE-G controls home appliances according to context information, and is divided into two parts: Agent layer and Service layer. The Service layer consists of Lookup Service (LUS), Home Controller, Grid Service Provider (GridSP). The Service layer provides home and grid services with user request. LUS provides searching and registering those services. The Agent layer consists of Grid Agent (GA) and Home Context Agent (HCA). Grid technology can be used to automatically execute the home network service depending on context information. Agent layer provides user ubiquity, autonomous execution and large computation in this system. GA consists of Execution Manager (EM), MDS Manager (MM), and File Manager (FM) in order to enable those functionalities. Also, HCA provides autonomous adaptability to user. It gathers and interprets context information, then assigns the information to GA. As a result, in CAHE-G, Grid computing provides the capabilities: autonomous execution, interoperability, and the standard approaches to, and mechanisms for basic problems. Moreover, context-aware characteristic gives merits such as user mobility and adaptability.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A network architecture for the Web of Things</title>
	<abstract>The "Web of Things" is emerging as an exciting vision for seamlessly integrating everyday objects like home appliances, digital picture frames, health monitoring devices and energy meters into the Internet using the Web's well-known standards and blueprints. The key idea is to represent resources on these devices as URIs and use HTTP verbs (GET, PUT, POST, DELETE) as the uniform interface to manipulate them. Unfortunately, practical considerations such as bandwidth or energy constraints, firewalls/NATs and mobility pose interesting challenges in the realization of this ideal vision. This paper describes these challenges, identifies some potential solutions and presents the design and implementation of a gateway-based network architecture to address these concerns. To the best of our knowledge, it represents the first attempt within the Web of Things community to tackle these issues in a comprehensive manner.</abstract>
	<search_task_number>11</search_task_number>
	<query>network of appliances in homes </query>
	<relevance>1</relevance>
  </item>



  <item>
    <title>Does a one-size recommendation system fit all? the effectiveness of collaborative filtering based recommendation systems across different domains and search modes</title>
	<abstract>Collaborative filtering (CF) is a personalization technology that generates recommendations for users based on others' evaluations. CF is used by numerous e-commerce Web sites for providing personalized recommendations. Although much research has focused on refining collaborative filtering algorithms, little is known about the effects of user and domain characteristics on the accuracy of collaborative filtering systems. In this study, the effects of two factors—product domain and users' search mode—on the accuracy of CF are investigated. The effects of those factors are tested using data collected from two experiments in two different product domains, and from two large CF datasets, EachMovie and Book-Crossing. The study shows that the search mode of the users strongly influences the accuracy of the recommendations. CF works better when users look for specific information than when they search for general information. The accuracy drops significantly when data from different modes are mixed. The study also shows that CF is more accurate for knowledge domains than for consumer product domains. The results of this study imply that for more accurate recommendations, collaborative filtering systems should be able to identify and handle users' mode of search, even within the same domain and user group.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>PipeCF: a scalable DHT-based collaborative filtering recommendation system</title>
	<abstract>Collaborative Filtering (CF) technique has proved to be one of the most successful techniques in recommendation systems in recent years. However, traditional centralized CF system has suffered from its shortage in scalability as their calculation complexity increases quickly both in time and space when the record in user database increases. In this paper, we propose a decentralized CF algorithm, called PipeCF, based on distributed hash table (DHT) method. We also propose two novel approaches to improve the scalability and prediction accuracy of DHT-based CF algorithm. The experimental data show that our DHT-based CF system has better prediction accuracy, efficiency and scalability than traditional CF systems.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Collaborative filtering in social tagging systems based on joint item-tag recommendations</title>
	<abstract>Tapping into the wisdom of the crowd, social tagging can be considered an alternative mechanism - as opposed to Web search - for organizing and discovering information on the Web. Effective tag-based recommendation of information items, such as Web resources, is a critical aspect of this social information discovery mechanism. A precise understanding of the information structure of social tagging systems lies at the core of an effective tag-based recommendation method. While most of the existing research either implicitly or explicitly assumes a simple tripartite graph structure for this purpose, we propose a comprehensive information structure to capture all types of co-occurrence information in the tagging data. Based on the proposed information structure, we further propose a unified user profiling scheme to make full use of all available information. Finally, supported by our proposed user profile, we propose a novel framework for collaborative filtering in social tagging systems. In our proposed framework, we first generate joint item-tag recommendations, with tags indicating topical interests of users in target items. These joint recommendations are then refined by the wisdom from the crowd and projected to the item space for final item recommendations. Evaluation using three real-world datasets shows that our proposed recommendation approach significantly outperformed state-of-the-art approaches.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Analysis of social voting patterns on digg</title>
	<abstract>The social Web is transforming the way information is created and distributed. Blog authoring tools enable users to publish content, while sites such as Digg and Del.icio.us are used to distribute content to a wider audience. With content fast becoming a commodity, interest in using social networks to promote and find content has grown, both on the side of content producers (viral marketing) and consumers (recommendation). Here we study the role of social networks in promoting content on Digg, a social news aggregator that allows users to submit links to and vote on news stories. Digg's goal is to feature the most interesting stories on its front page, and it aggregates opinions of its many users to identify them. Like other social networking sites, Digg allows users to designate other users as "friends" and see what stories they found interesting. We studied the spread of interest in news stories submitted to Digg in June 2006. Our results suggest that pattern of the spread of interest in a story on the network is indicative of how popular the story will become. Stories that spread mainly outside of the submitter's neighborhood go on to be very popular, while stories that spread mainly through submitter's social neighborhood prove not to be very popular. This effect is visible already in the early stages of voting, and one can make a prediction about the potential audience of a story simply by analyzing where the initial votes come from.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A collaborative filtering algorithm and evaluation metric that accurately model the user experience</title>
	<abstract>Collaborative Filtering (CF) systems have been researched for over a decade as a tool to deal with information overload. At the heart of these systems are the algorithms which generate the predictions and recommendations.In this article we empirically demonstrate that two of the most acclaimed CF recommendation algorithms have flaws that result in a dramatically unacceptable user experience.In response, we introduce a new Belief Distribution Algorithm that overcomes these flaws and provides substantially richer user modeling. The Belief Distribution Algorithm retains the qualities of nearest-neighbor algorithms which have performed well in the past, yet produces predictions of belief distributions across rating values rather than a point rating value.In addition, we illustrate how the exclusive use of the mean absolute error metric has concealed these flaws for so long, and we propose the use of a modified Precision metric for more accurately evaluating the user experience.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A multi-stage collaborative filtering approach for mobile recommendation</title>
	<abstract>Location-based and personalized services are the key factors for promoting user satisfaction. However, most service providers did not consider the needs of mobile user in terms of their location and event-participation. Consequently, the service provider may lose the chance for better service and profit. In this paper, we present a Multi-stage Collaborative Filtering (MSCF) process to provide event recommendation based on mobile user's location. To achieve this purpose, the Collaborative Filtering (CF) technique is employed and the Adaptive Resonance Theory (ART) network is applied to cluster mobile users according to their personal profile. Sequential pattern mining is, then, used to discover the correlations between events for recommendation. The MSCF is designed not only to recommend for the old registered mobile user (ORMU), but also to handle the cold-start problem for new registered mobile user (NRMU). This research is designed to achieve the followings.

(1) To present a personalized event recommendation system for mobile users.

(2) To discover mobile users' moving patterns.

(3) To provide recommendations based on mobile users' preferences.

(4) To overcome the cold-start problem for new registered mobile user.

The experimental results of this research show that the MSCF is able to accomplish the above purposes and shows better outcome for cold-start problem when comparing with user-based CF and item-based CF.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>MobHinter: epidemic collaborative filtering and self-organization in mobile ad-hoc networks</title>
	<abstract>We focus on collaborative filtering dealing with self-organizing communities, host mobility, wireless access, and ad-hoc communications. In such a domain, knowledge representation and users profiling can be hard; remote servers can be often unreachable due to client mobility; and feedback ratings collected during random connections to other users' ad-hoc devices can be useless, because of natural differences between human beings. Our approach is based on so called Affinity Networks, and on a novel system, called MobHinter, that epidemically spreads recommendations through spontaneous similarities between users. Main results of our study are two fold: firstly, we show how to reach comparable recommendation accuracies in the mobile domain as well as in a complete knowledge scenario; secondly, we propose epidemic collaborative strategies that can reduce rapidly and realistically the cold start problem.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Recommendation based on object typicality</title>
	<abstract>Current recommendation methods are mainly classified into content-based, collaborative filtering and hybrid methods. These methods are based on similarity measurements among items or users. In this paper, we investigate recommendation systems from a new perspective based on object typicality and propose a novel typicality-based recommendation approach. Experiments show that our method outperforms compared methods on recommendation quality.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>CARES: a ranking-oriented CADAL recommender system</title>
	<abstract>A recommender system is useful for a digital library to suggest the books that are likely preferred by a user. Most recommender systems using collaborative filtering approaches leverage the explicit user ratings to make personalized recommendations. However, many users are reluctant to provide explicit ratings, so ratings-oriented recommender systems do not work well. In this paper, we present a recommender system for CADAL digital library, namely CARES, which makes recommendations using a ranking-oriented collaborative filtering approach based on users' access logs, avoiding the problem of the lack of user ratings. Our approach employs mean AP correlation coefficients for computing similarities among users' implicit preference models and a random walk based algorithm for generating a book ranking personalized for the individual. Experimental results on real access logs from the CADAL web site show the effectiveness of our system and the impact of different values of parameters on the recommendation performance.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Rating aggregation in collaborative filtering systems</title>
	<abstract>Recommender systems based on user feedback rank items by aggregating users' ratings in order to select those that are ranked highest. Ratings are usually aggregated using a weighted arithmetic mean. However, the mean is quite sensitive to outliers and biases, and thus may not be the most informative aggregate. We compare the accuracy and robustness of three different aggregators: the mean, median and mode. The results show that the median may often be a better choice than the mean, and can significantly improve recommendation accuracy and robustness in collaborative filtering systems.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Design and evaluation of a command recommendation system for software applications</title>
	<abstract>We examine the use of modern recommender system technology to aid command awareness in complex software applications. We first describe our adaptation of traditional recommender system algorithms to meet the unique requirements presented by the domain of software commands. A user study showed that our item-based collaborative filtering algorithm generates 2.1 times as many good suggestions as existing techniques. Motivated by these positive results, we propose a design space framework and its associated algorithms to support both global and contextual recommendations. To evaluate the algorithms, we developed the CommunityCommands plug-in for AutoCAD. This plug-in enabled us to perform a 6-week user study of real-time, within-application command recommendations in actual working environments. We report and visualize command usage behaviors during the study, and discuss how the recommendations affected users behaviors. In particular, we found that the plug-in successfully exposed users to new commands, as unique commands issued significantly increased.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>PocketLens: Toward a personal recommender system</title>
	<abstract>Recommender systems using collaborative filtering are a popular technique for reducing information overload and finding products to purchase. One limitation of current recommenders is that they are not portable. They can only run on large computers connected to the Internet. A second limitation is that they require the user to trust the owner of the recommender with personal preference data. Personal recommenders hold the promise of delivering high quality recommendations on palmtop computers, even when disconnected from the Internet. Further, they can protect the user's privacy by storing personal information locally, or by sharing it in encrypted form. In this article we present the new PocketLens collaborative filtering algorithm along with five peer-to-peer architectures for finding neighbors. We evaluate the architectures and algorithms in a series of offline experiments. These experiments show that Pocketlens can run on connected servers, on usually connected workstations, or on occasionally connected portable devices, and produce recommendations that are as good as the best published algorithms to date.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Selectively acquiring ratings for product recommendation</title>
	<abstract>Accurate prediction of customer preferences on products is the key to any recommender systems to realize its promised strategic values such as improved customer satisfaction and therefore enhanced loyalty. In this paper, we propose proactively acquiring ratings from customers for a newly introduced product to quickly improve the accuracy of the predicted ratings generated by a collaborative filtering recommendation algorithm for the entire customer population. We formally introduce the problem of identifying the most informative ratings to acquire and termed it as the product rating acquisition problem. We proposed an active learning sampling method for this problem that is generic to any recommendation algorithms. Using the Netflix Prize dataset, we experimented with our proposed method, a uniform random sampling method, and a degree-based sampling method that is biased toward customers with large numbers of ratings for the user-based and item-based neighborhood recommendation algorithms. The experimental results showed that even with the random sampling method, acquiring 10% of all ratings in addition to a randomly selected 10% initial ratings achieved 4.5% improvement on overall rating prediction accuracy of the movie. In addition, our proposed active learning sampling method consistently outperformed the random and degree-based sampling for the better-performing item-based algorithm and achieved more than 8% improvement by acquiring 10% of the ratings.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Effective missing data prediction for collaborative filtering</title>
	<abstract>Memory-based collaborative filtering algorithms have been widely adopted in many popular recommender systems, although these approaches all suffer from data sparsity and poor prediction quality problems. Usually, the user-item matrix is quite sparse, which directly leads to inaccurate recommendations. This paper focuses the memory-based collaborative filtering problems on two crucial factors: (1) similarity computation between users or items and (2) missing data prediction algorithms. First, we use the enhanced Pearson Correlation Coefficient (PCC) algorithm by adding one parameter which overcomes the potential decrease of accuracy when computing the similarity of users or items. Second, we propose an effective missing data prediction algorithm, in which information of both users and items is taken into account. In this algorithm, we set the similarity threshold for users and items respectively, and the prediction algorithm will determine whether predicting the missing data or not. We also address how to predict the missing data by employing a combination of user and item information. Finally, empirical studies on dataset MovieLens have shown that our newly proposed method outperforms other state-of-the-art collaborative filtering algorithms and it is more robust against data sparsity.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>UTA-Rec: a recommender system based on multiple criteria analysis</title>
	<abstract>UTARec, a Recommender System that incorporates Multiple Criteria Analysis methodologies is presented. The system's performance and capability of addressing certain shortfalls of existing Recommender Systems is demonstrated in the case of movie recommendations. UTARec's accuracy is measured in terms of Kendall's tau and ROC curve analysis and is also compared to a Multiple Rating Collaborative Filtering (MRCF) approach. The results indicate that the proposed Multiple Criteria Analysis methodology can certainly improve the recommendation process by producing highly accurate results, from a user oriented perspective.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A social recommendation framework based on multi-scale continuous conditional random fields</title>
	<abstract>This paper addresses the issue of social recommendation based on collaborative filtering (CF) algorithms. Social recommendation emphasizes utilizing various attributes information and relations in social networks to assist recommender systems. Although recommendation techniques have obtained distinct developments over the decades, traditional CF algorithms still have these following two limitations: (1) relational dependency within predictions, an important factor especially when the data is sparse, is not being utilized effectively; and (2) straightforward methods for combining features like linear integration suffer from high computing complexity in learning the weights by enumerating the whole value space, making it difficult to combine various information into an unified approach. In this paper, we propose a novel model, Multi-scale Continuous Conditional Random Fields (MCCRF), as a framework to solve above problems for social recommendations. In MCCRF, relational dependency within predictions is modeled by the Markov property, thus predictions are generated simultaneously and can help each other. This strategy has never been employed previously. Besides, diverse information and relations in social network can be modeled by state and edge feature functions in MCCRF, whose weights can be optimized globally. Thus both problems can be solved under this framework. In addition, We propose to utilize Markov chain Monte Carlo (MCMC) estimation methods to solve the difficulties in training and inference processes of MCCRF. Experimental results conducted on two real world data have demonstrated that our approach outperforms traditional CF algorithms. Additional experiments also show the improvements from the two factors of relational dependency and feature combination, respectively.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A random walk method for alleviating the sparsity problem in collaborative filtering</title>
	<abstract>Collaborative Filtering is one of the most widely used approaches in recommendation systems which predicts user preferences by learning past user-item relationships. In recent years, item-oriented collaborative filtering methods came into prominence as they are more scalable compared to user-oriented methods. Item-oriented methods discover item-item relationships from the training data and use these relations to compute predictions. In this paper, we propose a novel item-oriented algorithm, Random Walk Recommender, that first infers transition probabilities between items based on their similarities and models finite length random walks on the item space to compute predictions. This method is especially useful when training data is less than plentiful, namely when typical similarity measures fail to capture actual relationships between items. Aside from the proposed prediction algorithm, the final transition probability matrix computed in one of the intermediate steps can be used as an item similarity matrix in typical item-oriented approaches. Thus, this paper suggests a method to enhance similarity matrices under sparse data as well. Experiments on MovieLens data show that Random Walk Recommender algorithm outperforms two other item-oriented methods in different sparsity levels while having the best performance difference in sparse datasets.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Preventing shilling attacks in online recommender systems</title>
	<abstract>Collaborative filtering techniques have been successfully employed in recommender systems in order to help users deal with information overload by making high quality personalized recommendations. However, such systems have been shown to be vulnerable to attacks in which malicious users with carefully chosen profiles are inserted into the system in order to push the predictions of some targeted items. In this paper we propose several metrics for analyzing rating patterns of malicious users and evaluate their potential for detecting such shilling attacks. Building upon these results, we propose and evaluate an algorithm for protecting recommender systems against shilling attacks. The algorithm can be employed for monitoring user ratings and removing shilling attacker profiles from the process of computing recommendations, thus maintaining the high quality of the recommendations.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Naïve filterbots for robust cold-start recommendations</title>
	<abstract>The goal of a recommender system is to suggest items of interest to a user based on historical behavior of a community of users. Given detailed enough history, item-based collaborative filtering (CF) often performs as well or better than almost any other recommendation method. However, in cold-start situations - where a user, an item, or the entire system is new - simple non-personalized recommendations often fare better. We improve the scalability and performance of a previous approach to handling cold-start situations that uses filterbots, or surrogate users that rate items based only on user or item attributes. We show that introducing a very small number of simple filterbots helps make CF algorithms more robust. In particular, adding just seven global filterbots improves both user-based and item-based CF in cold-start user, cold-start item, and cold-start system settings. Performance is better when data is scarce, performance is no worse when data is plentiful, and algorithm efficiency is negligibly affected. We systematically compare a non-personalized baseline, user-based CF, item-based CF, and our bot-augmented user- and item-based CF algorithms using three data sets (Yahoo! Movies, MovieLens, and EachMovie) with the normalized MAE metric in three types of cold-start situations. The advantage of our "naïve filterbot" approach is most pronounced for the Yahoo! data, the sparsest of the three data sets.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Exploiting contextual information in recommender systems</title>
	<abstract>Recommender Systems help an on-line user to tame information overload and are being used now in complex domains where it could be beneficial to exploit context-awareness, e.g., in travel recommendation. Technically, in Recommender Systems we can interpret context as a set of constraints or preferences over the usage of items determined by the contextual conditions (e.g., today it is raining or the user is in a particular location). In fact, there is a lack of approaches to deal effectively with contextual data. This thesis investigates some approaches to exploit context in Recommender Systems. It provides a general architecture of context-aware Recommender Systems and analyzes separate components of this model. The main focus is to investigate new approaches that can bring a real added value to users. In this paper I also describe my initial results on item selection and item weighting for context-dependent Collaborative Filtering (CF). Moreover, I shall present my ongoing research on CF hybridization using context.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Unified relevance models for rating prediction in collaborative filtering</title>
	<abstract>Collaborative filtering aims at predicting a user's interest for a given item based on a collection of user profiles. This article views collaborative filtering as a problem highly related to information retrieval, drawing an analogy between the concepts of users and items in recommender systems and queries and documents in text retrieval.

We present a probabilistic user-to-item relevance framework that introduces the concept of relevance into the related problem of collaborative filtering. Three different models are derived, namely, a user-based, an item-based, and a unified relevance model, and we estimate their rating predictions from three sources: the user's own ratings for different items, other users' ratings for the same item, and ratings from different but similar users for other but similar items.

To reduce the data sparsity encountered when estimating the probability density function of the relevance variable, we apply the nonparametric (data-driven) density estimation technique known as the Parzen-window method (or kernel-based density estimation). Using a Gaussian window function, the similarity between users and/or items would, however, be based on Euclidean distance. Because the collaborative filtering literature has reported improved prediction accuracy when using cosine similarity, we generalize the Parzen-window method by introducing a projection kernel.

Existing user-based and item-based approaches correspond to two simplified instantiations of our framework. User-based and item-based collaborative filterings represent only a partial view of the prediction problem, where the unified relevance model brings these partial views together under the same umbrella. Experimental results complement the theoretical insights with improved recommendation accuracy. The unified model is more robust to data sparsity because the different types of ratings are used in concert.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Explaining collaborative filtering recommendations</title>
	<abstract>Automated collaborative filtering (ACF) systems predict a person's affinity for items or information by connecting that person's recorded interests with the recorded interests of a community of people and sharing ratings between like-minded persons. However, current recommender systems are black boxes, providing no transparency into the working of the recommendation. Explanations provide that transparency, exposing the reasoning and data behind a recommendation. In this paper, we address explanation interfaces for ACF systems - how they should be implemented and why they should be implemented. To explore how, we present a model for explanations based on the user's conceptual model of the recommendation process. We then present experimental results demonstrating what components of an explanation are the most compelling. To address why, we present experimental evidence that shows that providing explanations can improve the acceptance of ACF systems. We also describe some initial explorations into measuring how explanations can improve the filtering performance of users.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Using a trust network to improve top-N recommendation</title>
	<abstract>Top-N item recommendation is one of the important tasks of recommenders. Collaborative filtering is the most popular approach to building recommender systems which can predict ratings for a given user and item. Collaborative filtering can be extended for top-N recommendation, but this approach does not work accurately for cold start users that have rated only a very small number of items. In this paper we propose novel methods exploiting a trust network to improve the quality of top-N recommendation. The first method performs a random walk on the trust network, considering the similarity of users in its termination condition. The second method combines the collaborative filtering and trust-based approach. Our experimental evaluation on the Epinions dataset demonstrates that approaches using a trust network clearly outperform the collaborative filtering approach in terms of recall, in particular for cold start users.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Is trust robust?: an analysis of trust-based recommendation</title>
	<abstract>Systems that adapt to input from users are susceptible to attacks from those same users. Recommender systems are common targets for such attacks since there are financial, political and many other motivations for influencing the promotion or demotion of recommendable items [2].Recent research has shown that incorporating trust and reputation models into the recommendation process can have a positive impact on the accuracy and robustness of recommendations. In this paper we examine the effect of using five different trust models in the recommendation process on the robustness of collaborative filtering in an attack situation. In our analysis we also consider the quality and accuracy of recommendations. Our results caution that including trust models in recommendation can either reduce or increase prediction shift for an attacked item depending on the model-building process used, while highlighting approaches that appear to be more robust.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A study of heterogeneity in recommendations for a social music service</title>
	<abstract>We present a preliminarily study on the influence of different sources of information in Web 2.0 systems on recommendation. Aiming to identify which are the sources of information (ratings, tags, social contacts, etc.) most valuable for recommendation, we evaluate a number of content-based, collaborative filtering and social recommenders on a heterogeneous dataset obtained from Last.fm. Moreover, aiming to investigate whether and how fusion of such information sources can benefit individual recommendation approaches, we propose various metrics to measure coverage, overlap, diversity and novelty between different sets of recommendations. The obtained results show that, in Last.fm, social tagging and explicit social networking information provide effective and heterogeneous item recommendations. Moreover, they give first insights on the feasibility of exploiting the above non performance recommendation characteristics by hybrid approaches.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>TiVo: making show recommendations using a distributed collaborative filtering architecture</title>
	<abstract>We describe the TiVo television show collaborative recommendation system which has been fielded in over one million TiVo clients for four years. Over this install base, TiVo currently has approximately 100 million ratings by users over approximately 30,000 distinct TV shows and movies. TiVo uses an item-item (show to show) form of collaborative filtering which obviates the need to keep any persistent memory of each user's viewing preferences at the TiVo server. Taking advantage of TiVo's client-server architecture has produced a novel collaborative filtering system in which the server does a minimum of work and most work is delegated to the numerous clients. Nevertheless, the server-side processing is also highly scalable and parallelizable. Although we have not performed formal empirical evaluations of its accuracy, internal studies have shown its recommendations to be useful even for multiple user households. TiVo's architecture also allows for throttling of the server so if more server-side resources become available, more correlations can be computed on the server allowing TiVo to make recommendations for niche audiences.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Music recommendation by unified hypergraph: combining social media information and music content</title>
	<abstract>Acoustic-based music recommender systems have received increasing interest in recent years. Due to the semantic gap between low level acoustic features and high level music concepts, many researchers have explored collaborative filtering techniques in music recommender systems. Traditional collaborative filtering music recommendation methods only focus on user rating information. However, there are various kinds of social media information, including different types of objects and relations among these objects, in music social communities such as Last.fm and Pandora. This information is valuable for music recommendation. However, there are two challenges to exploit this rich social media information: (a) There are many different types of objects and relations in music social communities, which makes it difficult to develop a unified framework taking into account all objects and relations. (b) In these communities, some relations are much more sophisticated than pairwise relation, and thus cannot be simply modeled by a graph. In this paper, we propose a novel music recommendation algorithm by using both multiple kinds of social media information and music acoustic-based content. Instead of graph, we use hypergraph to model the various objects and relations, and consider music recommendation as a ranking problem on this hypergraph. While an edge of an ordinary graph connects only two objects, a hyperedge represents a set of objects. In this way, hypergraph can be naturally used to model high-order relations. Experiments on a data set collected from the music social community Last.fm have demonstrated the effectiveness of our proposed algorithm.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Reasonable tag-based collaborative filtering for social tagging systems</title>
	<abstract>In this paper, we present a tag-based collaborative filtering recommendation method for use with recently popular online social tagging systems. Combining the information provided by tagging systems with the effective recommendation abilities given by collaborative filtering, we provide a website recommendation system which provides relevant, credible recommendations that match the user's changing interests as well as the user's bookmarking profile. Based upon user testing, our system provides a higher level of relevant recommendations over other commonly used search and recommendation methods. We describe this system as well as the relevant user testing results and its implication towards use in online social tagging systems.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Hybrid algorithms for recommending new items</title>
	<abstract>Despite recommender systems based on collaborative filtering typically outperform content-based systems in terms of recommendation quality, they suffer from the new item problem, i.e., they are not able to recommend items that have few or no ratings. This problem is particularly acute in TV applications, where the catalog of available items (e.g., TV programs) is very dynamic. On the contrary, content-based recommender systems are able to recommend both old and new items but the general quality of the recommendations in terms of relevance to the users is low. In this article we present two different approaches for building hybrid collaborative+content recommender systems, whose purpose is to produce relevant recommendations, while overcoming the new item issue. The approaches have been tested on two datasets: a version of the well--known Movielens dataset enriched with content meta--data, and an implicit dataset collected from 15'000 IPTV users over a period of six months.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Multi-criteria service recommendation based on user criteria preferences</title>
	<abstract>Research in recommender systems is now starting to recognise the importance of multiple selection criteria to improve the recommendation output. In this paper, we present a novel approach to multi-criteria recommendation, based on the idea of clustering users in "preference lattices" (partial orders) according to their criteria preferences. We assume that some selection criteria for an item (product or a service) will dominate the overall ranking, and that these dominant criteria will be different for different users. Following this assumption, we cluster users based on their criteria preferences, creating a "preference lattice". The recommendation output for a user is then based on ratings by other users from the same or close clusters. Having introduced the general approach of clustering, we proceed to formulate three alternative recommendation methods instantiating the approach: (a) using the aggregation function of the criteria, (b) using the overall item ratings, and (c) combining clustering with collaborative filtering. We then evaluate the accuracy of the three methods using a set of experiments on a service ranking dataset, and compare them with a conventional collaborative filtering approach extended to cover multiple criteria. The results indicate that our third method, which combines clustering and extended collaborative filtering, produces the highest accuracy.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Discovery-oriented collaborative filtering for improving user satisfaction</title>
	<abstract>Many recommender systems employed in commercial web sites use collaborative filtering. The main goal of traditional collaborative filtering techniques is improvement of the accuracy of recommendation. Nevertheless, such techniques present the problem that they include many items that the user already knows. These recommendations appear to be good when we consider accuracy alone. On the other hand, when we consider users' satisfaction, they are not necessarily good because of the lack of discovery. In our work, we infer items that a user does not know by calculating the similarity of users or items based on information about what items users already know. We seek to recommend items that the user would probably like and does not know by combining the above method and the most popular method of collaborative filtering.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Improving recommendation lists through topic diversification</title>
	<abstract>In this work we present topic diversification, a novel method designed to balance and diversify personalized recommendation lists in order to reflect the user's complete spectrum of interests. Though being detrimental to average accuracy, we show that our method improves user satisfaction with recommendation lists, in particular for lists generated using the common item-based collaborative filtering algorithm.Our work builds upon prior research on recommender systems, looking at properties of recommendation lists as entities in their own right rather than specifically focusing on the accuracy of individual recommendations. We introduce the intra-list similarity metric to assess the topical diversity of recommendation lists and the topic diversification approach for decreasing the intra-list similarity. We evaluate our method using book recommendation data, including offline analysis on 361, !, 349 ratings and an online study involving more than 2, !, 100 subjects.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Distributed collaborative filtering with domain specialization</title>
	<abstract>User data scarcity has always been indicated among the major problems of collaborative filtering recommender systems. That is, if two users do not share sufficiently large set of items for whom their ratings are known, then the user-to-user similarity computation is not reliable and a rating prediction for one user can not be based on the ratings of the other. This paper shows that this problem can be solved, and that the accuracy of collaborative recommendations can be improved by: a) partitioning the collaborative user data into specialized and distributed repositories, and b) aggregating information coming from these repositories. This paper explores a content-dependent partitioning of collaborative movie ratings, where the ratings are partitioned according to the genre of the movie and presents an evaluation of four aggregation approaches. The evaluation demonstrates that the aggregation improves the accuracy of a centralized system containing the same ratings and proves the feasibility and advantages of a distributed collaborative filtering scenario.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Self-organizing collaborative filtering in global-scale massive multi-user virtual environments</title>
	<abstract>Due to the huge amount of available information in today's society, it becomes more and more difficult for the consumer to locate the most useful information for a specific topic. Recommender systems using collaborative filtering (CF) are a popular technique for reducing information overload and finding useful information on the Internet. However, in massive global-scale multi-user virtual environments different approaches are required from those used within the currently dominant centralized infrastructures or lately investigated P2P approaches. Within this paper we present a novel collaborative filtering algorithm used within the HyperVerse -- a P2P-based self-organizing middleware service for massively distributed virtual worlds -- to generate and manage recommendations for HyperVerse object favorites. Due to its global extent considering users and possible ratings, using a monolithic database-backed recommendation service or huge profile- or item-rating-matrices does not scale in our scenario. The decentralized approach presented within this paper creates per user ratings in an adaptive and transparent way by comparing public favorites of passer-by users with personal peer data, weighted by self-adjusting buddy lists.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A collaborative constraint-based meta-level recommender</title>
	<abstract>Recommender Systems (RS) have become popular for their ability to make useful suggestions to online shoppers. Knowledge-based RS represent one branch of these types of applications that employ means-end knowledge to map abstract user requirements to product characteristics. Before setting up such a system, the knowledge has to be acquired from domain experts and formalized using constraints or a comparable representation mechanism. However, the initial acquisition of the knowledge base and its maintenance are effort intensive tasks. Here, we propose a system that learns rule-based preferences from successful interactions in historic transaction data. It is realized as a meta-level hybrid that employs collaborative filtering to derive preferences from a user's nearest neighbors that are processed by a knowledge-based RS to derive recommendations. An evaluation using a commercial dataset showed that this approach outperforms the prediction accuracy of a knowledge base provided by domain experts. In addition, the approach is applicable for supporting domain experts in the maintenance and validation tasks associated with providing personalization knowledge bases.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Analyzing category correlations for recommendation system</title>
	<abstract>Since the late 20th century, the Internet users have noticeably increased and these users have provided lots of information on the Web and searched for information from the Web. Now there are huge amount of new information on the Web everyday. However, not all data are reliable and valuable. This implies that it becomes more and more difficult to find a satisfactory result from the Web. We often iterate searching several times to find what we are looking for. Researcher suggests a recommendation system to solve this problem. Instead of searching several times, a recommendation system proposes relevant information. In the Web 2.0 era, a recommendation system often relies on the collaborative filtering from users. In general, the collaborative filtering approach works based on user information such as gender, location or preference. However, it may cause the cold-star problem or the sparsity problem since it requires initial user information. Recently, there are several attempts to tackle these collaborative filtering problems. One of such attempts is to use category correlation of contents. For instance, a movie has genre information given by movie experts and directors. We notice that these category information are more reliable compared with user ratings. Moreover, a newly created content always has category information; namely, we can avoid the cold-start problem. We consider a movie recommendation system. We revisit the previous algorithm using genre correlation and improve the algorithm. We also test the modified algorithm and analyze the results with respect to a characteristic of genre correlations.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>TagiCoFi: tag informed collaborative filtering</title>
	<abstract>Besides the rating information, an increasing number of modern recommender systems also allow the users to add personalized tags to the items. Such tagging information may provide very useful information for item recommendation, because the users' interests in items can be implicitly reflected by the tags that they often use. Although some content-based recommender systems have made preliminary attempts recently to utilize tagging information to improve the recommendation performance, few recommender systems based on collaborative filtering (CF) have employed tagging information to help the item recommendation procedure. In this paper, we propose a novel framework, called tag informed collaborative filtering (TagiCoFi), to seamlessly integrate tagging information into the CF procedure. Experimental results demonstrate that TagiCoFi outperforms its counterpart which discards the tagging information even when it is available, and achieves state-of-the-art performance.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A social network-aware top-N recommender system using GPU</title>
	<abstract>A book recommender system is very useful for a digital library. Good book recommender systems can effectively help users find interesting and relevant books from the massive resources, by providing individual recommendation book list for each end-user. By now, a variety of collaborative filtering algorithms have been invented, which are the cores of most recommender systems. However, because of the explosion of information, especially in the Internet, the improvement of the efficiency of the collaborative filting (CF) algorithm becomes more and more important. In this paper, we first propose a parallel Top-N recommendation algorithm in CUDA (Compute Unified Device Architecture) which combines the collaborative filtering and trust-based approach to deal with the cold-start user problem. Then based on this algorithm, we present a parallel book recommender system on a GPU (Graphics Processor unit) for CADAL digital library platform. Our experimental results show our algorithm is very efficient to process the large-scale datasets with good accuracy, and we report the impact of different values of parameters on the recommendation performance.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A unified approach to building hybrid recommender systems</title>
	<abstract>Content-based recommendation systems can provide recommendations for "cold-start" items for which little or no training data is available, but typically have lower accuracy than collaborative filtering systems. Conversely, collaborative filtering techniques often provide accurate recommendations, but fail on cold start items. Hybrid schemes attempt to combine these different kinds of information to yield better recommendations across the board.

We describe unified Boltzmann machines, which are probabilistic models that combine collaborative and content information in a coherent manner. They encode collaborative and content information as features, and then learn weights that reflect how well each feature predicts user actions. In doing so, information of different types is automatically weighted, without the need for careful engineering of features or for post-hoc hybridization of distinct recommender systems.

We present empirical results in the movie and shopping domains showing that unified Boltzmann machines can be used to combine content and collaborative information to yield results that are competitive with collaborative techniques in recommending items that have been seen before, and also effective at recommending cold-start items.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Recommendation algorithm combining the user-based classified regression and the item-based filtering</title>
	<abstract>With the expansion of the Internet services, providing personalized product recommendations has become one of the most important ways to attract customers. Especially, collaborative recommender systems have achieved widespread success on the web. Information of products is recommended to the users based on their nearest "neighbors" who have similar interests. It is widely known that there is a sparsity problem in such systems. However, according to our research, there are other problems: one is that the typical collaborative algorithm loses some important parameter when it predicts the ratings, because there might be a strong similarity between the users who give very different ratings. Another is that the classification information of resources is not used. To solve these problems, we have proposed a recommendation algorithm combining the user-based classified regression and the item-based filtering. The experiment results show that performance is improved after applying the new algorithm.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Collaborative competitive filtering: learning recommender using context of user choice</title>
	<abstract>While a user's preference is directly reflected in the interactive choice process between her and the recommender, this wealth of information was not fully exploited for learning recommender models. In particular, existing collaborative filtering (CF) approaches take into account only the binary events of user actions but totally disregard the contexts in which users' decisions are made. In this paper, we propose Collaborative Competitive Filtering (CCF), a framework for learning user preferences by modeling the choice process in recommender systems. CCF employs a multiplicative latent factor model to characterize the dyadic utility function. But unlike CF, CCF models the user behavior of choices by encoding a local competition effect. In this way, CCF allows us to leverage dyadic data that was previously lumped together with missing data in existing CF models. We present two formulations and an efficient large scale optimization algorithm. Experiments on three real-world recommendation data sets demonstrate that CCF significantly outperforms standard CF approaches in both offline and online evaluations</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Scouts, promoters, and connectors: the roles of ratings in nearest neighbor collaborative filtering</title>
	<abstract>Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors. The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce. We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest neighbor collaborative filtering. In particular, we formulate three roles--scouts, promoters, and connectors--that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected (resp.). These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole. For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling. We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Google news personalization: scalable online collaborative filtering</title>
	<abstract>Several approaches to collaborative filtering have been studied but seldom have studies been reported for large (several millionusers and items) and dynamic (the underlying item set is continually changing) settings. In this paper we describe our approach to collaborative filtering for generating personalized recommendations for users of Google News. We generate recommendations using three approaches: collaborative filtering using MinHash clustering, Probabilistic Latent Semantic Indexing (PLSI), and covisitation counts. We combine recommendations from different algorithms using a linear model. Our approach is content agnostic and consequently domain independent, making it easily adaptable for other applications and languages with minimal effort. This paper will describe our algorithms and system setup in detail, and report results of running the recommendations engine on Google News.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Preference networks: probabilistic models for recommendation systems</title>
	<abstract>Recommender systems are important to help users select relevant and personalised information over massive amounts of data available. We propose an unified framework called Preference Network (PN) that jointly models various types of domain knowledge for the task of recommendation. The PN is a probabilistic model that systematically combines both content-based filtering and collaborative filtering into a single conditional Markov random field. Once estimated, it serves as a probabilistic database that supports various useful queries such as rating prediction and top-N recommendation. To handle the challenging problem of learning large networks of users and items, we employ a simple but effective pseudo-likelihood with regularisation. Experiments on the movie rating data demonstrate the merits of the PN.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Accounting for taste: using profile similarity to improve recommender systems</title>
	<abstract>Recommender systems have been developed to address the abundance of choice we face in taste domains (films, music, restaurants) when shopping or going out. However, consumers currently struggle to evaluate the appropriateness of recommendations offered. With collaborative filtering, recommendations are based on people's ratings of items. In this paper, we propose that the usefulness of recommender systems can be improved by including more information about recommenders. We conducted a laboratory online experiment with 100 participants simulating a movie recommender system to determine how familiarity of the recommender, profile similarity between decision-maker and recommender, and rating overlap with a particular recommender influence the choices of decision-makers in such a context. While familiarity in this experiment did not affect the participants' choices, profile similarity and rating overlap had a significant influence. These results help us understand the decision-making processes in an online context and form the basis for user-centered social recommender system design.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Analysis of a low-dimensional linear model under recommendation attacks</title>
	<abstract>Collaborative filtering techniques have become popular in the past decade as an effective way to help people deal with information overload. Recent research has identified significant vulnerabilities in collaborative filtering techniques. Shilling attacks, in which attackers introduce biased ratings to influence recommendation systems, have been shown to be effective against memory-based collaborative filtering algorithms. We examine the effectiveness of two popular shilling attacks (the random attack and the average attack) on a model-based algorithm that uses Singular Value Decomposition (SVD) to learn a low-dimensional linear model. Our results show that the SVD-based algorithm is much more resistant to shilling attacks than memory-based algorithms. Furthermore, we develop an attack detection method directly built on the SVD-based algorithm and show that this method detects random shilling attacks with high detection rates and very low false alarm rates.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Who predicts better?: results from an online study comparing humans and an online recommender system</title>
	<abstract>Algorithmic recommender systems attempt to predict which items a target user will like based on information about the user's prior preferences and the preferences of a larger community. After more than a decade of widespread use, researchers and system users still debate whether such "impersonal" recommender systems actually perform as well as human recommenders. We compare the performance of MovieLens algorithmic predictions with the recommendations made, based on the same user profiles, by active MovieLens users. We found that algorithmic collaborative filtering outperformed humans on average, though some individuals outperformed the system substantially and humans on average outperformed the system on certain prediction tasks.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Personalized recommendation driven by information flow</title>
	<abstract>We propose that the information access behavior of a group of people can be modeled as an information flow issue, in which people intentionally or unintentionally influence and inspire each other, thus creating an interest in retrieving or getting a specific kind of information or product. Information flow models how information is propagated in a social network. It can be a real social network where interactions between people reside; it can be, moreover, a virtual social network in that people only influence each other unintentionally, for instance, through collaborative filtering. We leverage users' access patterns to model information flow and generate effective personalized recommendations. First, an early adoption based information flow (EABIF) network describes the influential relationships between people. Second, based on the fact that adoption is typically category specific, we propose a topic-sensitive EABIF (TEABIF) network, in which access patterns are clustered with respect to the categories. Once an item has been accessed by early adopters, personalized recommendations are achieved by estimating whom the information will be propagated to with high probabilities. In our experiments with an online document recommendation system, the results demonstrate that the EABIF and the TEABIF can respectively achieve an improved (precision, recall) of (91.0%, 87.1%) and (108.5%, 112.8%) compared to traditional collaborative filtering, given an early adopter exists.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A matrix factorization technique with trust propagation for recommendation in social networks</title>
	<abstract>Recommender systems are becoming tools of choice to select the online information relevant to a given user. Collaborative filtering is the most popular approach to building recommender systems and has been successfully employed in many applications. With the advent of online social networks, the social network based approach to recommendation has emerged. This approach assumes a social network among users and makes recommendations for a user based on the ratings of the users that have direct or indirect social relations with the given user. As one of their major benefits, social network based approaches have been shown to reduce the problems with cold start users. In this paper, we explore a model-based approach for recommendation in social networks, employing matrix factorization techniques. Advancing previous work, we incorporate the mechanism of trust propagation into the model. Trust propagation has been shown to be a crucial phenomenon in the social sciences, in social network analysis and in trust-based recommendation. We have conducted experiments on two real life data sets, the public domain Epinions.com dataset and a much larger dataset that we have recently crawled from Flixster.com. Our experiments demonstrate that modeling trust propagation leads to a substantial increase in recommendation accuracy, in particular for cold start users.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Improved recommendation based on collaborative tagging behaviors</title>
	<abstract>Considering the natural tendency of people to follow direct or indirect cues of other people's activities, collaborative filtering-based recommender systems often predict the utility of an item for a particular user according to previous ratings by other similar users. Consequently, effective searching for the most related neighbors is critical for the success of the recommendations. In recent years, collaborative tagging systems with social bookmarking as their key component from the suite of Web 2.0 technologies allow users to freely bookmark and assign semantic descriptions to various shared resources on the web. While the list of favorite web pages indicates the interests or taste of each user, the assigned tags can further provide useful hints about what a user thinks of the pages.

In this paper, we propose a new collaborative filtering approach TBCF (Tag-based Collaborative Filtering) based on the semantic distance among tags assigned by different users to improve the effectiveness of neighbor selection. That is, two users could be considered similar not only if they rated the items similarly, but also if they have similar cognitions over these items. We tested TBCF on real-life datasets, and the experimental results show that our approach has significant improvement against the traditional cosine-based recommendation method while leveraging user input not explicitly targeting the recommendation system.</abstract>
	<search_task_number>2</search_task_number>
	<query>collaborative filtering recommendation systems</query>
	<relevance>1</relevance>
  </item>



  <item>
    <title>Business Metadata: Capturing Enterprise Knowledge: Capturing Enterprise Knowledge</title>
	<abstract>People have a hard time communicating, and also have a hard time finding business knowledge in the environment. With the sophistication of search technologies like Google, business people expect to be able to get their questions answered about the business just like you can do an internet search. The truth is, knowledge management is primitive today, and it is due to the fact that we have poor business metadata management. This book is about all the groundwork necessary for IT to really support the business properly. By providing not just data, but the context behind the data. For the IT professional, it will be tactically practical--very "how to" and a detailed approach to implementing best practices supporting knowledge management. And for the the IT or other manager who needs a guide for creating and justifying projects, it will help provide a strategic map. * First book that helps businesses capture corporate (human) knowledge and unstructured data, and offer solutions for codifying it for use in IT and management.* Written by Bill Inmon, one of the fathers of the data warehouse and well-known author, and filled with war stories, examples, and cases from current projects.* Very practical, includes a complete metadata acquisition methodology and project plan to guide readers every step of the way.* Includes sample unstructured metadata for use in self-testing and developing skills.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>User-assisted query translation for interactive cross-language information retrieval</title>
	<abstract>Interactive Cross-Language Information Retrieval (CLIR), a process in which searcher and system collaborate to find documents that satisfy an information need regardless of the language in which those documents are written, calls for designs in which synergies between searcher and system can be leveraged so that the strengths of one can cover weaknesses of the other. This paper describes an approach that employs user-assisted query translation to help searchers better understand the system's operation. Supporting interaction and interface designs are introduced, and results from three user studies are presented. The results indicate that experienced searchers presented with this new system evolve new search strategies that make effective use of the new capabilities, that they achieve retrieval effectiveness comparable to results obtained using fully automatic techniques, and that reported satisfaction with support for cross-language searching increased. The paper concludes with a description of a freely available interactive CLIR system that incorporates lessons learned from this research.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Dispatcher: enabling active botnet infiltration using automatic protocol reverse-engineering</title>
	<abstract>Automatic protocol reverse-engineering is important for many security applications, including the analysis and defense against botnets. Understanding the command-and-control (C&amp;C) protocol used by a botnet is crucial for anticipating its repertoire of nefarious activity and to enable active botnet infiltration. Frequently, security analysts need to rewrite messages sent and received by a bot in order to contain malicious activity and to provide the botmaster with an illusion of successful and unhampered operation. To enable such rewriting, we need detailed information about the intent and structure of the messages in both directions of the communication despite the fact that we generally only have access to the implementation of one endpoint, namely the bot binary. Current techniques cannot enable such rewriting. In this paper, we propose techniques to extract the format of protocol messages sent by an application that implements a protocol specification, and to infer the field semantics for messages both sent and received by the application. Our techniques enable applications such as rewriting the C&amp;C messages for active botnet infiltration. We implement our techniques into Dispatcher, a tool to extract the message format and field semantics of both received and sent messages. We use Dispatcher to analyze MegaD, a prevalent spam botnet employing a hitherto undocumented C&amp;C protocol, and show that the protocol information extracted by Dispatcher can be used to rewrite the C&amp;C messages.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A framework for understanding file format conversions</title>
	<abstract>This paper addresses the workshop question: "Can data generated from the infancy of the digital age be ingestible by software today?" We have prototyped a set of e-services that serve as a framework for understanding content preservation, automation and computational requirements on preservation of electronic records. The framework consists of e-services for (a) finding file format conversion software, (b) executing file format conversions using available software, and (c) evaluating information loss across conversions. While the target audience for the technology is the US National Archives, these basic e-services are of interest to any manager of electronic records and to all citizens trying to keep their files current with the rapidly changing information technology. The novelty of the framework is in organizing the information about file format conversions, providing services about file format conversion paths, in prototyping a general architecture for reusing existing third-party software with import/export capabilities, and in evaluating information loss due to file format conversions. The impact of these e-services is in the widely accessible conversion software registry (CSR), conversion engine (Polyglot) and comparison engine (Versus) which can increase the productivity of the digital preservation community and other users of digital files.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Time and space-efficient architecture for a corpus-based text-to-speech synthesis system</title>
	<abstract>This paper proposes a time and space-efficient architecture for a text-to-speech synthesis system (TTS). The proposed architecture can be efficiently used in those applications with unlimited domain, requiring multilingual or polyglot functionality. The integration of a queuing mechanism, heterogeneous graphs and finite-state machines gives a powerful, reliable and easily maintainable architecture for the TTS system. Flexible and language-independent framework efficiently integrates all those algorithms used within the scope of the TTS system. Heterogeneous relation graphs are used for linguistic information representation and feature construction. Finite-state machines are used for time and space-efficient representation of language resources, for time and space-efficient lookup processes, and the separation of language-dependent resources from a language-independent TTS engine. Its queuing mechanism consists of several dequeue data structures and is responsible for the activation of all those TTS engine modules having to process the input text. In the proposed architecture, all modules use the same data structure for gathering linguistic information about input text. All input and output formats are compatible, the structure is modular and interchangeable, it is easily maintainable and object oriented. The proposed architecture was successfully used when implementing the Slovenian PLATTOS corpus-based TTS system, as presented in this paper.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>English learners’ dictionaries on CD-ROM as reference and language learning tools</title>
	<abstract>This paper discusses the findings of two empirical studies which investigated how students of English used two English learners’ dictionaries on CD-ROM. The outcome of these studies will indicate in what way such dictionaries can serve as reference and language learning tools. It is argued that an English learner’s dictionary on CD-ROM could be much more than an ‘ordinary’ reference work because it includes facilities that are not available in book form, such as audio and visual elements, exercises and games. The studies suggest, however, that a number of changes in content and design will have to be made in order to make such dictionaries more effective and beneficial to their users. Learners, on the other hand, may need to acquire special skills in order to benefit from all the information an English learner’s dictionary on CD-ROM contains.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item> 
  <item>
    <title>Forensic triage for mobile phones with DEC0DE</title>
	<abstract>We present DEC0DE, a system for recovering information from phones with unknown storage formats, a critical problem for forensic triage. Because phones have myriad custom hardware and software, we examine only the stored data. Via flexible descriptions of typical data structures, and using a classic dynamic programming algorithm, we are able to identify call logs and address book entries in phones across varied models and manufacturers. We designed DEC0DE by examining the formats of one set of phone models, and we evaluate its performance on other models. Overall, we are able to obtain high performance for these unexamined models: an average recall of 97% and precision of 80% for call logs; and average recall of 93% and precision of 52% for address books. Moreover, at the expense of recall dropping to 14%, we can increase precision of address book recovery to 94% by culling results that don't match between call logs and address book entries on the same phone.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>   
  <item>
    <title>Polyglot: automatic extraction of protocol message format using dynamic binary analysis</title>
	<abstract>Protocol reverse engineering, the process of extracting the application-level protocol used by an implementation, without access to the protocol specification, is important for many network security applications. Recent work [17] has proposed protocol reverse engineering by using clustering on network traces. That kind of approach is limited by the lack of semantic information on network traces. In this paper we propose a new approach using program binaries. Our approach, shadowing, uses dynamic analysis and is based on a unique intuition - the way that an implementation of the protocol processes the received application data reveals a wealth of information about the protocol message format. We have implemented our approach in a system called Polyglot and evaluated it extensively using real-world implementations of five different protocols: DNS, HTTP, IRC, Samba and ICQ. We compare our results with the manually crafted message format, included in Wireshark, one of the state-of-the-art protocol analyzers. The differences we find are small and usually due to different implementations handling fields in different ways. Finding such differences between implementations is an added benefit, as they are important for problems such as fingerprint generation, fuzzing, and error detection.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>CALLing for help: researching language learning strategies using help facilities in a web-based multimedia program</title>
	<abstract>This paper summarizes the research described in a PhD thesis (Pujolă, 2000) which presents a description of how learners use the help facilities of a web-based multimedia CALL program, called ImPRESSions, designed to foster second language learners’ reading and listening skills and language learning strategies. The study investigates the variation of strategy use in a CALL environment: Twenty two Spanish adult students of English worked with the program in four sessions and their computer movements were digital-video screen recorded. Together with direct observation and retrospective questions a detailed picture of learners’ deployment of strategies was drawn. As the emphasis was on the process rather than the product, the description and analysis of the data focus on the observation of the language learning strategies learners deployed when using the help facilities provided: Dictionary, Cultural Notes, Transcript, Subtitles and Play Controls, Feedback and an Experts module specifically designed to provide the language learner training component of the program. The qualitative analysis of the data indicates that many variables have an influence on the amount and quality of the use of the help provided by the program, from the learners’ individual differences to the fact that the CALL environment may prompt learners to behave or work in a different way from a more conventional type of learning. The results of the study provide information for future CALL material design and the type of research offers new possibilities for CALL research methods.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item> 
  <item>
    <title>The learning vector quantization algorithm applied to automatic text classification tasks</title>
	<abstract>Automatic text classification is an important task for many natural language processing applications. This paper presents a neural approach to develop a text classifier based on the Learning Vector Quantization (LVQ) algorithm. The LVQ model is a classification method that uses a competitive supervised learning algorithm. The proposed method has been applied to two specific tasks: text categorization and word sense disambiguation. Experiments were carried out using the REUTERS-21578 text collection (for text categorization) and the SENSEVAL-3 corpus (for word sense disambiguation). The results obtained are very promising and show that our neural approach based on the LVQ algorithm is an alternative to other classification systems.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>    
  <item>
    <title>Navigation and Acquisition of Spatial Knowledge in a Virtual Maze</title>
	<abstract>Spatial behavior in humans and animals includes a wide variety of behavioral competences and makes use of a large number of sensory cues. Here we studied the ability of human subjects to search locations, to find shortcuts and novel paths, to estimate distances between remembered places, and to draw sketch maps of the explored environment; these competences are related to goal-independent memory of space, or cognitive maps. Information on spatial relations was restricted to two types: a visual motion sequence generated by simulated movements in a virtual maze and the subject's own movement decisions defining the path through the maze. Visual information was local (i.e., no global landmarks or compass informa tion was provided). Other position and movement information (vestibular or proprioceptive) was excluded. The amount of visual information provided was varied over four experimental conditions. The results indicate that human subjects are able to learn a virtual maze from sequences of local views and movements. The information acquired is local, consisting of recognized positions and movement decisions associated to them. Although simple associations of this type can be shown to be present in some subjects, more complete configurational knowledge is acquired as well. The results are discussed in a view-based framework of navigation and the representation of spatial knowledge by means of a view graph.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>Multilingual information access: the contribution of evaluation</title>
	<abstract>Since evaluation of cross-language information retrieval systems began at TREC in 1997 and NTCIR in 1998 and, in particular, with the launch of the Cross-Language Evaluation Forum (CLEF) in 2000, considerable progress has been made in this particular sector of IR. Advances can be considered in two stages. The first stage regarded in particular the development of text retrieval systems from simple so-called "bilingual" systems in which a query in one language is used to search a document collection in another to truly "multilingual" retrieval systems where a query in one language can find relevant results from a collection of documents in multiple languages. In the second stage, the focus was no longer just on multilingual document retrieval but was diversified to include different kinds of text retrieval across languages (e.g multilingual question answering) and retrieval on different kinds of media (e.g. collections containing images or speech). However, although the results from the research perspective have been interesting, there has been little real take-up by the applications communities. In the paper we describe the results achieved by CLEF over the years and propose a third stage for multilingual system evaluation which gives far more attention to questions regarding usability and user satisfaction but also provides ways for the results achieved to be transferred to the operational context.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>1</relevance>
  </item> 
  <item>
    <title>Exploring the effects of language skills on multilingual web search</title>
	<abstract>Multilingual access is an important area of research, especially given the growth in multilingual users of online resources. A large body of research exists for Cross-Language Information Retrieval (CLIR); however, little of this work has considered the language skills of the end user, a critical factor in providing effective multilingual search functionality. In this paper we describe an experiment carried out to further understand the effects of language skills on multilingual search. Using the Google Translate service, we show that users have varied language skills that are non-trivial to assess and can impact their multilingual searching experience and search effectiveness.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>1</relevance>
  </item>   
  <item>
    <title>A prolog-based framework for search, integration and empirical analysis on software evolution data</title>
	<abstract>Software projects use different repositories for storing project and evolution information such as source code, bugs and patches. An integrated system that combines these multiple repositories and can answer a broad range of queries regarding the project's evolution history would be beneficial to both software developers and researchers. For example, the list of source code changes or the list of developers associated with a bug fix are frequent queries for both developers and researchers. Integrating and gathering this information is a tedious, cumbersome, error-prone process when done manually, especially for large projects. Previous approaches to this problem use frameworks that limit the user to a set of pre-defined query templates, or use query languages with limited power. In this paper, we argue the need for a framework built with recursively enumerable languages, that can answer temporal queries, and supports negation and recursion. As a first step toward such a framework, we present a Prolog-based system that we built, along with an evaluation of real-world integrated data from the Firefox project. Our system allows for elegant and concise, yet powerful queries, and can be used by developers and researchers for frequent development and empirical analysis tasks.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>Relations as an abstraction for BDD-based program analysis</title>
	<abstract>In this article we present Jedd, a language extension to Java that supports a convenient way of programming with Binary Decision Diagrams (BDDs). The Jedd language abstracts BDDs as database-style relations and operations on relations, and provides static type rules to ensure that relational operations are used correctly. The article provides a description of the Jedd language and reports on the design and implementation of the Jedd translator and associated runtime system. Of particular interest is the approach to assigning attributes from the high-level relations to physical domains in the underlying BDDs, which is done by expressing the constraints as a SAT problem and using a modern SAT solver to compute the solution. Further, a runtime system is defined that handles memory management issues and supports a browsable profiling tool for tuning the key BDD operations. The motivation for designing Jedd was to support the development of interrelated whole program analyses based on BDDs. We have successfully used Jedd to build Paddle, a framework of context-sensitive program analyses, including points-to analysis and call graph construction, as well as several client analyses.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item> 
  <item>
    <title>Expressive and modular predicate dispatch for Java</title>
	<abstract>Predicate dispatch is an object-oriented (OO) language mechanism for determining the method implementation to be invoked upon a message send. With predicate dispatch, each method implementation includes a predicate guard specifying the conditions under which the method should be invoked, and logical implication of predicates determines the method overriding relation. Predicate dispatch naturally unifies and generalizes several common forms of dynamic dispatch, including traditional OO dispatch, multimethod dispatch, and functional-style pattern matching. Unfortunately, prior languages supporting predicate dispatch have had several deficiencies that limit the practical utility of this language feature. We describe JPred, a backward-compatible extension to Java supporting predicate dispatch. While prior languages with predicate dispatch have been extensions to toy or nonmainstream languages, we show how predicate dispatch can be naturally added to a traditional OO language. While prior languages with predicate dispatch have required the whole program to be available for typechecking and compilation, JPred retains Java's modular typechecking and compilation strategies. While prior languages with predicate dispatch have included special-purpose algorithms for reasoning about predicates, JPred employs general-purpose, off-the-shelf decision procedures. As a result, JPred's type system is more flexible, allowing several useful programming idioms that are spuriously rejected by those other languages. After describing the JPred language informally, we present an extension to Featherweight Java that formalizes the language and its modular type system, which we have proven sound. Finally, we discuss two case studies that illustrate the practical utility of JPred, including its use in the detection of several errors.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>   
  <item>
    <title>Geometric encoding: forging the high performance context sensitive points-to analysis for Java</title>
	<abstract>Context sensitive points-to analysis suffers from the scalability problem. We present the geometric encoding to capture the redundancy in the points-to analysis. Compared to BDD and EPA, the state of the art, the geometric encoding is much more efficient in processing the encoded facts, especially for the high-order context sensitivity with the heap cloning. We also developed two precision preserving techniques, constraints distillation and 1-CFA SCC modeling, to further improve the efficiency, in addition to the precision performance trade-off scheme. We evaluate our points-to algorithm with two variants of the geometric encoding, Geom and HeapIns, on 15 widely cited Java benchmarks. The evaluation shows that the Geom based algorithm is 11x and 68x faster than the worklist/BDD based 1-object-sensitive analysis in Paddle, and the speedup steeply goes up to 24x and 111x, if the HeapIns algorithm is used. Meanwhile, being very efficient in time, the precision is still equal to and sometime better than the 1-object-sensitive analysis.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>    
  <item>
    <title>"Lift and Shift": Moving the Back Office to India</title>
	<abstract>The recent growth in offshoring business processes is driven by the need for cost savings, but, because of the potential for both the quantity and quality of work that may be done overseas, has larger implications for the service economy in developed countries. This paper uses India as a case study to examine the business, knowledge-related, and technological considerations that drive the globalization of business process fulfillment. It also examines the industrial structure that is emerging in India for the work and draws conclusions about its future and its implications for service workers in developed countries.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>   
  <item>
    <title>XJ: facilitating XML processing in Java</title>
	<abstract>The increased importance of XML as a data representation format has led to several proposals for facilitating the development of applications that operate on XML data. These proposals range from runtime API-based interfaces to XML-based programming languages. The subject of this paper is XJ, a research language that proposes novel mechanisms for the integration of XML as a first-class construct into Java™. The design goals of XJ distinguish it from past work on integrating XML support into programming languages --- specifically, the XJ design adheres to the XML Schema and XPath standards. Moreover, it supports in-place updates of XML data thereby keeping with the imperative nature of Java. We have built a prototype compiler for XJ, and our preliminary experiments demonstrate that the performance of XJ programs can approach that of traditional low-level API-based interfaces, while providing a higher level of abstraction.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>   
  <item>
    <title>JTL: the Java tools language</title>
	<abstract>We present an overview of JTL (the Java Tools Language, pronounced "Gee-tel"), a novel language for querying JAVA [8] programs. JTL was designed to serve the development of source code software tools for JAVA, and as a small language which to aid programming language extensions to JAVA. Applications include definition of pointcuts for aspect-oriented programming, fixing type constraints for generic programming, specification of encapsulation policies, definition of micro-patterns, etc. We argue that the JTL expression of each of these is systematic, concise, intuitive and general.JTL relies on a simply-typed relational database for program representation, rather than an abstract syntax tree. The underlying semantics of the language is restricted to queries formulated in First Order Predicate Logic augmented with transitive closure (FOPL).Special effort was taken to ensure terse, yet readable expression of logical conditions. The JTL pattern <B>public abstract class</B>, for example, matches all abstract classes which are publicly accessible, while <B>class</B> (<B>public</B> clone();) matches all classes in which method clone is public. To this end, JTL relies on a DATALOG-like syntax and semantics, enriched with quantifiers and pattern matching which all but entirely eliminate the need for recursive calls.JTL's query analyzer gives special attention to the fragility of the "closed world assumption" in examining JAVA software, and determines whether a query relies on such an assumption.The performance of the JTL interpreter is comparable to that of JQuery after it generated its database cache, and at least an order of magnitude faster when the cache has to be rebuilt.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>   
  <item>
    <title>Overview of WebCLEF 2005</title>
	<abstract>We describe WebCLEF, the multilingual web track, that was introduced at CLEF 2005. We provide details of the tasks, the topics, and the results of WebCLEF participants. The mixed monolingual task proved an interesting addition to the range of tasks in cross-language information retrieval. Although it may be too early to talk about a solved problem, effective web retrieval techniques seem to carry over to the mixed monolingual setting. The multilingual task, in contrast, is still very far from being a solved problem. Remarkably, using non-translated English queries proved more successful than using translations of the English queries.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>1</relevance>
  </item>   
  <item>
    <title>Refinement-based context-sensitive points-to analysis for Java</title>
	<abstract>We present a scalable and precise context-sensitive points-to analysis with three key properties: (1) filtering out of unrealizable paths, (2) a context-sensitive heap abstraction, and (3) a context-sensitive call graph. Previous work [21] has shown that all three properties are important for precisely analyzing large programs, e.g., to show safety of downcasts. Existing analyses typically give up one or more of the properties for scalability. We have developed a refinement-based analysis that succeeds by simultaneously refining handling of method calls and heap accesses, allowing the analysis to precisely analyze important code while entirely skipping irrelevant code. The analysis is demanddriven and client-driven, facilitating refinement specific to each queried variable and increasing scalability. In our experimental evaluation, our analysis proved the safety of 61% more casts than one of the most precise existing analyses across a suite of large benchmarks. The analysis checked the casts in under 13 minutes per benchmark (taking less than 1 second per query) and required only 35MB of memory, far less than previous approaches.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>   
  <item>
    <title>HydroJ: object-oriented pattern matching for evolvable distributed systems</title>
	<abstract>In an evolving software system, components must be able to change independently while remaining compatible with their peers. One obstacle to independent evolution is the brittle parameter problem: the ability of two components to communicate can depend on a number of inessential details of the types, structure, and/or contents of the values communicated. If these details change, then the components can no longer communicate, even if the essential parts of the message remain unaffected.We present HydroJ, an extension of Java that addresses this problem. In HydroJ, components communicate using self-describing, semi-structured messages, and programmers use pattern matching to define the handling of messages. This design stems from two central ideas: first, that self-describing messages reduce dependence on inessential message format details; and second, that object-oriented pattern matching naturally focuses on the essential information in a message and is insensitive to inessential information.We have developed these ideas in the context of Rain, a distributed, heterogeneous messaging system for ubiquitous computing. To evaluate the design, we have constructed a prototype HydroJ compiler, implemented some Rain services in HydroJ, studied the evolution of an existing Rain service over time, and formalized HydroJ's key features in a core language.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>   
  <item>
    <title>Embodied conversational agents in Wizard-of-Oz and multimodal interaction applications</title>
	<abstract>Embodied conversational agents employed in multimodal interaction applications have the potential to achieve similar properties as humans in faceto-face conversation. They enable the inclusion of verbal and nonverbal communication. Thus, the degree of personalization of the user interface is much higher than in other human-computer interfaces. This, of course, greatly contributes to the naturalness and user friendliness of the interface, opening-up a wide area of possible applications. Two implementations of embodied conversational agents in human-computer interaction are presented in this paper: the first one in a Wizard-of-Oz application and the second in a dialogue system. In the Wizard-of-Oz application, the embodied conversational agent is applied in a way that it conveys the spoken information of the operator to the user with whom the operator communicates. Depending on the scenario of the application, the user may or not be aware of the operator's involvement. The operator can communicate with the user based on audio/visual, or only audio, communication. This paper describes an application setup, which enables distant communication with the user, where the user is unaware of the operator's involvement. A real-time viseme recognizer is needed to ensure a proper response from the agent. In addition, implementation of the embodied conversational agent Lili hosting an entertainment show, which is broadcast by RTV Slovenia, will be described in more detail. Employment of the embodied conversational agent as a virtual major-domo named Maja, within an intelligent ambience, using speech recognition system and TTS system PLATTOS, will be also described.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>   
  <item>
    <title>A constructivist approach to teaching sentences in Indian languages</title>
	<abstract>This paper describes project IELIL, which uses constructivist approach and intelligent tutoring system to construct a learning environment for Indian language. Our approach is focused on teaching Marathi language by taking into account the interim results obtained in teaching simple sentences. The Intelligent Tutoring System provides a way to store relevant knowledge, track the learner's progress, give appropriate feedback to him and this is supported by constructivist framework which provides an interactive environment to help the learner to construct his knowledge. The approach is based on the way children learn their first language where no visible attempt is placed on teaching grammar rules. These grammar rules are induced by the learner during the learning process. Further work to extend the system to a richer sentence structure is in progress.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>   
  <item>
    <title>Detecting inefficiently-used containers to avoid bloat</title>
	<abstract>Runtime bloat degrades significantly the performance and scalability of software systems. An important source of bloat is the inefficient use of containers. It is expensive to create inefficiently-used containers and to invoke their associated methods, as this may ultimately execute large volumes of code, with call stacks dozens deep, and allocate many temporary objects. This paper presents practical static and dynamic tools that can find inappropriate use of containers in Java programs. At the core of these tools is a base static analysis that identifies, for each container, the objects that are added to this container and the key statements (i.e., heap loads and stores) that achieve the semantics of common container operations such as ADD and GET. The static tool finds problematic uses of containers by considering the nesting relationships among the loops where these semantics-achieving statements are located, while the dynamic tool can instrument these statements and find inefficiencies by profiling their execution frequencies. The high precision of the base analysis is achieved by taking advantage of a context-free language (CFL)-reachability formulation of points-to analysis and by accounting for container-specific properties. It is demand-driven and client-driven, facilitating refinement specific to each queried container object and increasing scalability. The tools built with the help of this analysis can be used both to avoid the creation of container-related performance problems early during development, and to help with diagnosis when problems are observed during tuning. Our experimental results show that the static tool has a low false positive rate and produces more relevant information than its dynamic counterpart. Further case studies suggest that significant optimization opportunities can be found by focusing on statically-identified containers for which high allocation frequency is observed at run time.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>   
  <item>
    <title>An ontology-based semantic integration for digital museums</title>
	<abstract>This paper describes the design and implementation through prototyping of the architecture of a system for browsing and retrieving museum information based on concepts. Ontology for the museum domain, based on the CIDOC Concept Reference Model, is being developed as mediation of the architecture. The challenges of developing such a global ontology model and a mapping mechanism between the global schema and data sources of local museums are discussed. Web Services technology is employed to integrate heterogeneous and distributed data sources of local museums. Experimentations with our prototype have demonstrated that this architecture is stable and efficient.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>   
  <item>
    <title>Thorn: robust, concurrent, extensible scripting on the JVM</title>
	<abstract>Scripting languages enjoy great popularity due to their support for rapid and exploratory development. They typically have lightweight syntax, weak data privacy, dynamic typing, powerful aggregate data types, and allow execution of the completed parts of incomplete programs. The price of these features comes later in the software life cycle. Scripts are hard to evolve and compose, and often slow. An additional weakness of most scripting languages is lack of support for concurrency - though concurrency is required for scalability and interacting with remote services. This paper reports on the design and implementation of Thorn, a novel programming language targeting the JVM. Our principal contributions are a careful selection of features that support the evolution of scripts into industrial grade programs - e.g., an expressive module system, an optional type annotation facility for declarations, and support for concurrency based on message passing between lightweight, isolated processes. On the implementation side, Thorn has been designed to accommodate the evolution of the language itself through a compiler plugin mechanism and target the Java virtual machine.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>   
  <item>
    <title>Recommending Adaptive Changes for Framework Evolution</title>
	<abstract>In the course of a framework’s evolution, changes ranging from a simple refactoring to a complete rearchitecture can break client programs. Finding suitable replacements for framework elements that were accessed by a client program and deleted as part of the framework’s evolution can be a challenging task. We present a recommendation system, SemDiff, that suggests adaptations to client programs by analyzing how a framework was adapted to its own changes. In a study of the evolution of one open source framework and three client programs, our approach recommended relevant adaptive changes with a high level of precision. In a second study of the evolution of two frameworks, we found that related change detection approaches were better at discovering systematic changes and that SemDiff was complementary to these approaches by detecting non-trivial changes such as when a functionality is imported from an external library.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>   
  <item>
    <title>A nanopass infrastructure for compiler education</title>
	<abstract>Compilers structured as a small number of monolithic passes are difficult to understand and difficult to maintain. Adding new optimizations often requires major restructuring of existing passes that cannot be understood in isolation. The steep learning curve is daunting, and even experienced developers find it hard to modify existing passes without introducing subtle and tenacious bugs. These problems are especially frustrating when the developer is a student in a compiler class.An attractive alternative is to structure a compiler as a collection of many small passes, each of which performs a single task. This "micropass" structure aligns the actual implementation of a compiler with its logical organization, simplifying development, testing, and debugging. Unfortunately, writing many small passes duplicates code for traversing and rewriting abstract syntax trees and can obscure the meaningful transformations performed by individual passes.To address these problems, we have developed a methodology and associated tools that simplify the task of building compilers composed of many fine-grained passes. We describe these compilers as "nanopass" compilers to indicate both the intended granularity of the passes and the amount of source code required to implement each pass. This paper describes the methodology and tools comprising the nanopass framework.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>   
  <item>
    <title>Deriving input syntactic structure from execution</title>
	<abstract>Program input syntactic structure is essential for a wide range of applications such as test case generation, software debugging and network security. However, such important information is often not available (e.g., most malware programs make use of secret protocols to communicate) or not directly usable by machines (e.g., many programs specify their inputs in plain text or other random formats). Furthermore, many programs claim they accept inputs with a published format, but their implementations actually support a subset or a variant. Based on the observations that input structure is manifested by the way input symbols are used during execution and most programs take input with top-down or bottom-up grammars, we devise two dynamic analyses, one for each grammar category. Our evaluation on a set of real-world programs shows that our technique is able to precisely reverse engineer input syntactic structure from execution.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Ether: malware analysis via hardware virtualization extensions</title>
	<abstract>Malware has become the centerpiece of most security threats on the Internet. Malware analysis is an essential technology that extracts the runtime behavior of malware, and supplies signatures to detection systems and provides evidence for recovery and cleanup. The focal point in the malware analysis battle is how to detect versus how to hide a malware analyzer from malware during runtime. State-of-the-art analyzers reside in or emulate part of the guest operating system and its underlying hardware, making them easy to detect and evade. In this paper, we propose a transparent and external approach to malware analysis, which is motivated by the intuition that for a malware analyzer to be transparent, it must not induce any side-effects that are unconditionally detectable by malware. Our analyzer, Ether, is based on a novel application of hardware virtualization extensions such as Intel VT, and resides completely outside of the target OS environment. Thus, there are no in-guest software components vulnerable to detection, and there are no shortcomings that arise from incomplete or inaccurate system emulation. Our experiments are based on our study of obfuscation techniques used to create 25,000 recent malware samples. The results show that Ether remains transparent and defeats the obfuscation tools that evade existing approaches.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Reverse engineering for mobile systems forensics with Ares</title>
	<abstract>We present Ares, a reverse engineering technique for assisting in the analysis of data recovered for the investigation of mobile and embedded systems. The focus of investigations into insider activity is most often on the data stored on the insider's computers and digital device - call logs, email messaging, calendar entries, text messages, and browser history - rather than on the status of the system's security. Ares is novel in that it uses a data-driven approach that incorporates natural language processing techniques to infer the layout of input data that has been created according to some unknown specification. While some other reverse engineering techniques based on instrumentation of executables offer high accuracy, they are hard to apply to proprietary phone architectures. We evaluated the effectiveness of Ares on call logs and contact lists from ten used Nokia cell phones. We created a rule set by manually reverse engineering a single Nokia phone. Without modification to that grammar, Ares parsed most phones' data with 90% of the accuracy of a commercial forensics tool based on manual reverse engineering, and all phones with at least 50% accuracy even though the endianess for one phone changed.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>ONOMATOPEDIA: onomatopoeia online example dictionary system extracted from data on the web</title>
	<abstract>Japanese is filled with onomatopoeia words, which describe sounds or actions like "click" or "bow-wow." In general, mastering onomatopoeia phrases is hard for foreign speakers, and example-based dictionaries are known to be useful for learning Japanese onomatopoeia. To construct such dictionaries, we need to collect as many examples as possible. This paper proposes an online onomatopoeia example-based dictionary named ONOMATOPEDIA, which comprises extensive example sentences collected from the Web. Inappropriate sentences are often included in web search results, for example, sentences that contain onomatopoeia words used as nick-names, or sentences that include uncommon usage patterns. We propose a model for extracting appropriate sentences as learning examples. Further, we propose a clustering algorithm for sentences having onomatopoeia that takes into account onomatopoeic words that could be used in different meanings depending on the context.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Loop-extended symbolic execution on binary programs</title>
	<abstract>Mixed concrete and symbolic execution is an important technique for finding and understanding software bugs, including security-relevant ones. However, existing symbolic execution techniques are limited to examining one execution path at a time, in which symbolic variables reflect only direct data dependencies. We introduce loop-extended symbolic execution, a generalization that broadens the coverage of symbolic results in programs with loops. It introduces symbolic variables for the number of times each loop executes, and links these with features of a known input grammar such as variable-length or repeating fields. This allows the symbolic constraints to cover a class of paths that includes different numbers of loop iterations, expressing loop-dependent program values in terms of properties of the input. By performing more reasoning symbolically, instead of by undirected exploration, applications of loop-extended symbolic execution can achieve better results and/or require fewer program executions. To demonstrate our technique, we apply it to the problem of discovering and diagnosing buffer-overflow vulnerabilities in software given only in binary form. Our tool finds vulnerabilities in both a standard benchmark suite and 3 real-world applications, after generating only a handful of candidate inputs, and also diagnoses general vulnerability conditions.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>Accelerating the creation of customized, language-Specific IDEs in Eclipse</title>
	<abstract>Full-featured integrated development environments have become critical to the adoption of new programming languages. Key to the success of these IDEs is the provision of services tailored to the languages. However, modern IDEs are large and complex, and the cost of constructing one from scratch can be prohibitive. Generators that work from language specifications reduce costs but produce environments that do not fully reflect distinctive language characteristics. We believe that there is a practical middle ground between these extremes that can be effectively addressed by an open, semi-automated strategy to IDE development. This strategy is to reduce the burden of IDE development as much as possible, especially for internal IDE details, while opening opportunities for significant customizations to IDE services. To reduce the effort needed for customization we provide a combination of frameworks, templates, and generators. We demonstrate an extensible IDE architecture that embodies this strategy, and we show that this architecture can be used to produce customized IDEs, with a moderate amount of effort, for a variety of interesting languages.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>Recommending adaptive changes for framework evolution</title>
	<abstract>In the course of a framework's evolution, changes ranging from a simple refactoring to a complete rearchitecture can break client programs. Finding suitable replacements for framework elements that were accessed by a client program and deleted as part of the framework's evolution can be a challenging task. We present a recommendation system, SemDiff, that suggests adaptations to client programs by analyzing how a framework adapts to its own changes. In a study of the evolution of the Eclipse JDT framework and three client programs, our approach recommended relevant adaptive changes with a high level of precision, and detected non-trivial changes typically undiscovered by current refactoring detection techniques.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>Capturing polymorphic creations: towards ontological heterogeneity and transmodiology</title>
	<abstract>This paper addresses the vast practice encompassed under the placeholder term polymorphic creations: contemporary tie-ins, pervasive gaming, telematic arts and so on. A position is put forward where a heterogenous model is championed in favour of a shared ontology. This is contextualized according to polymorphic practices and larger cultural shifts. This paper is, therefore, a theoretical analysis of potential polymorphic-specific methodologies.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>V2E: combining hardware virtualization and softwareemulation for transparent and extensible malware analysis</title>
	<abstract>A transparent and extensible malware analysis platform is essential for defeating malware. This platform should be transparent so malware cannot easily detect and bypass it. It should also be extensible to provide strong support for heavyweight instrumentation and analysis efficiency. However, no existing platform can meet both requirements. Leveraging hardware virtualization technology, analysis platforms like Ether can achieve good transparency, but its instrumentation support and analysis efficiency is poor. In contrast, software emulation provides strong support for code instrumentation and good analysis efficiency by using dynamic binary translation. However, analysis platforms based on software emulation can be easily detected by malware and thus is poor in transparency. To achieve both transparency and extensibility, we propose a new analysis platform that combines hardware virtualization and software emulation. The essence is precise heterogeneous replay: the malware execution is recorded via hardware virtualization and then replayed in software. Our design ensures the execution replay is precise. Moreover, with page-level recording granularity, the platform can easily adjust to analyze various forms of malware (a process, a kernel module, or a shared library). We implemented a prototype called V2E and demonstrated its capability and efficiency by conducting an extensive evaluation with both synthetic samples and 14 realworld emulation-resistant malware samples.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>Fast algorithm for creating space efficient dispatching tables with application to multi-dispatching</title>
	<abstract>The dispatching problem can be solved very efficiently in the single-inheritance~(SI) setting. In this paper we show how to extend one such solution to the multiple-inheritance~(MI) setting. This generalization comes with an increase to the space requirement by a small factor of κ This factor can be thought of as a metric of the complexity of the topology of the inheritance hierarchy.On a data set of~35 hierarchies totaling some~64 thousand types, our dispatching data structure, based on a novel type slicing technique, exhibits very significant improvements over previous dispatching techniques, not only in terms of the time for creating the underlying data structure, but also in terms of total space used.The cost is in the dispatching time, which is no longer constant, but doubly logarithmic in the number of types. Conversely, by using a simple binary search, dispatching time is logarithmic in the number of different implementations. In practice dispatching uses one indirect branch and, on average, only~2.5 binary branches.Our results also have applications to the space-efficient implementation of the more general problem of dispatching multi-methods.A by-product of our type slicing technique is an incremental algorithm for constant-time subtyping tests with favorable memory requirements. (The incremental version of the subtyping problem is to maintain the subtyping data structure in presence of additions of types to the inheritance hierarchy.)</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>Supporting incremental change in large system models</title>
	<abstract>When reengineering large systems, software developers would like to assess and compare the impact of multiple change scenarios without actually performing these changes. A change can be effected by applying a tool to the source code, or by a manual refactoring. In addition, tools run over a model are costly to redevelop. It raises an interesting challenge for tools implementors: how to support modification of large source code models to enable comparison of multiple versions. One naive approach is to copy the entire model after each modification. However, such an approach is too expensive in memory and execution time. In this paper we explore different implementations that source code metamodels support multiple versions of a system. We propose a solution based on dynamic binding of entities between multiple versions, providing good access performance while minimizing memory consumption.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>Detection and analysis of cryptographic data inside software</title>
	<abstract>Cryptographic algorithms are widely used inside software for data security and integrity. The search of cryptographic data (include algorithms, input-output data and intermediated states of operation) is important to security analysis. However, various implementations of cryptographic algorithms lead the automatic detection and analysis to be very hard. This paper proposes a novel automatic cryptographic data detection and analysis approach. This approach is based on execution tracing and data pattern extraction techniques, searching the data pattern of cryptographic algorithms, and automatically extracting detected Cryptographic algorithms and input-output data. We implement and evaluate our approach, and the result shows our approach can detect and extract common symmetric ciphers and hash functions in most kinds of programs with accuracy, effectiveness and universality.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>Thwarting zero-day polymorphic worms with network-level length-based signature generation</title>
	<abstract>It is crucial to detect zero-day polymorphic worms and to generate signatures at network gateways or honeynets so that we can prevent worms from propagating at their early phase. However, most existing network-based signatures are specific to exploit and can be easily evaded. In this paper, we propose generating vulnerability-driven signatures at network level without any host-level analysis of worm execution or vulnerable programs. As the first step, we design a network-based length-based signature generator (LESG) for the worms exploiting buffer overflow vulnerabilities. The signatures generated are intrinsic to buffer overflows, and are very difficult for attackers to evade. We further prove the attack resilience bounds even under worst-case attacks with deliberate noise injection. Moreover, LESG is fast and noisetolerant and has efficient signature matching. Evaluation based on real-world vulnerabilities of various protocols and real network traffic demonstrates that LESG is promising in achieving these goals.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>Links to projects. mathematical software, icms2006: developer's meeting</title>
	<abstract>This document is a collection of links and descriptions of projects related to icms2006—developer’s meeting.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>Testing concurrent systems: an interpretation of intuitionistic logic</title>
	<abstract>We present a natural confluence of higher-order hereditary Harrop formulas (HH formulas), Constraint Logic Programming (CLP, [JL87]), and Concurrent Constraint Programming (CCP, [Sar93]) as a fragment of (intuitionistic, higher-order) logic. This combination is motivated by the need for a simple executable, logical presentation for static and dynamic semantics of modern programming languages. The power of HH formulas is needed for higher-order abstract syntax, and the power of constraints is needed to naturally abstract the underlying domain of computation. Underpinning the combination is a sound and complete operational interpretation of a two-sided sequent presentation of (a large fragment of) intuitionistic logic in terms of behavioral testing of concurrent systems. Formulas on the left hand side of a sequent style presentation are viewed as a system of concurrent agents, and formulas on the right hand side as tests against this evolving system. The language permits recursive definitions of agents and tests, allows tests to augment the system being tested and allows agents to be contingent on the success of a test. We present a condition on proofs, operational derivability (OD), and show that the operational semantics generates only operationally derivable proofs. We show that a sequent in this logic has a proof iff it has an operationally derivable proof.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>Inferring protocol state machine from network traces: a probabilistic approach</title>
	<abstract>Application-level protocol specifications (i.e., how a protocol should behave) are helpful for network security management, including intrusion detection and intrusion prevention. The knowledge of protocol specifications is also an effective way of detecting malicious code. However, current methods for obtaining unknown protocol specifications highly rely on manual operations, such as reverse engineering which is a major instrument for extracting application-level specifications but is time-consuming and laborious. Several works have focus their attentions on extracting protocol messages from real-world trace automatically, and leave protocol state machine unsolved.In this paper, we propose Veritas, a system that can automatically infer protocol state machine from real-world network traces. The main feature of Veritas is that it has no prior knowledge of protocol specifications, and our technique is based on the statistical analysis on the protocol formats. We also formally define a new model - probabilistic protocol state machine (P-PSM), which is a probabilistic generalization of protocol state machine. In our experiments, we evaluate a text-based protocol and two binary-based protocols to test the performance of Veritas. Our results show that the protocol state machines that Veritas infers can accurately represent 92% of the protocol flows on average. Our system is general and suitable for both text-based and binary-based protocols. Veritas can also be employed as an auxiliary tool for analyzing unknown behaviors in real-world applications.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Proceedings of the 14th ACM conference on Computer and communications security</title>
	<abstract>These proceedings contain the papers selected for presentation at the 14th ACM Conference on Computer and Communications Security (CCS 2007), held October 29 to November 2, 2007 in Alexandria, VA, USA.In response to the call for papers 302 papers were submitted to the conference. These papers were evaluated on the basis of their significance, novelty, and technical quality. Each paper was reviewed by at least three members of the program committee. Reviewing was double-blind, meaning that the program committee was not able to see the names and affiliations of the authors, and the authors were not told which committee members reviewed which papers. After the reviews were written, the program committee met online for three weeks of intensive discussion. Of the papers submitted, 55 were selected for presentation at the conference, giving an acceptance rate of about 18%.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>An architecture for composing embedded domain-specific languages</title>
	<abstract>Embedded domain-specific languages (EDSLs) are said to be easier to compose than DSLs that are implemented by preprocessors. However, existing approaches focus on composition scenarios where the use of abstractions from one domain does not affect the interpretation of abstractions from another domain. This leads to programs that exhibit scattering and tangling symptoms if multiple EDSLs with crosscutting domain semantics are used. To address this issue, we propose an architecture for embedding DSLs that makes use of meta-object protocols and aspect-oriented concepts to support crosscutting composition of EDSLs. This enables to write modularized EDSL programs where each program addresses one concern.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>Fast algorithms for compressed multimethod dispatch table generation</title>
	<abstract>The efficiency of dynamic dispatch is a major impediment to the adoption of multimethods in object-oriented languages. In this article, we propose a simple multimethod dispatch scheme based on compressed dispatch tables. This scheme is applicable to any object-oriented language using a method precedence order that satisfies a specific monotonous property (e.g., as Cecil and Dylan) and guarantees that dynamic dispatch is performed in constant time, the latter being a major requirement for some languages and applications. We provide efficient algorithms to build the dispatch tables, provide their worst-case complexity, and demonstrate the effectiveness of our scheme by real measurements performed on two large object-oriented applications. Finally, we provide a detailed comparison of our technique with other existing techniques.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>MDE-based FPGA physical design: fast model-driven prototyping with Smalltalk</title>
	<abstract>The integrated circuit industry continues to progress rapidly deepening the gap in between the technological breakthroughs and the electronic design automation industry. This gap is even more problematic in the context of physical design, the last automation level between applications and the technology. The challenges of meeting the physical and performance constraints necessitate innovation at the algorithmic level, and at the methodological level.This study presents a methodological approach to physical design automation relying on model-driven engineering. Relying on the flexibility, and adaptability of the Smalltalk environment we propose an agile framework enabling fast physical design tool-flow prototyping. We illustrate our approach by using the Madeo FPGA toolkit as a legacy codebase that is incrementally changed to adopt this model-driven development strategy.Some pragmatic achievements are presented to illustrate the principal axes of this approach: algorithmic improvements through plug-and-play routines, domain-model extension for emerging technologies, as well as model evolution toward a meta-described environment.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>Sponge: a case study in practice-based collaborative art research</title>
	<abstract>In this paper, we describe the origins, thematics, projects and practices of the art research collective Sponge. In particular, we focus on Sponge as a useful case study in transdisciplinary, collaborative practice-based research in creative art and design production and specifically, on Sponge as a unique example of a community of practice that spans artistic production, techno-scientific research, and critical studies. Issues essential to collaborative work practices such as shared language, construction of boundary objects, accommodation of differing epistemic cultures as well Sponge's thematic interest in performance, materiality and agency are examined in the context of several large scale artistic projects produced in the US, Canada and Europe. Finally, we examine the relationship between Sponge and the second author's Topological Media Lab in trying to come to terms with the differing scales and life cycles in partnering between the university-based research lab and the sphere of artistic and cultural production.</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>   
  <item>
    <title>Alphabet Soup</title>
	<abstract>The Internationalization of Linux, Part 1: Mr. Turnbull takes a look at the problems faced when different character sets and the need for standardization</abstract>
	<search_task_number>5</search_task_number>
	<query>polyglot information searching</query>
	<relevance>0</relevance>
  </item>           


  <item>
    <title>
      The software engineering impacts of cultural factors on multi-cultural software development teams
    </title>
    <abstract>
      This paper is based on our experiences in trying to apply software engineering practices to development projects staffed by developers from three distinct cultures; Japan, India, and the United States. The development of commercial software products has always been difficult. The standard balancing act that occurs between features, schedules, and resources is at the core of the difficulty. We found that cultural differences also had a large impact on our software engineering work.Much has been written and said about software engineering methods that can be applied to development projects to reduce and control these core difficulties. Methods that were thought to be &quot;best practices&quot; turned out to be ineffective or very difficult to implement. Our understanding of the possible root causes for these difficulties greatly increased when we began to study some of the cultural dynamics within the team. This paper describes our observations in terms of how these cultural factors impacted the software engineering techniques used on the projects.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Knowledge-oriented software engineering process in a multi-cultural context
    </title>
    <abstract>
      In software engineering, leading trends can be detected that will affect the characteristic features of a product and its development process. On a product level, the growth of size and complexity is apparent--but on the one hand only. On the other hand, there is also a growing demand for simple and reasonable small software products executed by handheld terminals and smartphones; these applications are in many cases expected to collaborate with databases over the Internet. In addition, different kinds of service concepts (ASP, SaaS) are becoming recognized alternatives to the traditional way of buying software. Increasingly, software products are also distributed in a wide geographical scope to users with different cultural backgrounds and expectations. In software engineering work, as a consequence of this growth in size and complexity, the development work is more and more often distributed. The software business itself is becoming global because of acquisitions, offshoring, and international subcontracting. The globalization of work sets new requirements to the engineering processes: in international teams the organisational and cultural differences of the development subteams have to be recognized. In this paper, the focus is on the software development and its global dimension--especially the roles of multi-cultural and cross-organizational issues in software engineering. Our paper presents the results of the first phase of our three phases research project related to &quot;Culture-Aware Software Engineering.&quot; The main result of the first phase is the multi-cultural software engineering working model introduced in our paper. Culture is seen as one example of the context, i.e. the situation at hand. The concept of culture has also different meanings, which have to be understood in well-organized software engineering. Software engineering work is analyzed as a knowledge creation process, in which both explicit and tacit knowledge are recognized and the transformation between these establishes baselines along the development life cycle.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Structural factors that affect global software development learning team performance
    </title>
    <abstract>
      A team performance model provided an organizing framework for studying multi-cultural distributed learning teams. Structural equation modeling was used to test for relationships among individual, cultural and attitudes about collaborative work factors and team performance. The paper describes this model and its theoretical basis and reports on results from two pilot projects involving 152 students from the US, Panama, UK, and Turkey. While the model shows satisfactory fit, the results suggest that other factors may also influence how well students work together on global software projects. Future research, followed by model development, should incorporate these factors to capture the complexity of the educational and training environments.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Making Sense of Product Requirements
    </title>
    <abstract>
      This paper takes a historical perspective to more than 40 years of software development within a company that is delivering its products to a diverse set of customers throughout the world. By examining the company’s past, we wish to find origins and potential remedies for the challenges that many companies presently face in determining just which, of many, features shall be implemented to forthcoming versions of their software products. We have concluded that the product-related design problems that once were manageable with rational thinking have gradually evolved into problems that have multiple and conflicting interpretations, different value orientations, unclear goals, contradictions and paradoxes. These problems occur due to the demands imposed by larger and much more diverse sets of critical stakeholders drawn from the new global business environment with its multi-cultural needs and greater numbers of highly domain-skilled, computer naïve users. For such problems sense making rather than decision making has begun to be the central organizational issue.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      How Globally Distributed Software Teams Can Improve Their Collaboration Effectiveness?
    </title>
    <abstract>
      While advanced collaboration tools are available in the market, feedback from organizations suggests a sub-optimal use and insufficient value leveraged from these tools. Our research focuses on identifying patterns of collaboration among distributed team members by mapping the interaction scenarios and associated goals, task contexts, success factors and constraints of collaboration with the choice of mechanisms used. Based on this analysis and utilizing a Goal Oriented Collaboration Strategy Framework, we analyze the effectiveness of collaboration mechanisms in specific scenarios involving a multi-geo, multi-cultural and multi-organization software development project. We recommend a planning based approach that identifies a basket of collaboration practices and tool features that distributed teams should use to enhance their collaboration effectiveness. We believe this research will not only help distributed software teams to improve their effectiveness but will also provide insights to tool vendors for appropriate tool design.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Introduction to education and training track
    </title>
    <abstract>
      The attendees of ICSE comprise some of the top researchers in software engineering and also many educators of software engineering. Traditionally, however, these two groups do not talk to each other about educational issues. Then there are the practitioners who attend ICSE who have their own opinions about the relevance, strengths, and shortcomings of current software engineering education offered in universities. The goal of this year's track on Software Engineering Education and Training at ICSE is to bring these three communities together to discuss some urgent questions that have profound effect on how we structure our educational programs. Considering the tremendous changes taking place in the software engineering industry, and in the industrial world in general, it seems appropriate to confront the needs of the software engineering educators.Consider just the following increasingly common developments:Outsourcing of software projectsPervasiveness of software in all areas of commerce, industry, and societyIncreasingly distributed platformsOpen-source developmentGlobalization, leading to international (multi-cultural) distributed software teamsHow should these developments change the way we teach software engineering? Should textbooks be updated? Should software engineering play a different role in the computer science curriculum, that is, be more pervasive? How are professors in universities handling these issues?These are some of the questions we address in this track. In particular, we consider current challenges, current solutions, and future challenges. We are pleased to have six distinguished researchers to present their views and fifteen presenters from universities around the world presenting their innovative approaches in their classrooms. We expect lively and active discussion between the speakers and the audience.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Studying the influence of culture in global software engineering: thinking in terms of cultural models
    </title>
    <abstract>
      Culture appears to have a greater influence on software-engineering practice than originally envisioned. Many recent studies have reported that cultural factors greatly impact global software-engineering (GSE) practice. However, many of these studies characterize culture as a set of dimensions (e.g., Hofstede's), which significantly limits the meaning of culture. In this paper, we discuss the limitations of such a dimensional approach to studying culture by highlighting the aspects of culture that such dimensions fail to capture. Next, we present the idea of thinking of culture in terms of cultural models (inspired by Shore's work), and illustrate this idea by presenting cultural models adopted by the software-engineering domain. Then, based on this idea of cultural models, we present a conceptual reference framework for studying the influence of culture in the global software-engineering setting. Finally, we present some examples that use this framework, which illustrates the benefits of such a framework for studying culture's influence on GSE practice.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Global and task effects in information-seeking among software engineers
    </title>
    <abstract>
      Information-seeking strategies were explored in software engineers. A complex interaction of task and geographic differences was revealed. There is a general tendency across software engineers to favor non-social sources, such as documentation, for tasks where the goal is to seek factual information. More social sources are preferred when seeking information to diagnostic, problem-solving questions. Within this effect, some geographic variations assert themselves in a way that might partly be interpreted in terms of national cultural differences in Individualism vs. Collectivism. Implications of geographic differences in information-seeking for collaboration within global software development teams are discussed.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Practitioner's challenges in designing trust into online systems
    </title>
    <abstract>
      It is widely recognised that successful online systems are not those that simply fulfil some functional specifications, but rather systems that are developed to also meet a number of non-functional requirements such as security, reliability and trust. It is also generally accepted that the development of quality online systems is not anymore just the task of single individuals or mono-cultural teams but it often involves diverse and disperse teams with members from different cultures and backgrounds. Some non-functional requirements (such as security or reliability) enjoy a general consensus, independent of the cultural background of the developers. Unfortunately, this is not the case for trust, despite the growing interest in trustworthy information systems. The very nature of trust indicates that it is understood differently by different individuals and relates to personal experiences more than other non-functional requirements. In this paper we identify the field of study to support the inclusion of considerations regarding trust in the design of online systems, to provide the understanding and support that is in par with security or reliability.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Cultural influences and differences in software process improvement programs
    </title>
    <abstract>
      Implementing software process improvement (SPI) program successfully in a software organization is possibly the most challenging issue that the industry faces today. It is even more challenging to implement an SPI program in software organizations of developing countries like Bangladesh because of the difference in norms and values of the national culture, and organizational culture as compared to those of the developed European and Western countries. This research extends the prior research on SPI in Bangladesh (Wong &amp; Hasan, 2006) and addresses the factors involved in the implementation of SPI programs in software organizations. A study of ten top software companies in Bangladesh was conducted to investigate whether cultural factors hindered the process improvement program and whether this hindrance led to deterioration of the business goals. The results showed that the lack of cultural awareness and lack of skills from the management perspective acted as barriers during the implementation of SPI programs and so affected the business goals sought by these organizations.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      A tale of two teams: success and failure in virtual team meetings
    </title>
    <abstract>
      Interaction between two teams with the same team leader and with similar size and goals moved from weekly face-to-face meetings to virtual meetings because of the temporary displacement of the team leader to a time zone six hours ahead of the rest of the team. One team focused primarily on software development and the second team on developing and testing a research instrument. The Software Team floundered through multiple different meeting arrangements and eventually agreed to disperse until the leader returned to the same time zone. In contrast, the Research Instrument Team kept a single meeting time that was set before it moved to virtual gatherings, and continued to be an active and productive team. This paper explores what factors led to this divergence in team success and concludes that the implicit temporal structures entraining the members of the Software Team coupled with an inability to repair member unhappiness and an unequal dispersion of skill sets among virtual and co-located members led to one team's eventual shutdown.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Cultural differences in software engineering
    </title>
    <abstract>
      The effect of cultural differences is often overlooked or neglected when analysing attractive, cost-effective options for software development. This papers aims to highlight people issues that arise out of cultural differences between interacting software development teams, particularly between Indians and non-Indians. The author's intent is to merely bring out the differences and not to provide solutions or recommendations or to identify root causes for the behavior. This is an experience paper, mostly based on observations and sharing of personal experiences from various colleagues and coworkers.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Cultural patterns in software process mishaps: incidents in global projects
    </title>
    <abstract>
      This paper describes a current and ongoing research project being conducted at the University of British Columbia, Canada. The paper begins by briefly describing past anthropological and sociological culture research. This research will inform our current exploration into the issues surrounding culture and its role in Global Software Development efforts. It then clarifies why this research is particularly important. The paper continues with a description of the current phase of this research, which is an exploratory qualitative approach rooted in Grounded Theory, and of the next phase, which will be a more quantitative approach looking at specific &quot;problem areas&quot; that were identified during the first phase.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Reflections on software engineering education
    </title>
    <abstract>
      The “engineering” focus in software engineering education leaves instructors vulnerable to several traps. It also misleads students as to SE's essential human and social dimensions. In this paper we argue that there's more to SE than engineering. A major challenge is to reconcile the engineering dimension with the human and social dimension.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      The future of software processes
    </title>
    <abstract>
      In response to increasing demands being put onto software-intensive systems, software processes will evolve significantly over the next two decades. This paper identifies seven relatively surprise-free trends – increased emphasis on users and end value; increasing software criticality and need for dependability; increasingly rapid change; increasingly complex systems of systems; increasing needs for COTS, reuse, and legacy software integration; and computational plenty – and two “wild card” trends: increasing software autonomy and combinations of biology and computing; and discusses their likely influences on software processes between now and 2025. It also discusses limitations to software process improvement, and areas of significant software process research and education needs.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      Cross-cultural user-experience design
    </title>
    <abstract>
      User interfaces for desktop, Web, mobile, and vehicle platforms extend across culturally diverse user communities, sometimes within a single country or language group, and certainly across the globe. If user interfaces are to be usable, useful, and appealing to such a wide range of users, user-interface/user-experience developers must account for cultural aspects in globalizing/localizing products and services. In this course, participants will learn practical principles and techniques that are immediately useful for both analysis and design tasks. Where time permits, they will have an opportunity to put their understanding into practice through a series of group exercises. Some handout materials are available in Mandarin.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Tool support for disseminating and improving development practices
    </title>
    <abstract>
      Knowledge management in software engineering and software process improvement activities pose challenges as initiatives are deployed. Most existing approaches are either too expensive to deploy or do not take an organization's specific needs into consideration. There is thus a need for scalable improvement approaches that leverage knowledge already residing in the organizations. This paper presents tool support for an Experience Factory approach for disseminating and improving practices used in an organization. Experiences from using practices in development projects are captured in postmortems and provide iteratively improved decision support for identifying what practices work well and what needs improvement. An initial evaluation of using the tool for organizational improvement has been performed utilizing both academia and industry. The results from the evaluation indicate that organizational characteristics influence how practices and experiences can be used. Experiences collected in postmortems are estimated to have little effect on improvements to practices used throughout the organization. However, in organizations where different practices are used in different parts of the organization, making practices available together with experiences from use, as well as having context information, can influence decisions on what practices to use in projects.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      Stakeholder dissonance: disagreements on project outcome and its impact on team motivation across three countries
    </title>
    <abstract>
      When a project perceived to be a failure by one set of stakeholders is perceived as a success by another set of stakeholders we have outcome disagreement. Our objective is to discover if team motivation is affected when developers and managers disagree on a project's outcome. We also investigate if culture influences team motivation. We collected questionnaire data on 290 completed projects from software engineering practitioners based in Australia, Chile, and USA. We asked if the respondent considered their project was successful and if higher level management considered the project a success. We found that more projects were perceived successful by management than by developers. Also, successful projects are associated with higher levels of team motivation than failed projects or projects with outcome disagreement. Culture makes a difference to levels of team motivation for both failed projects, and projects with outcome disagreement. An over-riding influence on team motivation is agreement with other stakeholders. To motivate practitioners, stakeholders need to agree on what constitutes a successful or a failed project before the start of the project.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Global software development: technical, organizational, and social challenges
    </title>
    <abstract>
      The International Workshop on Global Software Development was held in Portland (Oregon, USA) on May 9 2003, as part of the International Conference on Software Engineering 2003. A multicultural group of twenty-five people contributed to a successful workshop that debated the continued challenges of software development in global teams. The workshop consisted of an invited talk, sixteen short presentations, and fruitful discussions. We present an overview of the workshop motivation and then focus on the workshop's technical program. The workshop web site, including papers and slides, can be found at &lt;http://gsd2003.cs.uvic.ca&gt;.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Towards a framework for work package allocation for GSD
    </title>
    <abstract>
      Global software development is an inexorable trend in the software industry. The impact of the trend in conventional software development can be found in many of its aspects. One of them is task or work package allocation. Task allocation was traditionally driven by resource competency and availability but GSD introduces new complexities to this process including time-zones differences, costs and cultural differences. In this work a report on the construction of a framework for work-package allocation within GSD projects is presented. This framework lies on three main pillars: individual and organizational competency, organizational customization and sound assessment methods.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Making IT offshoring work for the Japanese industries
    </title>
    <abstract>
      IT offshoring has now become imperative not only for the IT industry but also for the manufacturing industry in Japan. The Japanese hardware industry which has long been globally competitive now faces unexpected challenges, i.e. swelling volumes of programs embedded in electronic devices and systems. The impending serious shortages of IT engineers will soon erode the competitiveness of the industry. Increased global competition and serious manpower shortages are the major background for the IT offshoring. Japanese offshoring is yet at an immature stage, turning mainly to China due to cultural similarity, Japanese language proficiency, and geographical proximity. Through analyzing both failed and successful projects, the authors try to explore fundamental causes for the poor performance and provisionally determine critical factors contributing to the successful offshoring.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Introducing global supply chains into software engineering education
    </title>
    <abstract>
      This paper describes lessons from running software development projects across three globally distributed educational institutions. What was innovative about this study was that two of the institutions were located in service providing countries, conventional onshore/ offshore roles were reversed, and students were exposed to the realities of global supply chain management. Three teams of US students were tasked to develop three different software products for Cambodian clients, while sub-contracting the database component to third-party teams of Indian students. This paper details the role of the three institutions, the prerequisites for planning and logistics for running such educational projects, and summarises the findings, while drawing broader parallels with the commercial world of offshore and outsourced development. It ends with recommendations for software engineering education to better reflect the needs and skills demanded of right sourcing in the global marketplace. These extend more generally to global software engineering.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Risk areas in embedded software industry projects
    </title>
    <abstract>
      A powerful way to understand where gaps are in the expertise of embedded system designers is to look at what goes wrong in real industry projects. In this paper we summarize the &quot;red flag&quot; issues found in approximately 90 design reviews of embedded system products conducted over a ten year period across a variety of embedded system industries. The problems found can be roughly categorized into the areas of process, requirements, architecture, design, implementation, verification/validation, dependability, project management, and people. A few problem areas, such as watchdog timers and real time scheduling, are standard embedded education topics. But many areas, such as peer reviews, requirements, SQA, and user interface design might be worthy of increased attention in texts and education programs.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      Online collaboration: Collaborative behavior patterns and factors affecting globally distributed team performance
    </title>
    <abstract>
      Studying the collaborative behavior of online learning teams and how this behavior is related to communication mode and task type is a complex process. Research about small group learning suggests that a higher percentage of social interactions occur in synchronous rather than asynchronous mode, and that students spend more time in task-oriented interaction in asynchronous discussions than in synchronous mode. This study analyzed the collaborative interaction patterns of global software development learning teams composed of students from Turkey, US, and Panama. Data collected from students' chat histories and forum discussions from three global software development projects were collected and compared. Both qualitative and quantitative analysis methods were used to determine the differences between a group's communication patterns in asynchronous versus synchronous communication mode. K-means clustering with the Ward method was used to investigate the patterns of behaviors in distributed teams. The results show that communication patterns are related to communication mode, the nature of the task, and the experience level of the leader. The paper also includes recommendations for building effective online collaborative teams and describes future research possibilities.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Identifying key success factors for globally distributed software development project using simulation: a case study
    </title>
    <abstract>
      With the increased pressure to reduce cost, reduce development time,and improve quality, many software companies are moving toward using aGlobally Distributed Software Development (GSD) paradigm. Due to the challengesand difficulties with GSD, researchers and practitioners are attempting toidentify key success factors for GSD projects. Based on our previous work, wefound that the key success factors can be different depending upon specific projectcharacteristics. To ensure a successful outcome, project managers have tofocus on the right success factors for their particular project. In this paper, weillustrate how a GSD simulation model can be used to represent a specific projectand to identify key success factors for that project. We use a case studyfrom an actual software development firm. We also perform sensitivity analysisto assess the magnitude of the performance impact for the key factors for thespecific project. For the case study site, which uses a combination of phasebasedand module-based task allocation strategies, we found that team memberfamiliarity, frequency of team meetings, and communication frequency eachhave a strong impact on total project effort and duration.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      A heuristics-based approach to reverse engineering of electronic services
    </title>
    <abstract>
      Since the beginning of the electronic era, public administrations and enterprises have been developing services, through which citizens, businesses and customers can conduct their transactions with the offering entity. Each electronic service contains a substantial amount of knowledge in the form help texts, rules of use or legislation excerpts, examples, validation checks, etc. This knowledge has been extracted from domain experts when the services were developed, especially in the phases of analysis and design, and was subsequently translated into software. In the latter format though, knowledge cannot be readily used in organizational processes, such as knowledge sharing and development of new services. In this paper, we present an approach for reverse engineering electronic services in order to create knowledge items of high levels of abstraction, which can be used in knowledge sharing environments as well as in service development platforms. The proposed approach has been implemented and configured to generate artifacts for the SmartGov service development platform. Finally, an evaluation of the proposed approach is presented to assess its efficiency regarding various aspects of the reverse engineering process.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      OSS opportunities in open source software -- CRM and OSS standards
    </title>
    <abstract>
      In many industry verticals, open source software is proving to be a credible alternative to proprietary software. When compared to proprietary software, open source software can offer a number of benefits such as stronger customer involvement, lower costs and better quality. Despite these benefits, the take-up of open source software by incumbent telecommunications providers has been virtually zero, choosing instead to focus on major proprietary commercial-off-theshelf (COTS) packages due to prior strategic investments and negative preconceptions about open source software. Although an immediate change in take-up is not expected, the pressure to reduce operational support systems (OSS) costs is steering telecommunications providers to research potential opportunities for using open source software within their OSS. This paper addresses some of those opportunities. The general opportunities are presented using specific examples from current research. Potential short-term opportunities, using readily available open source customer relationship management (CRM) applications, and potential mid-to long-term opportunities, using open source OSS integration software, are analysed and evaluated. The paper concludes with a recommendation on what telecommunications providers should do to further investigate the opportunities presented by open source OSS.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      University-industry collaboration journey towards product lines
    </title>
    <abstract>
      Product Lines for mission critical Command and Control systems was a starting point for a long lasting research collaboration between National University of Singapore (NUS) and ST Electronics (Info-Software Systems) Pte Ltd (STEE-InfoSoft). Collaboration was intensified by a joint research project, also involving University of Waterloo and Netron Inc. that led to development of reuse technology called XVCL. The contribution of this paper is twofold: First, we describe collaboration modes, factors that were critical to sustain collaboration, and benefits for university and industry gained over years. Among the main benefits, STEE-InfoSoft advanced its reuse practice by applying XVCL in several software Product Line projects, while NUS team received early feedback from STEE-InfoSoft which helped refine XVCL reuse methods and keep academic research in sync with industrial realities. Academic findings and industrial pilots have opened new unexpected research directions. Second, we draw lessons learned from many projects, to explain the general nature and significance of problems addressed with the XVCL approach.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      Automotive engineering curriculum development: case study for Clemson University
    </title>
    <abstract>
      The automotive manufacturing industry has transitioned in the past 20 years from a central technical focus to an integrated and globally distributed supply chain. As car makers outsource not only a greater portion of their manufacturing, but also their technical design responsibility, a more thorough understanding of both design and manufacturing changes' effect on total vehicle and total production system performance and cost is critical. The distribution of technical responsibility in automotive manufacturing has motivated the development of a specific curriculum in Automotive Engineering at Clemson University in South Carolina, USA, with core focus on the interaction between systems, both in design and manufacturing. In this development, a detailed survey of automotive Original Equipment Manufacturers and major suppliers was carried out. The differences in perceived need between these organization types is explored, and the incorporation of these perceived needs to a new Automotive Engineering curriculum is presented.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      Towards efficient web engineering approaches through flexible process models
    </title>
    <abstract>
      After more than a decade of web developments and some deafening fiascos, it has become clear that it is not possible to face the development of large scale web systems without following a systematic and well-defined process to guarantee quality, measurability, maintainability and reusability. There are a number of hypermedia/web engineering methods that provide mechanisms to specify the product requirements, including those concerning structure, navigation, interaction, presentation and access. But apart from product requirements there are also process requirements which in the web arena are constantly changing. Hence, to be put in practice without disturbing the project goals nor compromising its success, methods have to rely on empirical and flexible process models that can be easily adapted to fit process requirements. Little attention has been paid to the process model in most hypermedia/web methods which usually apply a classical iterative process based on the use of prototypes that are tested with users. In this paper we describe how we did apply the ADM web engineering method following a flexible star life cycle in the context of a specific web project highlighting the main benefits of this approach. In particular we will describe the different cycles applied in the ARCE project, illustrating the application of a usability engineering life cycle in a real case.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      Using established Web Engineering knowledge in model-driven approaches
    </title>
    <abstract>
      A lot of research is currently conducted in the field of Model-Driven Development (MDD), especially regarding its applications to specific domains. Another field that enjoys a great amount of popularity is the Web. As a result, one of the domains MDD is applied to quite frequently is that of Web Applications. However, Web Engineering differs significantly from general Software Engineering and a number of well-established non-MDD solutions already exist in that field. This leads to several interesting questions, which have been left unanswered so far. In this paper, we address this shortcoming by analyzing whether the problems encountered in the field of Web Engineering can really be solved with MDD approaches. We also answer the questions whether MDD will be able to solve these problems better and/or cheaper than traditional Web Engineering approaches and whether the current Web MDD propositions live up to this potential. While answering these questions, we will show that there exists a great synergy between the two groups and that the success of MDD in the Web domain will depend on exploiting the strengths of both.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      Proceedings of the 11th Annual conference on Genetic and evolutionary computation
    </title>
    <abstract>
      These proceedings contain the papers presented at the 11th Annual Genetic and Evolutionary Computation Conference (GECCO-2009), held in Montréal, Canada, July 8-12, 2009.After 2007, when GECCO was held in London, UK, this is the second time GECCO has been held outside the U.S. The generally high number of submissions of previous events has been maintained: 531 papers have been submitted for review, which is an increase of about 18% when compared to last year. Of these 531 papers, 220 were accepted as eight-page publications and 25 minutes presentations at the conference, yielding an acceptance ratio of 41,4%. In addition, 137 submissions (25,8%) have been accepted for poster presentations with two-page abstracts included in the proceedings. Last year, GECCO successfully moved over to electronic proceedings, and we continued with this publishing strategy as it greatly facilitates the handling of all conference materials.GECCO has lived up to its motto of one conference, many mini-conferences. This year, there were 15 separate tracks that operated independently from each other. Each track had its own track chair(s) and individual program committee. A member of one track's program committee was not allowed to simultaneously be a member of another track's committee. To reduce any bias reviewers might have, all reviews were conducted double blind, no authors' names were included in the reviewed papers. About 600 researchers participated in the reviewing process. We want to thank them for all their work, which is highly appreciated and absolutely vital for the quality of the conference.Track chairs have been asked to not accept more than 50% of their submissions as full papers. An appropriate acceptance rate is important in order to preserve the quality of the conference. Even though we were not bound by strong physical or environmental limitations on the number of accepted papers, we strove to keep our acceptance rate at the lower end. The scientific quality of the conference as well as that of the proceedings also is ensured by principles laid down in the GECCO by-laws of SIGEVO: (i) The GECCO conference shall be a broad-based conference encompassing the whole field of genetic and evolutionary computation. (ii) Papers will be published and presented as part of the main conference proceedings only after being peer reviewed. No invited papers shall be published (except for those of up to three invited plenary speakers). (iii) The peer review process shall be conducted consistent with the principle of division of powers performed by a multiplicity of independent program committees, each with expertise in the area of the paper being reviewed. (iv) The determination of the policy for the peer review process for each of the conference's independent program committees and the reviewing of papers for each program committee shall be performed by persons who occupy their positions by virtue of meeting objective and explicitly stated qualifications based on their previous scientific research activity or applications activity. (v) Emerging areas within the field of genetic and evolutionary computation shall be actively encouraged and incorporated in the activities of the conference by providing a semi-automatic method for their inclusion into the activities of the conference (with some procedural flexibility being extended to such emerging new areas). (vi) The percentage of submitted papers that are accepted as regular papers (i.e., papers other than poster papers) shall not exceed 50%.In addition to the presentation of the papers contained in these proceedings, GECCO-2009 also included free tutorials, workshops, a series of sessions on Evolutionary Computation in Practice, various competitions, late-breaking papers, and a job shop.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      Proceedings of the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers: Late Breaking Papers
    </title>
    <abstract>
      These proceedings contain the papers presented at the 11th Annual Genetic and Evolutionary Computation Conference (GECCO-2009), held in Montréal, Canada, July 8-12, 2009.After 2007, when GECCO was held in London, UK, this is the second time GECCO has been held outside the U.S. The generally high number of submissions of previous events has been maintained: 531 papers have been submitted for review, which is an increase of about 18% when compared to last year. Of these 531 papers, 220 were accepted as eight-page publications and 25 minutes presentations at the conference, yielding an acceptance ratio of 41,4%. In addition, 137 submissions (25,8%) have been accepted for poster presentations with two-page abstracts included in the proceedings. Last year, GECCO successfully moved over to electronic proceedings, and we continued with this publishing strategy as it greatly facilitates the handling of all conference materials.GECCO has lived up to its motto of one conference, many mini-conferences. This year, there were 15 separate tracks that operated independently from each other. Each track had its own track chair(s) and individual program committee. A member of one track's program committee was not allowed to simultaneously be a member of another track's committee. To reduce any bias reviewers might have, all reviews were conducted double blind, no authors' names were included in the reviewed papers. About 600 researchers participated in the reviewing process. We want to thank them for all their work, which is highly appreciated and absolutely vital for the quality of the conference.Track chairs have been asked to not accept more than 50% of their submissions as full papers. An appropriate acceptance rate is important in order to preserve the quality of the conference. Even though we were not bound by strong physical or environmental limitations on the number of accepted papers, we strove to keep our acceptance rate at the lower end. The scientific quality of the conference as well as that of the proceedings also is ensured by principles laid down in the GECCO by-laws of SIGEVO: (i) The GECCO conference shall be a broad-based conference encompassing the whole field of genetic and evolutionary computation. (ii) Papers will be published and presented as part of the main conference proceedings only after being peer reviewed. No invited papers shall be published (except for those of up to three invited plenary speakers). (iii) The peer review process shall be conducted consistent with the principle of division of powers performed by a multiplicity of independent program committees, each with expertise in the area of the paper being reviewed. (iv) The determination of the policy for the peer review process for each of the conference's independent program committees and the reviewing of papers for each program committee shall be performed by persons who occupy their positions by virtue of meeting objective and explicitly stated qualifications based on their previous scientific research activity or applications activity. (v) Emerging areas within the field of genetic and evolutionary computation shall be actively encouraged and incorporated in the activities of the conference by providing a semi-automatic method for their inclusion into the activities of the conference (with some procedural flexibility being extended to such emerging new areas). (vi) The percentage of submitted papers that are accepted as regular papers (i.e., papers other than poster papers) shall not exceed 50%.In addition to the presentation of the papers contained in these proceedings, GECCO-2009 also included free tutorials, workshops, a series of sessions on Evolutionary Computation in Practice, various competitions, late-breaking papers, and a job shop.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      Three new features of innovation brought about by information and communication technology
    </title>
    <abstract>
      This paper aims to examine the changes to innovation brought about by the internet. Based on a systematic survey on the current efforts made by industry and academia in expanding the frontier of innovation, it found that e-innovation add-on or collective changes to innovation brought about by Information and Communication Technology is expanding traditional innovation in three aspects: 1) having an obvious tendency to software development and application, 2) encouraging a distributed delivery, and 3) deploying different managing/operating rules and toolkits. These expansions are leading e-innovation to become a new paradigm for carrying out creative activities.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      The role of asynchronous discussions in increasing the effectiveness of remote synchronous requirements negotiations
    </title>
    <abstract>
      Important and yet very difficult process in software development, requirements engineering is plagued with additional challenges in the emergent dynamics of geographically distributed software teams. Our hypothesis is that a mix of lean and rich communication media are needed towards increasing the effectiveness of meetings in reaching mutual agreement when stakeholders are geographically dispersed.We studied tool-supported remote inspections in six educational global project teams in a multicultural software development environment. In this paper we present the preliminary results from comparing the effectiveness of the requirements negotiations when preceded by the asynchronous discussions to those negotiations with no prior asynchronous discussions.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      Proceedings of the 2011 annual conference extended abstracts on Human factors in computing systems
    </title>
    <abstract>
      Over the last year or so, we have been blessed with the challenge, the opportunity, and the distinct pleasure of organizing the CHI 2011 Conference on Human Factors in Computing Systems, the premier international conference for the field of human-computer interaction.CHI 2011 takes place in gorgeous, energetic, sophisticated Vancouver BC, a city renowned for its innovation in entertainment, sustainability, accessibility, and inclusivity. The New York Times calls it, &quot;a liquid city, a tomorrow city, equal parts India, China, England, France and the Pacific Northwest.&quot; Vancouver lays a beautiful backdrop for our conference, which boasts nearly 30 years of wonderful work.Behind the success of the conference is our diverse community of faculty and students, of researchers and practitioners, of young and, well, also of experienced. It is a community of designers, technologists, psychologists, social scientists, biologists, artists, engineers, anthropologists, musicians; the list goes on. Wherever we are, we are always a community of near and far. Most impressively, ours is a community that cares deeply about innovating, learning, sharing, and interacting; all with the common goal of using technology to shape the way people around the world live and play.Returning attendees will recognize the general conference format - 2 days of small intimate workshops, followed by 4 days of technical content, all surrounded by social and intellectual exchanges. In addition to the familiar venues that form the core of the conference, we have also arranged various special events, such an keynotes by Howard Rheingold and Ethan Zuckerman; invited talks by ACM SIGCHI award winners Terry Winograd, Larry Tesler, Alan Newell, and Clayton Lewis; an HCI museum exhibit hosted by Bill Buxton; and a panel celebrating Stu Card's achievements and contributions to the field of HCI.With the record number of submissions and accepted content this year, we hope that you will utilize the print and electronic programs, but also the daily CHI Madness presentations that provide a glimpse of the day ahead. In the interest of continuing to evolve the conference to best serve our needs, we will experiment with shorter talks this year (20 minute slots for long pieces of content and 10 for shorter ones) to infuse even more energy into the program. We will also have a pretty full slate of social media applications to help you connect with other attendees and to provide you with the fullest experience possible.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Proceedings of the 2011 annual conference on Human factors in computing systems
    </title>
    <abstract>
      Over the last year or so, we have been blessed with the challenge, the opportunity, and the distinct pleasure of organizing the CHI 2011 Conference on Human Factors in Computing Systems, the premier international conference for the field of human-computer interaction.CHI 2011 takes place in gorgeous, energetic, sophisticated Vancouver BC, a city renowned for its innovation in entertainment, sustainability, accessibility, and inclusivity. The New York Times calls it, &quot;a liquid city, a tomorrow city, equal parts India, China, England, France and the Pacific Northwest.&quot; Vancouver lays a beautiful backdrop for our conference, which boasts nearly 30 years of wonderful work.Behind the success of the conference is our diverse community of faculty and students, of researchers and practitioners, of young and, well, also of experienced. It is a community of designers, technologists, psychologists, social scientists, biologists, artists, engineers, anthropologists, musicians; the list goes on. Wherever we are, we are always a community of near and far. Most impressively, ours is a community that cares deeply about innovating, learning, sharing, and interacting; all with the common goal of using technology to shape the way people around the world live and play.Returning attendees will recognize the general conference format - 2 days of small intimate workshops, followed by 4 days of technical content, all surrounded by social and intellectual exchanges. In addition to the familiar venues that form the core of the conference, we have also arranged various special events, such an keynotes by Howard Rheingold and Ethan Zuckerman; invited talks by ACM SIGCHI award winners Terry Winograd, Larry Tesler, Alan Newell, and Clayton Lewis; an HCI museum exhibit hosted by Bill Buxton; and a panel celebrating Stu Card's achievements and contributions to the field of HCI.With the record number of submissions and accepted content this year, we hope that you will utilize the print and electronic programs, but also the daily CHI Madness presentations that provide a glimpse of the day ahead. In the interest of continuing to evolve the conference to best serve our needs, we will experiment with shorter talks this year (20 minute slots for long pieces of content and 10 for shorter ones) to infuse even more energy into the program. We will also have a pretty full slate of social media applications to help you connect with other attendees and to provide you with the fullest experience possible.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Internationalization of computer science education
    </title>
    <abstract>
      Internationalization of computer science education involves incorporating awareness, knowledge and skills of professional life in a global environment. Through an NSF CPATH1grant we have established a Pacific Rim community of computer science departments, high tech industry and international programs exploring a new model of computer science education that focuses on the knowledge, skills and competencies necessary for professional success and leadership in a global context. This paper describes our progress in building an international community of computer science educators, as well as our efforts in curricular innovation and establishment of international summer schools. Internationalization of computer science education will help attract the best and brightest students and broaden the appeal of computer science to a much more diverse population. Computer science will be seen as a pathway to a career not in an isolated cubicle but in the wide-open world.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Globally distributed content creation: developing consumable content for international markets
    </title>
    <abstract>
      This paper examines the globalization of content from the perspective of consumability. The authors describe how technical communicators can use a globally distributed approach to manage the translation of technical content. In so doing, the authors examine how technical communicators might use iterative development practices to improve the development of technical communication products (e.g., software manuals, online help, context-sensitive help, or dynamic help available on the Web) for international audiences of consumers.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Exploring virtual team-working effectiveness in the construction sector
    </title>
    <abstract>
      In defining a virtual team-working solution as with any new organisational form, success relies not merely on the introduction and adoption of Information and Communication Technologies (ICTs), but also on critically analysing the underlying social and organisational aspects. The paper investigates the effectiveness of virtual teams, and any other suitable form of virtual collaboration, in the Construction sector and explores the factors that influence their successful adoption. The positivist strand adopted in the research emphasises a particular approach that promotes software application hosting through a dedicated application service provider, as opposed to the traditional software-licensing model. The research identifies important socio-organisational challenges inherent to the project-based nature of Construction, including issues related to technology adoption, team identification, trust, and motivation. Action research techniques have been employed to conduct the research involving two Small and Medium-Sized Enterprises (SME) from France and Finland.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      Engaging with practices: design case studies as a research framework in CSCW
    </title>
    <abstract>
      Information and communications technology (ICT) pervades most aspects of our lives and changes everyday's practices in work and leisure time. When designing innovative ICTs, we need to engage with given practices, institutional arrangements, and technological infrastructures. We describe the research framework used at the University of Siegen. It is based on a collection of design case studies in particular fields of practice and identifies cross-cutting issues to compare and aggregate insights between these cases. To illustrate this framework, we describe our research activities and discuss three themes which became important in different design case studies.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      Adopting agile practices: an incipient pattern language
    </title>
    <abstract>
      The increasing popularity of Agile approaches to software development forces an increasing number of organizations to deal with issues of Agile adoption (and adaptation). This paper lays some groundwork for a pattern language that will facilitate the transition to agility. We introduce patterns that focus on the dynamics of adoption rather than the structure that results from adoption. To establish the desired foundation it is necessary to &quot;push the pattern envelope&quot; in terms of traditional pattern documentation format and relationships among patterns that form a pattern language.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      No sense of distance: improving cross-cultural communication with context-linked software tools
    </title>
    <abstract>
      Many studies have established the difficulties inherent in both cross-cultural and distance communication. Distance work interferes with close collaboration and trust. Physical distance and lack of time zone overlap can exacerbate cross-cultural misunderstandings. Nevertheless, international collaboration over distance is becoming increasingly common in many fields. Scientific collaborations, in particular, are becoming larger and more international in scope. There has been much research in the area of understanding cultural differences, but not as much in how technology might bridge such communication gaps in international scientific collaboration. In an effort to begin to form guidelines for such technology development, we undertook an empirical study of how computer-mediated communication tools facilitated cross-cultural communication over distance and led to greater team effectiveness in an international astrophysics collaboration.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Proceedings of the 2001 ACM/IEEE conference on Supercomputing (CDROM)
    </title>
    <abstract>
      Welcome to the Proceedings of the ACM/IEEE SC2001 Conference. These proceedings are nearly a direct (albeit more portable) copy of the conference Web site. The final snapshot for this Proceedings CD-ROM was taken on October 8, 2001. All information contained herein was current as of this date.By mirroring our comprehensive Web site in these Proceedings, we hope to provide you with a result which closely follows the numerous activities of this conference, serves as a complete archive of all related technical material, and accurately represents the spirit of SC2001.To view the technical papers you will need to be able to open Portable Document Format (PDF) files. At the time these papers were formatted for this CD-ROM, version 4 of &quot;Acrobat Reader&quot; from Adobe was the most common and effective reader for PDF files. Additionally, much of the material included in these Proceedings requires a JavaScript-capable Web browser to be viewed as intended.On behalf of the SC2001 Conference Committee, it is my sincere hope that you find the contents of these Proceedings useful. I encourage you to send in any comments or suggestions you have regarding this CD-ROM to webmaster@sc2001.org. Directions for obtaining additional copies of these Proceedings can be found on the ordering information page. Thank you for your support of the SCXY Conference series.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      An Empirical Investigation of Collaborative Conflict Management Style in Group Support System-Based Global Virtual Teams
    </title>
    <abstract>
      Virtual teams cut across national, organizational, and functional boundaries, often resulting in diversity in team composition. This paper presents the results of a laboratory study involving groupware-supported, culturally homogeneous, and heterogeneous virtual teams where collaborative conflict management style, a team's cultural orientation as measured by the degree of individualism-collectivism, and group diversity affect several group performance variables. Collaborative conflict management style was positively related to performance, group diversity was found to have a moderating influence between collaborative style and group performance, and collaborative style was influenced by the individualistic-collectivistic orientations. Consistent with prior research, we found that collectivistic orientations help enhance the level of collaborative conflict management style prevailing in teams. Our research also indicates that the process to motivate team members may differ depending on their orientation.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      Collaborative design: Managing task interdependencies and multiple perspectives
    </title>
    <abstract>
      This paper focuses on two characteristics of collaborative design with respect to cooperative work: the importance of work interdependencies linked to the nature of design problems; and the fundamental function of design cooperative work arrangement, which is the confrontation and combination of perspectives. These two intrinsic characteristics of the design work stress specific cooperative processes: coordination processes in order to manage task interdependencies, establishment of common ground and negotiation mechanisms in order to manage the integration of multiple perspectives in design.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      Internal and contextual factors, knowledge processes and performance: From the Chinese provider's perspective
    </title>
    <abstract>
      This paper explores the influences of two internal factors, i.e. supplier team's IT-based skills and communication with client's team, and two contextual factors, i.e. supplier team's understanding of client's culture and collaboration with client's team, on knowledge processes and performance in global sourcing of IT services from the Chinese provider's perspective. Knowledge processes are characterized by knowledge sharing, knowledge-based coordination and expertise management, and performance is measured by product success and personal satisfaction. Data have been collected in 13 companies in Xi'an Software Park, with 26 in-depth, semi-structured interviews held with top and middle managers, and 200 structured questionnaires distributed to knowledge workers who are involved in global sourcing projects. The results indicate that supplier team's IT-based skills, communication with client's team, cultural understanding of client's culture and collaboration with client's team are positively associated with knowledge process and performance. Also, knowledge sharing, knowledge-based coordination and expertise management are found to be crucial for those influential factors to function positively and contribute to the performance. The findings of this study suggest that the effects of key factors on knowledge processes and performance in global sourcing of IT services appear to transcend the social and cultural differences; however, contextual factors seem to have more significant influences on knowledge processes and performance in global sourcing of IT services.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	1
    </relevance>
  </item>
  <item>
    <title>
      UbiPlay: an interactive playground and visual programming tools for children
    </title>
    <abstract>
      Children develop important skills at playgrounds. Physical play promotes health and reacting to other children establishes social behavior patterns. By augmenting playground elements with sensor technology, video displays, and computer software, we pursued to take the experience further. This paper describes what was achieved; UbiPlay, a technology platform for programmable interactive playgrounds. UbiPlay allows children to create and play games in interactive playground environments. We present a play space built using the technology and results from qualitative evaluations, performed with 44 school children between the ages of 10 and 12. Results indicate that end-user programmable playgrounds like ours can provide much stimulus and excitement for children.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0	
    </relevance>
  </item>
  <item>
    <title>
      'Why didn't somebody tell me?': climate, information asymmetry, and bad news about troubled projects
    </title>
    <abstract>
      The reluctance to transmit bad news is a problem that is endemic to many organizations. When large projects go awry, it often takes weeks, months, and sometimes even years, before senior management becomes fully aware of what has happened. Accurate communication concerning a project and its status is therefore critical if organizations are to avoid costly and embarrassing debacles. This paper describes the results of an experiment designed to explore some key variables that may influence an individual's willingness to report bad news in an information systems project context. We extend a basic theoretical model derived from the whistle-blowing literature by considering relevant constructs from agency theory. We then test the entire model using a controlled experiment that employs a role-playing scenario. The results explain a significant portion of the variance in the reluctance to report negative status information. Implications for research and practice are discussed, along with directions for future research.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>
  <item>
    <title>
      Computer integrated construction: A review and proposals for future direction
    </title>
    <abstract>
      We present a review of the computer integrated construction (CIC) research space spanning approximately 20 years. This review reveals a strong focus on data and application integration for most of that time. We argue that whilst valuable in its own right, such research and the software solutions it yields fall short of the potential for CIC, giving our rationale for these beliefs. Thus we propose a re-focussing of CIC research on the relatively under-represented area of semantically described and coordinated process oriented systems to better support the kind of short term virtual organisation that typifies the working environment in the construction sector. Finally we present an outline vision for such a system, supported by a generic system architecture and a simple business model for its deployment, noting opportunities for future work in its realisation.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      multi-cultural software development
    </query>
    <relevance>
	0
    </relevance>
  </item>



  <item>
    <title>Perceptually based brush strokes for nonphotorealistic visualization</title>
	<abstract>An important problem in the area of computer graphics is the visualization of large, complex information spaces. Datasets of this type have grown rapidly in recent years, both in number and in size. Images of the data stored in these collections must support rapid and accurate exploration and analysis. This article presents a method for constructing visualizations that are both effective and aesthetic. Our approach uses techniques from master paintings and human perception to visualize a multidimensional dataset. Individual data elements are drawn with one or more brush strokes that vary their appearance to represent the element's attribute values. The result is a nonphotorealistic visualization of information stored in the dataset. Our research extends existing glyph-based and nonphotorealistic techniques by applying perceptual guidelines to build an effective representation of the underlying data. The nonphotorealistic properties the strokes employ are selected from studies of the history and theory of Impressionist art. We show that these properties are similar to visual features that are detected by the low-level human visual system. This correspondence allows us to manage the strokes to produce perceptually salient visualizations. Psychophysical experiments confirm a strong relationship between the expressive power of our nonphotorealistic properties and previous findings on the use of perceptual color and texture patterns for data display. Results from these studies are used to produce effective nonphotorealistic visualizations. We conclude by applying our techniques to a large, multidimensional weather dataset to demonstrate their viability in a practical, real-world setting.</abstract>
	<search_task_number>13</search_task_number>
	<query>data visualize display dataset</query>
	<relevance>1</relevance>
  </item>




  <item>
    <title>Semantic web portal: a platform for better browsing and visualizing semantic data</title>
	<abstract>One of the main shortcomings of Semantic Web technologies is that there are few user-friendly ways for displaying, browsing and querying semantic data. In fact, the lack of effective interfaces for end users significantly hinders further adoption of the Semantic Web. In this paper, we propose the Semantic Web Portal (SWP) as a light-weight platform that unifies off-the-shelf Semantic Web tools helping domain users organize, browse and visualize relevant semantic data in a meaningful manner. The proposed SWP has been demonstrated, tested and evaluated in several different use cases, such as a middle-sized research group portal, a government dataset catalog portal, a patient health center portal and a Linked Open Data portal for bio-chemical data. SWP can be easily deployed into any middle-sized domain and is also useful to display and visualize Linked Open Data bubbles.</abstract>
	<search_task_number>13</search_task_number>
	<query>data visualize display dataset</query>
	<relevance>1</relevance>
  </item>


  <item>
    <title>Assisted navigation for large information spaces</title>
	<abstract>This paper presents a new technique for visualizing large, complex collections of data. The size and dimensionality of these datasets make them challenging to display in an effective manner. The images must show the global structure of spatial relationships within the dataset, yet at the same time accurately represent the local detail of each data element being visualized. We propose combining ideas from information and scientific visualization together with a navigation assistant, a software system designed to help users identify and explore areas of interest within their data. The assistant locates data elements of potential importance to the user, clusters them into spatial regions, and builds underlying graph structures to connect the regions and the elements they contain. Graph traversal algorithms, constraint-based viewpoint construction, and intelligent camera planning techniques can then be used to design animated tours of these regions. In this way, the navigation assistant can help users to explore any of the areas of interest within their data. We conclude by demonstrating how our assistant is being used to visualize a multidimensional weather dataset.</abstract>
	<search_task_number>13</search_task_number>
	<query>data visualize display dataset</query>
	<relevance>1</relevance>
  </item>



  <item>
    <title>Multiscale acquisition and presentation of very large artifacts: The case of portalada</title>
	<abstract>The dichotomy between full detail representation and the efficient management of data digitization is still a big issue in the context of the acquisition and visualization of 3D objects, especially in the field of the cultural heritage. Modern scanning devices enable very detailed geometry to be acquired, but it is usually quite hard to apply these technologies to large artifacts. In this article we present a project aimed at virtually reconstructing the impressive (7×11 m.) portal of the Ripoll Monastery, Spain. The monument was acquired using triangulation laser scanning technology, producing a dataset of 2212 range maps for a total of more than 1 billion triangles. All the steps of the entire project are described, from the acquisition planning to the final setup for dissemination to the public. We show how time-of-flight laser scanning data can be used to speed-up the alignment process. In addition we show how, after creating a model and repairing imperfections, an interactive and immersive setup enables the public to navigate and display a fully detailed representation of the portal. This article shows that, after careful planning and with the aid of state-of-the-art algorithms, it is now possible to preserve and visualize highly detailed information, even for very large surfaces.</abstract>
	<search_task_number>13</search_task_number>
	<query>data visualize display dataset</query>
	<relevance>0</relevance>
  </item>


  <item>
    <title>Technical section: Visualizing MR diffusion tensor fields by dynamic fiber tracking and uncertainty </title>
	<abstract>Recent advances in magnetic resonance imaging have provided methods for the acquisition of high-resolution diffusion tensor fields. Their 3D-visualization with streamline-based techniques-called fiber tracking-allow analysis of cerebral white matter tracts for diagnostic, therapeutic as well as neuro-scientific purposes. The illusiveness of fiber visualizations and the inability to reliably visualize branching structures are problems still waiting for solutions. In this paper we present an on-the-fly approach to the tracking of branching and crossing fibers by dynamically setting secondary seeds in regions where branching is assumed, thus avoiding computationally intensive preprocessing steps. Moreover, we propose an uncertainty mapping technique that uses color-coding to enrich 3D fiber displays with information on their validity. Probability values for fiber samples are computed from dataset features as well as characteristics of the tracking process. In contrast to data optimization and pre-processing approaches, our algorithms focus on highly interactive visualization scenarios in collaborative environments.</abstract>
	<search_task_number>13</search_task_number>
	<query>data visualize display dataset</query>
	<relevance>0</relevance>
  </item>


  <item>
    <title>A Spatiotemporal Database for Ozone in the Conterminous U.S.</title>
	<abstract>This paper considers a set of ozone data in the conterminous U.S., which records the ozone concentration levels at a set of monitoring sites during 1994 and 1999. Existing GIS techniques are insufficient in handling such kind of spatiotemporal data in terms of data interpolation, visualization, representation and querying. We adopt 3-D shape functions from finite element methods for the spatiotemporal interpolation of the ozone dataset and analyze interpolation errors. The 3-D shape function based method estimates ozone concentration levels with less than 10 percent Mean Absolute Percentage Error. We give two approaches for visualizing the data: (i) combining the ArcGIS visualization tool with shape function interpolation results to visualize the ozone data for each year from 1994 and 1999, (ii) using Matlab to visualize the interpolated ozone data in a 3-D vertical profile display. For the spatiotemporal data representation, we use the constraint data model, because it can give an efficient and accurate representation of interpolation results. Finally, we give some practical query examples.</abstract>
	<search_task_number>13</search_task_number>
	<query>data visualize display dataset</query>
	<relevance>1</relevance>
  </item>

	<item>
	    <title>Visual analytics for partition comparison and evaluation</title>
		<abstract>Due to the rapid increase of data size and complexity in the world of information technology, cluster analysis with the assistance of visualization has become a major methodology for data analysis and exploration. This research focuses on visualization for mutual comparison and evaluation of multiple partitions, known as cluster stability analysis. In this context, a partition is constructed from the decomposition of a dataset into a family of disjoint groups. Partitions may refer to flat clustering results, categorical dimensions, binned numerical dimensions, predetermined class labeling dimensions, or prior knowledge in mutually exclusive format (one data item is associated with one and only one outcome). Cluster stability analysis is a kind of comparative cluster analysis, and the primary step in identifying "optimal" cluster structure, building cluster ensemble, or conducting cluster validation. Visualization for mutual comparison and evaluation of multiple partitions is a new research area in which there have been very few publications.
In this research, we define and extend a cluster stability metric system to record stability, group stability, and partition similarity measures. We describe a visualization tool CComViz (Cluster Comparison Visualiz ation) that performs mutual comparison and evaluation of multiple partitions of the same dataset. CComViz utilizes a novel algorithm for the layout of record and dimension order in a display. We use it to visualize data stability, data flow, density distribution and hierarchy, data correlation at the record, the group, and the dimension levels within a single graphical display. We show how CComViz can, using brushing and linking, also help in the identification of interesting records and patterns in exploratory data analysis.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>1</relevance>
	</item>

	<item>
	    <title>TreemapBar: Visualizing Additional Dimensions of Data in Bar Chart</title>
		<abstract>Bar chart is a very common and simple graph that is mainly used to visualize simple x, y plots of data for numerical comparisons by partitioning the categorical data values into bars and typically limited to operate on highly aggregated dataset. In today’s growing complexity of business data with multi dimensional attributes using bar chart itself is not sufficient to deal with the representation of such business dataset and it also not utilizes the screen space efficiently.Nevertheless, bar chart is still useful because of its shape create strong visual attention to users at first glance than other visualization techniques. In this article, we present a treemap bar chart + tablelens interaction technique that combines the treemap and bar chart visualizations with a tablelens based zooming technique that allows users to view the detail of a particular bar when the density of bars increases. In our approach, the capability of the original bar chart and treemaps for representing complex business data is enhanced and the utilization of display space is also optimized.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>1</relevance>
	</item>

	<item>
	    <title>Large Datasets at a Glance: Combining Textures and Colors in Scientific Visualization</title>
		<abstract>This paper presents a new method for using texture and color to visualize multivariate data elements arranged on an underlying height field. We combine simple texture patterns with perceptually uniform colors to increase the number of attribute values we can display simultaneously. Our technique builds multicolored perceptual texture elements (or pexels) to represent each data element. Attribute values encoded in an element are used to vary the appearance of its pexel. Texture and color patterns that form when the pexels are displayed can be used to rapidly and accurately explore the dataset. Our pexels are built by varying three separate texture dimensions: height, density, and regularity. Results from computer graphics, computer vision, and human visual psychophysics have identified these dimensions as important for the formation of perceptual texture patterns. The pexels are colored using a selection technique that controls color distance, linear separation, and color category. Proper use of these criteria guarantees colors that are equally distinguishable from one another. We describe a set of controlled experiments that demonstrate the effectiveness of our texture dimensions and color selection criteria. We then discuss new work that studies how texture and color can be used simultaneously in a single display. Our results show that variations of height and density have no effect on color segmentation, but that random color patterns can interfere with texture segmentation. As the difficulty of the visual detection task increases, so too does the amount of color on texture interference increase. We conclude by demonstrating the applicability of our approach to a real-world problem, the tracking of typhoon conditions in Southeast Asia.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>1</relevance>
	</item>


	<item>
	    <title>Volume illustration</title>
		<abstract>With the fast growing size and dimensionality of scientific datasets, exploring and rendering data features has become an important topic in visualization. Scientific illustrations have been widely used as visual representations in science and engineering because of their capability to display a large amount of information in a relatively succinct manner.
This dissertation investigates new efficient rendering algorithms to improve the visual representation qualities of scientific datasets by integrating the effectiveness of scientific; illustrations with visualization techniques. The main contribution is a volume illustration framework that can visualize volumetric datasets efficiently through conveying object features and simulating multiple illustrative styles.

Specifically, a stipple rendering algorithm explores a set of feature enhancements to improve the general understanding of scientific datasets; multiple illustrations styles are achieved through non-periodic 3D pattern and texture generation methods based on Wang Cubes: and an example-based approach creates 3D rendering from 2D illustration examples to simulate professional scientific illustrations.

This volume illustration framework can be used to explore features from a dataset interactively and express them efficiently. By taking advantage of geometry-based and hardware-accelerated rendering techniques, important features can be highlighted in an illustrative way at an interactive rendering speed, with a small storage overhead and short preprocessing delay.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>1</relevance>
	</item>

	<item>
	    <title>Interactive visualizations for trees and graphs</title>
		<abstract>Graphs are a very commonly used information structure, and have been applied to a broad range of fields from computer science to biology. There are several important issues to consider when designing graph visualizations. One of the most difficult problems researchers face is how to visualize large graphs. While an algorithm may produce good layouts for graphs of several hundred nodes, it may not scale well to several thousand nodes. And, as the size of the graph increases, performance will degrade rapidly, making it difficult to build an interactive system. Label readability will also suffer, hindering users' abilities to understand the graph data and perform many tasks. Finally, even if a system can lay out and display large graphs, the cognitive demands placed on the user by the visualization may be overwhelming.
This dissertation describes and applies several design principles to various graph visualization domains to address these issues. Tightly-coupled and highly customized views were used for graph visualization in a novel way. A new tree layout approach to graph visualization was proposed with appropriate visualization and interaction techniques. When visualizing graphs as trees, a guiding metaphor "Plant a seed and watch it grow" was used to support information gathering and detailed exploration of the graph's local structure.

Three graph visualization systems guided by these design principles were also developed and evaluated. First, PaperLens provides an abstract overview of the full dataset and shows relationships through interactive highlighting. It offers a novel alternative to the more common node-link diagram approach to graph visualization. Second, the development and evaluation of TaxonTree provided valuable insights that led to the design of TreePlus, a general interactive graph visualization component. Finally, TreePlus takes a tree layout approach to graph visualization, transforming a graph into a tree plus cross links (the links not represented by the spanning tree) using visualization, animation and interaction techniques to reveal the graph structure while preserving the label readability.

Other contributions of this work include the development of a task taxonomy for graph visualization and several specific applications of the graph visualization systems described above.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>1</relevance>
	</item>

	<item>
	    <title>Knowledge discovery query language (KDQL)</title>
		<abstract>KDD is a rapidly expanding field with promise for great applicability. Knowledge discovery became the new database technology for the incoming years. The need for automated discovery tools caused an explosion in the number and type of tools available commercially and in the public domain. These requirements encouraged us to propose a new KDD model so called ODBC_KDD(2) described in [39]. "One of the ODBC_KDD(2) model requirements is the implementation of a query language that could handle DM rules"[40]. This query language called Knowledge Discovery Query Language (KDQL). KDQL is a companion of two major tasks in KDD such as DM and Data Visualization. These requirements motivates us to think for the possibility of joining the two tasks of KDD commonly known as Data Mining (DM) and Data Visualization (DV) together in one single KDD process. Integrating DM and DV requires a new database concept. This database concept is called "i-extended database". I-extended database will be retrieved by the use of KDQL. This I-extended database described in details in [42]. KDQL RULES operations were also theoretically proposed in this paper and some examples were given as well. KDQL RULES are used only to find out the association rules in i-extended database we have.

The development and results of this paper would contribute to the data mining and visualization fields in several ways. The formulation of a set of heuristics for algorithms selection will help to clarify the matching between a specific problem and the set of best-suited algorithms or techniques (i.e. association rules) for solving it. These guidelines are expected to be useful and applicable to real DM projects.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>




	<item>
	    <title>Visualization in Medicine: Theory, Algorithms, and Applications</title>
		<abstract>Visualization in Medicine is the first book on visualization and its application to problems in medical diagnosis, education, and treatment. The book describes the algorithms, the applications and their validation (how reliable are the results?), and the clinical evaluation of the applications (are the techniques useful?). It discusses visualization techniques from research literature as well as the compromises required to solve practical clinical problems. 

The book covers image acquisition, image analysis, and interaction techniques designed to explore and analyze the data. The final chapter shows how visualization is used for planning liver surgery, one of the most demanding surgical disciplines. The book is based on several years of the authors' teaching and research experience. Both authors have initiated and lead a variety of interdisciplinary projects involving computer scientists and medical doctors, primarily radiologists and surgeons.

* A core field of visualization and graphics missing a dedicated book until now
* Written by pioneers in the field and illustrated in full color
* Covers theory as well as practice</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>1</relevance>
	</item>

	<item>
	    <title>Java Data Mining: Strategy, Standard, and Practice: A Practical Guide for architecture, design, and implementation</title>
		<abstract>Whether you are a software developer, systems architect, data analyst, or business analyst, if you want to take advantage of data mining in the development of advanced analytic applications, Java Data Mining, JDM, the new standard now implemented in core DBMS and data mining/analysis software, is a key solution component. This book is the essential guide to the usage of the JDM standard interface, written by contributors to the JDM standard. 

The book discusses and illustrates how to solve real problems using the JDM API. The authors provide you with:

* Data mining introduction--an overview of data mining and the problems it can address across industries; JDM's place in strategic solutions to data mining-related problems;
* JDM essentials--concepts, design approach and design issues, with detailed code examples in Java; a Web Services interface to enable JDM functionality in an SOA environment; and illustration of JDM XML Schema for JDM objects; 
* JDM in practice--the use of JDM from vendor implementations and approaches to customer applications, integration, and usage; impact of data mining on IT infrastructure; a how-to guide for building applications that use the JDM API. 
* Free, downloadable KJDM source code referenced in the book available here

* Data mining introduction--an overview of data mining and the problems it can address across industries; JDM's place in strategic solutions to data mining-related problems;
* JDM essentials--concepts, design approach and design issues, with detailed code examples in Java; a Web Services interface to enable JDM functionality in an SOA environment; and illustration of JDM XML Schema for JDM objects; 
* JDM in practice--the use of JDM from vendor implementations and approaches to customer applications, integration, and usage; impact of data mining on IT infrastructure; a how-to guide for building applications that use the JDM API. 
* Free, downloadable KJDM source code referenced in the book available here</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>

	<item>
	    <title>E-discovery: Creating and Managing an Enterprisewide Program: A Technical Guide to Digital Investigation and Litigation Support</title>
		<abstract>One of the hottest topics in computer forensics today, electronic discovery (e-discovery) is the process by which parties involved in litigation respond to requests to produce electronically stored information (ESI). According to the 2007 Socha-Gelbmann Electronic Discovery Survey, it is now a $2 billion industry, a 60% increase from 2004, projected to double by 2009. The core reason for the explosion of e-discovery is sheer volume; evidence is digital and 75% of modern day lawsuits entail e-discovery.

A recent survey reports that U.S. companies face an average of 305 pending lawsuits internationally. For large U.S. companies ($1 billion or more in revenue)that number has soared to 556 on average, with an average of 50 new disputes emerging each year for nearly half of them. To properly manage the role of digital information in an investigative or legal setting, an enterprise--whether it is a Fortune 500 company, a small accounting firm or a vast government agency--must develop an effective electronic discovery program. Since the amendments to the Federal Rules of Civil Procedure, which took effect in December 2006, it is even more vital that the lifecycle of electronically stored information be understood and properly managed to avoid risks and costly mistakes.

This books holds the keys to success for systems administrators, information security and other IT department personnel who are charged with aiding the e-discovery process.

*Comprehensive resource for corporate technologists, records managers, consultants, and legal team members to the e-discovery process, with information unavailable anywhere else

*Offers a detailed understanding of key industry trends, especially the Federal Rules of Civil Procedure, that are driving the adoption of e-discovery programs

*Includes vital project management metrics to help monitor workflow, gauge costs and speed the process

*Companion Website offers e-discovery tools, checklists, forms, workflow examples, and other tools to be used when conducting e-discovery strategy</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	
	<item>
	    <title>The Best Damn Cybercrime and Digital Forensics Book Period</title>
		<abstract>Electronic discovery refers to a process in which electronic data is sought, located, secured, and searched with the intent of using it as evidence in a legal case. Computer forensics is the application of computer investigation and analysis techniques to perform an investigation to find out exactly what happened on a computer and who was responsible. IDC estimates that the U.S. market for computer forensics will be grow from $252 million in 2004 to $630 million by 2009. Business is strong outside the United States, as well. By 2011, the estimated international market will be $1.8 billion dollars. The Techno Forensics Conference has increased in size by almost 50% in its second year; another example of the rapid growth in the market. 

This book is the first to combine cybercrime and digital forensic topics to provides law enforcement and IT security professionals with the information needed to manage a digital investigation. Everything needed for analyzing forensic data and recovering digital evidence can be found in one place, including instructions for building a digital forensics lab.

* Digital investigation and forensics is a growing industry
* Corporate I.T. departments needing to investigate incidents related to corporate espionage or other criminal activities are learning as they go and need a comprehensive step-by-step guide to e-discovery
* Appeals to law enforcement agencies with limited budgets</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>

	<item>
	    <title>Proceedings of the 9th international semantic web conference on The semantic web - Volume Part II</title>
		<abstract>An abstract is not available.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>

	<item>
	    <title>Finding representative workloads for computer system design</title>
		<abstract>This work explores how improved workload characterization can be used for a better selection of representative workloads within the computer system and processor design process. We find that metrics easily available in modern computer systems provide sufficient distinctive capability for workload selection, thus avoiding the need to characterize a large number of workloads in simulation.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	
	<item>
	    <title>Adaptive Web SitesA Knowledge Extraction from Web Data Approach</title>
		<abstract>An abstract is not available.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>

	<item>
	    <title>Developing Virtual Reality Applications: Foundations of Effective Design</title>
		<abstract>Virtual Reality systems enable organizations to cut costs and time, maintain financial and organizational control over the development process, digitally evaluate products before having them created, and allow for greater creative exploration. In this book, VR developers Alan Craig, William Sherman, and Jeffrey Will examine a comprehensive collection of current,unique, and foundational VR applications in a multitude of fields, such as business, science, medicine, art, entertainment, and public safety among others.

An insider's view of what works, what doesn't work, and why, Developing Virtual Reality Applications explores core technical information and background theory as well as the evolution of key applications from their genesis to their most current form. Developmental techniques are cross-referenced between different applications linking information to describe overall VR trends and fundamental best practices. This synergy, coupled with the most up to date research being conducted, provides a hands-on guide for building applications, and an enhanced, panoramic view of VR development. Developing Virtual Reality Applications is an indispensable one-stop reference for anyone working in this burgeoning field.






Dozens of detailed application descriptions provide practical ideas for VR development in ALL areas of interest!

Development techniques are cross referenced between different application areas, providing fundamental best practices!

Includes a media-rich companion website with hours of footage from application demonstrations</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>

	<item>
	    <title>Communications of the ACM: Volume 53 Issue 6</title>
		<abstract>An abstract is not available.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	
	<item>
	    <title>High-Performance Embedded Computing: Architectures, Applications, and Methodologies</title>
		<abstract>Over the past several years, embedded systems have emerged as an integral though unseen part of many consumer, industrial, and military devices. The explosive growth of these systems has resulted in embedded computing becoming an increasingly important discipline. The need for designers of high-performance, application-specific computing systems has never been greater, and many universities and colleges in the US and worldwide are now developing advanced courses to help prepare their students for careers in embedded computing.

High-Performance Embedded Computing: Architectures, Applications, and Methodologies is the first book designed to address the needs of advanced students and industry professionals. Focusing on the unique complexities of embedded system design, the book provides a detailed look at advanced topics in the field, including multiprocessors, VLIW and superscalar architectures, and power consumption. Fundamental challenges in embedded computing are described, together with design methodologies and models of computation. HPEC provides an in-depth and advanced treatment of all the components of embedded systems, with discussions of the current developments in the field and numerous examples of real-world applications.

Covers advanced topics in embedded computing, including multiprocessors, VLIW and superscalar architectures, and power consumption

Provides in-depth coverage of networks, reconfigurable systems, hardware-software co-design, security, and program analysis

Includes examples of many real-world embedded computing applications (cell phones, printers, digital video) and architectures (the Freescale Starcore, TI OMAP multiprocessor, the TI C5000 and C6000 series, and others)</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>

	<item>
	    <title>Fast detection of communication patterns in distributed executions</title>
		<abstract>Understanding distributed applications is a tedious and difficult task. Visualizations based on process-time diagrams are often used to obtain a better understanding of the execution of the application. The visualization tool we use is Poet, an event tracer developed at the University of Waterloo. However, these diagrams are often very complex and do not provide the user with the desired overview of the application. In our experience, such tools display repeated occurrences of non-trivial communication patterns, appearing throughout the trace data and cluttering the display space. This paper describes an event abstraction facility which tries to simplify the execution visualization shown by Poet by efficiently detecting and abstracting such patterns.A user can define patterns, subject to only very few constraints, and store them in a hierarchical pattern library. We also provide the user with the possibility to annotate the source code as a help in the abstraction process. We detect these communication patterns by employing an enhanced efficient multiple string matching algorithm. The results indicate that the matching process is indeed very fast. A user can experiment with multiple patterns at potentially different levels in the hierarchy, checking for their occurrence in the trace file, while trying to gain some understanding in a short period of time.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>

	<item>
	    <title>Physical and Logical Security Convergence: Powered By Enterprise Security Management: Powered By Enterprise Security Management</title>
		<abstract>Government and companies have already invested hundreds of millions of dollars in the convergence of physical and logical security solutions, but there are no books on the topic.

This book begins with an overall explanation of information security, physical security, and why approaching these two different types of security in one way (called convergence) is so critical in today's changing security landscape. It then details enterprise security management as it relates to incident detection and incident management. This is followed by detailed examples of implementation, taking the reader through cases addressing various physical security technologies such as: video surveillance, HVAC, RFID, access controls, biometrics, and more.

*This topic is picking up momentum every day with every new computer exploit, announcement of a malicious insider, or issues related to terrorists, organized crime, and nation-state threats 
*The author has over a decade of real-world security and management expertise developed in some of the most sensitive and mission-critical environments in the world 
*Enterprise Security Management (ESM) is deployed in tens of thousands of organizations worldwide</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Computational Intelligence: Concepts to Implementations</title>
		<abstract>Russ Eberhart and Yuhui Shi have succeeded in integrating various natural and engineering disciplines to establish Computational Intelligence. This is the first comprehensive textbook, including lots of practical examples. -Shun-ichi Amari, RIKEN Brain Science Institute, Japan

This book is an excellent choice on its own, but, as in my case, will form the foundation for our advanced graduate courses in the CI disciplines. -James M. Keller, University of Missouri-Columbia

The excellent new book by Eberhart and Shi asserts that computational intelligence rests on a foundation of evolutionary computation. This refreshing view has set the book apart from other books on computational intelligence. The book has an emphasis on practical applications and computational tools, which are very useful and important for further development of the computational intelligence field. -Xin Yao, The Centre of Excellence for Research in Computational Intelligence and Applications, Birmingham

The "soft" analytic tools that comprise the field of computational intelligence have matured to the extent that they can, often in powerful combination with one another, form the foundation for a variety of solutions suitable for use by domain experts without extensive programming experience.

Computational Intelligence: Concepts to Implementations provides the conceptual and practical knowledge necessary to develop solutions of this kind. Focusing on evolutionary computation, neural networks, and fuzzy logic, the authors have constructed an approach to thinking about and working with computational intelligence that has, in their extensive experience, proved highly effective. 

Features
· Moves clearly and efficiently from concepts and paradigms to algorithms and implementation techniques by focusing, in the early chapters, on the specific concepts and paradigms that inform the authors' methodologies.

· Explores a number of key themes, including self-organization, complex adaptive systems, and emergent computation.

· Details the metrics and analytical tools needed to assess the performance of computational intelligence tools.

· Concludes with a series of case studies that illustrate a wide range of successful applications.

· Presents code examples in C and C++.

· Provides, at the end of each chapter, review questions and exercises suitable for graduate students, as well as researchers and practitioners engaged in self-study.

· Makes available, on a companion website, a number of software implementations that can be adapted for real-world applications.

· Moves clearly and efficiently from concepts and paradigms to algorithms and implementation techniques by focusing, in the early chapters, on the specific concepts and paradigms that inform the authors' methodologies.

· Explores a number of key themes, including self-organization, complex adaptive systems, and emergent computation.

· Details the metrics and analytical tools needed to assess the performance of computational intelligence tools.

· Concludes with a series of case studies that illustrate a wide range of successful applications.

· Presents code examples in C and C++.

· Provides, at the end of each chapter, review questions and exercises suitable for graduate students, as well as researchers and practitioners engaged in self-study.

· Makes available, on a companion website, a number of software implementations that can be adapted for real-world applications.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>PRESIDIO: A Framework for Efficient Archival Data Storage</title>
		<abstract>The ever-increasing volume of archival data that needs to be reliably retained for long periods of time and the decreasing costs of disk storage, memory, and processing have motivated the design of low-cost, high-efficiency disk-based storage systems. However, managed disk storage is still expensive. To further lower the cost, redundancy can be eliminated with the use of interfile and intrafile data compression. However, it is not clear what the optimal strategy for compressing data is, given the diverse collections of data.

To create a scalable archival storage system that efficiently stores diverse data, we present PRESIDIO, a framework that selects from different space-reduction efficent storage methods (ESMs) to detect similarity and reduce or eliminate redundancy when storing objects. In addition, the framework uses a virtualized content addressable store (VCAS) that hides from the user the complexity of knowing which space-efficient techniques are used, including chunk-based deduplication or delta compression. Storing and retrieving objects are polymorphic operations independent of their content-based address. A new technique, harmonic super-fingerprinting, is also used for obtaining successively more accurate (but also more costly) measures of similarity to identify the existing objects in a very large data set that are most similar to an incoming new object.

The PRESIDIO design, when reported earlier, had comprehensively introduced for the first time the notion of deduplication, which is now being offered as a service in storage systems by major vendors. As an aid to the design of such systems, we evaluate and present various parameters that affect the efficiency of a storage system using empirical data.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>



	<item>
	    <title>GPGPU: general purpose computation on graphics hardware</title>
		<abstract>The graphics processor (GPU) on today's commodity video cards has evolved into an extremely powerful and flexible processor. The latest graphics architectures provide tremendous memory bandwidth and computational horsepower, with fully programmable vertex and pixel processing units that support vector operations up to full IEEE floating point precision. High level languages have emerged for graphics hardware, making this computational power accessible. Architecturally, GPUs are highly parallel streaming processors optimized for vector operations, with both MIMD (vertex) and SIMD (pixel) pipelines. Not surprisingly, these processors are capable of general-purpose computation beyond the graphics applications for which they were designed. Researchers have found that exploiting the GPU can accelerate some problems by over an order of magnitude over the CPU.However, significant barriers still exist for the developer who wishes to use the inexpensive power of commodity graphics hardware, whether for in-game simulation of physics of for conventional computational science. These chips are designed for and driven by video game development; the programming model is unusual, the programming environment is tightly constrained, and the underlying architectures are largely secret. The GPU developer must be an expert in computer graphics and its computational idioms to make effective use of the hardware, and still pitfalls abound. This course provides a detailed introduction to general purpose computation on graphics hardware (GPGPU). We emphasize core computational building blocks, ranging from linear algebra to database queries, and review the tools, perils, and tricks of the trade in GPU programming. Finally we present some interesting and important case studies on general-purpose applications of graphics hardware.The course presenters are experts on general-purpose GPU computation from academia and industry, and have presented papers and tutorials on the topic at SIGGRAPH, Graphics Hardware, Game Developers Conference, and elsewhere.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Visualizing geospatial data</title>
		<abstract>This course reviews concepts and highlights new directions in GeoVisualization. We review four levels of integrating geospatial data and geographic information systems (GIS) with scientific and information visualization (VIS) methods. These include:• Rudimentary: minimal data sharing between the GIS and Vis systems• Operational: consistency of geospatial data• Functional: transparent communication between the GIS and Vis systems• Merged: one comprehensive toolkit environmentWe review how to apply both information and scientific visualization fundamentals to the visual display of geospatial and geoinformatics data. Distributed GeoVisualization systems that allow for collaborative synchronous and asynchronous visual exploration and analysis of geospatial data via the Web, Internet, and large-screen group-enabled displays are discussed. This includes the application of intelligent agent and spatial data mining technologies. Case study examples are shown in real time during the course.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>The IT Regulatory and Standards Compliance Handbook:: How to Survive Information Systems Audit and Assessments</title>
		<abstract>This book provides comprehensive methodology, enabling the staff charged with an IT security audit to create a sound framework, allowing them to meet the challenges of compliance in a way that aligns with both business and technical needs. This "roadmap" provides a way of interpreting complex, often confusing, compliance requirements within the larger scope of an organization's overall needs.


Key Features:

* The ulitmate guide to making an effective security policy and controls that enable monitoring and testing against them
* The most comprehensive IT compliance template available, giving detailed information on testing all your IT security, policy and governance requirements
* A guide to meeting the minimum standard, whether you are planning to meet ISO 27001, PCI-DSS, HIPPA, FISCAM, COBIT or any other IT compliance requirement
* Both technical staff responsible for securing and auditing information systems and auditors who desire to demonstrate their technical expertise will gain the knowledge, skills and abilities to apply basic risk analysis techniques and to conduct a technical audit of essential information systems from this book
* This technically based, practical guide to information systems audit and assessment will show how the process can be used to meet myriad compliance issues</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Point-Based Graphics</title>
		<abstract>The polygon-mesh approach to 3D modeling was a huge advance, but today its limitations are clear. Longer render times for increasingly complex images effectively cap image complexity, or else stretch budgets and schedules to the breaking point.

Point-based graphics promises to change all that, and this book explains how. Comprised of contributions from leaders in the development and application of this technology, Point-Based Graphics examines it from all angles, beginning with the way in which the latest photographic and scanning devices have enabled modeling based on true geometry, rather than appearance.

From there, it's on to the methods themselves. Even though point-based graphics is in its infancy, practitioners have already established many effective, economical techniques for achieving all the major effects associated with traditional 3D Modeling and rendering. You'll learn to apply these techniques, and you'll also learn how to create your own. The final chapter demonstrates how to do this using Pointshop3D, an open-source tool for developing new point-based algorithms. A copy of this tool can be found on the companion website.



The first book on a major development in graphics by the pioneers in the field
* This technique allows 3D images to be manipulated as easily as Photoshop works with 2D images
* Includes CD-ROM with the open source software program Pointshop3D for experimentation with point graphics</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Communications of the ACM: Volume 54 Issue 8</title>
		<abstract>An abstract is not available.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Level set and PDE methods for computer graphics</title>
		<abstract>Level set methods, an important class of partial differential equation (PDE) methods, define dynamic surfaces implicitly as the level set (iso-surface) of a sampled, evolving nD function. The course begins with preparatory material that introduces the concept of using partial differential equations to solve problems in computer graphics, geometric modeling and computer vision. This will include the structure and behavior of several different types of differential equations, e.g. the level set equation and the heat equation, as well as a general approach to developing PDE-based applications. The second stage of the course will describe the numerical methods and algorithms needed to actually implement the mathematics and methods presented in the first stage. The course closes with detailed presentations on several level set/PDE applications, including image/video inpainting, pattern formation, image/volume processing, 3D shape reconstruction, image/volume segmentation, image/shape morphing, geometric modeling, anisotropic diffusion, and natural phenomena simulation.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Projectors: advanced graphics and vision techniques</title>
		<abstract>An abstract is not available.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Data fusion</title>
		<abstract>The development of the Internet in recent years has made it possible and useful to access many different information systems anywhere in the world to obtain information. While there is much research on the integration of heterogeneous information systems, most commercial systems stop short of the actual integration of available data. Data fusion is the process of fusing multiple records representing the same real-world object into a single, consistent, and clean representation.

This article places data fusion into the greater context of data integration, precisely defines the goals of data fusion, namely, complete, concise, and consistent data, and highlights the challenges of data fusion, namely, uncertain and conflicting data values. We give an overview and classification of different ways of fusing data and present several techniques based on standard and advanced operators of the relational algebra and SQL. Finally, the article features a comprehensive survey of data integration systems from academia and industry, showing if and how data fusion is performed in each.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Massive model visualization techniques: course notes</title>
		<abstract>Interactive display and visualization of large geometric and textured models is becoming a fundamental capability. There are numerous application areas, including games, movies, CAD, virtual prototyping, and scientific visualization. One of observations about geometric models used in interactive applications is that their model complexity continues to increase because of fundamental advances in 3D modeling, simulation, and data capture technologies. As computing power increases, users take advantage of the algorithmic advances and generate even more complex models and datasets. Therefore, there are many cases where we are required to visualize massive models that consist of hundreds of millions of triangles and, even, billions of triangles. However, interactive visualization and handling of such massive models still remains a challenge in computer graphics and visualization. In this monograph we discuss various techniques that enable interactive visualization of massive models. These techniques include visibility computation, simplification, levels-of-detail, and cache-coherent data management. We believe that the combinations of these techniques can make it possible to interactively visualize massive models in commodity hardware.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Communications of the ACM: Volume 53 Issue 11</title>
		<abstract>An abstract is not available.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Real-time volume graphics</title>
		<abstract>The tremendous evolution of programmable graphics hardware has made high-quality real-time volume graphics a reality. In addition to the traditional application of rendering volume data in scientific visualization, the interest in applying these techniques for real-time rendering of atmospheric phenomena and participating media such as fire, smoke, and clouds is growing rapidly. This course covers both applications in scientific visualization, e.g., medical volume data, and real-time rendering, such as advanced effects and illumination in computer games, in detail. Course participants will learn techniques for harnessing the power of consumer graphics hardware and high-level shading languages for real-time rendering of volumetric data and effects. Beginning with basic texture-based approaches including hardware ray casting, the algorithms are improved and expanded incrementally, covering local and global illumination, scattering, pre-integration, implicit surfaces and non-polygonal isosurfaces, transfer function design, volume animation and deformation, dealing with large volumes, high-quality volume clipping, rendering segmented volumes, higher-order filtering, and non-photorealistic volume rendering. Course participants are provided with documented source code covering details usually omitted in publications.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Building Mashups by Demonstration</title>
		<abstract>The latest generation of WWW tools and services enables Web users to generate applications that combine content from multiple sources. This type of Web application is referred to as a mashup. Many of the tools for constructing mashups rely on a widget paradigm, where users must select, customize, and connect widgets to build the desired application. While this approach does not require programming, the users must still understand programming concepts to successfully create a mashup. As a result, they are put off by the time, effort, and expertise needed to build a mashup. In this article, we describe our programming-by-demonstration approach to building mashup by example. Instead of requiring a user to select and customize a set of widgets, the user simply demonstrates the integration task by example. Our approach addresses the problems of extracting data from Web sources, cleaning and modeling the extracted data, and integrating the data across sources. We implemented these ideas in a system called Karma, and evaluated Karma on a set of 23 users. The results show that, compared to other mashup construction tools, Karma allows more of the users to successfully build mashups and makes it possible to build these mashups significantly faster compared to using a widget-based approach.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Building Intelligent Interactive Tutors: Student-centered strategies for revolutionizing e-learning</title>
		<abstract>Computers have transformed every facet of our culture, most dramatically communication, transportation, finance, science, and the economy. Yet their impact has not been generally felt in education due to lack of hardware, teacher training, and sophisticated software. Another reason is that current instructional software is neither truly responsive to student needs nor flexible enough to emulate teaching. The more instructional software can reason about its own teaching process, know what it is teaching, and which method to use for teaching, the greater is its impact on education. 

Building Intelligent Interactive Tutors discusses educational systems that assess a student's knowledge and are adaptive to a student's learning needs. Dr. Woolf taps into 20 years of research on intelligent tutors to bring designers and developers a broad range of issues and methods that produce the best intelligent learning environments possible, whether for classroom or life-long learning. The book describes multidisciplinary approaches to using computers for teaching, reports on research, development, and real-world experiences, and discusses intelligent tutors, web-based learning systems, adaptive learning systems, intelligent agents and intelligent multimedia.

*Combines both theory and practice to offer most in-depth and up-to-date treatment of intelligent tutoring systems available
*Presents powerful drivers of virtual teaching systems, including cognitive science, artificial intelligence, and the Internet
*Features algorithmic material that enables programmers and researchers to design building components and intelligent systems</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Cumulvs: Interacting with High-Performance Scientific Simulations, for Visualization, Steering and Fault Tolerance</title>
		<abstract>High-performance computer simulations are an increasingly popular alternative or complement to physical experiments or prototypes. However, as these simulations grow more massive and complex, it becomes challenging to monitor and control their execution. CUMULVS is a middleware infrastructure for visualizing and steering scientific simulations while they are running. Front-end "viewers" attach dynamically to simulation programs, to extract and collect intermediate data values, even if decomposed over many parallel tasks. These data can be graphically viewed or animated in a variety of commercial or custom visualization environments using a provided viewer library. In response to this visual feedback, scientists can "close the loop" and apply interactive control using computational steering of any user-defined algorithmic or model parameters. The data identification interfaces and gathering protocols can also be applied for parallel data exchange in support of coupled simulations, and for application-directed collection of key program data in checkpoints, for automated restart in response to software or hardware failures. CUMULVS was originally based on PVM, but interoperates well with simulations that use MPI or other parallel environments. Several alternate messaging systems are being integrated with CUMULVS to ease its applicability, e.g. to MPI. CUMULVS has recently been integrated with the Common Component Architecture (CCA) for visualization and parallel data redistribution (referred to as "MxN"), and also with Global Arrays. This paper serves as a comprehensive overview of the CUMULVS capabilities, their usage, and their development over several years.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Centralized and Distributed Anonymization for High-Dimensional Healthcare Data</title>
		<abstract>Sharing healthcare data has become a vital requirement in healthcare system management; however, inappropriate sharing and usage of healthcare data could threaten patients’ privacy. In this article, we study the privacy concerns of sharing patient information between the Hong Kong Red Cross Blood Transfusion Service (BTS) and the public hospitals. We generalize their information and privacy requirements to the problems of centralized anonymization and distributed anonymization, and identify the major challenges that make traditional data anonymization methods not applicable. Furthermore, we propose a new privacy model called LKC-privacy to overcome the challenges and present two anonymization algorithms to achieve LKC-privacy in both the centralized and the distributed scenarios. Experiments on real-life data demonstrate that our anonymization algorithms can effectively retain the essential information in anonymous data for data analysis and is scalable for anonymizing large datasets.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Perceptually-motivated graphics, visualization and 3D displays</title>
		<abstract>This course presents timely, relevant examples on how researchers have leveraged perceptual information for optimization of rendering algorithms, to better guide design and presentation in (3D stereoscopic) display media, and for improved visualization of complex or large data sets. Each presentation will provide references and short overviews of cutting-edge current research pertaining to that area. We will ensure that the most up-to-date research examples are presented by sourcing information from recent perception and graphics conferences and journals such as ACM Transactions on Perception, paying particular attention work presented at the 2010 Symposium on Applied Perception in Graphics and Visualization.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Communications of the ACM: Volume 55 Issue 4</title>
		<abstract>An abstract is not available.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>The application of data mining techniques to characterize agricultural soil profiles</title>
		<abstract>The advances in computing and information storage have provided vast amounts of data. The challenge has been to extract knowledge from this raw data; this has lead to new methods and techniques such as data mining that can bridge the knowledge gap. This research aimed to assess these new data mining techniques and apply them to a soil science database to establish if meaningful relationships can be found.

A large data set extracted from the WA Department of Agriculture and Food (AGRIC) soils database has been used to conduct this research. The database contains measurements of soil profile data from various locations throughout the south west agricultural region of Western Australia. The research establishes whether meaningful relationships can be found in the soil profile data at different locations. In addition, comparison was made between current data mining techniques such as cluster analysis and statistical methods to establish the most effective technique. The outcome of the research may have many benefits, to agriculture, soil management and environmental</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Introduction to linked data and its lifecycle on the web</title>
		<abstract>With Linked Data, a very pragmatic approach towards achieving the vision of the Semantic Web has recently gained much traction. The term Linked Data refers to a set of best practices for publishing and interlinking structured data on the Web. While many standards, methods and technologies developed within by the Semantic Web community are applicable for Linked Data, there are also a number of specific characteristics of Linked Data, which have to be considered. In this article we introduce the main concepts of Linked Data. We present an overview of the Linked Data lifecycle and discuss individual approaches as well as the state-of-the-art with regard to extraction, authoring, linking, enrichment as well as evolution of Linked Data. We conclude the chapter with a discussion of issues, limitations and further research and development challenges of Linked Data.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Communications of the ACM: Volume 52 Issue 10</title>
		<abstract>An abstract is not available.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Learning correlations using the mixture-of-subsets model</title>
		<abstract>Using a mixture of random variables to model data is a tried-and-tested method common in data mining, machine learning, and statistics. By using mixture modeling it is often possible to accurately model even complex, multimodal data via very simple components. However, the classical mixture model assumes that a data point is generated by a single component in the model. A lot of datasets can be modeled closer to the underlying reality if we drop this restriction. We propose a probabilistic framework, the mixture-of-subsets (MOS) model, by making two fundamental changes to the classical mixture model. First, we allow a data point to be generated by a set of components, rather than just a single component. Next, we limit the number of data attributes that each component can influence. We also propose an EM framework to learn the MOS model from a dataset, and experimentally evaluate it on real, high-dimensional datasets. Our results show that the MOS model learned from the data represents the underlying nature of the data accurately.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Communications of the ACM: Volume 54 Issue 7</title>
		<abstract>An abstract is not available.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>Data mining for web personalization</title>
		<abstract>In this chapter we present an overview of Web personalization process viewed as an application of data mining requiring support for all the phases of a typical data mining cycle. These phases include data collection and pre-processing, pattern discovery and evaluation, and finally applying the discovered knowledge in real-time to mediate between the user and the Web. This view of the personalization process provides added flexibility in leveraging multiple data sources and in effectively using the discovered models in an automatic personalization system. The chapter provides a detailed discussion of a host of activities and techniques used at different stages of this cycle, including the preprocessing and integration of data from multiple sources, as well as pattern discovery techniques that are typically applied to this data. We consider a number of classes of data mining algorithms used particularly forWeb personalization, including techniques based on clustering, association rule discovery, sequential pattern mining, Markov models, and probabilistic mixture and hidden (latent) variable models. Finally, we discuss hybrid data mining frameworks that leverage data from a variety of channels to provide more effective personalization solutions.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>


	<item>
	    <title>A labeled-tree approach to semantic and structural data interoperability applied in hydrology domain</title>
		<abstract>The issues of data integration and interoperability pose significant challenges in scientific hydrological and environmental studies, due largely to the inherent semantic and structural heterogeneities of massive datasets and non-uniform autonomous data sources. To address these data integration challenges, we propose a unified data integration framework, called Hydrological Integrated Data Environment (HIDE). HIDE is based on a labeled-tree data integration model referred to as DataNode tree. Using this framework, characteristics of datasets gathered from diverse data sources - with different logical and access organizations - can be extracted and classified as Time-Space-Attribute (TSA) labels and are subsequently arranged in a DataNode tree. The uniqueness of our approach is that it effectively combines the semantic aspects of the scientific domain with diverse datasets having different logical organizations to form a unified view. Further, we also adopt a metadata-based approach for specifying the TSA-DataNode tree in order to achieve flexibility and extensibility. The search engine of our HIDE prototype system evaluates a simple user query systematically on the TSA-DataNode tree, presenting integrated results in a standardized format that facilitates both effective and efficient data integration.</abstract>
		<search_task_number>13</search_task_number>
		<query>data visualize display dataset</query>
		<relevance>0</relevance>
	</item>

﻿

	<item>
		<title>The impact on musculoskeletal system during multitouch tablet interactions</title>
		<abstract>HCI researchers and technologists have heralded multitouch interaction as the technology to drive computing systems into the future. However, as we move towards a world where interaction is based on human body movements that are not well documented or studied, we face a serious and a grave risk of creating technology and systems that may lead to musculoskeletal disorders (MSD's). Designers need to be empowered with objective data on the impact of multitouch interactions on the musculoskeletal system to make informed choices in interaction design. In this paper we present an experiment that documents kinematic (movement) and kinetic measures (EMG) when interacting with a multitouch tablet. Results show that multitouch interaction can induce significant stress that may lead to MSDs and care must be taken when designing multitouch interaction.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>User-centered development of UI elements for selecting items on a digital map designed for heavy rugged tablet PCs in mass casualty incidents</title>
		<abstract>In a Mass Casualty Incident (MCI) time and good management are critical. Currently, the first arriving rescue units perform the triage algorithm on paper instead of a mobile device. By using mobile devices, the patients' triage state and position can be instantly shared through a network. We implemented a map application to visualize this data on a rugged tablet PC that is intended to be used by the Ambulant Incident Officer (AIO). Even though, using a mobile device offers more benefits, it also requires some mental efforts from the user. The goal of the SpeedUp project1 is to ensure the speed-up of the rescue process. It is crucial to carefully develop, introduce and evaluate the User Interface (UI) iteratively close to the target group and adapt it to an MCI situation. Thus, multiple UI concepts have been developed to compare, rate and optimize them. This paper represents a follow up study and focuses on approaches to select patients on a digital map displayed on a heavy rugged tablet PC. An evaluation is performed to estimate how intuitive, efficient, and ergonomic the UI is without the need of special training for the target group and to increase the acceptance of new devices.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Ergonomic requirements for input devices</title>
		<abstract>The aim of this literature research was to gather information on ergonomic requirements for input devices that are provided by investigations applying biomechanical criteria. Firstly, international and national standards, guidelines as well as checklist of this topic had been looked for and their propositions were summarised. Secondly, a query on Internet search engines and databases had been conducted. A ranking system for the selected articles had been installed in order to comprehensibly rate the information obtained from each study. For every regarded input device, i.e. keyboard, mouse, trackball, graphic tablet/stylus and additionally forearm/wrist support, biomechanically based assessment parameters were deducted and outlined. Finally, these findings were discussed with respect to the recommendations of the standards and an overall ergonomic design of office workplaces with VDTs. In conclusion, this will lead to the development of a checklist for keyboards and mice that should be evaluated by occupational health practitioner.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Consciousness, Agents and the Knowledge Game</title>
		<abstract>This paper has three goals. The first is to introduce the "knowledge game", a new, simple and yet powerful tool for analysing some intriguing philosophical questions. The second is to apply the knowledge game as an informative test to discriminate between conscious (human) and conscious-less agents (zombies and robots), depending on which version of the game they can win. And the third is to use a version of the knowledge game to provide an answer to Dretske's question "how do you know you are not a zombie?".</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Algorithm visualization using concept keyboards</title>
		<abstract>Software visualization and algorithm animation are topics of growing interest in computer science education. We propose a new interface to support the interaction between learners and the dynamic data structure of standard algorithms. Based on their source code users generate and configure so-called concept keyboards to explore the data structures and to execute the methods of the algorithms. This access is in contrast with a simple step interface which allows visualization of the steps of the algorithm in the right order. The results of the various evaluation phases are given. At this moment a larger study is being undertaken with undergraduates which focuses on the usability of the keyboard.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Eye-based head gestures</title>
		<abstract>A novel method for video-based head gesture recognition using eye information by an eye tracker has been proposed. The method uses a combination of gaze and eye movement to infer head gestures. Compared to other gesture-based methods a major advantage of the method is that the user keeps the gaze on the interaction object while interacting. This method has been implemented on a head-mounted eye tracker for detecting a set of predefined head gestures. The accuracy of the gesture classifier is evaluated and verified for gaze-based interaction in applications intended for both large public displays and small mobile phone screens. The user study shows that the method detects a set of defined gestures reliably.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Using paper mockups for evaluating soft keyboard layouts</title>
		<abstract>Five experiments were conducted to compare soft keyboard layouts. The methodology involved paper mockups and manual timing in a classroom situation. Students worked in pairs - one as experimenter, one as participant - swapping roles midway through the experiment. Participants used a stylus to tap the well-known "quick brown fox" phrase five times on each layout. Entry speeds, computed from the measured time to enter the phrase, were 26.5 to 34.5 wpm for the Qwerty keyboard layout, 12.3 to 14.7 wpm for the Opti layout, 15.7 wpm for the Fitaly layout, 12.1 and 12.3 for a Qwerty-Phone (QP) hybrid layout, and 19.0 to 23.0 wpm for the standard phone keypad layout. The merits and limitations of the evaluation method are discussed.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Rehabilitation of handwriting skills in stroke patients using interactive games: a pilot study</title>
		<abstract>This paper describes an interactive application that aims to support the rehabilitation of handwriting skills in people that suffer from paralysis after a stroke. The purpose of the application is to make the rehabilitation of handwriting skills fun and engaging. Four platform-independent games with adjustable levels of difficulty were created in order to target varying levels of skills. The application also features a performance history, audio-visual feedback, and posture reminders. It was evaluated with medical staff and patients from the Hoensbroeck Rehabilitation Centre in the Netherlands. The initial results indicated that the games are more motivating and fun than traditional pen and paper exercises. The feedback received from therapists supports our claim that the games are a useful addition to the rehabilitation of handwriting.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>HCI Beyond the GUI: Design for Haptic, Speech, Olfactory, and Other Nontraditional Interfaces</title>
		<abstract>As technology expands and evolves, one-dimensional, graphical user interface (GUI) design becomes increasingly limiting and simplistic. Designers must meet the challenge of developing new and creative interfaces that adapt to meet human needs and technological trends. HCI Beyond the GUI provides designers with this know how by exploring new ways to reach users that involve all of the human senses. Dr. Kortum gathers contributions from leading human factors designers to present a single reference for professionals, researchers, and students.
			Explores the human factors involved in the design and implementation of the nontraditional interfaces, detailing design strategies, testing methodologies, and implementation techniques
			Provides an invaluable resource for practitioners who design interfaces for children, gamers and users with accessibility needs
			Offers extensive case studies, examples and design guidelines</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Mobile Malware Attacks and Defense</title>
		<abstract>Malware has gone mobile, and the security landscape is changing quickly with emerging attacks on cell phones, PDAs, and other mobile devices. This first book on the growing threat covers a wide range of malware targeting operating systems like Symbian and new devices like the iPhone. Examining code in past, current, and future risks, protect your banking, auctioning, and other activities performed on mobile devices.
			* Visual Payloads
			View attacks as visible to the end user, including notation of variants.
			* Timeline of Mobile Hoaxes and Threats
			Understand the history of major attacks and horizon for emerging threates.
			* Overview of Mobile Malware Families
			Identify and understand groups of mobile malicious code and their variations.
			* Taxonomy of Mobile Malware
			Bring order to known samples based on infection, distribution, and payload strategies.
			* Phishing, SMishing, and Vishing Attacks
			Detect and mitigate phone-based phishing (vishing) and SMS phishing (SMishing) techniques.
			* Operating System and Device Vulnerabilities
			Analyze unique OS security issues and examine offensive mobile device threats.
			* Analyze Mobile Malware
			Design a sandbox for dynamic software analysis and use MobileSandbox to analyze mobile malware.
			* Forensic Analysis of Mobile Malware
			Conduct forensic analysis of mobile devices and learn key differences in mobile forensics.
			* Debugging and Disassembling Mobile Malware
			Use IDA and other tools to reverse-engineer samples of malicious code for analysis.
			* Mobile Malware Mitigation Measures
			Qualify risk, understand threats to mobile assets, defend against attacks, and remediate incidents.

			* Understand the History and Threat Landscape of Rapidly Emerging Mobile Attacks
			* Analyze Mobile Device/Platform Vulnerabilities and Exploits
			* Mitigate Current and Future Mobile Malware Threats</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Increasing the expressive power of task analysis: Systematic comparison and empirical assessment of tool-supported task models</title>
		<abstract>Task analysis is a critical step in the design process of interactive systems. The large set of task models available today may lead to the assumption that this step is well supported. However, very few task models are tool-supported. And in this latter category, few of them are based on a clear semantics (in this article, the word semantics is used with the following definition: ''the meaning of a word, phrase, sentence, or text'' from Compact Oxford English Dictionary(R)). This paper focuses on tool-supported task models and provides an assessment of the features that have been considered as essential in task modelling. It compares the different tool-supported methods, and evaluates the actual use of these features in K-MADe, a tool aimed at contributing to the incorporation of ergonomics into the design process of interactive systems through activity and task analysis. The originality of the K-MADe tool is to be based on a model whose expressive power lies on computable syntax while trying to be usable by every modelling knowledge designer. This facilitates task description and analysis, but also model query and the migration within software engineering models and software lifecycle steps. Evaluation results demonstrate the usefulness of an increased expressive power for task models, and their acceptance by users. They also enlighten some weaknesses in the K-MAD method and suggest further improvements. </abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Principles and grand challenges for the future: a prospectus for the computer-supported collaborative learning (CSCL) community</title>
		<abstract>Six principles of future learning environments have emerged from the CSCL research community. These include: greater "sightlines" into learner, teacher and peer cognition; an increasingly salient role for modeling; increased connectivity between people, concurrent with a greater sense of individualization or "one-to-oneness"; fluid contextual mobility in learning, such as between virtual and real contexts or interoperability of individual, social and machine knowledge forms; and higher interactional bandwidth, or capacity of the environment to mediate meaningful content. Four grand challenges - large, worthy, and difficult tasks should occupy the attention of the CSCL community. Each is a frontier: a more visible and vibrant role for the tools and metaphors of the CSCL community in a troubled era of globalization; means for extending collaboration beyond cognitive models to a broader range of human experience; vitality in learning and collaboration through the life cycle; and unlocking group "flow" in the science of collaboration.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Portable, but not mobile: a study of wireless laptops in the home</title>
		<abstract>We report a qualitative study of the use of physical space and wireless laptops by ten United States households. Although wireless laptops purportedly offer the opportunity and affordances to go "anywhere in the home," laptops were generally used in a small set of particular places rather than moving fluidly through the home: wireless laptops were portable, but not mobile per se We present factors that influence laptop movement in the home. We also present a model of people's use of space in the home, identifying a small set of favored places of long-term use and a larger set of kinetic places used for specific tasks. We discuss how the factors we have identified generally promote use of laptops in favored places and generally discourage use of laptops in kinetic places. We discuss how our findings are relevant to the design of technologies for the home.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Visual search-based design and evaluation of screen magnifiers for older and visually impaired users</title>
		<abstract>The purpose of this study was to achieve the following goals: apply some principles in visual search theory to design and develop a screen magnifier; evaluate if applying such features could increase user performance; and make some recommendations for designing a screen magnifier according to the findings of the study. In order to achieve these goals, a screen magnifier and an experimental tool to present stimulus words were developed. Seventy-two elderly Chinese adults took part in the experiment. The findings indicate that, for a screen magnifier, the overlapping mode is superior to the parallel mode; the yellow-highlighted background is superior to the non-highlighted background; for lower-density text, the dual output mode (visual+auditory) is superior to the mono output mode (visual only), while for the higher-density text, no difference was found between the mono and dual output modes. Based on the findings, several recommendations are made. The overlapping mode should be set as the default working mode. Yellow could be a reasonable choice of background color for a magnifier when the text is black. Auditory output support should be used, and users should be advised to use dual output when the text density is lower and mono output when the text density is higher. </abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Usability, communicability and cultural tourism in interactive systems: trends, economic effects and social impact</title>
		<abstract>We have developed a set of techniques and an analysis methodology aimed at boosting the quality of interactive tourism systems. The details of it will be presented in full and with real examples which have yielded interesting results in the last few years, both from the social and economical point of view, but with a huge wealth of cultural and natural heritage. We will also present a first guidelines to foster tourism in those villages that are willing to promote themselves in the national and international market at a low cost.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Shift: a technique for operating pen-based interfaces using touch</title>
		<abstract>Retrieving the stylus of a pen-based device takes time and requires a second hand. Especially for short intermittent interactions many users therefore choose to use their bare fingers. Although convenient, this increases targeting times and error rates. We argue that the main reasons are the occlusion of the target by the user's finger and ambiguity about which part of the finger defines the selection point. We propose a pointing technique we call Shift that is designed to address these issues. When the user touches the screen, Shift creates a callout showing a copy of the occluded screen area and places it in a non-occluded location. The callout also shows a pointer representing the selection point of the finger. Using this visual feedback, users guide the pointer into the target by moving their finger on the screen surface and commit the target acquisition by lifting the finger. Unlike existing techniques, Shift is only invoked when necessary--over large targets no callout is created and users enjoy the full performance of an unaltered touch screen. We report the results of a user study showing that with Shift participants can select small targets with much lower error rates than an unaided touch screen and that Shift is faster than Offset Cursor for larger targets.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Advances in human-computer interaction: graphics and animation components for interface design</title>
		<abstract>We present an analysis of communicability methodology in graphics and animation components for interface design, called CAN (Communicability, Acceptability and Novelty). This methodology has been under development between 2005 and 2010, obtaining excellent results in cultural heritage, education and microcomputing contexts. In studies where there is a bi-directional interrelation between ergonomics, usability, user-centered design, software quality and the human-computer interaction. We also present the heuristic results about iconography and layout design in blogs and websites of the following countries: Spain, Italy, Portugal and France.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Handheld Augmented Reality for underground infrastructure visualization</title>
		<abstract>In this paper, we present an Augmented Reality (AR) system for aiding field workers of utility companies in outdoor tasks such as maintenance, planning or surveying of underground infrastructure. Our work addresses these issues using spatial interaction and visualization techniques for mobile AR applications and as well as for a new mobile device design. We also present results from evaluations of the prototype application for underground infrastructure spanning various user groups. Our application has been driven by feedback from industrial collaborators in the utility sector, and includes a translation tool for automatically importing data from utility company databases of underground assets.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>The "instructed-teacher": a computer science online learning pedagogical pattern</title>
		<abstract>This paper describes a computer science specific pedagogical pattern that has emerged from conducting a three semester design-research project investigating teaching computing online. The "Instructed-Teacher" pedagogical pattern distinguished itself as an effective interactive strategy for eliciting and developing students' mental models. The pattern is presented and key observations regarding its implementation are shared. The teaching and research context is described in order to assist transferability and inform validity.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>The multimodal presentation dashboard</title>
		<abstract>The multimodal presentation dashboard allows users to control and browse presentation content such as slides and diagrams through a multimodal interface that supports speech and pen input. In addition to control commands (e.g. "take me to slide 10"), the system allows multimodal search over content collections. For example, if the user says "get me a slide about internet telephony," the system will present a ranked series of candidate slides that they can then select among using voice, pen, or a wireless remote. As presentations are loaded, their content is analyzed and language and understanding models are built dynamically. This approach frees the user from the constraints of linear order allowing for a more dynamic and responsive presentation style.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Formally expressing the users' objects world in task models</title>
		<abstract>While past research presents objects as essential entities in task modeling, they are in fact rarely used. In the first tool that truly considered objects, K-MADe, two main reasons may explain this limited use: an incomplete module of object description (in the tool, the expression coverage of the object concept is not wide enough) and the usability problems of K-MADe's specific interface. This paper presents a study on the object expression coverage. From case studies, we identify limitations and we infer modification on the K-MADe object entity definitions. These modifications aim to increase the expressive power of objects.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Creating shared mental models: the support of visual language</title>
		<abstract>Cooperative design involves multiple stakeholders that often hold different ideas of the problem, the ways to solve it, and to its solutions (i.e., mental models; MM). These differences can result in miscommunication, misunderstanding, slower decision making processes, and less chance on cooperative decisions. In order to facilitate the creation of a shared mental model (sMM), visual languages (VL) are often used. However, little scientific foundation is behind this choice. To determine whether or not this gut feeling is justified, a research was conducted in which various stakeholders had to cooperatively redesign a process chain, with and without VL. To determine whether or not a sMM was created, scores on agreement in individual MM, communication, and cooperation were analyzed. The results confirmed the assumption that VL can indeed play an important role in the creation of sMM and, hence, can aid the processes of cooperative design and engineering.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Ergonomic interactive testing in a mixed-reality environment</title>
		<abstract>The field of computer graphics is greatly increasing its overall performance enabling consequently the implementation of most of the product design process phases into virtual environments. The deriving benefits of using virtual practices in product development have been proved intrinsically highly valuable, since they foster the reduction of time to market, process uncertainty, and the translation of most prototyping activities into the virtual environment. In this paper we present the developed platform in mixed environment for ergonomic validation. Specifically we defined a methodology for testing both aspects related to design and ergonomic validation by allowing the tester to interact visually and physically with the car dashboard control devices and related interface by the mean of a rotatory haptic device. By experimental session it has been highlighted that it is possible gathering qualitative data about the design, and finding typical occlusion problems, but also quantitative data can be collected by testing the infotainment interface and the consequent users' distraction during the device use.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Ergonomic interactive testing in a mixed-reality environment</title>
		<abstract>The field of computer graphics is greatly increasing its overall performance enabling consequently the implementation of most of the product design process phases into virtual environments. The deriving benefits of using virtual practices in product development have been proved intrinsically highly valuable, since they foster the reduction of time to market, process uncertainty, and the translation of most prototyping activities into the virtual environment. In this paper we present the developed platform in mixed environment for ergonomic validation. Specifically we defined a methodology for testing both aspects related to design and ergonomic validation by allowing the tester to interact visually and physically with the car dashboard control devices and related interface by the mean of a rotatory haptic device. By experimental session it has been highlighted that it is possible gathering qualitative data about the design, and finding typical occlusion problems, but also quantitative data can be collected by testing the infotainment interface and the consequent users' distraction during the device use.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>A plea for modesty</title>
		<abstract>From time to time a movement arises that promises to save the world, or at least to make it vastly better. The extraordinary achievements of digital computing make it a locus of such movements today. Yet we should be wary; when movements fail they provoke backlash that rejects the more limited gains that they might have afforded. Today "computational thinking" has a considerable following, and I would like to discuss some problems with its discourse. It is too often presented in terms that could be interpreted as arrogant or that are overstated. Its descriptions too often lack appropriate examples, and perhaps as a result, it gets misunderstood in casual writing.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>From task model to wearable computer configuration</title>
		<abstract>This article describes a method for choosing of interaction peripherals for wearable computer in Mobile and Augmented Reality context. This method is based on a transformational process starting by user tasks modeling and their decomposition on user, machine and interaction tasks. Interaction tasks are expressed by interaction atoms (device independent) which are realized by interaction techniques related to interaction devices. A referential of interaction devices helps designers in choosing devices in relation with tasks to be supported and contextual requirements (functional and non-functional). A well organized selection process based on several devices/criteria matrixes allows explicit comparison of configurations in regard with usability criteria.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>XML in formal specification, verification and generation of mobile HCI</title>
		<abstract>Our work is carried out in the framework of a global approach for Human-Computer Interaction design and automatic generation. This approach is based on a formal modeling of the Human-Computer Interaction. We propose a Model Based Design Approach (MBDA). We are concerned with identifying the user tasks and requirements and further with the automatic graphical interface validation and generation. Therefore we use Petri Nets. Indeed, the Petri Nets are very efficient in formal modeling of HCI. Our research focuses on mobile HCI. It aims to analyze the ubiquitous environment using ontology described in OWL2 standard. We face difficulties in modeling ontology in XML using Petri Nets. Thus, it becomes necessary to adopt approaches for manipulation of Petri nets via XML as PNML or XML Nets.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>CornerPen: smart phone is the pen</title>
		<abstract>The use of finger on the touchscreen is one of the most prevalent forms of input on mobile devices. However, due to the size of the finger tip, precise input is difficult and the presence of the finger on the touchscreen can often occlude the content in interaction. In this paper, we propose to install a sensor in the corner of the mobile device (e.g. smart phone) and transform the mobile device into a digital pen for making input for itself or other external devices. The system, dubbed "CornerPen" has many potential advantages in addition to those of the traditional pen-based input (vs. finger based) such as less occlusion, leveraging on tactile memory, and larger interaction surface. We have implemented and experimentally tested the CornerPen against the nominal finger-based touchscreen input system using two tasks, namely, flickbased icon browsing (search) and selection and free-form text input. Our results showed while the subjects did acknowledge the problem of occlusion with finger-based input on the touchscreen, the CornerPen approach still was not particularly effective nor preferred for the intended purpose, i.e. making precise input, and only exhibited comparable performance for simple flick/tab like input actions.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Comparative study between AZERTY-type and K-Hermes virtual keyboards dedicated to users with cerebral palsy</title>
		<abstract>The aim of this paper is to compare two virtual keyboards for people with cerebral palsy; many of these users have difficulty performing actions using their upper limbs due to large numbers of unwanted movements. The first is a classical QWERTY type keyboard, called Clavicom NG. The second is the K-Hermes proposed in this paper. K-Hermes is a reduced and monotape keyboard; its entry principles are inspired by the T9 keyboard. The aim of the experiment is to demonstrate the reduced effort and increased speed of typing with the keyboard suggested for people with Cerebral Palsy.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>IHM et démocratie quand une machine à voter n'est pas utilisable par les seniors</title>
		<abstract>The objective of this paper is to show the ergonomics of voting machines currently used in France and their problems they involve for older voters. Initially we do a quick assessment of problems that have posed so far the various voting systems and the problem of access to technology by older people. We compared the interface of one of these voting computers with a multimodal interface that we have created with some usability guidelines. This comparison was made with users tests carried out on 20 elderly users. The results show a significant exclusion of older people with the voting machine used in France and open a track to develop a computer to vote for those who could not vote under normal conditions.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>État de l'art des méthodes de saisie de données sur dispositifs nomades: typologie des approches</title>
		<abstract>This article presents a state of the art on the data input methods for mobile devices (PDA, mobile phone, tablet PC ...). A typology of the different methods is proposed and some results of evaluation are given.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Professional Android 2 Application Development</title>
		<abstract>Build unique mobile applications with the latest Android SDK Written by an Android authority, this up-to-date resource shows you how to leverage the features of Android 2 to enhance existing products or create innovative new ones. Serving as a hands-on guide to building mobile apps using Android, the book walks you through a series of sample projects that introduces you to Android's new features and techniques. Using the explanations and examples included in these pages, you'll acquire the foundation needed to write compelling mobile applications that use Android, along with the flexibility to quickly adapt to future enhancements. Professional Android 2 Application Development: Reviews Android as a development platform andbest practices for mobile development Provides an in-depth look at the Android application components Details creating layouts and Views to produce compelling resolution independent user interfaces Examines Intents and Content Providers for sharing data Introduces techniques for creating map-based applications and using location-based services such as GPS Looks at how to create and use background Services, Notifications, and Alarms Demonstrates how to create interactive homescreen components Explores the Bluetooth, telephony, and networking APIs Examines using hardware, including the camera and sensors such as the compass and accelerometers Wrox Professional guides are planned and written by working programmers to meet the real-world needs of programmers, developers, and IT professionals. Focused and relevant, they address the issues technology professionals face every day. They provide examples, practical solutions, and expert education in new technologies, all designed to help programmers do a better job. wrox.com Programmer Forums Join our Programmer to Programmer forums to ask and answer programming questions about this book, join discussions on the hottest topics in the industry, and connect with fellow programmers from around the world. Code Downloads Take advantage of free code samples from this book, as well as code samples from hundreds of other books, all ready to use. Read More Find articles, ebooks, sample chapters and tables of contents for hundreds of books, and more reference resources on programming topics that matter to you. Note: CD-ROM/DVD and other supplementary materials are not included as part of eBook file.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Portable tool for finalizing freehand drawings: activity analysis and design requirements</title>
		<abstract>Within a multidisciplinary team of designers, architects and mechanical engineers, and ergonomists, we participate in a research project (ICC) in design and creative interface. This paper describes a participative and iterative approach and reviews the results of field studies involved in the design of a portable tool for finalizing freehand drawings. The results are discussed in terms of Activity Theory and its contribution to this field.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Effectiveness of annotating by hand for non-alphabetical languages</title>
		<abstract>Unlike documents, annotation for multimedia information needs to be input as text, not in the form of symbols such as underlines and circles. This is problematic with keyboard input for non-alphabetical languages, especially the East Asian languages such as Chinese and Japanese, because it is labor intensive and imposes a high cognitive load. This study provides a quantitative analysis of the effectiveness of making annotations by hand during a note-taking task in Japanese. Although the lessons learned from this study come from Japanese text input, they are also generally applicable to other East Asian Languages which use ideographic characters such as Chinese. In our study, we focused on both the ergonomic and cognitive aspects and found that during annotation and note-taking task input by hand is more effective than input by keyboard. Finally, we anatomized the keyboard input problem and discuss it in this paper.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Handheld devices for mobile augmented reality</title>
		<abstract>In this paper, we report on four generations of display-sensor platforms for handheld augmented reality. The paper is organized as a compendium of requirements that guided the design and construction of each generation of the handheld platforms. The first generation, reported in [17]), was a result of various studies on ergonomics and human factors. Thereafter, each following iteration in the design-production process was guided by experiences and evaluations that resulted in new guidelines for future versions. We describe the evolution of hardware for handheld augmented reality, the requirements and guidelines that motivated its construction.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Imaging technologies applied to chronic wounds: a survey</title>
		<abstract>Chronic wounds are a major problem in healthcare worldwide. The assessment and treatment of chronic wounds include monitoring color and size (area or volume) at regular intervals by an expert. This evaluation is often based on qualitative observation and manual measurements of the wound (using a caliper or tracing methods). Over the last two decades, several researchers have focused on developing technologies to assess the clinical improvement of chronic wounds. This article aims to provide a survey on imaging technologies applied to chronic wounds. Their accuracy, precision, reliability, ergonomics and usage are compared. In general terms, the survey aggregates the different methods into 3 groups: planimetric techniques, volumetric techniques and color classification. Finally, a discussion is provided on open topics and what progress needs to be done in this area of research. Among other key points, vision-based technologies for wound assessment should emphasize clinical validation, correlation of clinical findings with quantitative metrics and application to tele-dermatology.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Mobile application for determination of users' text entry speed</title>
		<abstract>The report examines the existing metrics for determination of users' text entry speed. The functional and ergonomic requirements for a mobile application that can perform such task are defined. The realization of a concrete mobile application (m-TESM) is described and appropriate conclusions are made.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Cognitive scaffolding in Web3D learning systems: a case study for form and structure</title>
		<abstract>In this paper, we describe a case study in usability engineering for Web3D learning systems and introduce a new step to the typical methods of the usability design. Pedagogical applications present a challenge to the usual usability engineering process in that the end-users of the system (students) cannot describe the requirements of the system. For this situation, we engage the latest evidence and principles of cognition to help map requirements to information design for an interactive learning system.

			Our system seeks to improve the structural understanding of architects and to teach relationships between form and structure in long-span systems. We provide both explanatory multimedia resources and interactive resources including a Web-based modeling and simulation tool that aids architecture students with better understanding of the relationship between structure and form in design. We describe our design process and the system and examine the qualitative impact of the cognitive ergonomic process. This extra step in the usability design process of mapping expert knowledge to human perception and cognition can increase awareness of the requirements of a learning system and improve the effectiveness of the subsequent design.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Curve: revisiting the digital desk</title>
		<abstract>Current desktop workspace environments consist of a vertical area (e.g., a screen with a virtual desktop) and a horizontal area (e.g., the physical desk). Daily working activities benefit from different intrinsic properties of both of these areas. However, both areas are distinct from each other, making data exchange between them cumbersome. Therefore, we present Curve, a novel interactive desktop environment, which combines advantages of vertical and horizontal working areas using a continous curved connection. This connection offers new ways of direct multi-touch interaction and new ways of information visualization. We describe our basic design, the ergonomic adaptions we made, and discuss technical challenges we met and expect to meet while building and configuring the system.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>The absent interface in design engineering</title>
		<abstract>In order to respond to the difficulties encountered by CAD software applications in really assisting the conceptual designer, we propose a tool that is capable of interpreting design sketches and feeding data to various project evaluators, right from the early phases in the design process. For that purpose, we use the concept of the absent interface, which is the only interface that is compatible with the cognitive process involved in sketching. In this paper, we present the principles of such an interface, illustrated by EsQUIsE, a software prototype for capturing and interpreting architectural sketches, which has been under development for several years.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Investigating teamwork and taskwork in single- and multi-display groupware systems</title>
		<abstract>Multi-display groupware (MDG) systems, which typically comprise both public and personal displays, promise to enhance collaboration, yet little is understood about how they differ in use from single-display groupware (SDG) systems. While research has established the technical feasibility of MDG systems, evaluations have not addressed the question of how users' behave in such environments, how their interface design can impact group behavior, or what advantages they offer for collaboration. This paper presents a user study that investigates the impact of display configuration and software interface design on taskwork and teamwork. Groups of three completed a collaborative optimization task in single- and multi-display environments, under different task interface constraints. Our results suggest that MDG configurations offer advantages for performing individual task duties, whereas SDG conditions offer advantages for coordinating access to shared resources. The results also reveal the importance of ergonomic design considerations when designing co-located groupware systems.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Unleashed: Web tablet integration into the home</title>
		<abstract>To understand how web access from a portable tablet appliance changes the way people use the Internet, MediaOne gave families pen-based tablet computers with a wireless connection to our high-speed data network. We used ethnographic and usability methods to understand how tablets would be integrated into household activities and to define user requirements for such devices. Participants viewed the tablet as conceptually different from a PC. The tablet enabled a high degree of multitasking with household activities, yet flaws in form and function affected use. Results suggest that correctly designed portable Internet appliances will fill a special role in peoples' daily lives, particularly if these devices share information with each other. They will allow spontaneous access to information and communication anywhere.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Design and validation of two-handed multi-touch tabletop controllers for robot teleoperation</title>
		<abstract>Controlling the movements of mobile robots, including driving the robot through the world and panning the robot's cameras, typically requires many physical joysticks, buttons, and switches. Operators will often employ a technique called "chording" to cope with this situation. Much like a piano player, the operator will simultaneously actuate multiple joysticks and switches with his or her hands to create a combination of complimentary movements. However, these controls are in fixed locations and unable to be reprogrammed easily. Using a Microsoft Surface multi-touch table, we have designed an interface that allows chording and simultaneous multi-handed interaction anywhere that the user wishes to place his or her hands. Taking inspiration from the biomechanics of the human hand, we have created a dynamically resizing, ergonomic, and multi-touch controller (the DREAM Controller). This paper presents the design and testing of this controller with an iRobot ATRV-JR robot.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Ergodic HMM-UBM system for on-line signature verification</title>
		<abstract>We propose a novel approach for on-line signature verification based on building HMM user models by adapting an ergodic Universal Background Model (UBM). State initialization of this UBM is driven by a dynamic signature feature. This approach inherits the properties of the GMM-UBM mechanism, such as minimizing overfitting due to scarcity of user training data and allowing a world-model type of likelihood normalization. This system is experimentally compared to a baseline state-of-the-art HMM-based online signature verification system using two different databases: the well known MCYT-100 corpus and a subset of the signature part of the BIOSECURE-DS2 corpus. The HMM-UBM approach obtains promising results, outperforming the baseline HMM-based system on all the experiments.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Interaction techniques for using handhelds and PCs together in a clinical setting</title>
		<abstract>In the present study we compare interaction techniques for using handheld devices together with stationary displays in a hospital setting. A set of prototype implementations were developed and tested for a pre-surgery scenario with pairs of physicians and patients. The participants were asked to rank the interaction techniques in order of preference. The results show highest ranking for a distributed user interface where the GUI elements reside on the handheld and where the stationary display is used for showing media content. An analysis of the factors affecting the usability shows that in addition to GUI usability, the interaction techniques were ranked based on ergonomic and social factors specific to the use situation. The latter include the physicality of the patient bed and how computing devices potentially interrupt the face-to-face communication between physician and patient. The study illustrates how the usability of interaction techniques for ubiquitous computing is affected by the ergonomic and social factors of each specific use context.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Communicating through gestures without visual feedback</title>
		<abstract>Human-machine interaction by gestures helps to improve communication: it utilizes means of communication which are common to humans but alien to machines. We consider ergonomic, functional and semantic issues in gesture-based interfaces without visual feedback. Such situations arise with the unobtrusive usage of wearable devices but, more importantly, with interfaces for visually impaired persons. We consider the latter scenario -- that of blind users -- in view of forthcoming haptic interface technology and investigate properties of gestural interaction models.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>IMPROVE: designing effective interaction for virtual and mixed reality environments</title>
		<abstract>In this paper we present evaluation results of an innovative application designed to make collaborative design review in the architectural and automotive domain more effective. Within IMPROVE, a European research project in the area of advanced displays, we are combining high resolution multi-tile displays, TabletPCs and head-mounted displays with innovative 2D and 3D Interaction Paradigms to better support collaborative mobile mixed reality design reviews. Our research and development is motivated by application scenarios in the automotive domain involving FIAT Elasis from Naples, Italy and in the architectural domain involving Page/Park architects from Glasgow, Scotland. User evaluation took place at Glasgow (UK), Naples (ITA) and Darmstadt (GER), where we tested the integrated IMPROVE prototype application. The tests were conducted based on several heuristics such as ergonomics and psychomotorial factors and they were conducted based on guidelines recommended by ISO 9241 to verify whether the developed interfaces were suitable for the applications scenarios. Evaluation results show that there is a strong demand for more interactive design review systems, allowing users greater flexibility and greater choice of input and visualization modalities as well as their combination.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Intelligent software analyzer design for biotechnological processes</title>
		<abstract>This work presents a technology of artificial intelligence methods-based software analyzers design for biotechnological processes. The design of intelligent software analyzers of two hard-to measure intercellular metabolites, important in the early stage of brewing fermentation - ergosterol and glycogen, is demonstrated by implementing ANFIS and compare it with the artificial neural networks (ANN)-training techniques of Levenberg-Marquardt and Radial Basis Functions-NN (RBFNN). The application of intelligent software analyzers in modern control technologies is discussed.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Touch-display keyboards: transforming keyboards into interactive surfaces</title>
		<abstract>In spite of many advances in GUI workstations, the keyboard has remained limited to text entry and basic command invocation. In this work, we introduce the Touch-Display Keyboard (TDK), a novel keyboard that combines the physical-ergonomic qualities of the conventional keyboard with dynamic display and touch-sensing embedded in each key. The TDK effectively transforms the keyboard into an interactive surface that is seamlessly integrated with the interaction space of GUIs, extending graphical output, mouse interaction and three-state input to the keyboard. This gives rise to an entirely new design space of interaction across keyboard, mouse and screen, for which we provide a first systematic analysis in this paper. We illustrate the emerging design opportunities with a host of novel interaction concepts and techniques, and show how these contribute to expressiveness of GUIs, exploration and learning of keyboard interfaces, and interface customization across graphics display and physical keyboard.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Improving efficiencies and patient safety in healthcare through human factors and ergonomics</title>
		<abstract>Efficiencies and patient safety can be improved through modeling of healthcare systems with consideration of human factors and ergonomics. With focus on predicting individual and system performance, safety and satisfaction can be considered from various perspectives including the clinician, patient, pharmacist and healthcare organization. The basis for predictions are models and hypotheses developed from scientific principles, methods and technology implementation strategies demonstrated, observed and reported through research. This paper provides a brief review and reappraisal of recent articles and book chapters to give some insights into how efficiencies and patient safety can be improved through human factors and ergonomics. Current needs and potential future research are also outlined. Web-resources from the World Health Care Congress, Agency for Healthcare Research &amp; Quality, Institute for Healthcare Improvement, National Institutes of Health, the Veterans Administration, Regenstrief Center for Healthcare Engineering (RCHE) and the Institutional Review Board at Purdue, and recent special broadcasts from C-SPAN and PBS support and motivate further research. In considering the new research, it is proposed that a great deal more focus is needed on the process, methods and structuring of research studies, than on the implementations of a particular healthcare practice, technology or model. Students at cross-disciplinary research centers like RCHE at Purdue University are focusing on developing their ability to evaluate research based on the selected course readings and a "List of 10 ways" to look at a research paper systematically for critical analysis. With foundations in human factors and ergonomics it is expected that new potential healthcare systems engineering related research projects can be considered and developed through the semester projects and built upon as extensions that may lead to theses and dissertations. Recent project examples will be discussed. Research issues will be outlined along various health systems issues as follows: Modeling Systems and Economic Aspects, Healthcare Information Technologies, Electronic Prescribing and Computerized Physician Order Entry Systems, Cognitive Aspects and Human---Computer Interaction, Healthcare Work Analysis &amp; Design, International and Cultural Issues, Human Factors, Ergonomics and Patient Safety, Technology Adoption, Data Collection and Analysis.</abstract>
		<search_task_number>16</search_task_number>
		<query>tablet and ergo*</query>
		<relevance>0</relevance>
	</item>


  <item>
    <title>Use of computers by blind computer science students</title>
	<abstract>The use of computers plays an important role in just about any student's education. Using screen reader software, blind students can perform most of the same operations sighted students do using computers. A screen reader analyzes the state of the computer to determine things like which window currently has focus, which item on a list is highlighted, etc. The software then presents this information using either speech or Braille output. The screen reader will typically also provide extra keyboard shortcuts to allow common operations to be easily performed, and will sometimes present alternative interfaces for certain applications. This allows blind Computer Science students to do online research, develop and test software, read textbooks, and more. This method of access does have some limitations, but the number of limitations is decreasing as time passes.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Model-directed web transactions under constrained modalities</title>
	<abstract>Online transactions (e.g., buying a book on the Web) typically involve a number of steps spanning several pages. Conducting such transactions under constrained interaction modalities as exemplified by small screen handhelds or interactive speech interfaces - the primary mode of communication for visually impaired individuals - is a strenuous, fatigue-inducing activity. But usually one needs to browse only a small fragment of a Web page to perform a transactional step such as a form fillout, selecting an item from a search results list, etc. We exploit this observation to develop an automata-based process model that delivers only the "relevant" page fragments at each transactional step, thereby reducing information overload on such narrow interaction bandwidths. We realize this model by coupling techniques from content analysis of Web documents, automata learning and statistical classification. The process model and associated techniques have been incorporated into Guide-O, a prototype system that facilitates online transactions using speech/keyboard interface (Guide-O-Speech), or with limited-display size handhelds (Guide-O-Mobile). Performance of Guide-O and its user experience are reported.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Cueing for drooling in Parkinson's disease</title>
	<abstract>We present the development of a socially acceptable cueing device for drooling in Parkinson's disease (PD). Sialorrhea, or drooling, is a significant problem associated with PD and has a strong negative emotional impact on those who experience it. Previous studies have shown the potential for managing drooling by using a cueing device. However, the devices used in these studies were deemed unacceptable by their users due to factors such as hearing impairment and social embarrassment. We conducted exploratory scoping work and high fidelity iterative prototyping with people with PD to get their input on the design of a cueing aid and this has given us an insight into challenges that confront users with PD and limit device usability and acceptability. The key finding from working with people with PD was the need for the device to be socially acceptable.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>An overview of cognitive skills and impairments for cognitive work</title>
	<abstract>A practical taxonomy of IST functional requirements is developed here based upon occupational and cognitive factors to consider systematically:• the IST requirements of individuals with different cognitive skills and impairments; and,• the opportunities and solutions that IST developers can generate.The focus is on functionality and accessibility to IST function in the work-place. Whilst vocational factors provide the context of use, cognitive notions proved more useful and precise when considering systematically the range of accessibility solutions that need to be developed in IST. Analysis shows that this taxonomy captures the range of potential accessibility solutions.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Multimodal collaborative handwriting training for visually-impaired people</title>
	<abstract>"McSig" is a multimodal teaching and learning environ-ment for visually-impaired students to learn character shapes, handwriting and signatures collaboratively with their teachers. It combines haptic and audio output to realize the teacher's pen input in parallel non-visual modalities. McSig is intended for teaching visually-impaired children how to handwrite characters (and from that signatures), something that is very difficult without visual feedback. We conducted an evaluation with eight visually-impaired children with a pretest to assess their current skills with a set of character shapes, a training phase using McSig and then a post-test of the same character shapes to see if there were any improvements. The children could all use McSig and we saw significant improvements in the character shapes drawn, particularly by the completely blind children (many of whom could draw almost none of the characters before the test). In particular, the blind participants all expressed enjoyment and excitement about the system and using a computer to learn to handwrite.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Proceedings of the 2008 international cross-disciplinary conference on Web accessibility (W4A)</title>
	<abstract>The World Wide Web (Web) is returning to its origins. Surfers are not just passive readers but content creators. Wikis allow open editing and access, blogs enable personal expression. MySpace, Bebo and Facebook encourage social networking by enabling designs to be 'created' and 'wrapped' around content. Flickr and YouTube are examples of sites that allow sharing of photos, audio and video which through informal taxonomies can be discovered and shared in the most efficient ways possible. Template based tools enable fast professional looking Web content creation using automated placement, with templates for blogging, picture sharing, and social networking. The Web is becoming ever more democratised as a publishing medium, regardless of technical ability. But with this change comes new challenges for accessibility. New tools, new types of content creator --- where does accessibility fit in this process?
The call for participation in W4A 2008 asked you to consider whether the conjugation of authoring tools and user agents represents an opportunity for automatically generated Web Accessibility or yet another problem for Web Accessibility? Will form-based and highly graphical interfaces excluded disabled people from creation, expression and social networking? And what about educating users --- and customers --- in accessible design? How, for example, do we collectively demand that the producers of the next MySpace or Second Life adhere to the W3C Authoring Tool Accessibility Guidelines (ATAG)? What effect will this have on the wider Web? We posed the question: What happens when surfers become authors and designers?
We have collected together an excitingly diverse range of papers for W4A 2008, each contributing in their own way to helping provide an answer to this question. Papers have ranged from topics as diverse as evaluating the accessibility of Wikipedia, one of the most popular user-generated resources on the Web, and considering the accessibility challenges of geo-referenced information often found in user-generated content. We hear about the challenges of raising awareness of accessibility, through experiences of accessibility education in Brazil, the particular challenges of encouraging accessible design to embrace the needs of older Web users, and the challenges of providing appropriate guidance to policymakers and technology developers alike that gives them freedom to provide innovative and holistic accessible Web solutions while building on the technical framework provided by W3C WAI. We also see a continuing focus on Web 2.0; several papers focusing directly on making Web 2.0 technologies as accessible as possible, or adapting assistive technology to cope more effectively with the increasingly interactive behaviour of Web 2.0 Web sites.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>WISE:: a wizard interface supporting enhanced usability</title>
	<abstract>The current state of software which targets older adults' ability to use computers focuses on physical issues while largely ignoring the cognitive issues. As a larger percentage of Americans are considered "old" (60+), the lack of a system tailored to the needs of this age demographic has resulted in a part of the population that is disconnected from the rest of the world. This paper describes WISE, an alternative OS and application UI that specifically targets the cognitive deficits of older adults.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Children with special needs: comparing tactile and tangible interaction</title>
	<abstract>In this paper a comparison of the same computer game with two interaction styles is achieved: through tactile interaction in a digital board or using tangible interaction on a tabletop. Tests were carried out with children with special needs, who have different degrees of disability. The aim of the paper is to compare usability and accessibility of each interface, as well as the experiences of children playing with them. Preliminary results indicate the necessity to provide feedback continuously, both hearing and visual, in order to facilitate the understanding of each task and its progression. In addition, the outcome obtained from a questionnaire show a significant preference for the tangible version of the game.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Vi-bowling: a tactile spatial exergame for individuals with visual impairments</title>
	<abstract>Lack of sight forms a significant barrier to participate in physical activity. Consequently, individuals with visual impairments are at greater risk for developing serious health problems, such as obesity. Exergames are video games that provide physical exercise. For individuals with visual impairments, exergames have the potential to reduce health disparities as they may be safer to play and can be played without the help of others. This paper presents VI Bowling, a tactile/audio exergame that can be played using an inexpensive motion-sensing controller. VI Bowling explores tactile dowsing: a novel technique for performing spatial sensorimotor challenges, which can be used for motor learning. VI Bowling was evaluated with six blind adults. All players enjoyed VI Bowling and the challenge tactile dowsing provided. Players could throw their ball with an average error of 9.76 degrees using tactile dowsing. Participants achieved an average active energy expenditure of 4.61 kJ/Min while playing VI Bowling, which is comparable to walking.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>URC based accessible TV</title>
	<abstract>This paper presents a new architecture to make TV sets accessible for all. Our proposal is based on the ISO/IEC 24752 Universal Remote Console Framework standard. This standard defines an abstract user interface layer called the "user interface socket" and allows the development of pluggable user interfaces for any type of user. Since this architecture is standards based, the development of compatible pluggable interfaces is open to any third party. Besides, a prototype of the proposed architecture has been built. In this prototype, the Universal Control Hub (UCH) has been used as the URC framework implementation, a PC with Windows Vista Media Center has been used as a TV Set-top box, and two pluggable UIs have been developed. These UIs consist of an accessible DHTML page for the visually impaired, and a multimodal client, which allows interaction via touch screen and speech. Finally, these two pluggable UIs have been validated with user groups with special needs and preliminary results show encouraging results.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Isolating the effects of visual impairment: exploring the effect of AMD on the utility of multimodal feedback</title>
	<abstract>This study examines the effects of multimodal feedback on the performance of older adults with an ocular disease, Age-Related Macular Degeneration (AMD), when completing a simple computer-based task. Visually healthy older users (n = 6) and older users with AMD (n = 6) performed a series of drag-and-drop tasks that incorporated a variety of different feedback modalities. The user groups were equivalent with respect to traditional visual function metrics and measured subject cofactors, aside from the presence or absence of AMD. Results indicate that users with AMD exhibited decreased performance, with respect to required feedback exposure time. Some non-visual and multimodal feedback forms show potential as solutions to enhance performance, for those with AMD as well as for visually healthy older adults.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Assistive device for the blind based on object recognition: an application to identify currency bills</title>
	<abstract>We have developed a real-time portable object recognition system based on a bio-inspired image analysis software to increase blind people autonomy by localizing and identifying surrounding objects. A working prototype of this system has been tested on the issue of currency bill recognition encountered by most of the blind people. Seven blind persons were involved in an experiment which demonstrated that the usability of the system was good enough for such a device to be used daily in real-life situations.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Usability and usefulness of GPS based localization technology used in dementia care</title>
	<abstract>Dementia is a chronic brain disease affecting cognitive functioning. People with dementia have a higher risk of getting lost. In recent years GPS based technology has been utilised to locate lost persons with dementia. We interviewed six families using such technology focusing on perceived usability, user-friendliness and usefulness. The informants also completed the QUEST 2.0 questionnaire which measures satisfaction with assistive technology. By and large the informants found the equipment easy to use, and it was viewed by all as being very useful. There were a number of usability issues which adversely affected usage, e.g. system stability, secure fastening, size, user interface issues and varying GPS-reception. The QUEST 2.0 results corresponded with the findings in the interviews. Further usability studies, as well as R&amp;D to address issues such as security and privacy protection and use in the public health sector are needed.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Designing assistive technology for blind users</title>
	<abstract>This project reports on an observational and interview study of a non-sighted person to develop design insights for enhancing interactions between a blind person and everyday technological artifacts found in their home such as wristwatches, cell phones or software applications. Analyzing situations where work-arounds compensate for task failures reveals important insights for future artifact design for the blind such as the value of socialization, tactile and audio feedback, and facilitation of user independence.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Conceptual modeling of service-oriented programmable smart assistive environments</title>
	<abstract>The imperative need of older people to lead a more independent life in their familiar environment can not be addressed only by the vast progress of the information and communication technologies but also modeling efforts are needed that take into serious account these specific needs. In this paper, we describe our approach concerning the conceptual design of a service oriented programmable smart system that integrates assistive environments for the elderly. Specifically, we present the role of the conceptual model emphasizing at its respective meta-model, while we document the design and development of the conceptual model authoring environment for enabling the scenario modeling of the ambient assistive living application domain.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>TAIG: textually accessible information graphics</title>
	<abstract>Information graphics (such as bar charts and line graphs) are an important component of many documents. Unfortunately, these representations present serious access challenges for individuals with sight impairments. This paper describes our ongoing research on the TAIG system which is a part of a larger system whose long term goal is to enable visually impaired users to gain access to the content of information graphics and therefore benefit from these valuable resources. TAIG first provides the user with a brief textual summary of the graphic with the inferred overall message as the core content, and then will respond to follow-up questions which may request further detail about the graphic.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Teaching web development at a distance</title>
	<abstract>Distance education (DE) is proliferating with no signs of slowing down. This paper aims to fill the gap of lacking literature by providing instructional details of teaching a Web development course in the format of asynchronous DE and offering practical instructional strategies. The unique contribution of this paper lies in exemplifying the online delivery of a highly technical course that has been traditionally taught in face-to-face settings, as well as the application of problem-based learning (PBL) methods to DE.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Proceedings of the XV Brazilian Symposium on Multimedia and the Web</title>
	<abstract>WebMedia is an annual event promoted by the Brazilian Computer Society (SBC), and it is the most important forum for researchers and professionals in Multimedia, Hypermedia and Web areas in Brazil. In its 15th edition WebMedia aims to join researchers, students and practitioners, from Brazil and abroad, for discussing problems related to the Symposium topics.
This year WebMedia is held in Fortaleza, Ceará, from October 05 to 07. In this edition WebMedia 2009 is being organized together with three other relevant events: the XXIV Brazilian Symposium on Data Bases (SBBD), the XXIII Brazilian Symposium on software Engineering (SBES) and the VI Brazilian Symposium on Collaborative Systems (SBSC). The topics of interest intersection provided by those symposia is a great opportunity for interchange of ideas and work among researchers, practitioners and students.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The impact of accessibility assessment in macro scale universal usability studies of the web</title>
	<abstract>This paper presents a modelling framework, Web Interaction Environments, to express the synergies and differences of audiences, in order to study universal usability of the Web. Based on this framework, we have expressed the implicit model of WCAG and developed an experimental study to assess the Web accessibility quality of Wikipedia at a macro scale. This has resulted on finding out that template mechanisms such as those provided by Wikipedia lower the burden of producing accessible contents, but provide no guarantee that hyperlinking to external websites maintain accessibility quality. We discuss the black-boxed nature of guidelines such as WCAG and how formalising audiences helps leveraging universal usability studies of the Web at macro scales.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Don't listen! I am dictating my password!</title>
	<abstract>Speech recognition is a promising alternative input technology for individuals with upper-body motor impairments that hinder the use of the standard keyboard and mouse. A recent long-term field study found that the users employed speech techniques for a variety of tasks beyond generating text documents [1]. One challenge with hands-free speech-based interactions is user authentication, which requires the users to speak their user IDs and passwords character by character. Unfortunately, speaking a password presents both security and privacy threats as well as usability problems. To address this challenge, we propose a new speech-based authentication model. An initial proof-of-concept prototype has been implemented and a pilot study was conducted. Preliminary results suggest several problems for further examination.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Supporting children with special needs through multi-perspective behavior analysis</title>
	<abstract>In past years, ubiquitous computing technologies have been successfully deployed for supporting children with special needs. One focus of current research has been on post-hoc behavior analysis based on video footage where one or multiple cameras were used to review situations in which children behaved in a certain way. As miniaturized cameras as well as portable devices are becoming available at low costs, we envision a new quality in supporting the diagnosis, observation, and education of children with special needs. In contrast to existing approaches that use cameras in fix locations, we suggest to use multiple mobile camera perspectives. In this way observation data from fellow classmates, teachers, and caregivers can be considered, even in highly dynamic outdoor situations. In this paper we present MuPerBeAn, a platform that allows multi-perspective video footage from mobile cameras to be collected, synchronously reviewed, and annotated. We report on interviews with caregivers and parents and present a qualitative study based on two scenarios involving a total of seven children with autism (CWA). Our findings show that observing multiple mobile perspectives can help children as well as teachers to better reflect on situations, particularly during education.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Mozilla accessibility on Unix/Linux</title>
	<abstract>Web Accessibility (Ally) has been developed in recent years. Till recent days, no web browser on Unix/Linux has Ally support. This means the powerful Unix and the free Linux operating system is unusable by the disabled. People who had a computer, but could not afford $500 for Microsoft Windows, were unable to use his computer. This was the problem we were trying to address by adding Ally support into Mozilla on Unix/Linux. Mozilla was chosen because it's the most important open source cross-platform web browser and the dominant web browser on Unix/Linux. Although Mozilla is designed to be cross-platform, Ally support is platform-specific. This paper addresses how we solved the problem of architecting Ally support for Mozilla on Unix/Linux.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>AudioNav: a mixed reality navigation system for individuals who are visually impaired</title>
	<abstract>This research is to design a novel indoor navigation tool called AudioNav for users with visual impairment. Current navigation systems rely upon expensive physical augmentation of the environment or expensive sensing equipment. The proposed system is infrastructure-free and low cost as it uses only a virtual representation of a building to be navigated.
Our proposed system: 1) extracts a floor-plan and recognizes landmarks in the three-dimensional model of a building, 2) locates and tracks the user inside the building while there is no GPS reception, 3) finds the most suitable path based on the user's special needs, and provides step-by-step direction to the destination using voice, speech, or haptic feedback.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Vision-based user interfaces for health applications: a survey</title>
	<abstract>This paper proposes a survey of vision-based human computer interfaces for several key-fields in health care: data visualization for image-guided diagnosis, image-guided therapy planning and surgery, the operating room, assistance to motor-impaired patients, and monitoring and support of elderly. The emphasis is on the contribution of the underlying computer vision techniques to the usability and usefullness of interfaces for each specific domain.It is also shown that end-user requirements have a significant impact on the algorithmic design of the computer vision techniques embedded in the interfaces. </abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Human-centered computing: a multimedia perspective</title>
	<abstract>Human-Centered Computing (HCC) is a set of methodologies that apply to any field that uses computers, in any form, in applications in which humans directly interact with devices or systems that use computer technologies. In this paper, we give an overview of HCC from a Multimedia perspective. We describe what we consider to be the three main areas of Human-Centered Multimedia (HCM): media production, analysis, and interaction. In addition, we identify the core characteristics of HCM, describe example applications, and propose a research agenda for HCM.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Dialog generation for voice browsing</title>
	<abstract>In this paper we present our voice browser system, HearSay, which provides efficient access to the World Wide Web to people with visual disabilities. HearSay includes content-based segmentation of Web pages and a speech-driven interface to the resulting content. In our latest version of HearSay, we focus on general-purpose browsing. In this paper we describe HearSay's new dialog interface, which includes several different browsing strategies, gives the user control over the amount of information read out, and contains several different methods for summarizing information in part of a Web page. HearSay selects from its collection of presentation strategies at run time using classifiers trained on human-labeled data.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Towards an evaluation framework for assistive environments</title>
	<abstract>As the world population is aging, there is an increasing need to support independent living of elderly people. Assistive environments incorporate the latest pervasive and ubiquitous technologies and provide a viable alternative to traditional assistive living. In this paper, we propose an evaluation framework to assess the quality of assistive environments. An assistive environment can be successful only if the potential users are willing to adopt it. The proposed framework identifies a set of attributes that are considered critical to user adoption. Sample metrics, as well as possible approaches to measure them, are also suggested to quantify those attributes. The framework is illustrated using an experimental assistive apartment environment that is being built at the University of Texas at Arlington.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Dynamic patterns as an alternative to conventional text for the partially sighted</title>
	<abstract>When the partially sighted can no longer read even enlarged conventional text then possible aids include Touch Braille and speech synthesis. Touch Braille has proved to be very useful for those with no vision and also for the partially sighted who can only read with difficulty and hence liable to suffer from early fatigue. However, both of these aids lack any visual feedback useful for those with some residual sight. Visual Braille may also be of use but is limited to a given set of standard patterns. It should be noted that many partially sighted people may still be able to see a few areas of different color which maybe sufficient to allow different combinations of colored elements to represent a useful character set.The authors propose a possible alternative reading aid called Dynamic Pattern System (DPS) where each pattern or series of patterns can represent an ASCII character. Each pattern consists of a number of distinct areas, each of which can assume one of a set number of colors. A particular set of patterns representing a character set is known as a patternset. There are many conditions that can lead to partial sight and hence these patternsets may be individualised to each user thereby making full use of their limited vision. This can be regarded as an interface matching solution.The input or output of a DPS can be read as standard text by conventional text-based word processors, web browsers and other standard IT equipment. Using a DPS it may be possible for some partially sighted users to communicate with each other each using their own individualised patternset. Furthermore a fully sighted person may be able to both send and receive conventional text when communicating with a person using a DPS. An extensive literature search has failed to find any comparable work.Hence the advantages of such a system include:-&amp;bull; Standard PCs (minimum 386, Windows 3.x, 4M RAM) with no special equipment may be used to encourage the full integration of some partially sighted individuals, who are unable to read even enlarged text, into a standard office environment.&amp;bull; A standard PC running DPS may be used by the fully sighted.&amp;bull; DPS is a very small (one floppy disk) and easy to use software driven user interface requiring no special hardware and hence offers a low cost solution.&amp;bull; Individualized patternsets may allow the most effective use of a user's limited vision.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Verbal associations to tactile patterns: a step towards textured legends in multimodal maps</title>
	<abstract>In this paper we present a pilot study designed to investigate how tactile patterns are spontaneously interpreted by subjects. Participants were confronted to a sample of 40 tactile patterns. The results show strong correlations in the way subjects spontaneously associate verbal description to a clearly identified set of patterns. In terms of application, the study aimed to provide empirical evidence that can support the selection of appropriate tactile patterns in multimodal maps for blind or visually impaired users.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Quality, quality in use, actual usability and user experience as key drivers for web application evaluation</title>
	<abstract>Due to the increasing interest in Web quality, usability and user experience, quality models and frameworks have become a prominent research area as a first step in evaluating them. The ISO 25010/25012 standards were recently issued which specify and evaluate software and data quality requirements. In this work we propose extending the ISO 25010 standard to incorporate new characteristics and concepts into a flexible modeling framework. Particularly, we focus on including information quality, and learnability in use characteristics, and actual usability and user experience concepts into the modeling framework. The resulting models and framework contribute towards a flexible, integrated approach to evaluate Web applications. The operability and particularly the learnability of a real Web application are evaluated using the framework.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Accessibility now!: teaching accessible computing at the introductory level</title>
	<abstract>As ASSETS attendees, we are clearly interested in promoting accessibility in computing. One way to do this is to teach courses on the topic. Most such courses are aimed at upper-level students. But why wait? It's possible to teach accessibility immediately at the introductory level, thereby affecting a greater number of students. I offer a description of a course in computer science that accomplishes this.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Autonomous navigation through the city for the blind</title>
	<abstract>Autonomous navigation in the city has become a necessity for people with visual disabilities, due to the fact that they now enjoy a higher degree of social insertion. As such, several technological solutions seek to assist with this autonomy. In this work, we present a study on the effect of the use of an easy-to-access, audio-based GPS software program on navigation through open spaces, and in particular on the stimulation of orientation and mobility skills in blind people. Results show that the use of the audio-based GPS software allowed blind users to be able to get to various destinations without the need for prior information on the environment, favoring the navigation of blind people in unfamiliar contexts, stimulating the use of different orientation and mobility skills, and finally providing help to users that habitually navigate spaces in the city only in the company of other people.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Digital divide and learning disabilities: counteracting educational exclusion in information society</title>
	<abstract>A 1998 inquiry by the U.S. government showed that 42% of all U.S. households had a computer and 26% regularly used e-mail or more far-reaching Internet-based services. More recent but not fully comparable studies have revealed, that gradually, the originally extremely remarkable differences in Internet use between men and women and young and elderly have decreased, which is probably due primarily to the increasing user-friendliness and controllability of graphical user interfaces. But at present, income-caused, educationally conditioned differences in Internet use are increasing dramatically.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>HMD versus PDA: a comparative study of the user out-of-box experience</title>
	<abstract>The out-of-box experience (OOBE) has been identified as a significant factor contributing to user perception and acceptance of products and technologies. Whilst there has been considerable emphasis placed on formalising methodological procedures for evaluating the OOBE and on the creation of positive user experiences through appropriate interfaces and applications, relatively little work has been undertaken examining how the OOBE is impacted when the experience itself covers a range of (possibly interconnected) devices. In this paper we report the results of an empirical study which examined the OOBE when a Personal Digital Assistant (PDA) and Head Mounted Device (HMD) were configured and then connected for inter-operability purposes. Our findings show that type of device has a considerable impact on the OOBE, with the ask of interconnecting devices having a detrimental effect on the OOBE. The OOBE, however, is in main unaffected by user type and gender.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Customizable keyboard</title>
	<abstract>Customizable Keyboard is an on-screen keyboard designed to be flexible and expandable. Instead of giving the user a keyboard layout Customizable Keyboard allows the user to create a layout that is accommodating to the user's needs. Customizable Keyboard also allows the user to select from a variety of ways to interact with the keyboard including but not limited to using the mouse pointer to select keys and different types of scan based systems. Customizable Keyboard provides more functionality than a typical onscreen keyboard including the ability to control infrared devices such as TVs and send Twitter® Tweets.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Is Wikipedia usable for the blind?</title>
	<abstract>Today wikis are becoming increasingly widespread, and offer great benefits in a variety of collaborative environments. Therefore, to be universally valuable, wiki systems should be easy to use for anyone, regardless of ability. This paper describes obstacles that a blind user may encounter when interacting via screen reader with Wikipedia, and offers some suggestions for improving usability.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>User experience of speech controlled media center for physically disabled users</title>
	<abstract>In this paper, we present results from a long-term user pilot study of speech controlled media center. The pilot users in this case were physically disabled and the system was installed in their apartment for six weeks. We designed a multimodal media center interface based on speech. Full speech control is provided with a hands-free speech recognition input method for people with physical disabilities. In addition, the application includes a zoomable context + focus GUI in tight combination with speech output. The resulting interface was designed following human-centered principles. Finally, the results of user experience studies are presented.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Voter-centered design: Toward a voter decision support system</title>
	<abstract>Electronic voting support systems should not focus only on ballot casting and recording. Instead, a user-centered perspective should be adopted for the design of a system that supports information gathering, organizing and sharing, deliberation, decision making, and voting. Relevant social science literature on political decision making and voting is used to develop requirements. A design concept is presented that supports extended information browsing using combined filtering from ballot materials and voter profiles. The system supports information sharing and participation in electronic dialogues. Voters may interweave information browsing, annotation, contextualized discussion, and ballot markup over extended time periods.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>VTAssist: a location-based feedback notification system for the disabled</title>
	<abstract>The needs of individuals with limited mobility have not been given as much importance as those of other users of internet-accessible mobile devices. Disabled users have information and service needs that are unique and a timely delivery of these services and information can be invaluable. This has motivated us toward the development of VTAssist, an application that attempts to provide critical location-based information about building accessibility to impaired users. This paper discusses our initial implementation of VTAssist and reports on preliminary evaluation results. Our results indicate that our system can help wheelchair users to better understand and navigate around environments.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Automated construction of web accessibility models from transaction click-streams</title>
	<abstract>Screen readers, the dominant assistive technology used by visually impaired people to access the Web, function by speaking out the content of the screen serially. Using screen readers for conducting online transactions can cause considerable information overload, because transactions, such as shopping and paying bills, typically involve a number of steps spanning several web pages. One can combat this overload by using a transaction model for web accessibility that presents only fragments of web pages that are needed for doing transactions. We can realize such a model by coupling a process automaton, encoding states of a transaction, with concept classifiers that identify page fragments "relevant" to a particular state of the transaction. In this paper we present a fully automated process that synergistically combines several techniques for transforming unlabeled click-stream data generated by transactions into a transactionmodel. These techniques include web content analysis to partition a web page into segments consisting of semantically related content, contextual analysis of data surrounding clickable objects in a page, and machine learning methods, such as clustering of page segments based on contextual analysis, statistical classification, and automata learning. The use of unlabeled click streams in building transaction models has important benefits: (i) visually impaired users do not have to depend on sighted users for creating manually labeled training data to construct the models; (ii) it is possible to mine personalized models from unlabeled transaction click-streams associated with sites that visually impaired users visit regularly; (iii) since unlabeled data is relatively easy to obtain, it is feasible to scale up the construction of domain-specific transaction models (e.g., separate models for shopping, airline reservations, bill payments, etc.); (iv) adjusting the performance of deployed models over timtime with new training data is also doable. We provide preliminary experimental evidence of the practical effectiveness of both domain-specific, as well as personalized accessibility transaction models built using our approach. Finally, this approach is applicable for building transaction models for mobile devices with limited-size displays, as well as for creating wrappers for information extraction from web sites.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>WCAG 2.0: a web accessibility standard for the evolving web</title>
	<abstract>Since the Web Content Accessibility Guidelines 1.0 (WCAG) became a W3C recommendation in May 1999, the Web has changed dramatically. This paper describes some of the major issues encountered because of these changes and the approaches developed to address them in WCAG 2.0.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Proceedings of the 2006 international cross-disciplinary workshop on Web accessibility (W4A): Building the mobile web: rediscovering accessibility?</title>
	<abstract>After the launch of the Mobile Web Initiative at the World Wide Web Conference 2005 we are beginning to realise that, today, mobile Web access suffers from interoperability and usability problems that make the Web difficult to use. With the move to small screen size, low bandwidth, and different operating modalities, technology is in effect simulating the sensory and cognitive impairments experienced by disabled users within the wider population of mobile device users. In this our third workshop we ask the question "Is engineering, designing, and building for the mobile Web just a rehash of the same old Web accessibility problems?"These proceedings bring together a cross section of the Web design, accessibility and mobile Web communities. The papers included here report on developments, discuss the issues, and suggest cross-pollinated solutions.Conventional workshops on accessibility tend to be single disciplinary in nature. However, we are concerned that this focus on a single participant group prevents the cross-pollination of ideas, needs, and technologies from other related but separate fields. As with the first, this second workshop is decidedly cross disciplinary in nature and brings together users, accessibility experts, graphic designers, and technologists from academia and industry to discuss how accessibility can be supported. We also encourage the participation of users and other interested parties as an additional balance to the discussion. Our aim is to focus on accessibility by encouraging participation from many disciplines. Views often bridge academia, commerce, and industry and arguments encompass a range of beliefs across the design-accessibility spectrum.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Using interactive objects for speech intervention</title>
	<abstract>Technological advances in physical computing and automatic speech recognition (ASR) have made the development of novel solutions for speech intervention possible. I plan to combine an ASR engine with programmable microcontrollers to develop exercises and activities based on interaction with smart objects for helping with speech therapy intervention for children.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Beyond the handset: designing for wireless communications usability</title>
	<abstract>Service-based wireless devices like wireless telephones require users to interact with aspects of the technology beyond the hardware and software of the handset. By entering into contractual relationships with service-providers, and by using network-based services, users interact with a larger system---one that has social and technological components. The operation of the wireless telephone requires the assimilation of heterogeneous sources of information from the device manufacturer, sales people, customer service representatives, marketing people, and members of the popular media, among others, which can easily confound users' understanding of this new class of technology. Opportunities for usability problems therefore scale beyond the handset, as do opportunities for better design. We report the results of a study of 19 novice wireless phone users who were closely tracked for the first 6 weeks after service acquisition. Taking a technology-as-system analytical approach, we describe the wireless telephony system as four socio-technical components: hardware, software, "netware," and "bizware." This particular organization of the system is intended for the practical application of designing for usability.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Where's my stuff?: design and evaluation of a mobile system for locating lost items for the visually impaired</title>
	<abstract>Finding lost items is a common problem for the visually impaired and is something that computing technology can help alleviate. In this paper, we present the design and evaluation of a mobile solution, called FETCH, for allowing the visually impaired to track and locate objects they lose frequently but for which they do not have a specific strategy for tracking. FETCH uses devices the user already owns, such as their cell phone or laptop, to locate objects around their house. Results from a focus group with visually impaired users informed the design of the system. We then studied the usability of a laptop solution in a laboratory study and studied the usability and usefulness of the system through a one-month deployment and diary study. These studies demonstrate that FETCH is usable and useful, but there is still room for improvement.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Sasayaki: augmented voice web browsing experience</title>
	<abstract>Auditory user interfaces have great Web-access potential for billions of people with visual impairments, with limited literacy, who are driving, or who are otherwise unable to use a visual interface. However a sequential speech-based representation can only convey a limited amount of information. In addition, typical auditory user interfaces lose the visual cues such as text styles and page structures, and lack effective feedback about the current focus. To address these limitations, we created Sasayaki (from whisper in Japanese), which augments the primary voice output with a secondary whisper of contextually relevant information, automatically or in response to user requests. It also offers new ways to jump to semantically meaningful locations. A prototype was implemented as a plug-in for an auditory Web browser. Our experimental results show that the Sasayaki can reduce the task completion times for finding elements in webpages and increase satisfaction and confidence.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A framework for filtering web accessibility guidelines</title>
	<abstract>This paper first presents a framework for filtering the Web Accessibility Guidelines according to contexts of use. It then presents a prototype that implements the framework and explains an evaluation of the prototype.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Capturing phrases for ICU-Talk, a communication aid for intubated intensive care patients.</title>
	<abstract>The need for intubated patients, within the intensive care setting, to communicate more effectively led to the development of ICU-Talk, an augmentative and alternative communication aid. The communication aid contains a database containing both core and patient-specific vocabulary. Many users of communication aids can provide direct input into the vocabulary, but intensive care patients are not in this position. This paper discusses the methods chosen to gather the vocabulary for an intensive care setting.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Video accessibility enhancement for hearing-impaired users</title>
	<abstract>There are more than 66 million people suffering from hearing impairment and this disability brings them difficulty in video content understanding due to the loss of audio information. If the scripts are available, captioning technology can help them in a certain degree by synchronously illustrating the scripts during the playing of videos. However, we show that the existing captioning techniques are far from satisfactory in assisting the hearing-impaired audience to enjoy videos. In this article, we introduce a scheme to enhance video accessibility using a Dynamic Captioning approach, which explores a rich set of technologies including face detection and recognition, visual saliency analysis, text-speech alignment, etc. Different from the existing methods that are categorized as static captioning, dynamic captioning puts scripts at suitable positions to help the hearing-impaired audience better recognize the speaking characters. In addition, it progressively highlights the scripts word-by-word via aligning them with the speech signal and illustrates the variation of voice volume. In this way, the special audience can better track the scripts and perceive the moods that are conveyed by the variation of volume. We implemented the technology on 20 video clips and conducted an in-depth study with 60 real hearing-impaired users. The results demonstrated the effectiveness and usefulness of the video accessibility enhancement scheme.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Model-based accessible user interface generation in ubiquitous environments</title>
	<abstract>This paper presents a system that automatically generates accessible interfaces tailored to the users' capabilities and needs in order to provide them with access to ubiquitous computing environments. The aim is to ensure that people with disabilities are able to use ubiquitous services provided by intelligent machines, such as ATMs and vending machines. The tailored interfaces are generated from a formal description specified by a User Interface Description Language, and based on user and context models represented by ontologies.</abstract>
	<search_task_number>17</search_task_number>
	<query>user interface accessibility disability </query>
	<relevance>1</relevance>
  </item>



  <item>
    <title>Ambient intelligence: A survey</title>
	<abstract>In this article we survey ambient intelligence (AmI), including its applications, some of the technologies it uses, and its social and ethical implications. The applications include AmI at home, care of the elderly, healthcare, commerce, and business, recommender systems, museums and tourist scenarios, and group decision making. Among technologies, we focus on ambient data management and artificial intelligence; for example planning, learning, event-condition-action rules, temporal reasoning, and agent-oriented technologies. The survey is not intended to be exhaustive, but to convey a broad range of applications, technologies, and technical, social, and ethical challenges.</abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Communications of the ACM: Volume 54 Issue 6</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Discovering routines from large-scale human locations using probabilistic topic models</title>
	<abstract>In this work, we discover the daily location-driven routines that are contained in a massive real-life human dataset collected by mobile phones. Our goal is the discovery and analysis of human routines that characterize both individual and group behaviors in terms of location patterns. We develop an unsupervised methodology based on two differing probabilistic topic models and apply them to the daily life of 97 mobile phone users over a 16-month period to achieve these goals. Topic models are probabilistic generative models for documents that identify the latent structure that underlies a set of words. Routines dominating the entire group's activities, identified with a methodology based on the Latent Dirichlet Allocation topic model, include “going to work late”, “going home early”, “working nonstop” and “having no reception (phone off)” at different times over varying time-intervals. We also detect routines which are characteristic of users, with a methodology based on the Author-Topic model. With the routines discovered, and the two methods of characterizing days and users, we can then perform various tasks. We use the routines discovered to determine behavioral patterns of users and groups of users. For example, we can find individuals that display specific daily routines, such as “going to work early” or “turning off the mobile (or having no reception) in the evenings”. We are also able to characterize daily patterns by determining the topic structure of days in addition to determining whether certain routines occur dominantly on weekends or weekdays. Furthermore, the routines discovered can be used to rank users or find subgroups of users who display certain routines. We can also characterize users based on their entropy. We compare our method to one based on clustering using K-means. Finally, we analyze an individual's routines over time to determine regions with high variations, which may correspond to specific events.</abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>1</relevance>
  
  </item>
    <item>
    <title>Proceedings of the 27th international conference extended abstracts on Human factors in computing systems</title>
	<abstract>Welcome to CHI 2009! CHI comprises many events, ranging from archival material stored in the ACM digital library, to transient interactions such as the presentations, panels, and poster discussions, to the many social interactions and activities that make CHI a collegial and intimate experience. All parts are important, but it is the archival material -- especially the papers and notes -- that establishes CHI as the leading academic conference in Human Computer Interaction. Yet there are significant challenges in managing paper and notes within CHI. The HCI field has been very successful at creating new generations of research and practitioners over the years. The many people who are part of this community see CHI as the place to share their knowledge and experiences with others, primarily by publishing and presenting papers and notes. This has stressed the system in many ways. As submissions increase, so do the difficulties in managing the review process, finding good reviewers and other volunteers, matching papers to those competent in the subject matter, deciding which papers to accept or reject, maintaining consistent standards across both paper and notes, and not falling into the trap of overly narrowing our view of what is an 'acceptable' CHI paper.

This year, we introduced several large changes to the CHI papers/notes process to mitigate these challenges, most which will be transparent to attendees. First, we reorganized the CHI program committee into nine topical subcommittees - each a mini program committee - comprising sub-committee chairs (SCs) and various associate chairs (ACs) knowledgeable on the topic. Authors could select the subcommittee that he or she felt could best handle their submission. We did this to improve the match of a submission to AC and ultimately to reviewers, to have more focused and relevant discussions in the program committee meeting, and to minimize the load on individual volunteers. Second, we combined papers and notes, where all were handled in exactly the same way. We did this to ensure a consistent decision standard across both submission types. Third, we introduced contribution types, where each type described a different way that a CHI submission could contribute to the field as well as typical questions such a contribution should address. Authors identified their submission by contribution type, and (hopefully) used the information to help structure their paper. The idea is that we wanted to encourage a broad variety of submissions from authors (rather than 'formula' papers), while also providing guidance to referees by supplying criteria appropriate to the type of contribution the submission was making.

It will likely take several years before the full impact of these changes are known. We know that subcommittees did help us manage the large number of submissions. We also believe there was an overall better match between referees and submissions, and that papers and notes were handled consistently. We don't yet know about the effect of contribution types: this is a cultural change where we are hoping that authors will be more willing to write papers that don't match a particular formula, and that reviewers will be more accepting of those submissions.

Now for the numbers. This year, there were 1130 submissions, comprising 711 full papers and 419 notes. This is the highest number of submissions ever to CHI. Of these, we accepted 24.5%. The papers/notes committee involved 107 volunteers: the 2 co-chairs, 10 sub-committee chairs, and 95 associate chairs (ACs). Each AC managed 10-14 submissions, and personally recruited at least three -- sometimes more -- referees knowledgeable in the paper's topic. Refereeing was through blind review. Each referee returned a recommendation along with a detailed review, and authors had opportunity to rebut these reviews. Additional reviews were sometimes solicited. Almost all program committee members then attended a two day meeting in Boston in December. Rigorous discussions took place at the PC meeting, and the majority of papers were read by a second AC as well. The decision process was highly visible so that the committee could calibrate itself.

Finally, the various committees nominated 5% of the submissions as potential best papers. A separate committee deliberated over these papers, where only 1% of papers and notes received a best paper award. In total, as you will see in the program, 32 papers and four notes were designated as honorable mentions, while seven papers and four notes honored as best papers. Congratulations to all authors who achieved this significant status!</abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 52 Issue 8</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Making by making strange: Defamiliarization and the design of domestic technologies</title>
	<abstract>This article argues that because the home is so familiar, it is necessary to make it strange, or defamiliarize it, in order to open its design space. Critical approaches to technology design are of both practical and social importance in the home. Home appliances are loaded with cultural associations such as the gendered division of domestic labor that are easy to overlook. Further, homes are not the same everywhere---even within a country. Peoples' aspirations and desires differ greatly across and between cultures. The target of western domestic technology design is often not the user, but the consumer. Web refrigerators that create shopping lists, garbage cans that let advertisers know what is thrown away, cabinets that monitor their contents and order more when supplies are low are central to current images of the wireless, digital home of the future. Drawing from our research in the United States, the United Kingdom, and Asia, we provide three different narratives of defamiliarization. A historical reading of American kitchens provides a lens with which to scrutinize new technologies of domesticity, an ethnographic account of an extended social unit in England problematizes taken-for-granted domestic technologies, and a comparative ethnography of the role of information and communication technologies in the daily lives of urban Asia's middle classes reveals the ways in which new technologies can be captured and domesticated in unexpected ways. In the final section of the article, we build on these moments of defamiliarization to suggest a broad set of challenges and strategies for design in the home.</abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>1</relevance>
  
  </item>
    <item>
    <title>Peter J. Denning Interview: August 6-8, 2007; Naval Postgraduate School, Monterey, California</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 55 Issue 3</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 51 Issue 12</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 55 Issue 4</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>1</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 51 Issue 10</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 54 Issue 12</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>How routine learners can support family coordination</title>
	<abstract>Researchers have detailed the importance of routines in how people live and work, while also cautioning system designers about the importance of people's idiosyncratic behavior patterns and the challenges they would present to learning systems. We wish to take up their challenge, and offer a vision of how simple sensing technology could capture and model idiosyncratic routines, enabling applications to solve many real world problems.

To identify how a simple routine learner can demonstrate this in support of family coordination, we conducted six months of nightly interviews with six families, focusing on how they make and execute plans. Our data reveals that only about 40% of events unfold in a routine manner. When deviations do occur, family members often need but do not have access to accurate information about their routines. With about 90% of their content concerning deviations, not routines, families do not rely on calendars to support them during these moments. We discuss how coordination tools, like calendars and reminder systems, would improve coordination and reduce stress when augmented with routine information, and how commercial mobile phones can support the automatic creation of routine models.</abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>1</relevance>
  
  </item>
    <item>
    <title>Of pill boxes and piano benches: "home-made" methods for managing medication</title>
	<abstract>We report on the results of an ethnographic study of how elders manage their medication with the objective of informing the de-sign of in-home assistive health technology to support "medication adherence." We describe the methods by which elders organize and remember to take their medication-methods that leverage a kind of distributed cognition. Elders devise medication manage-ment systems that rely on the spatial features of their homes, the temporal rhythms of their days, as well as the routines that occa-sion these places and times to help recall and prospective remem-bering. We show how mobile health care workers participate in the development and execution of these systems, and "read" them to infer an elder's state of health and ability to manage medication. From this analysis, we present five principles for the design of assistive technology that support the enhanced but on-going use of personalized medication management systems, and that also allow for remote health care assistance as it becomes needed.</abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>1</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Fast detection of communication patterns in distributed executions</title>
	<abstract>Understanding distributed applications is a tedious and difficult task. Visualizations based on process-time diagrams are often used to obtain a better understanding of the execution of the application. The visualization tool we use is Poet, an event tracer developed at the University of Waterloo. However, these diagrams are often very complex and do not provide the user with the desired overview of the application. In our experience, such tools display repeated occurrences of non-trivial communication patterns, appearing throughout the trace data and cluttering the display space. This paper describes an event abstraction facility which tries to simplify the execution visualization shown by Poet by efficiently detecting and abstracting such patterns.A user can define patterns, subject to only very few constraints, and store them in a hierarchical pattern library. We also provide the user with the possibility to annotate the source code as a help in the abstraction process. We detect these communication patterns by employing an enhanced efficient multiple string matching algorithm. The results indicate that the matching process is indeed very fast. A user can experiment with multiple patterns at potentially different levels in the hierarchy, checking for their occurrence in the trace file, while trying to gain some understanding in a short period of time.
</abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 54 Issue 2</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 54 Issue 8</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 54 Issue 5</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 52 Issue 11</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title></title>
	<abstract>Communications of the ACM: Volume 53 Issue 12</abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>interactions: Volume 16 Issue 2</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>1</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 54 Issue 4</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>XRDS: Crossroads, The ACM Magazine for Students: Volume 17 Issue 2</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 54 Issue 10</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>interactions: Volume 15 Issue 4</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Robots in the wild: understanding long-term use</title>
	<abstract>It has long been recognized that novelty effects exist in the interaction with technologies. Despite this recognition, we still know little about the novelty effects associated with domestic robotic appliances and more importantly, what occurs after the novelty wears off. To address this gap, we undertook a longitudinal field study with 30 households to which we gave Roomba vacuuming robots and then observed use over six months. During this study, which spans over 149 home visits, we encountered methodological challenges in understanding households' usage patterns. In this paper we report on our longitudinal research, focusing particularly on the methods that we used 1) to understand human-robot interaction over time despite the constraints of privacy and temporality in the home, and 2) to uncover information when routines became less conscious to the participants themselves.</abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>1</relevance>
  
  </item>
    <item>
    <title>interactions: Volume 17 Issue 6</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 54 Issue 3</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 52 Issue 9</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Charles W. Bachman interview: September 25-26, 2004; Tucson, Arizona</title>
	<abstract>Charles W. Bachman reviews his career. Born during 1924 in Kansas, Bachman attended high school in East Lansing, Michigan before joining the Army Anti Aircraft Artillery Corp, with which he spent two years in the Southwest Pacific Theater, during World War II. After his discharge from the military, Bachman earned a B.Sc. in Mechanical Engineering in 1948, followed immediately by an M.Sc. in the same discipline, from the University of Pennsylvania. On graduation, he went to work for Dow Chemical in Midland, Michigan. Bachman discusses his experiences as an engineer there, focusing on engineering economics issues, including his work on the development of methods for return-on-investment calculations, using punched card machines. In 1957, he was appointed the first head of Dows Central Data Processing department, where he was responsible for preparing the way for the arrival of its first big digital computer. Bachman organized a feasibility study to select a new machine and hired a staff of programmers and analysts. Through the IBM SHARE user group and its 9PAC project, he became involved with Harry Tellier, and Russ McGee in the design of programs to simply the file maintenance and report generation process. This was based on the earlier GE Hanford 702 File Maintenance and Report Generator System, which Bachman discusses in some detail. He also worked on the theory of information with Bill Orchard-Hays and Tom Steel. However, Dow canceled its IBM 709 order before the machine arrived, ending Bachmans association with SHARE and, shortly afterwards, with Dow Chemical.

In 1961, Bachman went to work for General Electric in New York City. He worked for a corporate staff group providing internal consulting services, and was assigned to an integrated systems project to produce a generic manufacturing control system for GE departments, using the firms new GE 225 computer. Here he worked with Stan Williams and Bill Helgeson. This system, MIACS, an early transaction processing system, was built around Bachmans new creation: the IDS (Integrated Data Store). IDS is called the first database management system, and Bachman explains its origin, development, capabilities, design and relationship to MIACS in detail. IDS used a virtual memory database, with a DDL and DML, and introduced the network data model. Bachman created the first data structure diagrams to illustrate the complex data structures required to picture the adaptive manufacturing control logic of MIACS. In 1964, Bachman transferred to GEs Computer Department in Arizona, where he worked on a number of other database related projects including the GE 400 IDS, GE 600 IDS, DataBASIC, a personal data storage system, and WEYCOS 1 and 2, both built for Weyerhaeuser Lumber. The latter two were an attempt to build a complex online management information system around a database. Bachman believes WEYCOS 2 produced the first database management system able to support multiple application programs executing simultaneously against the same database. Bachman also describes his work with Warren Simmons, Bill Olle and others on the CODASYL Data Base Task Group during the late 1960s. The database standards produced by this group were heavily influenced by IDS and by Bachmans ideas. Bachman also discusses the links between IDS and Cullinanes IDMS, a successful commercial system, built to run on the IBM 360 series of computers, which was based on IDS.

When Honeywell acquired GEs computer business in 1970, Bachman worked for the merged operation in Honeywells advanced research group, in Boston. He continued to work on database topics, investigating what he called the role data model and serving on an ANSI-SPARC committee intended to standardize database systems. While failing in this objective, this committee made a highly influential distinction between external, conceptual, and internal (database) schemata. In 1973, Bachman received the ACM Turing award. He recalls his reactions to this, and his subsequent service on committee to select future winners. Bachman was elected a Distinguished Fellow of the British Computer Society in 1977. Bachman also discusses his thoughts on the relational data model and its creator Ted Codd, including a well known 1974 meeting in which he debated Codd on the merits of the two approaches. Bachman also chaired the ISO committee working on Open Systems Interconnection (OSI), well known for its elaboration of a seven layer model for computer communication.

In 1981, Bachman went to work for Cullinane Database Systems, a major software company built around IDMS. While at Cullinane, he supported marketing, and continued the ISO/OSI committee work, while working on tools to model high level schemata for enterprise database design and translate them into data definitions. He also worked on the partnership set data model, which was granted a US patent, and used as the basis for a enhanced network data model and conceptual schema modeling. In 1983, he left to pursue this work at his own firm, Bachman Information Systems. He describes the rise and fall of this company, which obtained venture capital funding, grew rapidly and made a successful IPO. Despite enjoying success with its Data Analyst product with its reverse and forward engineering capabilities, the firm struggled financial and managerially, including an initially successful, but ill-fated, spell as an IBM Business Partner. It eventually merged with Cadre Systems to create Cayenne Software, which was acquired by Sterling Software in 1998, which was subsequently acquired by Computer Associates. Since then Bachman has worked as a database consultant to Constellar Systems and in 2006 was still working with Cbr Systems.</abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 52 Issue 3</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
  <item>
    <title>Communications of the ACM: Volume 53 Issue 10</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 54 Issue 11</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 52 Issue 10</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 53 Issue 2</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>interactions: Volume 18 Issue 3</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 53 Issue 11</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>User evaluation of Físchlár-News: An automatic broadcast news delivery system</title>
	<abstract>Technological developments in content-based analysis of digital video information are undergoing much progress, with ideas for fully automatic systems now being proposed and demonstrated. Yet because we do not yet have robust operational video retrieval systems that can be deployed and used, the usual HCI practise of conducting a usage study and an informed iterative system design is thus not possible. Físchlár-News is one of the first automatic, content-based broadcast news analysis and archival systems that process broadcast news video so that users can search, browse, and play it in an easy-to-use manner with a conventional web browser. The system incorporates a number of state-of-the-art research components, some of which are not yet considered mature technology, yet it has been built to be robust enough to be deployed to users who are interested in access to daily news throughout a university campus. In this article we report and discuss a user-evaluation study conducted with 16 users, each of whom utilized the system freely for a one month period. Results from a detailed qualitative analysis are presented, looking at collected questionnaires, incident diaries, and interaction-log data. The findings suggest that our users employed the system in conjunction with their other news update methods, such as watching TV news at home and browsing online news websites at their workplace, their major concerns being up-to-dateness and coverage of the news content. They tried to accommodate the system to fit their established web browsing habits, and they found local news content and the ability to play self-contained news stories on their desktop as major values of the system. Our study also resulted in a detailed wishlist of new features which will help in the further development of both our and others' systems.</abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Learning patterns of pick-ups and drop-offs to support busy family coordination</title>
	<abstract>Part of being a parent is taking responsibility for arranging and supplying transportation of children between various events. Dual-income parents frequently develop routines to help manage transportation with a minimal amount of attention. On days when families deviate from their routines, effective logistics can often depend on knowledge of the routine location, availability and intentions of other family members. Since most families rarely document their routine activities, making that needed information unavailable, coordination breakdowns are much more likely to occur. To address this problem we demonstrate the feasibility of learning family routines using mobile phone GPS. We describe how we (1) detect pick-ups and drop-offs; (2) predict which parent will perform a future pick-up or drop-off; and (3) infer if a child will be left at an activity. We discuss how these routine models give digital calendars, reminder and location systems new capabilities to help prevent breakdowns, and improve family life.</abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>1</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 52 Issue 12</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 54 Issue 1</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 53 Issue 5</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 53 Issue 3</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>interactions: Volume 17 Issue 5</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 51 Issue 3</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 52 Issue 5</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 51 Issue 9</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 51 Issue 8</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
  
  </item>
    <item>
    <title>Communications of the ACM: Volume 53 Issue 6</title>
	<abstract></abstract>
	<search_task_number>11</search_task_number>
	<query>Artificial Intelligence Help Home Daily Routines</query>
	<relevance>0</relevance>
    </item>



  <item>
    <title>A New Look at Filtering Techniques for Illumination
      Invariance in Automatic Face Recognition
    </title>
    <abstract>Illumination invariance remains the most researched, yet
      the most challenging aspect of automatic face recognition. In this
      paper we propose a novel, general recognition framework for
      efficient matching of individual face images, sets or sequences.
      The framework is based on simple image processing filters that
      compete with unprocessed greyscale input to yield a single
      matching score between individuals. It is shown how the
      discrepancy between illumination conditions between novel input
      and the training data set can be estimated and used to weigh the
      contribution of two competing representations. We describe an
      extensive empirical evaluation of the proposed method on 171
      individuals and over 1300 video sequences with extreme
      illumination, pose and head motion variation. On this challenging
      data set our algorithm consistently demonstrated a dramatic
      performance improvement over traditional filtering approaches. We
      demonstrate a reduction of 50-75% in recognition error rates, the
      best performing method-filter combination correctly recognizing
      96% of the individuals.</abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic 3D face recognition from depth and intensity Gabor
      features
    </title>
    <abstract>As is well known, traditional 2D face recognition based on
      optical (intensity or color) images faces many challenges, such as
      illumination, expression, and pose variation. In fact, the human
      face generates not only 2D texture information but also 3D shape
      information. In this paper, we investigate what contributions
      depth and intensity information makes to face recognition when
      expression and pose variations are taken into account, and we
      propose a novel system for combining depth and intensity
      information to improve face recognition systems. In our system,
      local features described by Gabor wavelets are extracted from
      depth and intensity images, which are obtained from 3D data after
      fine alignment. Then a novel hierarchical selecting scheme
      embedded in linear discriminant analysis (LDA) and AdaBoost
      learning is proposed to select the most effective and most robust
      features and to construct a strong classifier. Experiments are
      performed on the CASIA 3D face database and the FRGC V2.0
      database, two data sets with complex variations, including
      expressions, poses and long time lapses between two scans.
      Experimental results demonstrate the promising performance of the
      proposed method. In our system, all processes are performed
      automatically, thus providing a prototype of automatic face
      recognition combining depth and intensity information.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic face recognition by support vector machines</title>
    <abstract>Automatic face recognition, though being a hard problem,
      has a wide variety of applications. Support vector machine (SVM),
      to which model selection plays a key role, is a powerful technique
      for pattern recognition problems. Recently lots of researches have
      been done on face recognition by SVMs and satisfying results have
      been reported. However, as SVMs model selection details were not
      given, those results might have been overestimated. In this paper,
      we propose a general framework for investigating automatic face
      recognition by SVMs, with which different model selection
      algorithms as well as other important issues can be explored.
      Preliminary experimental results on the ORL face database show
      that, with the proposed hybrid model selection algorithm,
      appropriate SVMs models can be obtained with satisfying
      recognition performance.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Probabilistic learning for fully automatic face recognition
      across pose
    </title>
    <abstract>Recent pose invariant methods try to model the subject
      specific appearance change across pose. For this, however, almost
      all of the existing methods require a perfect alignment between a
      gallery and a probe image. In this paper we present a pose
      invariant face recognition method that does not require the facial
      landmarks to be detected as such and is able to work with only
      single training image of the subject. We propose novel extensions
      by introducing to use a more robust feature description as opposed
      to pixel-based appearances. Using such features we put forward to
      synthesize the non-frontal views to frontal. Furthermore, using
      local kernel density estimation, instead of commonly used normal
      density assumption, is suggested to derive the prior models. Our
      method does not require any strict alignment between gallery and
      probe images which makes it particularly attractive as compared to
      the existing state of the art methods. Improved recognition across
      a wide range of poses has been achieved using these extensions.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>An automatic face recognition system in the near infrared
      spectrum
    </title>
    <abstract>Face recognition is a challenging visual classification
      task, especially when the lighting conditions can not be
      controlled. In this paper, we present an automatic face
      recognition system in the near infrared (IR) spectrum instead of
      the visible band. By making use of the near infrared band, it is
      possible for the system to work under very dark visual
      illumination conditions. A simple hardware enables efficient eye
      localization, thus the face can be easily detected based on the
      position of the eyes. This system exploits the feature extraction
      capabilities of the Discrete Cosine Transform (DCT) which can be
      calculated very fast. Support Vector Machines (SVMs) are used for
      classification. The effectiveness of our system is verified by
      experimental results.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>An Efficient Multimodal 2D-3D Hybrid Approach to Automatic
      Face Recognition
    </title>
    <abstract>We present a fully automatic face recognition algorithm
      and demonstrate its performance on the FRGC v2.0 data. Our
      algorithm is multimodal (2D and 3D) and performs hybrid
      (feature-based and holistic) matching in order to achieve
      efficiency and robustness to facial expressions. The pose of a 3D
      face along with its texture is automatically corrected using a
      novel approach based on a single automatically detected point and
      the Hotelling transform. A novel 3D Spherical Face Representation
      (SFR) is used in conjunction with the SIFT descriptor to form a
      rejection classifier which quickly eliminates a large number of
      candidate faces at an early stage for efficient recognition in
      case of large galleries. The remaining faces are then verified
      using a novel region-based matching approach which is robust to
      facial expressions. This approach automatically segments the
      eyes-forehead and the nose regions, which are relatively less
      sensitive to expressions, and matches them separately using a
      modified ICP algorithm. The results of all the matching engines
      are fused at the metric level to achieve higher accuracy. We use
      the FRGC benchmark to compare our results to other algorithms
      which used the same database. Our multimodal hybrid algorithm
      performed better than others by achieving 99.74% and 98.31%
      verification rates at 0.001 FAR and identification rates of 99.02%
      and 95.37% for probes with neutral and non-neutral expression
      respectively.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Method of automatic face recognition based on fuzzy multiple
      feature combination
    </title>
    <abstract>In general, automatic face recognition technique includes
      two important steps: detection/location, recognition. In this
      paper, a method of automatic face recognition based on fuzzy
      multiple feature combination was proposed. Face detection and
      location used eigen space and gray image understand processing.
      The segmented face image was recognized by the methods:
      Eigenfaces, EigenUpper, EigenTzone and two-order Eigenfaces
      firstly. The fuzzy integration function was adopted to combine the
      fuzzy results, which were fuzzy fused from the elementary
      recognition results, in order to obtain new distance function and
      the final recognition results were given. The experiments on the
      Yale and ORL face database show that, the correct recognition
      rates of the method are all greater than 95% and the fuzzy
      multiple feature combination method is superior to that of
      classical eigenfaces method.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Extending the Feature Vector for Automatic Face Recognition
    </title>
    <abstract>Many features can be used to describe a human face but few
      have been used in combination. Extending the feature vector using
      orthogonal sets of measurements can reduce the variance of a
      matching measure, to improve discrimination capability. This paper
      investigates how different features can be used for
      discrimination, alone or when integrated into an extended feature
      vector.This study concentrates on improving feature definition and
      extraction from a frontal view image, incorporating and extending
      established measurements. These form an extended feature vector
      based on four feature sets: geometric (distance) measurements, the
      eye region, the outline contour, and the profile. The profile,
      contour, and eye region are described by the Walsh power spectrum,
      normalized Fourier descriptors, and normalized moments,
      respectively. Although there is some correlation between the
      geometrical measures and the other sets, their bases (distance,
      shape description, sequency, and statistics) are orthogonal and
      hence appropriate for this research.A database of face images was
      analyzed using two matching measures which were developed to
      control differently the contributions of elements of the feature
      sets. The match was evaluated for both measures for the separate
      feature sets and for the extended feature vector. Results
      demonstrated that no feature set alone was sufficient for
      recognition whereas the extended feature vector could discriminate
      between subjects successfully.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>An integrated system for automatic face recognition</title>
    <abstract>This paper presents an Automated Face Recognition (AFR)
      system capable of providing satisfactory results even with only
      one training image per individual. To obtain this result an
      innovative architecture has been devised with the ability to
      integrate organically new solutios with well-established, even
      classic, techniques, i.e., Principal Component Analysis (PCA) and
      Discrete Cosine Transforms (DCT). The process of identification
      thereby concludes successfully even under trying circumstances;
      that is, even in the presence of consistent variations in the
      orientation, scale and expression of the face under observation.
      Radial Basis Function (RBF) neural networks are used as
      classifiers, the output of which converge into a single block thst
      in turn adopts a decisional strategy. Experimental results on the
      Face Recognition Technology (FERET) database demonstrate the
      validity of our approach, and invite comparison with other systems
      of face recognition....
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Face Recognition for Film Character Retrieval in
      Feature-Length Films
    </title>
    <abstract>The objective of this work is to recognize all the frontal
      faces of a character in the closed world of a movie or situation
      comedy, given a small number of query faces. This is challenging
      because faces in a feature-length film are relatively uncontrolled
      with a wide variability of scale, pose, illumination, and
      expressions, and also may be partially occluded. We develop a
      recognition method based on a cascade of processing steps that
      normalize for the effects of the changing imaging environment. In
      particular there are three areas of novelty: (i) we suppress the
      background surrounding the face, enabling the maximum area of the
      face to be retained for recognition rather than a subset; (ii) we
      include a pose refinement step to optimize the registration
      between the test image and face exemplar; and (iii) we use robust
      distance to a sub-space to allow for partial occlusion and
      expression change. The method is applied and evaluated on several
      feature length films. It is demonstrated that high recall rates
      (over 92%) can be achieved whilst maintaining good precision (over
      93%).</abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Face Recognition System by Combining Four
      Individual Algorithms
    </title>
    <abstract>The objective of face recognition involves the extraction
      of different features of the human face from the face image for
      discriminating it from other persons. It is the problem of
      searching a face in reference database to find the matches as a
      given face. The purpose is to find a face that has highest
      similarity with a given face in the database. Many face
      recognition algorithms have been developed and used as an
      application of access control and surveillance. For enhancing the
      performance and accuracy of biometric face recognition system, we
      use a multi-algorithmic approach, where in a combination of four
      different individual face recognition techniques is used. In this
      paper, we develop a face recognition systems based on one
      combination of four individual techniques namely Principal
      Component Analysis (PCA), Discrete Cosine Transform (DCT),
      Template Matching using Correlation (Corr) and Partitioned
      Iterative Function System (PIFS). We fuse the scores of all of
      these four techniques in a single face recognition system. We
      perform a comparative study of face recognition rate of this face
      recognition system at two precision levels namely at Top-5 and at
      Top-10 IDs. We experiment it with a standard ORL face database.
      Experimentally, we find that recognition rate by PCA-DCT technique
      is better than by individual PCA and DCT techniques and
      recognition rate by PCA-DCT-Corr technique is better than the
      PCA-DCT technique. Overall, we find the system based on
      combination of all of the four individual techniques outperforms.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Face Recognition System Based on Local
      Fourier-Bessel Feature
    </title>
    <abstract>We present an automatic face verification system inspired
      by known properties of biological systems. In the proposed
      algorithm the whole image is converted from the spatial to polar
      frequency domain by a Fourier-Bessel Transform (FBT). Using the
      whole image is compared to the case where only face image regions
      (local analysis) are considered. The resulting representations are
      embedded in a dissimilarity space, where each image is represented
      by its distance to all the other images, and a Pseudo-Fisher
      discriminator is built. Verification test results on the FERET
      database showed that the local-based algorithm outperforms the
      global-FBT version. The local-FBT algorithm performed as
      state-of-the-art methods under different testing conditions,
      indicating that the proposed system is highly robust for
      expression, age, and illumination variations. We also evaluated
      the performance of the proposed system under strong occlusion
      conditions and found that it is highly robust for up to 50% of
      face occlusion. Finally, we automated completely the verification
      system by implementing face and eye detection algorithms. Under
      this condition, the local approach was only slightly superior to
      the global approach.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic 3D reconstruction for face recognition</title>
    <abstract>An analysis-by-synthesis framework for face recognition
      with variant pose, illumination and expression (PIE) is proposed
      in this paper. First, an efficient 2D-to-3D integrated face
      reconstruction approach is introduced to reconstruct a
      personalized 3D face model from a single frontal face image with
      neutral expression and normal illumination; Then, realistic
      virtual faces with different PIE are synthesized based on the
      personalized 3D face to characterize the face subspace; Finally,
      face recognition is conducted based on these representative
      virtual faces. Compared with other related works, this framework
      has the following advantages: 1) only one single frontal face is
      required for face recognition, which avoids the burdensome
      enrollment work; 2) the synthesized face samples provide the
      capability to conduct recognition under difficult conditions like
      complex PIE; and 3) the proposed 2D-to-3D integrated face
      reconstruction approach is fully automatic and more efficient. The
      extensive experimental results show that the synthesized virtual
      faces significantly improve the accuracy of face recognition with
      variant PIE.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Feature Extraction for Multiview 3D Face
      Recognition
    </title>
    <abstract>Current 2D face recognition systems encounter difficulties
      in recognizing faces with large pose variations. Utilizing the
      pose-invariant features of 3D face data has the potential to
      handle multiview face matching. A feature extractor based on the
      directional maximum is proposed to estimate the nose tip location
      and the pose angle simultaneously. A nose profile model
      represented by subspaces is used to select the best candidates for
      the nose tip. Assisted by a statistical feature location model, a
      multimodal scheme is presented to extract eye and mouth corners.
      Using the automatic feature extractor, a fully automatic 3D face
      recognition system is developed. The system is evaluated on two
      databases, the MSU database (300 multiview test scans from 100
      subjects) and the UND database (953 near frontal scans from 277
      subjects). The automatic system provides recognition accuracy that
      is comparable to the accuracy of a system with manually labeled
      feature points.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic face analysis system based on face recognition and
      facial physiognomy
    </title>
    <abstract>An automatic face analysis system is proposed which uses
      face recognition and facial physiognomy. It first detects human's
      face, extracts its features, and classifies the shape of facial
      features. It will analyze the person's facial physiognomy and then
      automatically make an avatar drawing using the facial features.
      The face analysis method of the proposed algorithm can recognize
      face at real-time and analyze facial physiognomy which is composed
      of inherent physiological characteristics of humans, orientalism,
      and fortunes with regard to human's life. The proposed algorithm
      can draw the person's avatar automatically based on face
      recognition. We conform that the proposed algorithm could
      contribute to the scientific and quantitative on-line face
      analysis fields as well as the biometrics.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic facial expression recognition on a single 3D face
      by exploring shape deformation
    </title>
    <abstract>Facial expression recognition has many applications in
      multimedia processing and the development of 3D data acquisition
      techniques makes it possible to identify expressions using 3D
      shape information. In this paper, we propose an automatic facial
      expression recognition approach based on a single 3D face. The
      shape of an expressional 3D face is approximated as the sum of two
      parts, a basic facial shape component (BFSC) and an expressional
      shape component (ESC). The BFSC represents the basic face
      structure and neutral-style shape and the ESC contains shape
      changes caused by facial expressions. To separate the BFSC and
      ESC, our method firstly builds a reference face for each input 3D
      non-neutral face by a learning method, which well represents the
      basic facial shape. Then, based on the BFSC and the original
      expressional face, a facial expression descriptor is designed. The
      surface depth changes are considered in the descriptor. Finally,
      the descriptor is input into an SVM to recognize the expression.
      Unlike previous methods which recognize a facial expression with
      the help of manually labeled key points and/or a neutral face, our
      method works on a single 3D face without any manual assistance.
      Extensive experiments are carried out on the BU-3DFE database and
      comparisons with existing methods are conducted. The experimental
      results show the effectiveness of our method.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic 3d face recognition using discriminant common
      vectors
    </title>
    <abstract>In this paper we propose a fully automatic scheme for 3D
      face recognition. In our scheme, the original 3D data is
      automatically converted into the normalized 3D data, then the
      discriminant common vector (DCV) is introduced for 3D face
      recognition. We also compare DCV with two common methods, i.e.,
      principal component analysis (PCA) and linear discriminant
      analysis (LDA). Our experiments are based on the CASIA 3D Face
      Database, a challenging database with complex variations. The
      experimental results show that DCV is superior to the other two
      methods.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Human Face Recognition System Using Fractal
      Dimension and Modified Hausdorff Distance
    </title>
    <abstract>In this paper, an efticient automatic human face
      recognition system is proposed. Fractal dimension is an efficient
      representation of texture which is used to locate the eyes in a
      human face. We propose a modified approach to estimate the fractal
      dimensions which is less sensitive to lighting conditions and
      provides information about the orientation of an image under
      consideration. Based on the position of the eyes, two face images
      are normalized, aligned and then compared by a new modified
      Hausdorff distance measure. As different facial regions have
      different degrees of importance for face recognition, the modified
      Hausdorff distance is weighted according to a weighted function
      derived from the spatial information of the human face.
      Experimental results show that our approach can achieve
      recognition rates of 76%, 84%, and 92% for the first one, the
      first five, first ten likely matched faces, respectively. If the
      position of the eyes is selected manually, the corresponding
      recognition rates are 82%, 95% and 98%, respectively. The average
      processing time for detecting the eyes and recognize a human face
      is less than two seconds.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic 3D face recognition combining global geometric
      features with local shape variation information
    </title>
    <abstract>Face recognition is a focused issue in pattern recognition
      over the past decades. In this paper, we have proposed a new
      scheme for face recognition using 3D information. In this scheme,
      the scattered 3D point cloud is first represented with a regular
      mesh using hierarchical mesh fitting. Then the local shape
      variation information is extracted to characterize the individual
      together with the global geometric features. Experimental results
      on 3D_RMA, a likely largest 3D face database available currently,
      demonstrate that the local shape variation information is very
      important to improve the recognition accuracy and that the
      proposed algorithm has promising performance with a low
      computational cost.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Evaluation of automatic 4D face recognition using surface and
      texture registration
    </title>
    <abstract>We introduce a novel technique for face recognition by
      using 4D face data that has been reconstructed from a stereo
      camera system. The 4D face data consists of a dense 3D mesh of
      vertices describing the facial geometry as well as a 2D texture
      map describing the facial appearance of each subject. The
      combination of geometry and texture information produces a
      complete photo-realistic model of each face. We propose a
      recognition algorithm based on two steps: The first step involves
      a 3D or 4D rigid registration of the faces. In the second step we
      introduce and evaluate different similarity metrics that measure
      the distance between pairs of closest points on two faces. A key
      advantage of the proposed technique is the fact that it can
      capture facial variations irrespective of the posture of the
      subject. We use this technique on 3D surface and texture data
      comprising 62 subjects at various postures and emotional
      expressions. Our results demonstrate that for subjects that look
      straight into the camera the recognition rate significantly
      increases when texture and geometry are combined in a 4D
      similarity metric.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic interpolation and recognition of face images by
      morphing
    </title>
    <abstract>This article presents a new method for generating an
      optimum image morph field, i.e. an optimum mapping of one image to
      another image by distorting the brightness and geometry of the
      former image. The mapping is calculated by maximizing the
      probability of the morph field in a Bayesian framework. In
      contrast to other techniques this new method needs no training and
      is derived based on group invariances and expected singularities,
      only. An infeasible exhaustive search is replaced by an new
      iterative approximation approach where, in a neighborhood around
      the current morph field solution, the probability distribution is
      approximated by a Gaussian probability distribution for which the
      most likely solution is evaluated by linear algebra techniques.
      New views are generated by applying linearly interpolated morph
      fields to the original reference image. Experiments demonstrate
      that this approach is well suited to interpolate between different
      views of a single face image or between images of different
      persons. Finally, a new face recognition algorithm makes use of
      the fact that morphs among images of a single person are confined
      to a five-dimensional subspace within the space of all possible
      morphs.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic facial feature extraction and 3D face modeling
      using two orthogonal views with application to 3D face recognition
    </title>
    <abstract>We present a fully automated algorithm for facial feature
      extraction and 3D face modeling from a pair of orthogonal frontal
      and profile view images of a person's face taken by calibrated
      cameras. The algorithm starts by automatically extracting
      corresponding 2D landmark facial features from both view images,
      then compute their 3D coordinates. Further, we estimate the
      coordinates of the features that are hidden in the profile view
      based on the visible features extracted in the two orthogonal face
      images. The 3D coordinates of the selected feature points obtained
      from the images are used first to align, then to locally deform
      the corresponding facial vertices of the generic 3D model.
      Preliminary experiments to assess the applicability of the
      resulted models for face recognition show encouraging results.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic pose-normalized 3d face modeling and recognition
      systems
    </title>
    <abstract>Pose-variation factors present a big problem in 2D face
      recognition. To solve this problem, we designed a 3D face
      acquisition system which was able to generate multi-view images.
      However, this created another pose-estimation problem in terms of
      normalizing the 3D face data. This paper presents an automatic
      pose-normalized 3D face data acquisition method that is able to
      perform both 3D face modeling and 3D face pose-normalization at
      once. The proposed method uses 2D information with the AAM (Active
      Appearance Model) and 3D information with a 3D normal vector. The
      proposed system is based on stereo vision and a structured light
      system which consists of 2 cameras and 1 projector. In orsder to
      verify the performance of the proposed method, we designed an
      experiment for 2.5D face recognition. Experimental results showed
      that the proposed method is robust against pose variation.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic decision method of effective transform coefficients
      for face recognition
    </title>
    <abstract>In this paper, we propose a novel face recognition method
      using the effective transform coefficients of face images. The
      method is based on extraction of effective vector in Discrete
      Cosine Transform (DCT) domain and Linear Discriminant Analysis
      (LDA) of effective vector. In general, face images have
      characteristics that they show larger energy congestion in
      horizontal frequency coefficients than in vertical or diagonal
      frequency coefficients. However, many previous methods have
      shortcomings that they don't utilize the facial energy
      characteristics. To overcome shortcomings above, the proposed
      method selects the effective coefficients of the face in DCT
      domain and then extracts feature vector through LDA analysis on
      DCT coefficients. Experimental results show that our method has
      improvements of recognition performance over the previous methods
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Face Classifications by Self-organization for Face
      Recognition
    </title>
    <abstract>We propose a method of face recognition that can
      consistently identify every face angle, assuming it will beused in
      open spaces such as a normal room. We obtain the learning images
      not from an ideal world but from the real world, where users can
      move around freely with noconstraints. We then automatically
      classify the face images that vary according to the user's
      position and posture by self-organization (unsupervised learning),
      and create a discrimination circuit using only the best face
      images for the recognition task. We show that the recognition rate
      for images with various facial angles in the real world can be
      improved by automatic classification through self-organization.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic 3D Face Detection, Normalization and Recognition
    </title>
    <abstract>A fully automatic 3D face recognition algorithm is
      presented. Several novelties are introduced to make the
      recognition robust to facial expressions and efficient. These
      novelties include: (1) Automatic 3D face detection by detecting
      the nose; (2) Automatic pose correction and normalization of the
      3D face as well as its corresponding 2D face using the Hotelling
      Transform; (3) A Spherical Face Representation and its use as a
      rejection classifier to quickly reject a large number of candidate
      faces for efficient recognition; and (4) Robustness to facial
      expressions by automatically segmenting the face into expression
      sensitive and insensitive regions. Experiments performed on the
      FRGC Ver 2.0 dataset (9,500 2D/3D faces) show that our algorithm
      outperforms existing 3D recognition algorithms. We achieved
      verification rates of 99.47% and 94.09% at 0.001 FAR and
      identification rates of 98.03% and 89.25% for probes with neutral
      and non-neutral expression respectively.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Matching Tensors for Pose Invariant Automatic 3D Face
      Recognition
    </title>
    <abstract>The face is an easily collectible and non-intrusive
      biometric used for the authentication and identification of
      individuals. 2D face recognition techniques are sensitive to
      changes in illumination, makeup and pose. We present a fully
      automatic 3D face recognition algorithm that overcomes these
      limitations. During the enrollment, 3D faces in the gallery are
      represented by third order tensors which are indexed by a 4D hash
      table. During online recognition, tensors are computed for a probe
      and are used to cast votes to the tensors in the gallery using the
      hash table. Gallery faces are ranked according to their votes and
      a similarity measure based on a linear correlation coefficient and
      registration error is calculated only for the high ranked faces.
      The face with the highest similarity is declared as the recognized
      face. Experiments were performed on a database of 277 subjects and
      a rank one recognition rate of 86.4% was achieved. Our results
      also show that our algorithmÂ¿s execution time is insensitive to
      the gallery size.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Neural mechanisms of context effects on face recognition:
      Automatic binding and context shift decrements
    </title>
    <abstract>Although people do not normally try to remember
      associations between faces and physical contexts, these
      associations are established automatically, as indicated by the
      difficulty of recognizing familiar faces in different contexts
      ("butcher-on-the-bus" phenomenon). The present fMRI study
      investigated the automatic binding of faces and scenes. In the
      face-face (F-F) condition, faces were presented alone during both
      encoding and retrieval, whereas in the face/scene-face (FS-F)
      condition, they were presented overlaid on scenes during encoding
      but alone during retrieval (context change). Although participants
      were instructed to focus only on the faces during both encoding
      and retrieval, recognition performance was worse in the FS-F than
      in the F-F condition ("context shift decrement" [CSD]), confirming
      automatic face-scene binding during encoding. This binding was
      mediated by the hippocampus as indicated by greater subsequent
      memory effects (remembered &gt; forgotten) in this region for the
      FS-F than the F-F condition. Scene memory was mediated by right
      parahippocampal cortex, which was reactivated during successful
      retrieval when the faces were associated with a scene during
      encoding (FS-F condition). Analyses using the CSD as a regressor
      yielded a clear hemispheric asymmetry in medial temporal lobe
      activity during encoding: Left hippocampal and parahippocampal
      activity was associated with a smaller CSD, indicating more
      flexible memory representations immune to context changes, whereas
      right hippocampal/rhinal activity was associated with a larger
      CSD, indicating less flexible representations sensitive to context
      change. Taken together, the results clarify the neural mechanisms
      of context effects on face recognition.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Databases for Unsupervised Face Recognition</title>
    <abstract>In this paper, an automatic system is presented to
      establish databases for face recognition from video. We propose a
      temporal-based face detector, which can improve the detection rate
      as well as the recognition rate when working together with an
      image-based face detection method. Several adaptive thresholds and
      a filter are introduced to improve face recognition performance
      and to keep the purity, variety and rapidity of the face
      databases. The system deals with multiple coexisting persons
      without requirement for any user interaction, and without
      requirement for any human supervision or assistance.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Asymmetric 3D-2D Face Recognition</title>
    <abstract>3D Face recognition has been considered as a major
      solution to deal with unsolved issues of reliable 2D face
      recognition in recent years, i.e. lighting and pose variations.
      However, 3D techniques are currently limited by their high
      registration and computation cost. In this paper, an asymmetric
      3D-2D face recognition method is presented, enrolling in textured
      3D whilst performing automatic identification using only 2D facial
      images. The goal is to limit the use of 3D data to where it really
      helps to improve face recognition accuracy. The proposed approach
      contains two separate matching steps: Sparse Representation
      Classifier (SRC) is applied to 2D-2D matching, while Canonical
      Correlation Analysis (CCA) is exploited to learn the mapping
      between range LBP faces (3D) and texture LBP faces (2D). Both
      matching scores are combined for the final decision. Moreover, we
      propose a new preprocessing pipeline to enhance robustness to
      lighting and pose effects. The proposed method achieves better
      experimental results in the FRGC v2.0 dataset than 2D methods do,
      but avoiding the cost and inconvenience of data acquisition and
      computation of 3D approaches.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A real time automatic access control system based on face and
      eye corners detection, face recognition and speaker identification
    </title>
    <abstract>We have recently developed a real time automatic access
      control system suing both face and voice identification. We show
      that combining these two modalities achieves high recognition
      accuracy, especially in difficult recognition scenarios. Our
      system also supports online training of the user models, i.e., a
      person who has not been registered in the database can be added on
      the spot in real time. This system is composed of a face and eye
      corners detection module, a face recognition module, a speaker
      identification module, and a user interface module. We present the
      algorithm of each module with more emphasis on our information
      maximization based face and eye corners detection and user
      interface design. We discuss some issues related to the
      integration of components into the total system. We report the
      experimental results of the system performance together with
      comparison with those of some other systems.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Frequency Band Selection for Illumination Robust
      Face Recognition
    </title>
    <abstract>Varying illumination conditions cause a dramatic change in
      facial appearance that leads to a significant drop in face
      recognition algorithms' performance. In this paper, to overcome
      this problem, we utilize an automatic frequency band selection
      scheme. The proposed approach is incorporated to a local
      appearance-based face recognition algorithm, which employs
      discrete cosine transform (DCT) for processing local facial
      regions. From the extracted DCT coefficients, the approach
      determines to the ones that should be used for classification.
      Extensive experiments conducted on the extended Yale face database
      B have shown that benefiting from frequency information provides
      robust face recognition under changing illumination conditions.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Face Recognition Across Pose with Automatic Estimation of
      Pose Parameters through AAM-Based Landmarking
    </title>
    <abstract>In this paper we present a fully automatic system for face
      recognition across pose where no frontal view is needed in
      enrollment or test. The system uses three Active Appearance
      Models(AAMs): the first one is a generic multi resolution AAM,
      while the remaining ones are trained to cope with left/right
      variations (i.e. pose-dependent AAMs). During the fitting stage,
      pose is automatically estimated using eigenvector analysis, and a
      synthetic face is generated through texture warping. Results over
      CMU PIE Database show promising results compared to the
      performance achieved with manually land marked faces.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Face Region Tracking for Highly Accurate Face
      Recognition in Unconstrained Environments
    </title>
    <abstract>In this paper, we present a combined real-time face region
      tracking and highly accurate face recognition technique for an
      intelligent surveillance system. High-resolution face images are
      very important to achieve an accurate identification of a human
      face. Conventional surveillance or security systems, however,
      usually provide poor image quality because they use only fixed
      cameras to passively record scenes. We implemented a real-time
      surveillance system that tracks a moving face using four
      pan-tilt-zoom (PTZ) cameras. While tracking, the
      region-of-interest (ROI) can be obtained by using a low-pass
      filter and background subtraction with the PTZ. Color information
      in the ROI is updated to extract features for optimal tracking and
      zooming. FaceIt®, which is one of the most popular face
      recognition software packages, is evaluated and then used to
      recognize the faces from the video signal. Experimentation with
      real human faces showed highly acceptable results in the sense of
      both accuracy and computational efficiency.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic 3D Face Recognition Using Fourier Descriptors
    </title>
    <abstract>3D face recognition is attracting more attention due to
      the recent development in 3D facial data acquisition techniques.
      It is strongly believed that 3D Face recognition systems could
      overcome the inherent problems of 2D face recognition such as
      facial pose variation, illumination, and variant facial
      expression. In this paper we present a novel technique for 3D face
      recognition system using a set of parameters representing the
      central region of the face. These parameters are essentially
      vertical and cross sectional profiles and are extracted
      automatically without any prior knowledge or assumption about the
      image pose or orientation. In addition, these profiles are stored
      in terms of their Fourier Coefficients in order to minimize the
      size of input data. Our approach is validated and verified against
      two different datasets of 3Dimages covers enough systematic and
      pose variation. High recognition rate was achieved.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Texture Synthesis for Face Recognition from Single
      Views
    </title>
    <abstract>One possible solution for pose- and illuminationinvariant
      face recognition is to employ appearancebased approaches, which
      rely greatly on correct facial textures. However, existing facial
      texture analysis algorithms are suboptimal, because they usually
      neglect specular reflections and require numerous training images
      for virtual view synthesis. This paper presents a novel texture
      synthesis approach from a single frontal view for face
      recognition. Using a generic 3D face shape, facial textures are
      analyzed with consideration of all of the ambient, diffuse, and
      specular reflections. Virtual views are synthesized under
      different poses and illuminations. The proposed approach was
      evaluated using the CMU-PIE face database. Encouraging results
      show that the proposed approach improves face recognition
      performances across pose and illumination variations.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Parameter Determination Based on a Modified
      Discriminant Criterion for Face Recognition
    </title>
    <abstract>This paper presents an Automatical Parameter Determination
      (APD) approach for face recognition. A modified discriminant
      criterion is first proposed via consideration of both Euclidean
      structure and the manifold structure in the face pattern space.
      Two parameter-formulae are derived by maximizing the discriminant
      index. The update equations are then obtained to automatically
      determine the optimal parameters and the optimal projection
      matrix. The proposed ADP algorithm is evaluated on the ORL face
      database. Compared with some state of the art approaches, the
      experimental results demonstrate that the proposed method gives
      better performance.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic temporal segment detection and affect recognition
      from face and body display
    </title>
    <abstract>Psychologists have long explored mechanisms with which
      humans recognize other humans' affective states from modalities,
      such as voice and face display. This exploration has led to the
      identification of the main mechanisms, including the important
      role played in the recognition process by the modalities'
      dynamics. Constrained by the human physiology, the temporal
      evolution of a modality appears to be well approximated by a
      sequence of temporal segments called onset, apex, and offset.
      Stemming from these findings, computer scientists, over the past
      15 years, have proposed various methodologies to automate the
      recognition process. We note, however, two main limitations to
      date. The first is that much of the past research has focused on
      affect recognition from single modalities. The second is that even
      the few multimodal systems have not paid sufficient attention to
      the modalities' dynamics: The automatic determination of their
      temporal segments, their synchronization to the purpose of
      modality fusion, and their role in affect recognition are yet to
      be adequately explored. To address this issue, this paper focuses
      on affective face and body display, proposes a method to
      automatically detect their temporal segments or phases, explores
      whether the detection of the temporal phases can effectively
      support recognition of affective states, and recognizes affective
      states based on phase synchronization/alignment. The experimental
      results obtained show the following: 1) affective face and body
      displays are simultaneous but not strictly synchronous; 2)
      explicit detection of the temporal phases can improve the accuracy
      of affect recognition; 3) recognition from fused face and body
      modalities performs better than that from the face or the body
      modality alone; and 4) synchronized feature-level fusion achieves
      better performance than decision-level fusion.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>An Algorithm for Automatic Side Face Portrait Recognition
      Based on Fourier Descriptor
    </title>
    <abstract>The aim of this study was to establish a method of
      automatic side face portrait recognition based on Fourier
      Descriptor. The method consisted of three steps: face outline
      extraction, Fourier Descriptor feature extraction and feature
      comparison. In the step of face outline extraction, the background
      was separated from the face first so as to get the overall face
      contour line, and then, determined two feature points of forehead
      and lower jaw, next input the contour line between the two points
      to extract the Fourier descriptor. This algorithm selected the
      Fourier descriptor which was invariable to the variability of
      rotation, zooming, and the invariable displacement to eliminate
      the influence caused by shooting angle’s deviation in the process
      of portrait photograph. The characteristic contrast adopted a
      simple method of distinction based on the statistical probability
      of characteristic vector space. At last the article discussed the
      test results of the algorithm
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Visual Recognition of Face and Body Action Units
    </title>
    <abstract>Expressive face and body gestures are among the main
      non-verbal communication channels in human-human interaction.
      Understanding human emotions through these nonverbal means is one
      of the necessary skills both for humans and also for the computers
      to interact intelligently and effectively with their human
      counterparts. Much progress has been achieved in affect assessment
      using a single measure type; however, reliable assessment
      typically requires the concurrent use of multiple modalities.
      Accordingly in this paper, we present preliminary results of
      automatic visual recognition of expressive face and upper-body
      action units (FAUs and BAUs) suitable for use in a vision-based
      affective multimodal framework.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Eye-Level Height System for Face and Iris
      Recognition Systems
    </title>
    <abstract>In this paper we present a fully automated mobile camera
      device platform that will automatically adjust to the eye-levels
      of the persons in front or approaching the system. This system
      serves as a front-end to aid the face recognition and iris
      recognition systems during both the enrollment and verification of
      different people of variable heights. Currently most systems are
      positioned at fixed heights and require subjects who are very tall
      or short to adjust themselves for enrollment. In some cases this
      requires the enrollment system operator (or officer) to adjust the
      camera system accordingly to the user's height. This leads to much
      failure to acquire errors. The current U.S. VISIT system,
      Immigration officers manually move the camera goose-necks to
      adjust to the height of the visitors for photographing. The system
      presented in this paper can fully automate this process by
      automatically adjusting the height of the camera (or other
      biometric device such as Iris acquisition systems) to provide a
      good enrollment/verification photo image for matching using the
      automated face detection to drive the biometric sensors to the
      appropriate height level. The need for such a system can help the
      results obtained by the Independent Testing of Iris Recognition
      Technology (ITIRT) [6] and face recognition systems that can lead
      to minimal user co-operation.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Dealing with Inaccurate Face Detection for Automatic Gender
      Recognition with Partially Occluded Faces
    </title>
    <abstract>Gender recognition problem has not been extensively
      studied in situations where the face cannot be accurately detected
      and it also can be partially occluded. In this contribution, a
      comparison of several characterisation methods of the face is
      presented and they are evaluated in four different experiments
      that simulate the previous scenario. Two of the characterisation
      techniques are based on histograms, LBP and local contrast values,
      and the other one is a new kind of features, called Ranking
      Labels, that provide spatial information. Experiments have proved
      Ranking Labels description is the most reliable in inaccurate
      situations.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Fusion of Face and Gait for Automatic Human Recognition
    </title>
    <abstract>In recent years, several techniques have been proposed
      which integrate face, a physical biometric, with gait, a
      behavioral biometric, with the aim of investigating if such a
      combination will improve upon the performance of methods which
      exclusively employ only one of these biometrics. An overview of
      some of the well-known approaches in this area, along with a
      discussion of the advantages offered and the challenges faced by
      such systems, is provided in this paper and the potential of this
      technology for further research and application is explored.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Pose-Invariant Face Recognition with Parametric Linear
      Subspaces
    </title>
    <abstract>We present a framework for pose-invariant face recognition
      using parametric linear subspace models as stored representations
      of known individuals. Each model can be fit to an input, resulting
      in faces of known people whose head pose is aligned to the input
      face. The model's continuous nature enables the pose alignment to
      be very accurate, improving recognition performance, while its
      generalization to unknown poses enables the models to be compact.
      Recognition systems with two types of parametric linear model are
      compared using a database of 20 persons. The results showed our
      system's robust recognition of faces with plus-minus 50 degree
      range of full 3D head rotation, while compressing the data by a
      factor of 20 and more.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>How Should We RepresentFaces for Automatic Recognition?
    </title>
    <abstract>We describe results obtained from a testbed used to
      investigate different codings for automatic face recognition. An
      eigenface coding of shape-free faces using manually located
      landmarks was more effective than the corresponding coding of
      correctly shaped faces. Configuration also proved an effective
      method of recognition, with rankings given to incorrect matches
      relatively uncorrelated with those from shape-free faces. Both
      sets of information combine to improve significantly the
      performance of either system. The addition of a system, which
      directly correlated the intensity values of shape-free images,
      also significantly increased recognition, suggesting extra
      information was still available. The recognition advantage for
      shape-free faces reflected and depended upon high-quality
      representation of the natural facial variation via a disjoint
      ensemble of shape-free faces; if the ensemble was comprised of
      nonfaces, a shape-free disadvantage was induced. Manipulation
      within the shape-free coding to emphasize distinctive features of
      the faces, by caricaturing, allowed further increases in
      performance; this effect was only noticeable when the independent
      shape-free and configuration coding was used. Taken together,
      these results strongly support the suggestion that faces should be
      considered as lying in a high-dimensional manifold, which is
      locally linearly approximated by these shapes and textures,
      possibly with a separate system for local features. Principal
      Components Analysis is then seen as a convenient tool in this
      local approximation.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>The impact of ARTMAP to appearance-based face verification
    </title>
    <abstract>We propose a novel approach for matching and preservation
      of face images based on Adaptive Resonance Theory MAPping (ARTMAP)
      network. ART networks possess incrementally growing structure and
      provide stable on-line learning, which ensures that all patterns
      presented to the network, will be learned and compactly stored.
      Moreover the network's weights will be adapted after each
      classification. These characteristics are important for successful
      recognition of an object, which patterns are quite changeable in
      time. In our implementation called FaceART the network is learned
      from raw images as well as from eigenfaces decomposition
      coefficients. In order to compare the error rates of the
      implemented system to existing academic face recognition systems
      the XM2VTS database with Lausanne protocol is employed. We show
      that compared to the nearest neighbor rule the presented
      classification approach has better verification performance and
      more compact template representation.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Matching Forensic Sketches and Mug Shots to Apprehend
      Criminals
    </title>
    <abstract>Emerging face recognition technology can use forensic
      sketches to help identify criminal suspects.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Fuzzy Relational Image Compression for Face Recognition Tasks
    </title>
    <abstract>Data compression is always advisable when it comes to
      handling and processing information quickly and efficiently. There
      are two main problems that need to be solved when it comes to
      handling data; store information in smaller spaces and processes
      it in the shortest possible time. When it comes to face
      recognition tasks, there is always the need to construct large
      image repositories from people. Images that have to occupy more
      and more space, and makes the image processing a complicated task;
      given the rapidly increase in the image resolution every day. In
      this work, we show a simple and efficient method that uses Fuzzy
      Relational Product (FRP) to compresses the information inside an
      image, building with this a compressed relational matrix that
      holds enough information to reconstruct a lossy representation of
      the original image, or to perform face recognition tasks. We
      describe the feature extraction, based on overlapped frames, which
      generates vectors that will later be fed to an Artificial Neural
      Network, in the pattern recognition stage. We also show the image
      repository construction and finally, the performance of the face
      recognition task that goes up to 97.5%.</abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Cross-Modality Automatic Face Model Training from Large Video
      Databases</title>
    <abstract>Face recognition is an important issue on video indexing
      and retrieval applications. Usually, supervised learning is used
      to build face models for various specific named individuals.
      However, a huge amount of labeling work is needed in a traditional
      supervised learning framework. In this paper, we propose an
      automatic cross-modality training scheme without supervision which
      uses automatic speech recognition of videos to build visual face
      models. Based on Multiple-Instance Learning algorithms, we
      introduce novel concepts of "Quasi-Positive bags" and "Extended
      Diverse Density", and use them to develop an automatic training
      scheme. We also propose to use the "Relative Sparsity" of a
      cluster to detect the anchorperson in the news videos. Experiments
      show that our algorithm can get correct models for the persons we
      are interested in. The automatic learned models are tested and
      compared with a supervised learning algorithm for face recognition
      in large news video databases, and show promising results.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>50 Years of Image Processing and Pattern Recognition in China
    </title>
    <abstract>This article briefly reviews the development of image
      recognition in and outside China. It presents theoretical research
      achievements and applied research as well as several typical
      applications of image recognition in China. Finally, it discusses
      future trends in image recognition integrated with cognitive
      science. This article is part of a special issue on AI in China.
    </abstract>
    <search_task_number>4</search_task_number>
    <query>automatic face recognition</query>
    <relevance>1</relevance>
  </item>



    <item>
        <title>Spam detection with a content-based random-walk algorithm</title>
        <abstract>In this work we tackle the problem of the spam detection on the Web. Spam web pages have become a problem for Web search engines, due to the negative effects that this phenomenon can cause in their retrieval results. Our approach is based on a random-walk algorithm that obtains a ranking of pages according to their relevance and their spam likelihood. We introduce the novelty of taking into account the content of the web pages to characterize the web graph and to obtain an a-priori estimation of the spam likekihood of the web pages. Our graph-based algorithm computes two scores for each node in the graph. Intuitively, these values represent how bad or good (spam-like or not) is a web page, according to its textual content and the relations in the graph. Our experiments show that our proposed technique outperforms other link-based techniques for spam detection.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Application of evolutionary algorithms in detecting SMS spam at access layer</title>
        <abstract>In recent years, Short Message Service (SMS) has been widely exploited in arbitrary advertising campaigns and the propagation of scam. In this paper, we first analyze the role of SMS spam as an increasing threat to mobile and smart phone users. Afterward, we present a filtering method for controlling SMS spam on the access layer of mobile devices. We analyze the role of different evolutionary and non evolutionary classifiers for our spam filter by assimilating the byte-level features of SMS. We evaluated our framework on real-world benign and spam datasets collected from Grumbletext and the users in our social networking community. The results of carefully designed experiments demonstrated that the evolutionary classifiers, like the Structural Learning Algorithm in Vague Environment (SLAVE), could efficiently detect spam messages at the access layer of a mobile device. To the best of our knowledge, the current work is the first SMS spam filter based on evolutionary classifier that works on the access layer of a mobile device. The results of our experiments show that our framework, using evolutionary algorithms, achieves a detection accuracy of more than 93%, with false alarm rate of 0.13$% in classifying spam SMS. Moreover, the memory requirement for incorporating SMS features is relatively small, and it takes less than one second to classify a message as spam or benign.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>A large-scale study of link spam detection by graph algorithms</title>
        <abstract>Link spam refers to attempts to promote the ranking of spammers' web sites by deceiving link-based ranking algorithms in search engines. Spammers often create densely connected link structure of sites so called "link farm". In this paper, we study the overall structure and distribution of link farms in a large-scale graph of the Japanese Web with 5.8 million sites and 283 million links. To examine the spam structure, we apply three graph algorithms to the web graph. First, the web graph is decomposed into strongly connected components (SCC). Beside the largest SCC (core) in the center of the web, we have observed that most of large components consist of link farms. Next, to extract spam sites in the core, we enumerate maximal cliques as seeds of link farms. Finally, we expand these link farms as a reliable spam seed set by a minimum cut technique that separates links among spam and non-spam sites. We found about 0.6 million spam sites in SCCs around the core, and extracted additional 8 thousand and 49 thousand sites as spams with high precision in the core by the maximal clique enumeration and by the minimum cut technique, respectively.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Clustering malware-generated spam emails with a novel fuzzy string matching algorithm</title>
        <abstract>In this paper, a fuzzy-matching clustering algorithm is introduced to group subjects found in spam emails which are generated by malware. A modified scoring strategy is applied in dynamic programming to find subjects that are similar to each other. A recursive seed selection strategy allows the algorithm to detect similar patterns even when the spammer creates a variation of the original pattern. A sliding threshold based on string length helps to minimize false-positives. The algorithm proves to be effective in detecting and grouping spam emails using templates. It also helps spam investigators to collect and sort large amount of malware-generated spam more efficiently without looking at the email content.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Identifying Web Spam with the Wisdom of the Crowds</title>
        <abstract>Combating Web spam has become one of the top challenges for Web search engines. State-of-the-art spam-detection techniques are usually designed for specific, known types of Web spam and are incapable of dealing with newly appearing spam types efficiently. With user-behavior analyses from Web access logs, a spam page-detection algorithm is proposed based on a learning scheme. The main contributions are the following. (1) User-visiting patterns of spam pages are studied, and a number of user-behavior features are proposed for separating Web spam pages from ordinary pages. (2) A novel spam-detection framework is proposed that can detect various kinds of Web spam, including newly appearing ones, with the help of the user-behavior analysis. Experiments on large-scale practical Web access log data show the effectiveness of the proposed features and the detection framework.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Identifying web spam with user behavior analysis</title>
        <abstract>Combating Web spam has become one of the top challenges for Web search engines. State-of-the-art spam detection techniques are usually designed for specific known types of Web spam and are incapable and inefficient for newly-appeared spam. With user behavior analyses into Web access logs, we propose a spam page detection algorithm based on Bayesian Learning. The main contributions of our work are: (1) User visiting patterns of spam pages are studied and three user behavior features are proposed to separate Web spam from ordinary ones. (2) A novel spam detection framework is proposed that can detect unknown spam types and newly-appeared spam with the help of user behavior analysis. Preliminary experiments on large scale Web access log data (containing over 2.74 billion user clicks) show the effectiveness of the proposed features and detection framework.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Automatic seed set expansion for trust propagation based anti-spamming algorithms</title>
        <abstract>Seed sets are of significant importance for trust propagation based anti-spamming algorithms, e.g., TrustRank. Conventional approaches require manual evaluation to construct a seed set, which restricts the seed set to be small in size, since it would cost too much and may even be impossible to construct a very large seed set manually. The small-sized seed set can cause detrimental effect on the final ranking results. Thus, it is desirable to automatically expand an initial seed set to a much larger one. In this paper, we propose the first automatic seed set expansion algorithm (ASE), which expands a small seed set by selecting reputable seeds that are found and guaranteed to be reputable through a joint recommendation link structure. Experimental results on the WEBSPAM-2007 dataset show that with the same manual evaluation efforts, ASE can automatically obtain a large number of reputable seeds with high precision, thus significantly improving the performance of the baseline algorithm in terms of both reputable site promotion and spam site demotion.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Efficient constraint evaluation in categorical sequential pattern mining for trajectory databases</title>
        <abstract>The classic Generalized Sequential Patterns (GSP) algorithm returns all frequent sequences present in a database. However, usually a few ones are interesting from a user's point of view. Thus, post-processing tasks are required in order to discard uninteresting sequences. To avoid this drawback, languages based on regular expressions (RE) were proposed to restrict frequent sequences to the ones that satisfy user-specified constraints. In all of these languages, REs are applied over items, which limits their applicability in complex real-world situations. We propose a much powerful language, based on regular expressions, denoted RE-SPaM, where the basic elements are constraints defined over the (temporal and non-temporal) attributes of the items to be mined. Expressions in this language may include attributes, functions over attributes, and variables. We specify the syntax and semantics of RE-SPaM, and present a comprehensive set of examples to illustrate its expressive power. We study in detail how the expressions can be used to prune the resulting sequences in the mining process. In addition, we introduce techniques that allow pruning sequences in the early stages of the process, reducing the need to access the database, making use of the categorization of the attributes that compose the items, and of the automaton that accepts the language generated by the RE. Finally, we present experimental results. Although in this paper we focus on trajectory databases, our approach is general enough for being applied to other settings.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Effective pruning strategies for sequential pattern mining</title>
        <abstract>In this paper, we systematically explore the search space of frequent sequence mining and present two novel pruning strategies, SEP (Sequence Extension Pruning) and IEP (Item Extension Pruning), which can be used in all Apriori-like sequence mining algorithms or lattice-theoretic approaches. With a little more memory overhead, proposed pruning strategies can prune invalidated search space and decrease the total cost of frequency counting effectively. For effectiveness testing reason, we optimize SPAM [2] and present the improved algorithm, SPAMSEPIPE, which uses SEP and IEP to prune the search space by sharing the frequent 2-sequences lists. A set of comprehensive performance experiments study shows that SPAMSEPIEP outperforms SPAM by a factor of 10 on small datasets and better than 30% to 50% on reasonably large dataset.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Identifying spam link generators for monitoring emerging web spam</title>
        <abstract>In this paper, we address the question of how we can identify hosts that will generate links to web spam. Detecting such spam link generators is important because almost all new spam links are created by them. By monitoring spam link generators, we can quickly find emerging web spam that can be used for updating existing spam filters. In order to classify spam link generators, we investigate various linkbased features including modified PageRank scores based on white and spam seeds, and these scores of neighboring hosts. An online learning algorithm is used to handle large scale data, and the effectiveness of various features is examined. Experiments on three yearly archives of Japanese Web show that we can predict spam link generators with a reasonable performance.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Clustering for semi-supervised spam filtering</title>
        <abstract>We present a novel investigation of email clustering, demonstrating that clustering can be a powerful tool for email spam filtering. We first extend the well-known notion that ham and spam emails can be divided into clusters, showing the striking result that almost any reasonable clustering algorithm will naturally partition an email dataset into almost entirely spam and entirely spam clusters. We then consider the specific semi-supervised spam filtering scenario of filtering when a large amount of training data is available, but only a few true labels can be obtained for that data. We present two spam filtering approaches for this scenario, both of which start with a clustering of training email. Our first approach uses the true labels of the medoids of each cluster to train a spam filter; our second approach functions similar to the first, except that the true label of each cluster's medoid is used as the label of every email within the cluster, giving a much larger set of labels for training, while still only requiring only a few labels. We evaluate our approaches using the TREC2005 and CEAS2008 spam email datasets. For a large range of different numbers of true labels, we show that both of our approaches significantly outperform training on the same number of randomly selected email messages. The results of our second approach are also better than those of a previously published state-of-the-art semi-supervised small sample spam filtering approach.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Image spam clustering: an unsupervised approach</title>
        <abstract>We propose an unsupervised image clustering framework for revealing the common origins, i.e. the spam gangs, of unsolicited emails. In particular, we target email spam with image attachments because spam information is harder to extract due to information hiding enabled by various image obfuscation techniques. To identify spam gangs, we observe that spam images from the same source are usually composed of visually similar elements which are arranged and altered in many different ways in order to trick the spam filter. We propose to infer spam images originated from the same spam gang by investigating spam email similarity in terms of their visual appearance and editing style. In particular, a data mining technique based on unsupervised image clustering is proposed in this paper to solve this problem. This is achieved by first dividing a spam image into different areas/segments, including texts, foreground graphic illustrations, and background areas. The proposed framework then extracts characteristic visual features from segmented areas, including text layout, visual features of foreground graphic illustrations and its spatial layout, and background texture features. In the clustering stage, all spam images are first categorized as illustrated images and text mainly images according to the existence of foreground illustration objects. Then illustrated images are clustered based on the color and/or foreground layout, while text mainly images are clustered based on the text layouts and/or background textures. A novel unsupervised ranked clustering algorithm is proposed for feature fusion, which is used in combination with the traditional hierarchical clustering algorithm for clustering. We test the proposed approach using different settings and combinations of features and measure the overall performance with V-measure.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Social spam detection</title>
        <abstract>The popularity of social bookmarking sites has made them prime targets for spammers. Many of these systems require an administrator's time and energy to manually filter or remove spam. Here we discuss the motivations of social spam, and present a study of automatic detection of spammers in a social tagging system. We identify and analyze six distinct features that address various properties of social spam, finding that each of these features provides for a helpful signal to discriminate spammers from legitimate users. These features are then used in various machine learning algorithms for classification, achieving over 98% accuracy in detecting social spammers with 2% false positives. These promising results provide a new baseline for future efforts on social spam. We make our dataset publicly available to the research community.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Mining spam email to identify common origins for forensic application</title>
        <abstract>In recent years, spam email has become a major tool for criminals to conduct illegal business on the Internet. Therefore, in this paper we describe a new research approach that uses data mining techniques to study spam emails with the focus on law enforcement forensic analysis. After we retrieve useful attributes from spam emails, we use a connected components clustering algorithm to form relationships between messages. These initial clusters are then refined by using a weighted edges model where membership in the cluster requires the weight to exceed a chosen threshold. The results of the cluster membership are validated by WHOIS data, by the IP address of the computer hosting the advertised sites, and through comparison of graphical images of website fetches. This technique has been successful in identifying relationships between spam campaigns that were not identified by human researchers, enabling additional data to be brought into a single investigation.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Understanding the network-level behavior of spammers</title>
        <abstract>This paper studies the network-level behavior of spammers, including: IP address ranges that send the most spam, common spamming modes (e.g., BGP route hijacking, bots), how persistent across time each spamming host is, and characteristics of spamming botnets. We try to answer these questions by analyzing a 17-month trace of over 10 million spam messages collected at an Internet "spam sinkhole", and by correlating this data with the results of IP-based blacklist lookups, passive TCP fingerprinting information, routing information, and botnet "command and control" traces.We find that most spam is being sent from a few regions of IP address space, and that spammers appear to be using transient "bots" that send only a few pieces of email over very short periods of time. Finally, a small, yet non-negligible, amount of spam is received from IP addresses that correspond to short-lived BGP routes, typically for hijacked prefixes. These trends suggest that developing algorithms to identify botnet membership, filtering email messages based on network-level properties (which are less variable than email content), and improving the security of the Internet routing infrastructure, may prove to be extremely effective for combating spam.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Link analysis for Web spam detection</title>
        <abstract>We propose link-based techniques for automatic detection of Web spam, a term referring to pages which use deceptive techniques to obtain undeservedly high scores in search engines. The use of Web spam is widespread and difficult to solve, mostly due to the large size of the Web which means that, in practice, many algorithms are infeasible. We perform a statistical analysis of a large collection of Web pages. In particular, we compute statistics of the links in the vicinity of every Web page applying rank propagation and probabilistic counting over the entire Web graph in a scalable way. These statistical features are used to build Web spam classifiers which only consider the link structure of the Web, regardless of page contents. We then present a study of the performance of each of the classifiers alone, as well as their combined performance, by testing them over a large collection of Web link spam. After tenfold cross-validation, our best classifiers have a performance comparable to that of state-of-the-art spam classifiers that use content attributes, but are orthogonal to content-based methods.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Detecting spam web pages through content analysis</title>
        <abstract>In this paper, we continue our investigations of "web spam": the injection of artificially-created pages into the web in order to influence the results from search engines, to drive traffic to certain pages for fun or profit. This paper considers some previously-undescribed techniques for automatically detecting spam pages, examines the effectiveness of these techniques in isolation and when aggregated using classification algorithms. When combined, our heuristics correctly identify 2,037 (86.2%) of the 2,364 spam pages (13.8%) in our judged collection of 17,168 pages, while misidentifying 526 spam and non-spam pages (3.1%).</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Know your neighbors: web spam detection using the web topology</title>
        <abstract>Web spam can significantly deteriorate the quality of search engine results. Thus there is a large incentive for commercial search engines to detect spam pages efficiently and accurately. In this paper we present a spam detection system that combines link-based and content-based features, and uses the topology of the Web graph by exploiting the link dependencies among the Web pages. We find that linked hosts tend to belong to the same class: either both are spam or both are non-spam. We demonstrate three methods of incorporating the Web graph topology into the predictions obtained by our base classifier: (i) clustering the host graph, and assigning the label of all hosts in the cluster by majority vote, (ii) propagating the predicted labels to neighboring hosts, and (iii) using the predicted labels of neighboring hosts as new features and retraining the classifier. The result is an accurate system for detecting Web spam, tested on a large and public dataset, using algorithms that can be applied in practice to large-scale Web data.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Filtering spam with behavioral blacklisting</title>
        <abstract>Spam filters often use the reputation of an IP address (or IP address range) to classify email senders. This approach worked well when most spam originated from senders with fixed IP addresses, but spam today is also sent from IP addresses for which blacklist maintainers have outdated or inaccurate information (or no information at all). Spam campaigns also involve many senders, reducing the amount of spam any particular IP address sends to a single domain; this method allows spammers to stay "under the radar". The dynamism of any particular IP address begs for blacklisting techniques that automatically adapt as the senders of spam change. This paper presents SpamTracker, a spam filtering system that uses a new technique called behavioral blacklisting to classify email senders based on their sending behavior rather than their identity. Spammers cannot evade SpamTracker merely by using "fresh" IP addresses because blacklisting decisions are based on sending patterns, which tend to remain more invariant. SpamTracker uses fast clustering algorithms that react quickly to changes in sending patterns. We evaluate SpamTracker's ability to classify spammers using email logs for over 115 email domains; we find that SpamTracker can correctly classify many spammers missed by current filtering techniques. Although our current datasets prevent us from confirming SpamTracker's ability to completely distinguish spammers from legitimate senders, our evaluation shows that SpamTracker can identify a significant fraction of spammers that current IP-based blacklists miss. SpamTracker's ability to identify spammers before existing blacklists suggests that it can be used in conjunction with existing techniques (e.g., as an input to greylisting). SpamTracker is inherently distributed and can be easily replicated; incorporating it into existing email filtering infrastructures requires only small modifications to mail server configurations.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Spam double-funnel: connecting web spammers with advertisers</title>
        <abstract>Spammers use questionable search engine optimization (SEO) techniques to promote their spam links into top search results. In this paper, we focus on one prevalent type of spam - redirection spam - where one can identify spam pages by the third-party domains that these pages redirect traffic to. We propose a five-layer, double-funnel model for describing end-to-end redirection spam, present a methodology for analyzing the layers, and identify prominent domains on each layer using two sets of commercial keywords. one targeting spammers and the other targeting advertisers. The methodology and findings are useful for search engines to strengthen their ranking algorithms against spam, for legitimate website owners to locate and remove spam doorway pages, and for legitimate advertisers to identify unscrupulous syndicators who serve ads on spam pages.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Countering web spam with credibility-based link analysis</title>
        <abstract>We introduce the concept of link credibility, identify the conflation of page quality and link credibility in popular Web link analysis algorithms, and discuss how to decouple link credibility from page quality. Our credibility-based link analysis exhibits three distinct features. First, we develop several techniques for semi-automatically assessing link credibility for all Web pages. Second, our link credibility assignment algorithms allow users to assess credibility in a personalized manner. Third, we develop a novel credibility-based Web ranking algorithm - CredibleRank - which incorporates credibility information directly into the quality assessment of each page on the Web. Our experimental study shows that our approach is significantly and consistently more spam-resilient than both PageRank and TrustRank.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Adaptive e-mail intention finding mechanism based on e-mail words social networks</title>
        <abstract>Through the rapid evaluation of spam, no fully successful solution for filtering spam has been found. However, the spammers still spread spam by using the same intentions such as advertising and phishing. In this investigation, we propose a mechanism of E-mail Words Social Network (EWSN) for profiling users' intentions related to interesting and uninteresting e-mails. An EWSN is constructed from the information in an individual user's mailbox, and expands e-mail information from the World Wide Web (WWW) via the search engine. Based on the web information and association rules among the words, words and relations are expanded as a words' social network. Via the EWSN, both interested and uninterested EWSNs can be constructed to analyze user intentions. Additionally, an efficiency detection mechanism based on the EWSN is proposed to classify e-mails. Finally, the adaptation algorithm of artificial immune system is applied to EWSN, which is thus adapted to follow the user's confirmed classification results. The experimental results indicate that the proposed system is very helpful for classifying spam e-mails by analyzing senders' intentions. Some ideas for analyzing interested nature of people, and profiling their backgrounds, are also presented.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Comment spam detection by sequence mining</title>
        <abstract>Comments are supported by several web sites to increase user participation. Users can usually comment on a variety of media types - photos, videos, news articles, blogs, etc. Comment spam is one of the biggest challenges facing this feature. The traditional approach to combat spam is to train classifiers using various machine learning techniques. Since the commonly used classifiers work on the entire comment text, it is easy to mislead them by embedding spam content in good content. In this paper, we make several contributions towards comment spam detection. (1) We propose a new framework for spam detection that is immune to embed attacks. We characterize spam by a set of frequently occurring sequential patterns. (2) We introduce a variant (called min-closed) of the frequent closed sequence mining problem that succinctly captures all the frequently occurring patterns. We prove as well as experimentally show that the set of min-closed sequences is an order of magnitude smaller than the set of closed sequences and yet has exactly the same coverage. (3) We describe MCPRISM, extension of the recently published PRISM algorithm that effectively mines min-closed sequences, using prime encoding. In the process, we solve the open problem of using the prime-encoding technique to speed up traditional closed sequence mining. (4) We finally need to whittle down the set of frequent subsequences to a small set without sacrificing coverage. This problem is NP-Hard but we show that the coverage function is submodular and hence the greedy heuristic gives a fast algorithm that is close to optimal. We then describe the experiments that were carried out on a large real world comment data and the publicly available Gazelle dataset. (1) We show that nearly 80% of spam on real world data can be effectively captured by the mined sequences at very low false positive rates. (2) The sequences mined are highly discriminative. (3) On Gazelle data, the proposed algorithmic enhancements are faster by at least by a factor and by an order of magnitude on the larger comment dataset.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Enhanced email spam filtering through combining similarity graphs</title>
        <abstract>Over the last decade Email Spam has evolved from being just an irritant to users to being truly dangerous. This has led web-mail providers and academic researchers to dedicate considerable resources towards tackling this problem [9, 21, 22, 24, 26]. However, we argue that some aspects of the spam filtering problem are not handled appropriately in existing work. Principal among these are adversarial spammer efforts -- spammers routinely tune their spam emails to bypass spam-filters, and contaminate ground truth via fake HAM/SPAM votes -- and the scale and sparsity of the problem, which essentially precludes learning with a very large set of parameters. In this paper we propose an approach that learns to filter spam by striking a balance between generalizing HAM/SPAM votes across users and emails (to alleviate sparsity) and learning local models for each user (to limit effect of adversarial votes); votes are shared only amongst users and emails that are "similar" to one another. Moreover, we define user-user and email-email similarities using spam-resilient features that are extremely difficult for spammers to fake. We give a methodology that learns to combine multiple features into similarity values while directly optimizing the objective of better spam filtering. A useful side effect of this methodology is that the number of parameters that need to be estimated is very small: this helps us use off-the-shelf learning algorithms to achieve good accuracy while preventing over-training to the adversarial noise in the data. Finally, our approach gives a systematic way to incorporate existing spam-fighting technologies such as IP blacklists, keyword based classifiers, etc into one framework. Experiments on a real-world email dataset show that our approach leads to significant improvements compared to two state-of-the-art baselines.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Robust PageRank and locally computable spam detection features</title>
        <abstract>Since the link structure of the web is an important element in ranking systems on search engines, web spammers widely use the link structure of the web to increase the rank of their pages. Various link-based features of web pages have been introduced and have proven effective at identifying link spam. One particularly successful family of features (as described in the SpamRank algorithm), is based on examining the sets of pages that contribute most to the PageRank of a given vertex, called supporting sets. In a recent paper, the current authors described an algorithm for efficiently computing, for a single specified vertex, an approximation of its supporting sets. In this paper, we describe several link-based spam-detection features, both supervised and unsupervised, that can be derived from these approximate supporting sets. In particular, we examine the size of a node's supporting sets and the approximate l2 norm of the PageRank contributions from other nodes. As a supervised feature, we examine the composition of a node's supporting sets. We perform experiments on two labeled real data sets to demonstrate the effectiveness of these features for spam detection, and demonstrate that these features can be computed efficiently. Furthermore, we design a variation of PageRank (called Robust PageRank) that incorporates some of these features into its ranking, argue that this variation is more robust against link spam engineering, and give an algorithm for approximating Robust PageRank.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Content based SMS spam filtering</title>
        <abstract>In the recent years, we have witnessed a dramatic increment in the volume of spam email. Other related forms of spam are increasingly revealing as a problem of importance, specially the spam on Instant Messaging services (the so called SPIM), and Short Message Service (SMS) or mobile spam.Like email spam, the SMS spam problem can be approached with legal, economic or technical measures. Among the wide range of technical measures, Bayesian filters are playing a key role in stopping email spam. In this paper, we analyze to what extent Bayesian filtering techniques used to block email spam, can be applied to the problem of detecting and stopping mobile spam. In particular, we have built two SMS spam test collections of significant size, in English and Spanish. We have tested on them a number of messages representation techniques and Machine Learning algorithms, in terms of effectiveness. Our results demonstrate that Bayesian filtering techniques can be effectively transferred from email to SMS spam.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Spam Filtering Using Statistical Data Compression Models</title>
        <abstract>Spam filtering poses a special problem in text categorization, of which the defining characteristic is that filters face an active adversary, which constantly attempts to evade filtering. Since spam evolves continuously and most practical applications are based on online user feedback, the task calls for fast, incremental and robust learning algorithms. In this paper, we investigate a novel approach to spam filtering based on adaptive statistical data compression models. The nature of these models allows them to be employed as probabilistic text classifiers based on character-level or binary sequences. By modeling messages as sequences, tokenization and other error-prone preprocessing steps are omitted altogether, resulting in a method that is very robust. The models are also fast to construct and incrementally updateable. We evaluate the filtering performance of two different compression algorithms; dynamic Markov compression and prediction by partial matching. The results of our empirical evaluation indicate that compression models outperform currently established spam filters, as well as a number of methods proposed in previous studies.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Lies and propaganda: detecting spam users in collaborative filtering</title>
        <abstract>Collaborative Filtering systems are essentially social systems which base their recommendation on the judgment of a large number of people. However, like other social systems, they are also vulnerable to manipulation by malicious social elements. Lies and Propaganda may be spread by a malicious user who may have an interest in promoting an item, or downplaying the popularity of another one. By doing this systematically, with either multiple identities, or by involving more people, a few malicious user votes and profiles can be injected into a collaborative recommender system. This can significantly affect the robustness of a system or algorithm, as has been studied in recent work [5, 7]. While current detection algorithms are able to use certain characteristics of spam profiles to detect them, they suffer from low precision, and require a large amount of training data. In this work, we provide a simple unsupervised algorithm, which exploits statistical properties of effective spam profiles to provide a highly accurate and fast algorithm for detecting spam.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Using spam farm to boost PageRank</title>
        <abstract>Nowadays web spamming has emerged to take the economic advantage of high search rankings and threatened the accuracy and fairness of those rankings. Understanding spamming techniques is essential for evaluating the strength and weakness of a ranking algorithm, and for fighting against web spamming. In this paper, we identify the optimal spam farm structure under some realistic assumptions in the single target spam farm model. Our result extends the optimal spam farm claimed by Gyöngyi and Garcia-Molina through dropping the assumption that leakage is constant. We also characterize the optimal spam farms under additional constraints, which the spammer may deploy to disguise the spam farm by deviating from the unconstrained optimal structure.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>How dynamic are IP addresses?</title>
        <abstract>This paper introduces a novel algorithm, UDmap, to identify dynamically assigned IP addresses and analyze their dynamics pattern. UDmap is fully automatic, and relies only on application-level server logs. We applied UDmap to a month-long Hotmail user-login trace and identified a significant number of dynamic IP addresses - more than 102 million. This suggests that the fraction of IP addresses that are dynamic is by no means negligible. Using this information in combination with a three-month Hotmail email server log, we were able to establish that 95.6% of mail servers setup on the dynamic IP addresses in our trace sent out solely spam emails. Moreover, these mail servers sent out a large amount of spam - amounting to 42.2% of all spam emails received by Hotmail. These results highlight the importance of being able to accurately identify dynamic IP addresses for spam filtering. We expect similar benefits to arise for phishing site identification and botnet detection. To our knowledge, this is the first successful attempt to automatically identify and understand IP address dynamics.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Discovering large dense subgraphs in massive graphs</title>
        <abstract>We present a new algorithm for finding large, dense subgraphs in massive graphs. Our algorithm is based on a recursive application of fingerprinting via shingles, and is extremely efficient, capable of handling graphs with tens of billions of edges on a single machine with modest resources.We apply our algorithm to characterize the large, dense subgraphs of a graph showing connections between hosts on the World Wide Web; this graph contains over 50M hosts and 11B edges, gathered from 2.1B web pages. We measure the distribution of these dense subgraphs and their evolution over time. We show that more than half of these hosts participate in some dense subgraph found by the analysis. There are several hundred giant dense subgraphs of at least ten thousand hosts; two thousand dense subgraphs at least a thousand hosts; and almost 64K dense subgraphs of at least a hundred hosts.Upon examination, many of the dense subgraphs output by our algorithm are link spam, i.e., websites that attempt to manipulate search engine rankings through aggressive interlinking to simulate popular content. We therefore propose dense subgraph extraction as a useful primitive for spam detection, and discuss its incorporation into the workflow of web search engines.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Topical TrustRank: using topicality to combat web spam</title>
        <abstract>Web spam is behavior that attempts to deceive search engine ranking algorithms. TrustRank is a recent algorithm that can combat web spam. However, TrustRank is vulnerable in the sense that the seed set used by TrustRank may not be sufficiently representative to cover well the different topics on the Web. Also, for a given seed set, TrustRank has a bias towards larger communities. We propose the use of topical information to partition the seed set and calculate trust scores for each topic separately to address the above issues. A combination of these trust scores for a page is used to determine its ranking. Experimental results on two large datasets show that our Topical TrustRank has a better performance than TrustRank in demoting spam sites or pages. Compared to TrustRank, our best technique can decrease spam from the top ranked sites by as much as 43.1%.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Tracking Web spam with HTML style similarities</title>
        <abstract>Automatically generated content is ubiquitous in the web: dynamic sites built using the three-tier paradigm are good examples (e.g., commercial sites, blogs and other sites edited using web authoring software), as well as less legitimate spamdexing attempts (e.g., link farms, faked directories). Those pages built using the same generating method (template or script) share a common “look and feel” that is not easily detected by common text classification methods, but is more related to stylometry. In this work we study and compare several HTML style similarity measures based on both textual and extra-textual features in HTML source code. We also propose a flexible algorithm to cluster a large collection of documents according to these measures. Since the proposed algorithm is based on locality sensitive hashing (LSH), we first review this technique. We then describe how to use the HTML style similarity clusters to pinpoint dubious pages and enhance the quality of spam classifiers. We present an evaluation of our algorithm on the WEBSPAM-UK2006 dataset.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Link spam alliances</title>
        <abstract>Link spam is used to increase the ranking of certain target web pages by misleading the connectivity-based ranking algorithms in search engines. In this paper we study how web pages can be interconnected in a spam farm in order to optimize rankings. We also study alliances, that is, interconnections of spam farms. Our results identify the optimal structures and quantify the potential gains. In particular, we show that alliances can be synergistic and improve the rankings of all participants. We believe that the insights we gain will be useful in identifying and combating link spam.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Combining email models for false positive reduction</title>
        <abstract>Machine learning and data mining can be effectively used to model, classify and discover interesting information for a wide variety of data including email. The Email Mining Toolkit, EMT, has been designed to provide a wide range of analyses for arbitrary email sources. Depending upon the task, one can usually achieve very high accuracy, but with some amount of false positive tradeoff. Generally false positives are prohibitively expensive in the real world. In the case of spam detection, for example, even if one email is misclassified, this may be unacceptable if it is a very important email. Much work has been done to improve specific algorithms for the task of detecting unwanted messages, but less work has been report on leveraging multiple algorithms and correlating models in this particular domain of email analysis.EMT has been updated with new correlation functions allowing the analyst to integrate a number of EMT's user behavior models available in the core technology. We present results of combining classifier outputs for improving both accuracy and reducing false positives for the problem of spam detection. We apply these methods to a very large email data set and show results of different combination methods on these corpora. We introduce a new method to compare multiple and combined classifiers, and show how it differs from past work. The method analyzes the relative gain and maximum possible accuracy that can be achieved for certain combinations of classifiers to automatically choose the best combination.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>DirichletRank: Solving the zero-one gap problem of PageRank</title>
        <abstract>Link-based ranking algorithms are among the most important techniques to improve web search. In particular, the PageRank algorithm has been successfully used in the Google search engine and has been attracting much attention recently. However, we find that PageRank has a “zero-one gap” problem which, to the best of our knowledge, has not been addressed in any previous work. This problem can be potentially exploited to spam PageRank results and make the state-of-the-art link-based antispamming techniques ineffective. The zero-one gap problem arises as a result of the current ad hoc way of computing transition probabilities in the random surfing model. We therefore propose a novel DirichletRank algorithm which calculates these probabilities using Bayesian estimation with a Dirichlet prior. DirichletRank is a variant of PageRank, but does not have the problem of zero-one gap and can be analytically shown substantially more resistant to some link spams than PageRank. Experiment results on TREC data show that DirichletRank can achieve better retrieval accuracy than PageRank due to its more reasonable allocation of transition probabilities. More importantly, experiments on the TREC dataset and another real web dataset from the Webgraph project show that, compared with the original PageRank, DirichletRank is more stable under link perturbation and is significantly more robust against both manually identified web spams and several simulated link spams. DirichletRank can be computed as efficiently as PageRank, and thus is scalable to large-scale web applications.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Densest subgraph in streaming and MapReduce</title>
        <abstract>The problem of finding locally dense components of a graph is an important primitive in data analysis, with wide-ranging applications from community mining to spam detection and the discovery of biological network modules. In this paper we present new algorithms for finding the densest subgraph in the streaming model. For any ε > 0, our algorithms make O(log1+ε n) passes over the input and find a subgraph whose density is guaranteed to be within a factor 2(1 + ε) of the optimum. Our algorithms are also easily parallelizable and we illustrate this by realizing them in the MapReduce model. In addition we perform extensive experimental evaluation on massive real-world graphs showing the performance and scalability of our algorithms in practice.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Transductive link spam detection</title>
        <abstract>Web spam can significantly deteriorate the quality of search engines. Early web spamming techniques mainly manipulate page content. Since linkage information is widely used in web search, link-based spamming has also developed. So far, many techniques have been proposed to detect link spam. Those approaches are basically built on link-based web ranking methods. In contrast, we cast the link spam detection problem into a machine learning problem of classification on directed graphs. We develop discrete analysis on directed graphs, and construct a discrete analogue of classical regularization theory via discrete analysis. A classification algorithm for directed graphs is then derived from the discrete regularization. We have applied the approach to real-world link spam detection problems, and encouraging results have been obtained.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>A Stochastic Algorithm for Feature Selection in Pattern Recognition</title>
        <abstract>We introduce a new model addressing feature selection from a large dictionary of variables that can be computed from a signal or an image. Features are extracted according to an efficiency criterion, on the basis of specified classification or recognition tasks. This is done by estimating a probability distribution P on the complete dictionary, which distributes its mass over the more efficient, or informative, components. We implement a stochastic gradient descent algorithm, using the probability as a state variable and optimizing a multi-task goodness of fit criterion for classifiers based on variable randomly chosen according to P. We then generate classifiers from the optimal distribution of weights learned on the training set. The method is first tested on several pattern recognition problems including face detection, handwritten digit recognition, spam classification and micro-array analysis. We then compare our approach with other step-wise algorithms like random forests or recursive feature elimination.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Extracting link spam using biased random walks from spam seed sets</title>
        <abstract>Link spam deliberately manipulates hyperlinks between web pages in order to unduly boost the search engine ranking of one or more target pages. Link based ranking algorithms such as PageRank, HITS, and other derivatives are especially vulnerable to link spam. Link farms and link exchanges are two common instances of link spam that produce spam communities -- i.e., clusters in the web graph. In this paper, we present a directed approach to extracting link spam communities when given one or more members of the community. In contrast to previous completely automated approaches to finding link spam, our method is specifically designed to be used interactively. Our approach starts with a small spam seed set provided by the user and simulates a random walk on the web graph. The random walk is biased to explore the local neighborhood around the seed set through the use of decay probabilities. Truncation is used to retain only the most frequently visited nodes. After termination, the nodes are sorted in decreasing order of their final probabilities and presented to the user. Experiments using manually labeled link spam data sets and random walks from a single seed domain show that the approach achieves over 95.12% precision in extracting large link farms and 80.46% precision in extracting link exchange centroids.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Local computation of PageRank contributions</title>
        <abstract>Motivated by the problem of detecting link-spam, we consider the following graph-theoretic primitive: Given a webgraph G, a vertex v in G, and a parameter δ ∈ (0, 1), compute the set of all vertices that contribute to v at least a δ fraction of v's PageRank. We call this set the δ-contributing set of v. To this end, we define the contribution vector of v to be the vector whose entries measure the contributions of every vertex to the PageRank of v. A local algorithm is one that produces a solution by adaptively examining only a small portion of the input graph near a specified vertex. We give an efficient local algorithm that computes an Ɛ-approximation of the contribution vector for a given vertex by adaptively examining O(1/Ɛ) vertices. Using this algorithm, we give a local approximation algorithm for the primitive defined above. Specifically, we give an algorithm that returns a set containing the δ-contributing set of v and at most O(1/δ) vertices from the δ/2-contributing set of v, and which does so by examining at most O(1/δ) vertices. We also give a local algorithm for solving the following problem: If there exist k vertices that contribute a ρ-fraction to the PageRank of v, find a set of k vertices that contribute at least a (ρ-Ɛ)-fraction to the PageRank of v. In this case, we prove that our algorithm examines at most O(k/Ɛ) vertices.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>User behavior oriented web spam detection</title>
        <abstract>Combating Web spam has become one of the top challenges for Web search engines. State-of-the-art spam detection techniques are usually designed for specific known types of Web spam and are incapable and inefficient for recently-appeared spam. With user behavior analyses into Web access logs, we propose a spam page detection algorithm based on Bayes learning. Preliminary experiments on Web access data collected by a commercial Web site (containing over 2.74 billion user clicks in 2 months) show the effectiveness of the proposed detection framework and algorithm.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Identifying link farm spam pages</title>
        <abstract>With the increasing importance of search in guiding today's web traffic, more and more effort has been spent to create search engine spam. Since link analysis is one of the most important factors in current commercial search engines' ranking systems, new kinds of spam aiming at links have appeared. Building link farms is one technique that can deteriorate link-based ranking algorithms. In this paper, we present algorithms for detecting these link farms automatically by first generating a seed set based on the common link set between incoming and outgoing links of Web pages and then expanding it. Links between identified pages are re-weighted, providing a modified web graph to use in ranking page importance. Experimental results show that we can identify most link farm spam pages and the final ranking results are improved for almost all tested queries.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Enhancing scalability in anomaly-based email spam filtering</title>
        <abstract>Spam has become an important problem for computer security because it is a channel for the spreading of threats such as computer viruses, worms and phishing. Currently, more than 85% of received emails are spam. Historical approaches to combat these messages, including simple techniques such as sender blacklisting or the use of email signatures, are no longer completely reliable. Many solutions utilise machine-learning approaches trained using statistical representations of the terms that usually appear in the emails. However, these methods require a time-consuming training step with labelled data. Dealing with the situation where the availability of labelled training instances is limited slows down the progress of filtering systems and offers advantages to spammers. In a previous work, we presented the first spam filtering method based on anomaly detection that reduces the necessity of labelling spam messages and only employs the representation of legitimate emails. We showed that this method achieved high accuracy rates detecting spam while maintaining a low false positive rate and reducing the effort produced by labelling spam. In this paper, we enhance that system applying a data reduction algorithm to the labelled dataset, finding similarities among legitimate emails and grouping them to form consistent clusters that reduce the amount of needed comparisons. We show that this improvement reduces drastically the processing time, while maintaining detection and false positive rates stable.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>SF-HME system: a hierarchical mixtures-of-experts classification system for spam filtering</title>
        <abstract>Many linear statistical models have been lately proposed in text classification related literature and evaluated against the Unsolicited Bulk Email filtering problem. Despite their popularity - due both to their simplicity and relative ease of interpretation - the non-linearity assumption of data samples is inappropriate in practice, due to its inability to capture the apparent non-linear relationships, which characterize these samples. In this paper, we propose the SF-HME, a Hierarchical Mixture-of-Experts system, attempting to overcome limitations common to other machine-learning based approaches when applied to spam mail classification. By reducing the dimensionality of data through the usage of the effective Simba algorithm for feature selection, we evaluated our SF-HME system with a publicly available corpus of emails, with very high similarity between legitimate and bulk email - and thus low discriminative potential - where the traditional rule based filtering approaches achieve considerable lower degrees of precision. As a result, we confirm the domination of our SF-HME method against other machine learning approaches, which appeared to present lesser degree of recall.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Who moderates the moderators?: crowdsourcing abuse detection in user-generated content</title>
        <abstract>A large fraction of user-generated content on the Web, such as posts or comments on popular online forums, consists of abuse or spam. Due to the volume of contributions on popular sites, a few trusted moderators cannot identify all such abusive content, so viewer ratings of contributions must be used for moderation. But not all viewers who rate content are trustworthy and accurate. What is a principled approach to assigning trust and aggregating user ratings, in order to accurately identify abusive content? In this paper, we introduce a framework to address the problem of moderating online content using crowdsourced ratings. Our framework encompasses users who are untrustworthy or inaccurate to an unknown extent --- that is, both the content and the raters are of unknown quality. With no knowledge whatsoever about the raters, it is impossible to do better than a random estimate. We present efficient algorithms to accurately detect abuse that only require knowledge about the identity of a single 'good' agent, who rates contributions accurately more than half the time. We prove that our algorithm can infer the quality of contributions with error that rapidly converges to zero as the number of observations increases; we also numerically demonstrate that the algorithm has very high accuracy for much fewer observations. Finally, we analyze the robustness of our algorithms to manipulation by adversarial or strategic raters, an important issue in moderating online content, and quantify how the performance of the algorithm degrades with the number of manipulating agents.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Subspace embeddings for the L1-norm with applications</title>
        <abstract>We show there is a distribution over linear mappings R:l1n -> l1O(d log d), such that with arbitrarily large constant probability, for any fixed d-dimensional subspace L, for all x ∈ L we have |x|1 ≤ |Rx|1 = O(d log d)|x|1. This provides the first analogue of the ubiquitous subspace Johnson-Lindenstrauss embedding for the l1-norm. Importantly, the target dimension and distortion are independent of the ambient dimension n. We give several applications of this result. First, we give a faster algorithm for computing well-conditioned bases. Our algorithm is simple, avoiding the linear programming machinery required of previous algorithms. We also give faster algorithms for least absolute deviation regression and l1-norm best fit hyperplane problems, as well as the first single pass streaming algorithms with low space for these problems. These results are motivated by practical problems in image analysis, spam detection, and tatistics, where the l1-norm is used in studies where outliers may be safely and effectively ignored. This is because the l1-norm is more robust to outliers than the l2-norm.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Robust collaborative filtering</title>
        <abstract>The widespread deployment of recommender systems has lead to user feedback of varying quality. While some users faithfully express their true opinion, many provide noisy ratings which can be detrimental to the quality of the generated recommendations. The presence of noise can violate modeling assumptions and may thus lead to instabilities in estimation and prediction. Even worse, malicious users can deliberately insert attack profiles in an attempt to bias the recommender system to their benefit. Robust statistics is an area within statistics where estimation methods have been developed that deteriorate more gracefully in the presence of unmodeled noise and slight departures from modeling assumptions. In this work, we study how such robust statistical methods, in particular M-estimators, can be used to generate stable recommendation even in the presence of noise and spam. To that extent, we present a Robust Matrix Factorization algorithm and study its stability. We conclude that M-estimators do not add significant stability to recommendation; however the presented algorithm can outperform existing recommendation algorithms in its recommendation quality.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Web spam identification through content and hyperlinks</title>
        <abstract>We present an algorithm, witch, that learns to detect spam hosts or pages on the Web. Unlike most other approaches, it simultaneously exploits the structure of the Web graph as well as page contents and features. The method is efficient, scalable, and provides state-of-the-art accuracy on a standard Web spam benchmark.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Adversarial learning</title>
        <abstract>Many classification tasks, such as spam filtering, intrusion detection, and terrorism detection, are complicated by an adversary who wishes to avoid detection. Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier [2]. In this paper, we introduce the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks. We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering.</abstract>
        <search_task_number>12</search_task_number>
        <query>spam algorithm</query>
        <relevance>1</relevance>
    </item>



  <item>
    <title>Clustering online game communities through SOM</title>
	<abstract>Nowadays, online games have an exponential increase in the market because many people interact for hours in a virtual gaming worlds called the Massive Multiplayer Online Role-Playing Games (MMORPGs). In this kind of environment players maintain relationships and build communities. To study the common characteristics and relationships of the communities formed in those games, it is possible to cluster a player's community. Moreover, player's community structure is common in various real-world networks; methods or algorithms for grouping such communities have attracted great attention in recent years. The analysis of those groups aim to better understand and examine the behaviour of players. In this paper, self-organizing maps were explored to obtain clusters of a player community from the game World of Warcraft (WoW). To improve the efficiency of the clustering methodology masks were applied that considered the player's individual score, player's guild degree (number of connections), and player's class. The results obtained indicate that the proposed methodology can be successfully applied to the clustering online game communities.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>What makes people experience flow? Social characteristics of online games</title>
	<abstract>Online games are games in which users play simultaneously with each other in a virtual environment. The success of online games relies on the repetitive visits of players, and thus on the very personal experiences of players. Drawing on the theory of flow, this paper empirically explores how the characteristics of online games affect the individual flow experience. Three antecedents to flow (skills, challenges and focused attention) have positive influence on the flow experience. Interactivity, whether it is human or machine, is found to influence the antecedents of the flow experience, and system performance is shown to have no or marginally significant effects. The results show that social characteristics of online games are more crucial to online game success than technological ones.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Falling in love with online games: The uses and gratifications perspective</title>
	<abstract>Playing online games is experience-oriented but few studies have explored the user's initial (trial) reaction to game playing and how this further influences a player's behavior. Drawing upon the Uses and Gratifications theory, we investigated players' multiple gratifications for playing (i.e. achievement, enjoyment and social interaction) and their experience with the service mechanisms offered after they had played an online game. This study explores the important antecedents of players' proactive ''stickiness'' to a specific online game and examines the relationships among these antecedents. The results show that both the gratifications and service mechanisms significantly affect a player's continued motivation to play, which is crucial to a player's proactive stickiness to an online game.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The moderating role of locus of control on the links between experiential motives and intention to play online games</title>
	<abstract>Online games represent a burgeoning market sector with growth potential. The distinctive entertainment-oriented features of such games provide experiential motives for users. However, most previous studies have focused on the single dimensional affective motive of online game use. The multi-dimensional affective motivational aspects of entertainment technologies have been relatively neglected. In addition, previous studies report inconsistent relationships between motives and online game usage. To fill these gaps in the literature, the present study proposes five experiential motives such as concentration, enjoyment, escape, epistemic curiosity, and social affiliation as predictors of intention to play online games. External locus of control is also introduced as a moderator in the links between experiential motives and intention. The model was tested with the data from 576 current online game players who answered online survey. Structural equation model analysis confirmed that three experiential motives with the exception of concentration and epistemic curiosity have a positive effect on intention. @g^2 difference test confirmed that the coefficients linking experiential motives such as concentration, enjoyment, and escape to intention are higher for people with external locus of control than for people with internal locus of control. Implications are discussed in conclusion.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Online gaming: a scoping study of massively multi-player online role playing games</title>
	<abstract>The popularity of Massively Multiplayer Online Role-Playing Games (MMORPGs) has risen greatly over the last few years. To date there has been very little published academic research concerning online gaming and even less on the different types of online games that exist. Given the lack of data, a scoping study was undertaken to examine the extent, range and nature of different MMORPGs. Data were collected relating to the twenty most popular MMORPGs. The primary aim was to present a summarised overview of all material reviewed. The secondary aim was to provide a thematic construction in order to present a narrative account of existing MMORPG literature. Overall, the scoping study found that whilst some games had received moderate, or even substantial attention, others have had no research conducted upon them at all. This presents a problem since the growth in both the player base and the industry suggests that a single psychological profile of `the gamer' cannot be relied upon, and as such further research is required. It is hoped that this study suggests ways forward and helps set research agendas for future research into MMORPGs.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Networked Graphics: Building Networked Games and Virtual Environments</title>
	<abstract>This broad-ranging book equips programmers and designers with a thorough grounding in the techniques used to create truly network-enabled computer graphics and games. Written for graphics/game/VE developers and students, it assumes no prior knowledge of networking. The text offers a broad view of what types of different architectural patterns can be found in current systems, and readers will learn the tradeoffs in achieving system requirements on the Internet. The book explains the foundations of networked graphics, then explores real systems in depth, and finally considers standards and extensions. Numerous case studies and examples with working code are featured throughout the text, covering groundbreaking academic research and military simulation systems, as well as industry-leading game designs.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Online flow experiences, problematic Internet use and Internet procrastination</title>
	<abstract>This study explores the theoretical and practical overlap between online procrastination, problematic Internet use, and flow on the Internet. At the theoretical level there is a great deal of interrelatedness between these three concepts (for example, all three concepts deal with the issue of a lack of control over time spent online and acknowledge the distracting and entertaining properties of the Internet); yet, one can also argue that the concepts are theoretically distinct (for example, flow is a total absorption in the work at hand, whereas procrastination is the avoidance of the work at hand). All three concepts have been used to describe either desirable (flow) or undesirable (procrastination and problematic Internet use) states when online. In this study a sample of 1399 Internet users was obtained from a survey placed on a South African online information technology magazine. Using the problematic Internet use questionnaire (PIUQ), the distraction subscale of the online cognition scale (OCS), and a modified version of the Flow scale it was found that there were strong positive relationships between all three variables (the strongest relationship being between problematic Internet use and online procrastination). The results also suggested that procrastination may be a connector between PIU and flow; also that PIU is a connector between procrastination and flow, but that flow is independent of the relationship between PIU and procrastination. These results are discussed in relation to previous studies on problematic Internet use and in particular, whether these relationships are unique to respondents involved in the information technology sector.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The impact of perceived risk, intangibility and consumer characteristics on online game playing</title>
	<abstract>Online game is the most popular entertainment application in the virtual world and online gamers demonstrate high attachment to playing online games. Previous studies have linked to the numerous negative outcomes in playing online games. The factors contribute to the negative consequences on using online game have been relatively neglected. The purpose of this study is to explore the impact of perceived risk, intangibility, gender and age (adolescent/adult) difference on online game playing consequences and intentions. Past usage frequency is also made to look into the influence additional purchase intention. A total of 1418 useful questionnaires (including 1018 from public interview and 400 from online questionnaire feedback) were collected for final data analysis. The results demonstrated the important roles that time risk, psychological risk, financial risk, physical intangibility, mental intangibility and generality play on the negative consequences associated with online game playing. The results also indicated that male and adolescent individuals spent much more time on online game and intented to act the entertainment more than females and adults did. Finally, past online game playing frequency was showed to be a positive predictor of future online game playing intention.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The effects of a player's network centrality on resource accessibility, game enjoyment, and continuance intention: A study on online gaming communities</title>
	<abstract>This study applies social capital theory to investigate how a player's network centrality in an online gaming community (i.e., a guild) affects his/her attitude and continuance intention toward a Massive Multiplayer Online Game (MMOG). Analysis of 347 usable responses shows that players' network centrality has a negative impact on their ties to players who belong to other guilds (i.e., non-guild interaction), but a positive effect on players' access to resources. However, players' network centrality fails to increase their perceived game enjoyment directly. Players' resource accessibility and perceived game enjoyment play mediating roles in the relationship between network centrality and attitude toward playing an MMOG, which in turn influences game continuance intention. The results also show that although players' non-guild interaction is negatively related to their resource accessibility from the networks, it is positively associated with perceived game enjoyment. The article concludes with implications and limitations of the study.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Exploring user-producer interaction in an online community: the case of Habbo Hotel</title>
	<abstract>This article attempts to explore the user-producer interaction in the online community of Habbo Hotel. Based on desk research, interviews, an online survey among more than 3000 Habbo Hotel users in The Netherlands and online discussion groups with 45 Habbos, three specific issues that illustrate the interactions between the users and producers are highlighted. First, different levels of user-generated content in Habbo Hotel will be discussed. Second, the communication between the users and producers will be analysed and, finally, the matter of online safety will be reviewed.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Online addiction: privacy risks in online gaming environments</title>
	<abstract>In this paper we investigated the levels of addiction and personal data disclosure within Massively Multiplayer Online Role Playing Game environments (MMORPG's). The study made use of an online survey which embraced a combination of a six point behavioural addiction framework, Self Determination Theory and Impression Management theory to assess addictive behaviour and consequential data disclosure amongst a sample representative of 188 Singaporean based MMORPG gamers. Results found that pathological gaming addiction had a direct effect on levels of personal and sensitive data disclosure and participants who were disclosing high amounts of data were considered more vulnerable to exploitation and predation.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Understanding factors affecting perceived sociability of social software</title>
	<abstract>Sociability is considered to be important to the success of social software. The goal of the current study is to identify factors that affect the users' perception of the sociability of social software and to examine the impact of sociability on the users' attitude and behavior intentions. In a pilot study, 35 web users were interviewed to gain understanding of how they use social software to supplement their social life and to explore the possible factors that influence the users' utilization of social software. In the first study, a questionnaire was developed, and 163 valid responses were collected. From the factor analysis results, seven important factors for social software design emerged, which accounts for 63.3% of the total variance. In the second study, 246 participants were asked to evaluate one of ten popular social applications with respect to the seven factors, their perceived sociability, and their attitudes and intention regarding the use of the applications. Results show that sociability is influenced by social climate, benefits and purposes, people, interaction richness, self-presentation, and support for formal interaction. System competency is not a sociability factor, but it significantly influences the user's experience. Sociability and system competency, when combined, can predict 43% of users' attitude towards social software and 51% of their intentions to use social software.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Review: Packet-level traffic analysis of online games from the genre characteristics perspective</title>
	<abstract>Recent online games exert increasing impact on network traffic. While considerable research efforts have been placed on the study of game network traffic, most studies examine either only a small sample of games or games within a specific genre. This paper aims to utilize extensive resources of all notable research studies on online game traffic published over the last decade, and obtain a critical understanding of traffic pattern from game genre perspective. A general classification will be given according to game genres in order to highlight the characteristics pertaining to different game types. Network traffic studies for various game genres will be analyzed individually in the packet level, i.e. interarrival time and packet size. Having highlighted the current research gaps on game network traffic study, this paper will also provide suggestions on areas worthy of future research. The research outcomes from previous studies, despite having some minor discrepancies, have evidenced major commonalities that offer consistent perspectives of online game traffic. These results can be correlated as a guide for general traffic modeling for each game genre that depicts the characteristics of network traffic trends.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Business models and operational issues in the Chinese online game industry</title>
	<abstract>The rapid growth of Internet usage has enabled many new online communities to develop. A particularly interesting phenomenon that has arisen through Internet communities is the virtual world (VW) style of online game. This paper identifies the challenges that developers of VWs will face in their efforts to find viable business models. This is a single case study of China as an exploratory project to determine the issues surrounding business models for virtual world developers and users. The paper discusses the feedback effects between broadband adoption and online games as well as issues such as culture, history, Waigua, private servers, virtual property trade, developer control, governance, and regulation. In spite of the profitability of major Chinese VW operators, close observation of the Chinese case suggests that even the most successful VW operators are still in the early stages of their business model development.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A systematic classification of cheating in online games</title>
	<abstract>Cheating is rampant in current game play on the Internet. However, it is not as well understood as one might expect. In this paper, we summarize the various known methods of cheating, and we define a taxonomy of online game cheating with respect to the underlying vulnerability (what is exploited?), consequence (what type of failure can be achieved?) and the cheating principal (who is cheating?). This taxonomy provides a systematic introduction to the characteristics of cheats in online games and how they can arise. It is intended to be comprehensible and useful not only to security specialists, but also to game developers, operators and players who are less knowledgeable and experienced in security. One of our findings is that although cheating in online games is largely due to various security failures, the four traditional aspects of security -- confidentiality, integrity, availability and authenticity -- are insufficient to explain it. Instead, fairness becomes a vital additional aspect, and its enforcement provides a convincing perspective for understanding the role of security techniques in developing and operating online games.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Personality traits, usage patterns and information disclosure in online communities</title>
	<abstract>Online communities of different types have become an important part of the daily internet life of many people within the last couple of years. Both research and business have shown interest in studying the possibilities and risks related to these relatively new phenomena. Frequently discussed aspects that are tightly bound to online communities are their implications and effects on privacy issues. Available literature has shown that users generally disclose very much (private) information on such communities, and different factors influencing this behaviour were identified and studied. However, the influence and predictive power of personality traits on information disclosure in online communities has not yet been the subject of analysis. In this paper we report the results of an online survey investigating the relations between personality traits (based on the Fife-Factor Model), usage patterns and information disclosure of participants in different types of online communities.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Attentional bias in excessive massively multiplayer online role-playing gamers using a modified Stroop task</title>
	<abstract>There is considerable dispute regarding the nature of excessive or problematic Internet-related behaviour and whether it constitutes a clinical addiction. Classification of excessive gaming is hindered by a lack of experimental research investigating behavioural responses from gamers and comparing these patterns to those found in established addictions. We investigated whether an attentional bias for gaming-related words existed for addicted Massively Multiplayer Online Role-Playing Gamers (MMORPGers) identified using the Addiction-Engagement Questionnaire. Forty frequent MMORPGers (15 female) and 19 non-MMORPGers (eight female) completed a computerised modified Stroop task comprised of game-related, negative and neutral word lists, Addiction-Engagement Questionnaire, Depression, Anxiety and Stress Scale 21, gaming-related variables. The results indicated that addicted MMORPGers had significantly longer reaction times to negative and MMORPG words compared to neutral words, whereas highly engaged and non-MMORPG participants showed no such bias. The presence of an attentional bias in addicted MMORPGers is comparable with research investigating this behavioural response in established addictions.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Current Perspectives on Personality and Internet Use</title>
	<abstract>The Internet provides a means to take part in various online activities, for example, leisure activities (e.g., online gaming), social activities (e.g., online chat), and information activities (e.g., online newspapers). Patterns of Internet consumption tend to vary greatly and this is said to be a possible function of personality. Therefore, knowing something about the personalities of those who favor specific activities online may provide a better insight into the motivational factors behind use. This article combines and reviews current literature regarding personality and Internet use, using Eysenck's three-factor personality theory as a framework of convergence. Although the Internet allows us to play with our identities, it would still seem that online behavior tends to somewhat mimic the behavior expected by one's off-line personality.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Mobile gaming: Industry challenges and policy implications</title>
	<abstract>Mobile games are a prime example of a successful mobile application and demonstrate the increasing range of platforms for the media and entertainment industries. Against this convergent background, this paper introduces the basic features of the mobile gaming market and its industrial ecosystem together with its main actors and activities. The focus of the paper lies in the challenges ahead for the evolution of mobile applications into a potentially dominant game platform and the possible disruptions along this road. The deep personal relationships between users and their mobile devices are considered to further explore the link between mobile games, players' strategies and pending techno-economic developments. The paper concludes with a brief discussion of some policy options to assist with the development of this domain.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Multinational web uses and gratifications: Measuring the social impact of online community participation across national boundaries</title>
	<abstract>This paper describes the rationale and findings from a multinational study of online uses and gratifications conducted in the United States, Korea, and the Netherlands in spring 2003. Survey questions developed in three languages by native speaking researchers was presented to approximately 400 respondents in each country via the Web. Web uses and gratifications were analyzed cross-nationally in a comparative fashion focusing on involvement in different types of on-line communities. Findings indicate that demographic characteristics, cultural values, and Internet connection type emerged as critical factors that explain why the same technology is adopted differently.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Behavioural biometrics: a survey and classification</title>
	<abstract>This study is a survey and classification of the state-of-the-art in behavioural biometrics which is based on skills, style, preference, knowledge, motor-skills or strategy used by people while accomplishing different everyday tasks such as driving an automobile, talking on the phone or using a computer. The authors examine current research in the field and analyse the types of features used to describe different types of behaviour. After comparing accuracy rates for verification of users using different behavioural biometric approaches, researchers address privacy issues which arise or might arise in the future with the use of behavioural biometrics.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>How much do you tell?: information disclosure behaviour indifferent types of online communities</title>
	<abstract>Online communities of different types have become an important part of the internet life of many people within the last couple of years. Both research and business have shown interest in studying the possibilities and risks of these relatively new phenomena. Very controversial aspects of these communities are their implications and effects on privacy issues, as research has shown that users generally provide information rather freely on such communities. However, no systematic comparison of differences in information disclosure behavior considering different types of communities is available. Furthermore only few is known about the information disclosure behavior related to demographic variables, usage contexts and usage patterns. To better understand these aspects of online communities we conducted an online survey that questioned users of various popular online communities about their information disclosure behavior and usage patterns of these sites. More than 850 users responded to our questionnaire. In this paper we present the main results of the analysis and provide linear regression models that allow understanding the involved factors in detail.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Explaining mobile community user participation from a social capital perspective</title>
	<abstract>This research analyses factors affecting mobile community user participation from a social capital perspective. The six factors of social capital are categorised into three dimensions: the structural dimension, the cognitive dimension, and the relational dimension. User participation intention includes two variables: the intention to view and the intention to post. We collected 370 valid questionnaires and performed data analysis using Structural Equation Modelling (SEM). The results showed that trust had the strongest effect on the intention to view, whereas identification with the community had the strongest effect on the intention to post. Thus, mobile community providers need to focus on the relational dimension of social capital in order to promote user participation in their communities.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Cheating Prevention in Virtual Worlds: Software, Economic, and Law Aspects</title>
	<abstract>This paper deals with networked applications in the emerging field of online virtual worlds. Example applications include, e.g., Massively Multiplayer Online Computer Games (MMOG), networked e-learning, training and simulations, etc. Highly interactive virtual worlds bring a new security challenge: the multiple participants of such applications do not always show cooperative and intended behavior, but rather may act in an illegal way (cheating) which is harmful for other participants, thus intentionally or accidentally procuring illicit advantages for themselves. The paper studies the new challenge of cheating in virtual-world applications in three areas: a) system and application programming, b) economics, and c) law. The main contributions of our work are as follows: 1) We present a systematic classification of cheating threats in virtual worlds, and describe software solutions that help prevent them in future Internet-based applications; 2) We enhance the classical economic analysis of crime and punishment for applying it to virtual worlds; 3) We describe our development approach for networked virtual worlds and its implementation as the Real-Time Framework (RTF) which has been designed at the University of Muenster; 4) Finally, we explore the law aspects of cheating in virtual applications in the context of the legal system in Germany. The consideration of informatics aspects together with the corresponding problems of economics and law allows us to tackle virtual-world security in a holistic, systematic manner.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Factors influencing users' decisions to adopt voice communication in online console games</title>
	<abstract>We used a Model of Technology Appropriation to understand users' initial reaction to, and use of, the voice communication channel provided by Xbox Live. We found that while users expected voice to be an advance over text-based communication, in practice they found it difficult to use, leading some to reject it. There appear to be usability and sociability problems with the way the voice channel is currently configured in some games. We argue that game developers will need to address these problems in order to realise the potential of voice in online multiplayer videogames.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Review: Engagement in digital entertainment games: A systematic review</title>
	<abstract>Since their introduction over 40 years ago, digital entertainment games have become one of the most popular leisure activities globally. While digital games clearly provide highly engaging activities, the nature of this engagement is not well understood. The current study aims to advance our understanding by reporting a systematic review of recent literature addressing engagement in computer games. The papers in the review comprise a sub-sample of papers relating to engagement in digital games that was selected from a broader literature search carried out on the outcomes and impacts of playing computer games. A diverse range of studies was identified that examined varied aspects of engagement in games including subjective experiences while playing games, the physiological concomitants of these experiences, motives for playing games, game usage and time spent playing games and the impact of playing on life satisfaction. A narrative review was carried out to capture these diverse aspects of engagement and to develop a more coherent understanding of engagement in computer games.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Log file analysis for disengagement detection in e-Learning environments</title>
	<abstract>Most e-Learning systems store data about the learner's actions in log files, which give us detailed information about learner behaviour. Data mining and machine learning techniques can give meaning to these data and provide valuable information for learning improvement. One area that is of particular importance in the design of e-Learning systems is learner motivation as it is a key factor in the quality of learning and in the prevention of attrition. One aspect of motivation is engagement, a necessary condition for effective learning. Using data mining techniques for log file analysis, our research investigates the possibility of predicting users' level of engagement, with a focus on disengaged learners. As demonstrated previously across two different e-Learning systems, HTML-Tutor and iHelp, disengagement can be predicted by monitoring the learners' actions (e.g. reading pages and taking test/quizzes). In this paper we present the findings of three studies that refine this prediction approach. Results from the first study show that two additional reading speed attributes can increase the accuracy of prediction. The second study suggests that distinguishing between two different patterns of disengagement (spending a long time on a page/test and browsing quickly through pages/tests) may improve prediction in some cases. The third study demonstrates the influence of exploratory behaviour on prediction, as most users at the first login familiarize themselves with the system before starting to learn.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The role of trait emotional intelligence in gamers' preferences for play and frequency of gaming</title>
	<abstract>This paper examines the role of trait emotional intelligence (trait EI) in gamers' preferences for play and frequency of gaming in a sample of 1051 young adult US/European gamers, who play frequently the online massively multiplayer game, World of Warcraft (WoW). Trait EI was shown to predict social and achievement preferences for play as well as frequency of gaming. In particular, trait EI was positively correlated to a preference for social practices per se and negatively correlated to a preference for achievement-oriented, instrumental practices. These findings advocate that gamers' preferences for play are in accordance with their emotion-related personality characteristics. Trait EI was also negatively associated with frequency of gaming suggesting that lower scorers on trait EI are more likely associated with more frequent game use.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A cross-cultural study of flow experience in the it environment: the beginning</title>
	<abstract>Flow (optimal) experience is being widely investigated in the IT environments: in human-computer interaction, computer-mediated communication and exploratory behaviour, consumer and marketing applications, educational practice, playing computer, video and online games, psychological rehabilitation of the disabled, web usability testing, etc. Though a universal experience, flow can be expected to be culture specific and culture dependent. Optimal experience has only rarely been studied from a cross-cultural perspective, mainly in the field of gaming activities. An overview of the earliest works in the field is presented, as well as empirical evidences of a study referring to the flow experience and interaction patterns inherent to the samples of Russian and French online players.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Digital distribution of games: the players' perspective</title>
	<abstract>The online distribution of games has in the past few years become increasingly commonplace. Currently the brick and mortar retailers and physical copies still hold a dominant position in the overall market but online market -- including online distribution of full game titles, subscription-based models, downloadable content, virtual commodities and related value-added services -- is quickly developing. Still we know very little of the player attitudes towards digital distribution of games. Therefore the study at hand focuses on examining the players' experiences and notions concerning online distribution. Based on the findings the following factors have a significant influence on how players consider digital distribution: amount of time used on game playing, the social activities related to games, familiarity with other forms of downloadable content. A notable majority of those who had downloaded games highlighted the importance of the following issues: wide variety of games available, ease of finding downloadable games, affordability and simple payment methods. At the same time, more than half of these people announced that they still preferred to have their games as physical copies.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Why MMORPG players do what they do: relating motivations to action categories</title>
	<abstract>This paper presents an analysis of relations between player motivation and behaviour in a Massively Multiplayer Online Role-Playing Game (MMORPG). Player motivation is measured in terms of percentile ranks of motivational components for MMORPG players defined by Nick Yee. Player behaviour is described through previously defined action categories for MMORPGs (Trading, Questing, Dungeons, Raiding, Player versus Player Combat, and Communication). We conduct a player survey and perform measurements on the client side for a group of 104 players of World of Warcraft. Additionally we examine the importance of both voice and textual communication in MMORPGs.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>People, places, and play: player experience in a socio-spatial context</title>
	<abstract>Digital games frequently give rise to engaging and meaningful social interactions, both over the internet and in the real and tangible world of the gamer. This is the focus of the present paper, which explores digital gaming as a situated experience, shaped by socio-spatial contingencies. In particular we discuss how co-players, audience, and their spatial organization shape play and player experience and review supporting evidence for this. We present a framework describing social processes underlying situated social play experience and how these are shaped by the player the game's socio-spatial and media context. The core of this framework describes various 'sociality characteristics', and discusses these both in terms of co-located and mediated social game settings.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Communicative patterns and flow experience of MUD players</title>
	<abstract>Group online role-playing was investigated. Russian MUD players (N = 347) took part in online survey research. Demographic analysis, way analysis, explorative and confirmatory factor analyses were done. The following hypotheses were proved to be correct: MUD players experience flow; flow is positively correlated with interactive patterns.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Personal Identity, Agency and the Multiplicity Thesis</title>
	<abstract>I consider whether there is a plausible conception of personal identity that can accommodate the `Multiplicity Thesis' (MT), the thesis that some ways of creating and deploying multiple distinct online personae can bring about the existence of multiple persons where before there was only one. I argue that an influential Kantian line of thought, according to which a person is a unified locus of rational agency, is well placed to accommodate the thesis. I set out such a line of thought as developed by Carol Rovane, and consider the conditions that would have to be in place for the possibility identified by MT to be realised. Finally I briefly consider the prospects for MT according to neo-Lockean and animalist views of personhood.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>CSCW at play: 'there' as a collaborative virtual environment</title>
	<abstract>Video games are of increasing importance, both as a cultural phenomenon and as an application of collaborative technology. In particular, many recent online games feature persistent collaborative virtual environments (CVEs), with complex social organisation and strong social bonds between players. This paper presents a study of <i>'There'</i>, one such game, focusing on how <i>There</i> has been appropriated by its players. In particular we describe how its flexibility has allowed players to develop their own forms of play within the game. Three aspects of <i>There</i> are discussed: first, how the environment supports a range of social activities around objects. Second, how the chat environment is used to produce overlapping chat and how the game itself provides topics for conversation. Lastly, how the 'place' of <i>There</i> is a fluid interaction space that supports safe interactions between strangers. The paper concludes by drawing design lessons concerning the importance of supporting shared online activity, interaction between strangers, and the difficulties of designing for play.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Influence of individual characteristics, perceived usefulness and ease of use on mobile entertainment adoption</title>
	<abstract>This study aims at empirically investigating the influence of individual characteristics, Perceived Usefulness (PU) and Perceived Ease of Use (PEOU) on the Consumer Intention to Use (CIU) mobile entertainment (m-entertainment&#47;ME) in Malaysia. This study employs the Technology Acceptance Model (TAM) in the context of m-entertainment (ME) by incorporating individual characteristics with TAM to provide better understanding and insights about the adoption of ME among Malaysians. The findings revealed that PU, PEOU, academic qualification and past adoption behaviour as factors that influence Malaysian ME adoption. This study contributed to better understanding and insights regarding factors that influence ME adoption. This study has successfully incorporated individual characteristics with the TAM. The enhanced model offers a greater understanding of user acceptance of ME in Malaysia.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Modeling ping times in first person shooter games</title>
	<abstract>In First Person Shooter (FPS) games the Round Trip Time (RTT), i.e., the sum of the network delay from client to server and the network delay from server to client, impacts the gamer's performance considerably. Game client software usually has a built-in process to measure this RTT (also referred to as ping time), and therefore gamers do not want to connect to servers with a long ping time. This paper develops a methodology to evaluate the ping time in a scenario where gamers access a common gaming server over an access network, consisting of a link per user that connects this user to a shared aggregation node that in turn is connected to the gaming server via a bottleneck link. First, a model for the traffic the users and the server generate, is proposed based on experimental results of previous papers. It turns out that the characteristics of the (downstream) traffic from server to clients differ substantially from the characteristics of the client-to-server (upstream) traffic. Then, two queuing models are developed (one for the upstream and one for the downstream direction) and combined such that a quantile of the RTT can be calculated given all traffic and network parameters (packet sizes, packet inter-arrival times, link rate, network load, ...). This methodology is subsequently used to assess the (quantile of the) RTT in a typical Digital Subscriber Line (DSL) access scenario. In particular, given the capacity dedicated to gaming traffic on the bottleneck link (between the aggregation node and gaming server), the number of gamers (or equivalently the gaming load the bottleneck link can support) is determined under the restriction that the quantile of the RTT should not exceed a predefined bound. It turns out that this tolerable load is surprisingly low in most circumstances. Finally, it is remarked that this conclusion depends to some extent on the details of the downstream traffic characteristics and that measurements reported in literature do not give conclusive evidence on the exact value of all parameters, such that, although the qualitative conclusion still remains valid, additional experiments could refine the detailed quantitative results.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Reviewing the need for gaming in education to accommodate the net generation</title>
	<abstract>There is a growing interest in the use of simulations and games in Dutch higher education. This development is based on the perception that students belong to the 'gamer generation' or 'net generation': a generation that has grown up with computer games and other technology affecting their preferred learning styles, social interaction patterns and technology use generally. It is often argued that in education this generation prefers active, collaborative and technology-rich learning, i.e. learning methods that involve extensive computer use and collaboration among students. Gaming is then proposed as a new teaching method which addresses these requirements. This article presents the results of a survey which studied whether this discourse is also applicable to higher education students from the Netherlands and whether games, considered as active, collaborative and technology-rich learning experiences, are of greater importance in the formal education of today's students. Of 1432 respondents from eight Dutch institutes of higher education surveyed between 2005 and 2009, about 25% fit our criteria of being a clear representative of the net generation. Furthermore, our analysis shows that there is little difference, and no statistically significant difference, in active, collaborative and technology-rich learning preferences between the representatives and non-representatives of the net generation. Furthermore, no large or statistically significant differences were found between representatives and non-representatives of the net generation with respect to the value they accorded to gaming in education. Overall our dataset did not fit the expectations raised by the net generation theory, with the percentage of students who fit the criteria being much lower than expected. However, regardless of whether they represented the net generation or not, in general our respondents preferred collaborative and technology-rich learning and deemed games a valuable teaching method.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Cognitive-bias toward gaming-related words and disinhibition in World of Warcraft gamers</title>
	<abstract>This study investigated cognitive biases toward gaming-related words and differences in cognitive performance among twelve World of Warcraft players (WWP) and thirty non-players (NP). We measured response to valenced common English and WoW jargon words using a computer-based go/no-go task. Sometimes positive valence words were the targets for the 'go' response, with negative-valence words as the distracters, sometimes the reverse. Target discrimination (d') and response disinhibition (C) were calculated using a signal detection analysis. Based on questionnaire responses, there were no differences between groups in depression, anxiety, smoking or drinking, but WWP reported significantly more screen and gaming time (17.31h/week versus 4.12 among NP). WWP had faster reaction time (RT) and better discrimination of targets from distracters (high d') but also showed higher disinhibition (low C). WWP also showed cognitive-bias toward game-related words in the form of higher d' for WoW jargon than common English and more disinhibition to positive-valence WoW jargon. Similar to past studies which have found alcoholics to have cognitive biases toward alcohol-related words, WWP who play frequently showed cognitive biases toward words related to the World of Warcraft game.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Internet inequality: The relationship between high school students' Internet use in different locations and their Internet self-efficacy</title>
	<abstract>This research study utilized the framework of digital inequality proposed by DiMaggio and Hargittai (2001) to examine the relationships among the subdimensions of Internet inequality and their outcomes. We firstly investigated the relationships between constructs of technical apparatus, autonomy of use, availability of social support, variation of use at different locations of Internet access (school, home, Internet cafe, and combinations of these locations) and Internet self-efficacy (ISE). Then the relationships between ISE and high school students' exploratory behavior and academic were also investigated. The survey was developed from reliable instruments used in previous research to measure the following variables: Internet Self-Efficacy, Internet accessibility at home and school, exploratory behaviors, academic performance, study use, leisure use, parents influence, superior influence, and training support. Internet access at the Internet cafe, gender, and self-reported academic achievement were added to the student survey. Bivariate correlation and regression statistical analyses were conducted to find significant relationships among these variables. ANOVA statistical analysis was used to find significant differences among groups. Significant findings indicated that digital inequality in Internet existed in school, home and Internet cafe and students with Internet access at home had the highest level of ISE. Our study also showed that different dimension of Internet inequality had different relationships with ISE. Home Internet accessibility positively related to ISE. Availability of social support from school had a greater effect than that from home as parents influence did not associate with ISE. And last, the variation of use was also related to ISE. Leisure use at Internet cafe, leisure use at home and study use at home positively associated with ISE. In addition, at home and Internet cafe, the relationship between leisure use and ISE was stronger than that between study use and ISE. As to the outcome of ISE, high levels of ISE were positively related to exploratory behaviors, and for those students who used the Internet at school and home, higher ISE related to better academic performance.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A business privacy model for virtual communities</title>
	<abstract>Virtual communities in online networks have created unprecedented opportunities for consumer cooperation, which, naturally, impacts business functions significantly. While virtual communities are now formed for almost every field of life that one could imagine, they still lack a business model by which consumers can make the most of their private data and presumed privacy rights. This paper proposes a business privacy model for virtual communities based on allowing consumers to capitalise on the value of their personal information and get something of value in return. The proposed business model employs a privacy risk quantification mechanism, a game theoretic negotiation protocol and a risk-based premium to determine the consumers' payoff.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Passive network forensics: behavioural classification of network hosts based on connection patterns</title>
	<abstract>Passive monitoring of the data entering and leaving an enterprise network can support a number of forensic objectives. We have developed analysis techniques for NetFlow data that use behavioural identification and can confirm individual host roles and behaviours expressed as connection patterns. By looking at the way a given machine interacts with others, it is often possible to determine the role of the machine based solely on the network data. Host behaviours as characterized by NetFlow data are not stationary. Evolutionary changes occur as the result of new applications, computational and communications paradigms. Compromised machines often undergo changes in behaviour that range from subtle to dramatic. We use behavioural changes to identify role shifts and to trace the malicious or unintentional propagation of that change to other machines. Observed behavioural characteristics from over a year of traffic captures containing ordinary behaviours as well as a variety of compromises of interest are presented as examples for the forensics practitioner or researcher.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Player experience and technical performance prospects for distributed 3D gaming in private and public settings</title>
	<abstract>Distributed gaming enables access to interactive media from devices based on different platforms. It facilitates users to enjoy video games in various environments without the need for using a single device or operating system. Understanding the potential and limitations of such gaming-on-demand systems is key for their adoption and further growth in public places. This paper presents an in-depth, quantitative study performed with the Games@Large (GaL) distributed-gaming system and its potential users at an Internet café in Genoa, Italy. The approach of the study was multilevel, covering the player experience and user acceptance aspects as well as technical performance peculiarities. Results show that the GaL system has a high potential at Internet cafes, in particular when playing a casual genre game. Furthermore, results provide recommendations for deploying such systems in terms of social setting and technical aspects. The methodology and findings of the GaL system tests can be applied to similar game streaming systems and used as input for theories on social digital game play.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Games without frontiers: On the moral and psychological implications of violating taboos within multi-player virtual spaces</title>
	<abstract>When assessing the appropriateness of massively multiplayer online role-playing games, it is our contention that questions dealing with the morality of their content - especially regarding the more 'adult' nature of potential interactions - are the wrong sorts of questions to ask. Instead, when considering the permissibility of such games, a more informative strategy is to focus on what gamers are able to deal with, psychologically, especially regarding taboo violation. Thus, we argue that there is nothing morally problematic with online gamespace per se, no matter how prohibitive the simulated behaviour is offline (as long as the space is frequented by adults only). Instead, we should concern ourselves with whether the potential moral freedoms afforded the online gaming community are psychologically healthy: For it is our contention that underlying any change to the gamer's behaviour offline is the need (in some) to seek psychological parity across domains (making congruent one's identity and actions in both the virtual and offline worlds). It is therefore not so much what games are doing to us that is of concern, here, but what we are doing to ourselves through the process of seeking psychological parity.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Design issues for Peer-to-Peer Massively Multiplayer Online Games</title>
	<abstract>Massively Multiplayer Online Games (MMOGs) are increasing in both popularity and scale, and while classical Client&#47;Server (C&#47;S) architectures convey some benefits, they suffer from significant technical and commercial drawbacks. This realisation has sparked intensive research interest in adapting MMOGs to Peer-to-Peer (P2P) architectures. This paper articulates a comprehensive set of six design issues to be addressed by P2P MMOGs, namely Interest Management (IM), game event dissemination, Non-Player Character (NPC) host allocation, game state persistency, cheating mitigation and incentive mechanisms. Design alternatives for each issue are systematically compared, and their interrelationships discussed. We further evaluate how well representative P2P MMOG architectures fulfil the design criteria.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Implicit learning as a design strategy for learning games: Alert Hockey</title>
	<abstract>Concussion education and prevention for youth hockey players has been an issue of recent concern amongst sport medicine practitioners and hockey's administrative bodies. This article details the assessment of a sports-action hockey video game that aims to reduce the aggressive and negligent behaviours that can lead to concussions. The game, termed Alert Hockey, was designed to modify game playing behaviour by embedding an implicit teaching mechanism within the gameplay. In Alert Hockey, participants were expected to learn by simply playing to win, in contrast to playing to learn. We studied learning in an experimental simulated environment where the possibility to win the game was exaggerated as a consequence of desirable safety behaviours (positive learning group) and effectively reduced as a consequence of undesirable (negative learning group) behaviour. The positive learning group significantly improved their mean score on a composite behavioural indicator compared with no significant change amongst control group participants. The results demonstrate that implicit learning embedded in a sports-action game can lead to changes in game-play behaviour.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The effects of interactive brand placements in online games on children's cognitive, affective, and conative brand responses</title>
	<abstract>This study investigated how persuasive messages integrated in an online game affects children's cognitive, affective, and conative responses to the brand, as well as their attitude toward the game itself. An experiment conducted among 2453 girls between the ages of 11 and 17 demonstrated that confrontation with interactive brand placement in the game resulted in more positive attitudes toward the game, higher top of mind awareness of the brand, more positive brand images, and more favorable behavioral intentions. In addition, consistent with persuasion literature and theories on child development, this study showed that there was a three-way interaction effect between exposure to the brand placement, age, and prior brand use for behavioral intentions. The youngest girls who had no prior experience with the brand were more strongly influenced by the brand placement than the oldest girls who had no prior brand experience.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A hybrid P2P communications architecture for zonal MMOGs</title>
	<abstract>Distributed Virtual Environments are becoming more popular in today's computing and communications among people. Perhaps the most widely used form of such environments is Massively Multiplayer Online Games (MMOG), which are in the form of client/server architecture that requires considerable server resources to manage a large number of distributed players. Peer-to-peer communication can achieve scalability at lower cost but may introduce other difficulties. Synchronous communication is a prime concern for multi-user collaborative applications like MMOGs where players need frequently interaction with each other to share their game states. In this article, we present a hybrid MMOG architecture called MM-VISA (Massively Multiuser VIrtual Simulation Architecture). In this architecture, servers and peers are coupled together to take the inherent advantages of the centralized architecture and the scalability of distributed systems. As the virtual world is decomposed into smaller manageable zones, the players' random movement causes reorganization at the P2P overlay structure. The frequent nature of movements along with unintelligent zone crossing approaches, currently implemented in MMOGs, breaks synchronous communication. To limit such problem, we consider players' gaming characteristics to intelligently define routing paths. A graph-theoretic framework is incorporated for overlay oriented real-time distributed virtual environments. We shall show that interest-driven zone crossing, dynamic shared region between adjacent zones, and clustering of entities based on their attributes significantly decrease unstable overlay situations. The effectiveness of the presented system is justified through simulation.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Strategy-based behavioural biometrics: a novel approach to automated identification</title>
	<abstract>Behavioural intrusion detection is a frequently used for insuring network security. We expand behaviour based intrusion detection approach to a new domain of game networks. Specifically, our research shows that a behavioural biometric signature can be generated based on the strategy used by an individual to play a game. We wrote software capable of automatically extracting behavioural profiles for each player in a game of Poker. Once a behavioural signature is generated for a player, it is continuously compared against player's current actions. Any significant deviations in behaviour are reported to the game server administrator as potential security breaches.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Future-proofing advergaming: a systematisation for the media buyer</title>
	<abstract>The idea of using games as carriers for goal-oriented strategically shaped rhetorical messages, i.e. advertising and propaganda, has been much talked about. Those who produce games take an interest in such messages as a way to find new revenue streams and new customers. Media strategists are interested in finding the audiences that are leaving traditional media and turning to games. It could be fruitful for media strategists and game producers to meet, but as the meaning of the term advergaming is becoming diluted, that meeting is becoming difficult. This paper is an attempt to facilitate such a meeting by giving an overview of the planned rhetorical functions of ludic activities. This will hopefully lead to a structure of concepts useful to the scientist as well as to the practicing communications strategy planner.</abstract>
	<search_task_number>18</search_task_number>
	<query>online gaming behaviour characteristics</query>
	<relevance>0</relevance>
  </item>



  <item>
    <title>Data visualization of teen birth rate data using freely available rapid prototyping tools</title>
	<abstract>Making sense of large data sets can be challenging without visual aids. The purpose of this project was to use freely available, web-based tools to rapidly visualize and enable the exploration of relationships within a complex data set. The data set leveraged was teen birth statistics in Texas from 2001 -- 2004. This information is used by researchers and public health administrators for public health decision making. Current tools used to explore this data set are table driven and are difficult to use. Using data presented in Exhibit, an open-source publishing framework, end users were able to successfully explore a complex data set. Given the users' enthusiastic response to the displays, we conclude that this tool is appropriate and useful for this purpose. The relatively low cost and effort to set up and maintain this display makes it ideal for organizations with low budgets and limited resources, but with a need to analyze complex data sets.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Aesthetic and auditory enhancements for multi-stream information sonification</title>
	<abstract>Sonification is an emerging modality of information representation, the auditory equivalent of visualization employing non-speech sound to display attributes of form, pattern, recurrence and trends in abstract data. Like data-art or visual and auditory art-forms driven by data content directly mapped to their rendering, sonification shares the goal of aesthetic representation (auditory graphing) in a way to better and more accessibly convey the message to broader consumer audiences. Often, the simple re-contextualization of dense abstract data in an auditory graph (or sonification) is sufficient to highlight long-term trends, to hear regularities (patterns) and anomalies in periodicity of time-series data and to assimilate very subtle and fine transformations. Sonification is also optimal for certain working or ambient situations that are visually rich or visually saturated, when we seek to command topical and peripheral attention with relevant cues. Auditory display is also an alternative to visualization for people with visual impairments. Exploring the premise that sonification should be both aesthetic and informative, i.e. listenable, attractive and engaging, this paper summarises the findings of 3 experiments conducted to determine ways to better represent and access dense information mapped on to more than one concurrent stream of information. Specifically, we show evidence that spatialization of informative events coinciding in time can be more clearly distinguished and that timbre (or tone colour / tone quality) characteristics can serve to further reinforce spatial and stream separation. These findings combine to develop comprehensible methods for representing complex data-sets. We consider human cognition, auditory perception and audio reproduction technologies that each influence the ability to display information sonically.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>If You Can't See the Pattern, Is It There?</title>
	<abstract>Analytic methods are capable of finding structure in even complex data sets and, indeed, some methods will find structure whether it is there or not. Confirming and understanding analytic results can be difficult unless some way of visualising them can be found. Both global overviews and displays of local detail are required and these have to be blended intelligently together. This paper discusses the development of coherent graphical tools for exploring and explaining large multidimensional data sets, emphasising the importance of using an interactive approach.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A knowledge integration framework for information visualization</title>
	<abstract>Users can better understand complex data sets by combining insights from multiple coordinated visual displays that include relevant domain knowledge. When dealing with multidimensional data and clustering results, the most familiar displays and comprehensible are 1- and 2-dimensional projections (histograms, and scatterplots). Other easily understood displays of domain knowledge are tabular and hierarchical information for the same or related data sets. The novel parallel coordinates view [6] powered by a direct-manipulation search, offers strong advantages, but requires some training for most users. We provide a review of related work in the area of information visualization, and introduce new tools and interaction examples on how to incorporate users' domain knowledge for understanding clustering results. Our examples present hierarchical clustering of gene expression data, coordinated with a parallel coordinates view and with the gene annotation and gene ontology.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Interconnected media for human-centered understanding</title>
	<abstract>Today, there are many systems with large amounts of complex data sets. Visualizing these systems in a way that enlightens the user and provides a profound understanding ofthe respective information space is one of the big information visualization research challenges. Keim states that it is no longer possible to display an overview of these systems as proposed in Shneiderman's information seeking mantra. To overcome this incapacity and to provide a solution to the dilemma of time multiplexing- vs. space multiplexing techniques, we propose the context-sensitive use of a collection of animated 3D metaphors. These metaphors are integrated in a flexible framework called HANNAH. This provides the possibility to interconnect media of various types in order to bridge the semantic gab as required for human-centered applications according to Elgammal.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Visualizing Data</title>
	<abstract>Enormous quantities of data go unused or underused today, simply because people can't visualize the quantities and relationships in it. Using a downloadable programming environment developed by the author, Visualizing Data demonstrates methods for representing data accurately on the Web and elsewhere, complete with user interaction, animation, and more. How do the 3.1 billion A, C, G and T letters of the human genome compare to those of a chimp or a mouse? What do the paths that millions of visitors take through a web site look like? With Visualizing Data, you learn how to answer complex questions like these with thoroughly interactive displays. We're not talking about cookie-cutter charts and graphs. This book teaches you how to design entire interfaces around large, complex data sets with the help of a powerful new design and prototyping tool called "Processing". Used by many researchers and companies to convey specific data in a clear and understandable manner, the Processing beta is available free. With this tool and Visualizing Data as a guide, you'll learn basic visualization principles, how to choose the right kind of display for your purposes, and how to provide interactive features that will bring users to your site over and over. This book teaches you: The seven stages of visualizing data -- acquire, parse, filter, mine, represent, refine, and interactHow all data problems begin with a question and end with a narrative construct that provides a clear answer without extraneous detailsSeveral example projects with the code to make them workPositive and negative points of each representation discussed. The focus is on customization so that each one best suits what you want to convey about your data set The book does not provide ready-made "visualizations" that can be plugged into any data set. Instead, with chapters divided by types of data rather than types of display, you'll learn how each visualization conveys the unique properties of the data it represents -- why the data was collected, what's interesting about it, and what stories it can tell. Visualizing Data teaches you how to answer questions, not simply display information.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Visualizing Data</title>
	<abstract>Enormous quantities of data go unused or underused today, simply because people can't visualize the quantities and relationships in it. Using a downloadable programming environment developed by the author, Visualizing Data demonstrates methods for representing data accurately on the Web and elsewhere, complete with user interaction, animation, and more. How do the 3.1 billion A, C, G and T letters of the human genome compare to those of a chimp or a mouse? What do the paths that millions of visitors take through a web site look like? With Visualizing Data, you learn how to answer complex questions like these with thoroughly interactive displays. We're not talking about cookie-cutter charts and graphs. This book teaches you how to design entire interfaces around large, complex data sets with the help of a powerful new design and prototyping tool called "Processing". Used by many researchers and companies to convey specific data in a clear and understandable manner, the Processing beta is available free. With this tool and Visualizing Data as a guide, you'll learn basic visualization principles, how to choose the right kind of display for your purposes, and how to provide interactive features that will bring users to your site over and over. This book teaches you: 
The seven stages of visualizing data -- acquire, parse, filter, mine, represent, refine, and interact.
How all data problems begin with a question and end with a narrative construct that provides a clear answer without extraneous details.
Several example projects with the code to make them work.
Positive and negative points of each representation discussed. The focus is on customization so that each one best suits what you want to convey about your data set.
The book does not provide ready-made "visualizations" that can be plugged into any data set. Instead, with chapters divided by types of data rather than types of display, you'll learn how each visualization conveys the unique properties of the data it represents -- why the data was collected, what's interesting about it, and what stories it can tell. Visualizing Data teaches you how to answer questions, not simply display information.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The visualization of uncertainty</title>
	<abstract>The graphical depiction of uncertainty information is emerging as a problem of great importance in the field of visualization. Scientific data sets are not considered complete without indications of error, accuracy, or levels of confidence, and this information is often presented as charts and tables alongside visual representations of the data. Uncertainty measures are often excluded from explicit representation within data visualizations because the increased visual complexity incurred can cause clutter, obscure the data display, and may lead to erroneous conclusions or false predictions. However, uncertainty is an essential component of the data, and its display must be integrated in order for a visualization to be considered a true representation of the data. The growing need for the addition of qualitative information into the visual representation of data, and the challenges associated with that need, command fundamental research on the visualization of uncertainty.
This dissertation seeks to advance approaches for uncertainty visualization by exploring techniques from scientific and information visualization, creating new visual devices to handle the complexities of uncertainty data, and combining the most effective display methods into the Ensemble-Vis framework for visual data analysis. Many techniques exist for graphical data display. However, their usage on data with uncertainty information is not straightforward. This work begins by first exploring existing methods for data visualization and assessing their application to uncertainty. New visual metaphors are then presented for the depiction of salient features of data distributions, including indications of uncertainty. These new methods are inspired by proven visual data analysis techniques, but account for the requirements of large, complex data sets. Finally, Ensemble-Vis is presented, which combines effective uncertainty visualization techniques with interactive selection, linking, and querying to provide a user-driven, component-based framework for data investigation, exploration, and analysis.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Visualization of Large Complex Datasets Using Virtual Reality</title>
	<abstract>Major advances in high performance computing software and hardware have made it possible for designers and analysts to obtain detailed information about the behavior of complex systems. However, this has lead to a proliferation of vast amounts of data, which is beyond the ability of humans to easily comprehend using traditional visualization methods.The method presented here combines emerging visualization tools with data filtering and data mining techniques. This method uses Virtual Reality (VR) coupled with multi-media techniques to display information within an environment that allows non-expert users to understand and interpret the data. A prototype visualization tool was developed under a NASA SBIR Phase II effort.This paper builds on the results of this work and suggests a visualization framework that can be used to examine, comprehend and interpret results in any complex data set, particularly those used in support of decision making activities.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The versatility of color mapping</title>
	<abstract>Extracting information from large amounts of data by using tables of numbers is difficult. Often, such data can be presented more effectively with graphics. The reduction in the cost of memory has allowed more powerful display systems to provide for the simultaneous display of hundreds, thousands, and even millions of colors. Effective and efficient manipulation of the colors in the display system is necessary to manage the use of such a large number of colors. These extended color capabilities can also be used to enrich the understanding of presentations of complex data sets. Applications which previously might have required the user to mentally correlate several displays can now display the same information in a single image with a corresponding increase in user understanding and accuracy of interpretation.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The Visual Display of Parallel Performance Data</title>
	<abstract>Data visualization can help users decipher scientific and engineering data and better comprehend large, complex data sets. The authors present a high-level abstract model for performance visualization that relates behavior abstractions to visual representations in a structured way. This model is based on two principles: Displays of performance information are linked directly to parallel performance models, and performance visualizations are designed and applied in an integrated environment. The authors explain some advantages of adhering to these principles. They begin by establishing a context for users to clearly understand performance information, defining terms such as perspective, semantic context, and subview mapping. Next, they describe the techniques used to scale graphical views as data sets become very large. Finally, they discuss concepts such as user perception and interaction, comparisons and cross-correlations between related views or representations, and information extraction. On the basis of this conceptual foundation, the authors present examples of practical applications for the model. These case studies address topics such as concurrency and communication in data-parallel computation, access patterns for data distributions, and critical paths in parallel computation. The authors conclude by discussing the relationship between performance visualization and general scientific visualization.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Design choices when architecting visualizations</title>
	<abstract>In this paper, we focus on some of the key design decisions we faced during the process of architecting a visualization system and present some possible choices, with their associated advantages and disadvantages. We frame this discussion within the context of Rivet, our general visualization environment designed for rapidly prototyping interactive, exploratory visualization tools for analysis. As we designed increasingly sophisticated visualizations, we needed to refine Rivet in order to be able to create these richer displays for larger and more complex data sets. The design decisions we discuss in this paper include the internal data model, data access, semantic meta-data information the visualization can use to create effective visual encodings, the need for data transformations in a visualization tool, modular objects for flexibility, and the tradeoff between simplicity and expressiveness when providing methods for creating visualizations.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Design choices when architecting visualizations</title>
	<abstract>In this paper, we focus on some of the key design decisions we faced during the process of architecting a visualization system and present some possible choices, with their associated advantages and disadvantages. We frame this discussion within the context of Rivet, our general visualization environment designed for rapidly prototyping interactive, exploratory visualization tools for analysis. As we designed increasingly sophisticated visualizations, we needed to refine Rivet in order to be able to create these richer displays for larger and more complex data sets.

The design decisions we discuss in this paper include: the internal data model, data access, semantic meta-data information the visualization can use to create effective visual encodings, the need for data transformations in a visualization tool, modular objects for flexibility, and the tradeoff between simplicity and expressiveness when providing methods for creating visualizations.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Visual Classification: Expert Knowledge Guides Machine Learning</title>
	<abstract>Humans use intuition and experience to classify everything they perceive, but only if the distinguishing patterns are visible. Machine-learning algorithms can learn class information from data sets, but the created classes' meaning isn't always clear. A proposed mixed-initiative approach combines intuitive visualizations with machine learning to tap into the strengths of human and machine classification. The use of visualizations in an expert-guided clustering technique allows the display of complex data sets in a way that allows human input into machine clustering. Test participants successfully employed this technique to classify analytic activities using behavioral observations of a creative-analysis task. The results demonstrate how visualization of the machine-learned classification can help users create more robust and intuitive categories.[Erratum: The print version contains an error that has been corrected in the online version. The first sentence of the "Method" section (p. 9) contained the URL www.nspace.com. This URL is not related to the nSpace analytic software environment discussed in the article. Instead of the URL, reference 2 should have been cited (P. Proulx et al., "nSpace and GeoTime: A VAST 2006 Case Study," IEEE Computer Graphics and Applications, vol. 27, no. 5, 2007, pp. 46-56). We apologize for these errors.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Parallel Performance Visualization: From Practice to Theory</title>
	<abstract>Parallel performance visualization presents fundamental challenges--complex data sets, the artificial aspects of parallel computation, the need for integrated analysis and visual models, and the dependence on the user's mental perspective. Despite these challenges, important concepts for visualization design and use have begun to emerge.A history of the practice of implementing and using visualization in performance analysis tools reveals an underlying theory in the form of an abstract model of performance visualization and applied visualization concepts. The model is based on the integration of performance evaluation models and performance displays. The concepts involve aspects of context, scaling, user perception and interaction, comparison, and extraction of information.Several performance scenarios demonstrate how to apply the model and concepts and principles in real tools to overcome the problems of understanding performance. These scenarios measure utilization and communication, utilization statistics, data access, and parallel performance optimization. The model and concepts form a solid basis for future research in parallel performance visualization.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An Optimization-based Approach to Dynamic Visual Context Management</title>
	<abstract>We are building an intelligent multimodal conversation system to aid users in exploring large and complex data sets. To tailor to diverse user queries introduced during a conversation, we automate the generation of system responses, including both spoken and visual outputs. In this paper, we focus on the problem of visual context management, a process that dynamically updates an existing visual display to effectively incorporate new information requested by subsequent user queries. Specifically, we develop an optimization-based approach to visual context management. Compared to existing approaches, which normally handle predictable visual context updates, our work offers two unique contributions. First, we provide a general computational framework that can effectively manage a visual context for diverse, unanticipated situations encountered in a user-system conversation. Moreover, we optimize the satisfaction of both semantic and visual constraints, which otherwise are difficult to balance using simple heuristics. Second, we present an extensible representation model that uses feature-based metrics to uniformly define all constraints. We have applied our work to two different applications and our evaluation has shown the promise of this work.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Perceptually-motivated graphics, visualization and 3D displays</title>
	<abstract>This course presents timely, relevant examples on how researchers have leveraged perceptual information for optimization of rendering algorithms, to better guide design and presentation in (3D stereoscopic) display media, and for improved visualization of complex or large data sets. Each presentation will provide references and short overviews of cutting-edge current research pertaining to that area. We will ensure that the most up-to-date research examples are presented by sourcing information from recent perception and graphics conferences and journals such as ACM Transactions on Perception, paying particular attention work presented at the 2010 Symposium on Applied Perception in Graphics and Visualization.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>High performance multivariate visual data exploration for extremely large data</title>
	<abstract>One of the central challenges in modern science is the need to quickly derive knowledge and understanding from large, complex collections of data. We present a new approach that deals with this challenge by combining and extending techniques from high performance visual data analysis and scientific data management. This approach is demonstrated within the context of gaining insight from complex, time-varying datasets produced by a laser wakefield accelerator simulation. Our approach leverages histogram-based parallel coordinates for both visual information display as well as a vehicle for guiding a data mining operation. Data extraction and subsetting are implemented with state-of-the-art index/query technology. This approach, while applied here to accelerator science, is generally applicable to a broad set of science applications, and is implemented in a production-quality visual data analysis infrastructure. We conduct a detailed performance analysis and demonstrate good scalability on a distributed memory Cray XT4 system.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Hierarchical parallel coordinates for exploration of large datasets</title>
	<abstract>Our ability to accumulate large, complex (multivariate) data sets has far exceeded our ability to effectively process them in search of patterns, anomalies, and other interesting features. Conventional multivariate visualization techniques generally do not scale well with respect to the size of the data set. The focus of this paper is on the interactive visualization of large multivariate data sets based on a number of novel extensions to the parallel coordinates display technique. We develop a multiresolutional view of the data via hierarchical clustering, and use a variation on parallel coordinates to convey aggregation information for the resulting clusters. Users can then navigate the resulting structure until the desired focus region and level of detail is reached, using our suite of navigational and filtering tools. We describe the design and implementation of our hierarchical parallel coordinates system which is based on extending the XmdvTool system. Lastly, we show examples of the tools and techniques applied to large (hundreds of thousands of records) multivariate data sets.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A New Approach of Data Clustering Using a Flock of Agents</title>
	<abstract>This paper presents a new bio-inspired algorithm (FClust) that dynamically creates and visualizes groups of data. This algorithm uses the concepts of a flock of agents that move together in a complex manner with simple local rules. Each agent represents one data. The agents move together in a 2D environment with the aim of creating homogeneous groups of data. These groups are visualized in real time, and help the domain expert to understand the underlying structure of the data set, like for example a realistic number of classes, clusters of similar data, isolated data. We also present several extensions of this algorithm, which reduce its computational cost, and make use of a 3D display. This algorithm is then tested on artificial and real-world data, and a heuristic algorithm is used to evaluate the relevance of the obtained partitioning.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>EMA-Tactons: vibrotactile external memory aids in an auditory display</title>
	<abstract>Exploring any new data set always starts with gathering overview information. When this process is done non-visually, interactive sonification techniques have proved to be effective and efficient ways of getting overview information, particularly for users who are blind or visually impaired. Under certain conditions, however, the process of data analysis cannot be completed due to saturation of the user's working memory. This paper introduces EMA-Tactons, vibrotactile external memory aids that are intended to support working memory during the process of data analysis, combining vibrotactile and audio stimuli in a multimodal interface. An iterative process led to a design that significantly improves the performance (in terms of effectiveness) of users solving complex data explorations. The results provide information about the convenience of using EMA-Tactons with other auditory displays, and the iterative design process illustrates the challenges of designing multimodal interaction techniques.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>From analysis to interactive exploration: building visual hierarchies from OLAP cubes</title>
	<abstract>We present a novel framework for comprehensive exploration of OLAP data by means of user-defined dynamic hierarchical visualizations. The multidimensional data model behind the OLAP architecture is particularly suitable for sophisticated analysis of large data volumes. However, the ultimate benefit of applying OLAP technology depends on the “intelligence” and usability of visual tools available to end-users.

The explorative framework of our proposed interface consists of the navigation structure, a selection of hierarchical visualization techniques, and a set of interaction features. The navigation interface allows users to pursue arbitrary disaggregation paths within single data cubes and, more importantly, across multiple cubes. In the course of interaction, the navigation view adapts itself to display the chosen path and the options valid in the current context. Special effort has been invested in handling non-trivial relationships (e.g., mixed granularity) within hierarchical dimensions in a way transparent to the user.

We propose a visual structure called Enhanced Decomposition Tree to to be used along with popular “state-of-the-art” hierarchical visualization techniques. Each level of the tree is produced by a disaggregation step, whereas the nodes display the specified subset of measures, either as plain numbers or as an embedded chart. The proposed technique enables a stepwise descent towards the desired level of detail while preserving the history of the interaction. Aesthetic hierarchical layout of the node-link tree ensures clear structural separation between the analyzed values embedded in the nodes and their dimensional characteristics which label the links. Our framework provides an intuitive and powerful interface for exploring complex multidimensional data sets.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A quantitative scale-setting approach for building multi-scale spatial databases</title>
	<abstract>Finding a balance between data redundancy and manipulation efficiency can sometimes be problematic when building multi-scale spatial databases (MSDBs). In order to render MSDBs real-timely, coarser representations with fewer vertices should be invoked once the response time exceeds the tolerable limitation. Of course, this would result in faster response speeds. With the vertex histogram making it possible to simulate the change in the number of vertices queried (NVQ) for the most complex area when the display scale shrinks, a NVQ-based approach is set forward to fix the representations by comparing the NVQ of the most complex area with the tolerable NVQ. This causes the number of levels and definite scales to be determined not only by the dataset itself but also by the generalization method. As a result, the arbitrary appointment of levels or scales when building multi-scale spatial datasets is avoided. It is argued that this approach facilitates the use of fewer representation levels to build a multi-scale database with a reasonable scale-setting scheme. Furthermore, the response time is restricted within the limit set in advance. A case study with a real spatial dataset verifies the effect of the proposed approach.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>MediVol: A Practical Initial Evaluation of Refined, 3D, Interactive Volumetric Representations of Soft Tissue Pathologies in Virtual Environments</title>
	<abstract>Effective processing of source data matched to appropriate visualisation can greatly enhance the user’s ability to explore and comprehend complex information. While this is a fundamental problem for many domains, in medical applications it is particularly important. None-invasive scanning technologies, such as MRI, have greatly enhanced our ability to ‘image’ the internal body, however the resultant visualisation is often difficult to comprehend due to both inadequacies in the scanning process and sub-optimal approaches to visualisation and data representation. These factors impose significant cognitive load on the user, requiring skill and experience to accurately comprehend detail and intense concentration, and in less experienced users, to understand the structures present. Our broader research aims to identify whether 3D representations of MRI data sets offer a more intuitive means of viewing the data and thereby enable easier understanding and comprehension of the scanned body region. As part of this research we have constructed a 3D MRI viewing application, raaMediVol, which utilises recent developments in 3D computer graphics hardware, to present an interactive environment that enables the user to view both traditional 2D slice representations and an enhanced 3D volumetric form that is freely explorable and configurable both on traditional 2D computer desktop displays and within Immersive Projection Technologies (IPTs)[1]. Initial evaluation of the two representational paradigms been undertaken through the comparative assessment of experienced clinicians’ performance in diagnosing a range of soft tissue pathologies within the shoulder, displayed in both traditional 2D slice, and evolved 3D volumetric representational form. An overview of the application, its technical operation, and the results of the evaluation trials are presented.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Perceiving ordinal data haptically under workload</title>
	<abstract>Visual information overload is a threat to the interpretation of displays presenting large data sets or complex application environments. To combat this problem, researchers have begun to explore how haptic feedback can be used as another means for information transmission. In this paper, we show that people can perceive and accurately process haptically rendered ordinal data while under cognitive workload. We evaluate three haptic models for rendering ordinal data with participants who were performing a taxing visual tracking task. The evaluation demonstrates that information rendered by these models is perceptually available even when users are visually busy. This preliminary research has promising implications for haptic augmentation of visual displays for information visualization.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Visual assessment of software evolution</title>
	<abstract>Configuration management tools have become well and widely accepted by the software industry. Software Configuration Management (SCM) systems hold minute information about the entire evolution of complex software systems and thus represent a good source for process accounting and auditing. However, it is still difficult to use the entire spectrum of information such tools maintain. Currently, significant effort is being done in the direction of mining this kind of software repositories for extracting data to support relevant assessments. In this article we propose a concerted set of visualization tools and techniques for the assessment of software evolution based on the information stored into SCM systems. Firstly, we introduce a generic way to obtain models of source code at different levels of detail and from different perspectives. Secondly, we propose a set of visual representations and techniques to efficiently and effectively depict the evolution of these code models. These techniques target specific questions and assessments, from the detailed code developer perspective to the overview required by system architects and project managers. We detail the concrete implementation of two such code models and corresponding visual representations. The file view describes code change at line level across multiple versions of a single file, or small number of files. The project view shows changes at file level across complete software projects. All our views share the same visual and interactive techniques, enabling users to easily switch among and correlate between them. We implement our visual techniques to quickly and compactly display and navigate the evolution of tens of thousands of artifacts on a single screen. We demonstrate our techniques with several use cases performed on real world, industry-size code bases and outline the concrete findings and ways our visualizations helped in understanding various types of code changes.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>First-person experience and usability of co-located interaction in a projection-based virtual environment</title>
	<abstract>Large screen projection-based display systems are very often not used by a single user alone, but shared by a small group of people. We have developed an interaction paradigm allowing multiple users to share a virtual environment in a conventional single-view stereoscopic projection-based display system, with each of the users handling the same interface and having a full first-person experience of the environment.Multi-viewpoint images allow the use of spatial interaction techniques for multiple users in a conventional projection-based display. We evaluate the effectiveness of multi-viewpoint images for ray selection and direct object manipulation in a qualitative usability study and show that interaction with multi-viewpoint images is comparable to fully head-tracked (single-user) interaction. Based on ray casting and direct object manipulation, using tracked PDA's as common interaction device, we develop a technique for co-located multi-user interaction in conventional projection-based virtual environments. Evaluation of the VRGEO Demonstrator, an application for the review of complex 3D geo-seismic data sets in the oil-and-gas industry, shows that this paradigm allows multiple users to each have a full first-person experience of a complex, interactive virtual environment.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Visualizing Cortical Waves and Timing from Data</title>
	<abstract>Waves are a fundamental mechanism for conveying information in many physical problems. Direct visualization techniques are often used to display wave fronts. However, the information derived from such visualizations may not be as central to an investigation as an understanding of how the location, structure and time course of the wave change as key experimental parameters are varied. In experimental data, these questions are confounded by noise and incomplete data. Recognition of waves in networks of neurons is additionally complicated by the presence of long-range physical connections and recurrent excitation. This paper applies visual techniques to analyze the structural details of waves in response data from the turtle visual cortex. We emphasize low-cost visualizations that allow comparisons across neural data sets and variables to reconstruct the choreography for a complex response.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>AppMap: exploring user interface visualizations</title>
	<abstract>In traditional graphical user interfaces, the majority of UI elements are hidden to the user in the default view, as application designers and users desire more space for their application data. We explore the benefits of dedicating additional screen space for presenting an alternative visualization of an application's user interface. Some potential benefits are to assist users in examining complex software, understanding the extent of an application's capabilities, and exploring the available features. We propose user interface visualizations, alternative representations of an application's interface augmented with usage information. We introduce a design space for UI visualizations and describe some initial prototypes and insights based on this design space. We then present AppMap, our new design, which displays the entire function set of AutoCAD and allows the user to interactively explore the visualization which is augmented with visual overlays displaying analytical data about the functions and their relations. In our initial studies, users welcomed this new presentation of functionality, and the unique information that it presents.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Scientific visualization today</title>
	<abstract>Scientific visualization is the field within computer science that focuses on the interactive display of complex data. It is a tool that can be used to compress a large amount of data into an image for a more efficient understanding. Scientific visualization can be considered a graphical process equivalent to numerical analysis. The methods used allows human to gain greater knowledge and insight into data through three increasing levels of comprehension: presentation, understanding, and prediction. Presentation graphics is the starting point (and often the ending point) in the scientific visualization process. At this level the goal is to communicate the data in ways that are easy to digest. From the presentation, researchers attempt to extract a deeper understanding of the processed data sets. Researchers can then attempts to predict behavior and form hypothesis aided by their enhanced understanding.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Mid-air pan-and-zoom on wall-sized displays</title>
	<abstract>Very-high-resolution wall-sized displays offer new opportunities for interacting with large data sets. While pointing on this type of display has been studied extensively, higher-level, more complex tasks such as pan-zoom navigation have received little attention. It thus remains unclear which techniques are best suited to perform multiscale navigation in these environments. Building upon empirical data gathered from studies of pan-and-zoom on desktop computers and studies of remote pointing, we identified three key factors for the design of mid-air pan-and-zoom techniques: uni- vs. bimanual interaction, linear vs. circular movements, and level of guidance to accomplish the gestures in mid-air. After an extensive phase of iterative design and pilot testing, we ran a controlled experiment aimed at better understanding the influence of these factors on task performance. Significant effects were obtained for all three factors: bimanual interaction, linear gestures and a high level of guidance resulted in significantly improved performance. Moreover, the interaction effects among some of the dimensions suggest possible combinations for more complex, real-world tasks.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Demonstration of LMMP (lunar mapping and modeling) using amazon's elastic compute cloud</title>
	<abstract>The Lunar Mapping and Modeling Project (LMMP) is currently being built by NASA. The goal is to provide a single point of access to the best state of knowledge of the moon's terrain, rock and crater fields, resource maps, lighting conditions and thermal conditions. The architecture and design employ a variety of technologies, allowing for execution of complex models, the processing of large data sets and the distribution of the information, over the internet, to both authenticated users and the general public. The architecture supports a variety of light-weight clients including a Flash based display, an iPad/iPhone interface and a set of programmatic APIs that allow rich clients to interact with the LMMP system.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Verbal assistance to visual search in complex visualizations</title>
	<abstract>We present an exploratory experiment of the possible contribution of multimodal messages to facilitating visual search in complex scenes (e.g., visualizations of very large data sets). The main objective is to evaluate the actual contribution of verbal information to improving the efficiency (i.e., accuracy and speed) of the detection and selection (using the mouse) of visual targets. The evaluation focuses on the performances and subjective satisfaction of future users in three situations where the main free variable is the target's preliminary presentation: (i) display of the isolated target, (ii) verbal designation + information on its location in the scene, and (iii) multimodal (visual + oral) presentation. Multimodal target presentations improved subjects' performances significantly; they also received the highest subjective ratings.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Computing environment for the statistical analysis of large and complex data</title>
	<abstract>Analyzing large data has become very feasible with recent advances in modern technology. Data acquisition has become very fine grained and is available across many scenarios from home power consumption, to network data. Such data sets which can collect data at the level of seconds, quickly become massive. The storage of such data is now possible because of the rapid fall in the hardware prices. It is has become the statisticians' challenge to analyze such massive data sets with the same level of comprehensive detail as is possible for much smaller analyses. Any detailed analysis of such data sets, necessarily creates many subsets and many more data structures. We need approaches to store and compute with them by taking advantage of modern technology such as distributed compute clusters. This forms the backdrop to the three chapters of the thesis: Visualization database for large data sets, a keystroke detection algorithm derived from analyzing hundreds of gigabytes of network data and a merger of the R and Hadoop programming environments that enables topics covered in the first two chapters.
Comprehensive visualization that preserves the information in the data requires a visualization database (VDB): many displays, some with many pages, and with one or more panels per page. A single display results from partitioning the data into subsets, and using the same method to display each subset in a sample of subsets, typically one per panel. The time of the analyst to produce a display is not increased by choosing a large subset over a small one, and every page does not necessarily need to be studied. Some displays might be studied in their entirety; for others, studying only a small fraction of the pages might suffice. On-the-fly computation without storage does not generally succeed because computation time is too large. The sizes and numbers of displays of VDBs require a rethinking all areas involved in data visualization, including the following: Methods of display design that enhance pattern perception to enable rapid page scanning; Automation algorithms for basic display elements such as the aspect ratio, scales across panels, line types and widths, and symbol types and sizes; Methods for subset sampling; Viewers designed for multi-panel, multi-page displays that scale across different amounts of physical screen area.

One such example of a detailed analysis of hundreds of gigabytes of data is the keystroke detection algorithm. This is a streaming algorithm detects SSH client keystroke packets in any TCP connection. Input data are timestamps and TCP-IP header fields of packets in both directions, measured at a monitor on the path between the hosts. The algorithm uses the packet dynamics just preceding and following a client packet with data to classify the packet as a keystroke or non-keystroke. The dynamics are described by classification variables derived from the arrival timestamps and the packet data sizes, sequence numbers, acknowledgement numbers, and flags. The algorithm succeeds because a keystroke creates an identifiable dynamical pattern. One application is identification of any TCP connection as an SSH interactive session, allowing detection of backdoor SSH servers. More generally, the algorithm demonstrates the potential for the use of detailed packet dynamics to classify connections.

The above analysis of network data would be extremely unwieldy (if not impossible) were it not using distributed file systems and computing frameworks. RHIPE is a software system that integrates the R open source project for statistical computing and visualization with the Apache Hadoop Distributed File System (HDFS) and the Apache MapReduce software framework for the distributed processing of massive data sets across a cluster. Distributed programming with massive data sets is by nature complex—issues such as data storage, scheduling and fault tolerance must all be handled. RHIPE uses its tight integration with the HDFS tostore data across the cluster. Similarly, it takes advantage of MapReduce to efficiently utilize all the processing cores of the cluster. Vital, but difficult to implement details, such as task scheduling, bandwidth optimization and recovery from failing computers are handled by Hadoop MapReduce. Most importantly, RHIPE hides these details from the R user, by providing an idiomatic R interface to Hadoop and HDFS cluster. The design of RHIPE strives for a balance between conceptual simplicity, ease of use and flexibility. Algorithms, designed for the MapReduce programming model, can be implemented using the R language, executed from R's REPL (read-eval-print-loop) and the results are directly returned to the user.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Building a foundation for human centric multi-dimensional data analysis</title>
	<abstract>This dissertation introduces a foundation for human centric, large-scale, multi-dimensional data analysis. This research enables collaborative workspaces by utilizing ultra-large, high-resolution display environments, distributed rendering techniques, and new interface modalities. Contributions include interactive visualization of ultra-large layered data sets, real-time distributed large-scale data acquisition, scalable distributed approaches for video playback in tiled display environments, natural exploratory techniques for multi-dimensional data, and multi-user interface technologies for distributed display environments.
Presented is a technique for the interactive visualization and interrogation of multi-dimensional gigapixel imagery, allowing several users to simultaneously compare and contrast complex data layers in a collaborative environment. This system is augmented through a distributed data gathering and visualization component, which allows researchers to pull, construct, and interrogate geospatial information from remote servers. Multimedia content can also be configured interactively, and viewed in many side-by-side comparisons using various color and temporal filters. Techniques also allow for the scalable playback of video content through a distributed architecture.

Additionally, multi-touch devices allow for hands-on analysis of massive, multi-dimensional data. The presented research introduces a set of natural metaphors, which allow for rapid analysis of global and local characteristics in the data set. The interface modalities can also be used for volumetric data, where position, gesture and pressure information are used for voxel density and depth specific operations. Finally, the combination of multi-touch devices and tiled display environments is presented, enabling multi-user collaborative environments.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>springScape</title>
	<abstract>The interpretation of microarray and other high-throughput data is highly dependent on the biological context of experiments. However, standard analysis packages are poor at simultaneously presenting both the array and related bioinformatic data. We have addressed this challenge by developing a system springScape based on ‘spring embedding’ and an ‘information landscape’ allowing several related data sources to be dynamically combined while highlighting one particular feature. Each data source is represented as a network of nodes connected by weighted edges. The networks are combined and embedded in the 2-D plane by spring embedding such that nodes with a high similarity are drawn close together. Complex relationships can be discovered by varying the weight of each data source and observing the dynamic response of the spring network. By modifying Procrustes analysis, we find that the visualizations have an acceptable degree of reproducibility. The ‘information landscape’ highlights one particular data source, displaying it as a smooth surface whose height is proportional to both the information being viewed and the density of nodes. The algorithm is demonstrated using several microarray data sets in combination with protein-protein interaction data and GO annotations. Among the features revealed are the spatio-temporal profile of gene expression and the identification of GO terms correlated with gene expression and protein interactions. The power of this combined display lies in its interactive feedback and exploitation of human visual pattern recognition. Overall, springScape shows promise as a tool for the interpretation of microarray data in the context of relevant bioinformatic information. Contact: d.jones@cs.ucl.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Topological Analysis Using Morse Theory and Auditory Display</title>
	<abstract>Simplicial complexes, or triangulated meshes, are widely used for computer graphics and for representing data in computational science and engineering. Often it is necessary to determine some topological characteristics of a complex, either for scientific study or to maintain properties while the data set is undergoing some transformation. In this paper we define the wave traversal algorithm and study its use for topological characterization of a complex. Wave traversal is a natural extension of breadth-first search of a graph to n-dimensional simplicial complexes. As defined, each wave is a subcomplex whose simplices have a certain distance from an arbitrary start vertex. We prove that if waves on a surface split and rejoin then there is a topological hole called a tunnel in the surface. We then prove stronger results about wave traversal and topology. We show that wave traversal can be extended to a discrete Morse function on triangulated manifolds. Critical points of a Morse function can be used to characterize the homotopy type of an n-manifold, and they give bounds on the betti numbers of the manifold. The Morse function we derive is straightforward to compute, but expensive computationally both in terms of space and time. So we show that we can achieve similar and even stronger results about the critical points using combinatorial methods, without first computing the function itself. We supply algorithms for doing this in the case of 2-manifolds. We also prove that barycentric subdivision eliminates degeneracies in wave traversal. Results from problems in other areas of research that have used the wave traversal paradigm could be re-examined in light of our findings. Finally, we demonstrate the use of wave traversal for creating an auditory display of a simplicial complex. This has application particularly when the complex is large or higher-dimensional, but audio has been shown to reinforce visual display in all cases. The auditory display is intended to convey information about the complex to the user in a interactive exploratory manner. We detail two of the auditory displays that we have developed using wave traversal.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Integrating Scientific Workflows and Large Tiled Display Walls: Bridging the Visualization Divide</title>
	<abstract>Modern in-silico science (or e-Science) is a complex process, often involving multiple steps conducted across different computing environments. Scientific workflow tools help scientists automate, manage and execute these steps, providing a robust and repeatable research environment. Increasingly workflows generate data sets that require scientific visualization, using a range of display devices such as local workstations, immersive 3D caves and large display walls. Traditionally, this display step handled outside the workflow, and output files are manually copied to a suitable visualization engine for display. This inhibits the scientific discovery process disconnecting the workflow that generated the data from the display and interpretation processes. In this paper we present a solution that links scientific workflows with a variety of display devises, including large tiled display walls. We demonstrate the feasibility of the system by a prototype implementation that leverages the Kepler workflow engine and the SAGE display software. We illustrate the use of the system with a case study in workflow driven microscopy.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Color display and interactive interpretation of three-dimensional data</title>
	<abstract>Three-dimensional results from engineering and scientific computations often involve the display and interpretation of a large volume of complex data. A method is developed for color display of 30 data with several interactive options to facilitate interpretation. The method is based on representing points whose values fall within a specified range as a single hue. An image is formed by overlaying successive 2D frames with increasing hue lightness towards the front. Interactive options to aid interpretations are viewpoint, contour lines, multiple range display, slicing, veiled surfaces, and stereo image pairs. The display method is successfully applied to several types of data. The overall structure and variations of the 30 data are observable, as well as transients which may be overlooked in a large input data set.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A taxonomy of glyph placement strategies for multidimensional data visualization</title>
	<abstract>Glyphs are graphical entities that convey one or more data values via attributes such as shape, size, color, and position. They have been widely used in the visualization of data and information, and are especially well suited for displaying complex, multivariate data sets. The placement or layout of glyphs on a display can communicate significant information regarding the data values themselves as well as relationships between data points, and a wide assortment of placement strategies have been developed to date. Methods range from simply using data dimensions as positional attributes to basing placement on implicit or explicit structure within the data set. This paper presents an overview of multivariate glyphs, a list of issues regarding the layout of glyphs, and a comprehensive taxonomy of placement strategies to assist the visualization designer in selecting the technique most suitable to his or her data and task. Examples, strengths, weaknesses, and design considerations are given for each category of technique. We conclude with some general guidelines for selecting a placement strategy, along with a brief description of some of our future research directions.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Visual feedback in querying large databases</title>
	<abstract>In this paper, we describe a database query system that provides visual relevance feedback in querying large databases. The goal of our system is to support the query specification process by using each pixel of the display to represent one data item of the database. By arranging and coloring the pixels according to their relevance for the query, the user gets a visual impression of the resulting data set. Using sliders for each condition of the query, the user may change the query dynamically and receives immediate feedback by the visual representation of the resulting data set. By using multiple windows for different parts of a complex query, the user gets visual feedback for each part of the query and, therefore, will easier understand the overall result. The system may be used to query any database that contains tens of thousands to millions of data items, but it is especially helpful to explore large data sets with an unknown distribution of values and to find the interesting hot spots in huge amounts of data. The direct feedback allows to visually display the influence of incremental query refinements and, therefore, allows a better, easier and faster query specification.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Interactive exploration of volumetric data sets with a combined visual and haptic interface</title>
	<abstract>The analysis of complex volumetric data sets is a critical component of many scientific and engineering applications. The difficulty of understanding the increasing amount of data generated by these applications motivates the need for effective and intuitive visualization approaches that allow users to extract relevant information from the data in a relatively short amount of time. Even though a great variety of visualization techniques are available today, the effectiveness of a particular approach is often limited by the ability of the user to interact with the presented information. Incorporating three-dimensional interaction metaphors into the visualization environment has great potential to assist the user during the data exploration process.
This dissertation develops and evaluates interactive visualization methods that combine three-dimensional graphical and haptic displays for the exploration of volumetric data sets. A number of techniques are suggested that are suitable for augmenting the traditional visualization interface with visual and haptic interaction methods for guiding the user during exploration. The advantage of the presented approach is that the rendering algorithms do not require a preprocessing stage to extract intermediate geometric representations from the data. Thus, the user has direct control over the visual and haptic display via a set of visualization parameters.

In addition to demonstrating the effectiveness of the presented methods over previous approaches, the dissertation describes a controlled experiment that examines the contribution of haptic guidance to vector field exploration tasks. The results of the evaluation suggest that properly combined visual and haptic feedback can significantly improve the speed and accuracy of volumetric data exploration. The presented methods are further illustrated with examples of representative visualization applications.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A display processor design</title>
	<abstract>This paper describes the results of a collaborative design effort aimed at development of a general purpose display system for the SDS-940 time-shared computer. The important features of the system evolved gradually from a number of separate design goals. We wanted a display system that would:

1. Contain an extensive but straightforward set of display generating commands.

2. Be able to generate pictures from highly complex data structures.

3. Allow easy access to display files from user programs in the main computer.

4. Provide some immediate feedback and interactive processing service to the display user, and be able to call upon the main computer for more extensive service.

5. Permit attachment of special purpose display generation and interactive hardware, as well as multiple display consoles.

6. Be capable of time-sharing its central resources among separate console-users.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Multi-Level Clustering and its Visualization for Exploratory Spatial Analysis</title>
	<abstract>Exploratory spatial analysis is increasingly necessary as larger spatial data is managed in electro-magnetic media. We propose an exploratory method that reveals a robust clustering hierarchy from 2-D point data. Our approach uses the Delaunay diagram to incorporate spatial proximity. It does not require prior knowledge about the data set, nor does it require preconditions. Multi-level clusters are successfully discovered by this new method in only O(nlogn) time, where n is the size of the data set. The efficiency of our method allows us to construct and display a new type of tree graph that facilitates understanding of the complex hierarchy of clusters. We show that clustering methods adopting a raster-like or vector-like representation of proximity are not appropriate for spatial clustering. We conduct an experimental evaluation with synthetic data sets as well as real data sets to illustrate the robustness of our method.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Organic Pie Charts</title>
	<abstract>We present a new visualization of the distance and cluster structure of high dimensional data. It is particularly well suited for analysis tasks of users unfamiliar with complex data analysis techniques as it builds on the well known concept of pie charts. The non-linear projection capabilities of Emergent Self-Organizing Maps (ESOM) are used to generate a topology-preserving ordering of the data points on a circle. The distance structure within the high dimensional space is visualized on the circle analogously to the U-Matrix method for two-dimensional SOM. The resulting display resembles pie charts but has an organic structure that naturally emerges from the data. Pie segments correspond to groups of similar data points. Boundaries between segments represent low density regions with larger distances among neighboring points in the high dimensional space. The representation of distances in the form of a periodic sequence of values makes time series segmentation applicable to automated clustering of the data that is in sync with the visualization. We discuss the usefulness of the method on a variety of data sets to demonstrate the applicability in applications such as document analysis or customer segmentation.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Modeling human performance on the traveling salesman problem: empirical studies and computational simulations</title>
	<abstract>The traveling salesperson problem (TSP) consists of attempting to find the shortest complete tour through a series of points (cities), starting and ending with the same point. This problem is a member of the set of computationally hard, or NP complete, problems, for which the best known solutions are obtained in exponential time relative to the problem size. Human solutions, however, are known to be both rapid and of high quality. This thesis attempts to determine the processes used by humans in solving these problems and encode them in the form of a running computer program.
For this purpose, TSP problem presentation software was developed to present problems and collect data at a finer grain scale than is possible with previous paper-and-pencil versions of the task. Data recording included real-time recording of mouse position at a 100Hz sampling rate. This software was used to conduct a set of six experiments. Experiment 1 validated the computer interface using a problem set from a previous TSP study (MacGregor and Ormerod, 1996). Experiment 2 employed a data set consisting of random problems matched on size to the problems employed in Experiment 1. Experiment 3 constrained the start of the problem solving episode by fixing a starting point. Experiment 4 introduced larger randomly constructed problems. Experiment 5 employed shaped problems intended to explore problem complexity due to contours. Experiment 6 employed an obscured interface that presented high-frequency spatial information only in the area immediately under the computer mouse, blurring distant areas.

The experiments conducted provide a number of findings. Problem solution times are linearly proportional to the number of problem points, with individual moves taking ∼1 second. Solution accuracy is generally within ∼5% of optimal path length for the majority of solvers and problems. Accuracy is lower on more complex problems where complexity is determined by factors including the number of points in a problem, problem shape, and the number of interior (non-convex hull) points.

Problem solving is preceded by 2-3 seconds during which the problem is scanned and a rough solution is developed. Experiment 6 demonstrated that planning requires only low-frequency spatial information provided by a blurred display. The pattern of mouse movement was similar in this condition to the normal interface experiments. This pattern involved initial travel that was indirect, consisting of multiple exploratory movements. Beyond the first two moves, mouse movements become more direct and there is a reduction of distance traveled and time and strokes taken.

Evidence for learning is minimal indicating that the strategies and processes used by human solvers are innate or well-practiced, and therefore general (e.g., perceptually based, weak methods, etc.).

These empirical findings guided the construction of a TSP solving algorithm designated the GL-TSP, that combines global perceptual processing and local serial problem solving. (Abstract shortened by UMI.)</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Exploring largeness, complexity and scalability from the OOT perspective (panel)</title>
	<abstract>This panel will lay the foundation for understanding the needs of Large Systems so that OO practitioners can:&amp;bull; appreciate the problems faced;&amp;bull; understand the issues involved; and&amp;bull; re-orient the approaches to provide a viable solution when participating in similar efforts.Specifically, this panel will establish the foundation for discussions on Large Systems by establishing concepts, exposing terminology, and highlighting the state-of-the art.Large applications are usually complex and display one or more of the following dimensions of largeness:&amp;bull; Processing power: requiring tens to hundreds of gigabytes of memory and hundreds of gigaflops performance&amp;bull; High connectivity: highly-connected, systems can show aggregate behavior with complex characteristics: they can become chaotic.&amp;bull; Online access: Archival of terabytes of information, with the need to provide online access to information&amp;bull; Archival and online retrieval: The two technologies (database and archival storage), however, currently do not interoperate. There is a need to develop interfaces to integrate these two technologies.&amp;bull; Data-intensive scientific applications: These involve constructing a data handling infrastructure that simplifies the effort required to maintain petabyte archives, identify relevant data sets within the archive, move, the data to processing platforms, and distribute the data sets across multiple nodes.&amp;bull; Internet access: Large applications often require simultaneous access of information by millions of users worldwide;they must provide acceptable response times.&amp;bull; Scaleable architecture: The systems must be prepared to support exponential growth of application load.Is OOT up to the task? OO practitioners will be encountering some of these application domains in the near future. Some may have already gained some experience in trying to solve these problems. However, the literature in OO does not provide sufficient evidence to believe that OO is ready for such large systems today. Part of the problem is the cross-disciplinary nature of these problems requires a steep learning curve for OO practitioners to be effective. The modeling of these problems with an OO approach is also a challenge. Current 00 methods do not do a good job of supporting multiple views of a domain, and multiple layers of a complex application domain.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Overview + detail in a Tomahawk mission-to-platform assignment tool: applying information visualization in support of an asset allocation planning task</title>
	<abstract>Information visualization techniques such as overview + detail displays have traditionally been applied and studied in domains with static data sets supporting information retrieval tasks. This study examines how these techniques can be extended to the design of interfaces for decision support systems (DSSs). Specifically, we developed a computerized decision support tool to assist Naval Tomahawk Strike Coordinators in the complex process of assigning a set of planned missions to a set of available launch platforms based on a number of different constraints and objectives, and compared user performance on two realistic scenarios (a within-subjects factor) across two versions of this tool (a between-subjects factor). The first version of the Mission-to-Platform Assignment Tool provided users with only a set of detail displays when assigning missions, whereas the second version had an additional, abstracted 'overview' display that allowed users to see the effect of early decisions on later decisions. The results showed that subjects performing this planning task with the overview + details display version completed scenarios, on average, 21% faster, with 22% fewer errors and with 74% fewer required workspace navigation activities than a comparable group using just the detail displays version. Subjects in the former group also rated their situational awareness 14% higher than those subjects without the overview display.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Investigating data with Andrews plots</title>
	<abstract>Most data that are collected are multivariate in nature, and much of them can be regarded as continuous. In the initial stages of analysis, graphic displays can be used to explore the data, but for multivariate data, traditional histograms or two- or three-dimensional scatter plots may miss complex relationships that exist in the data set. A number of methods for graphically displaying multivariate data have been suggested. However, these are not generally available in major statistical packages and are thus largely not used by researchers. One of the most appealing methods is that of Andrews Plots. This article discusses the potential uses of Andrews Plots and makes them accessible to users through the production of a freely available add-in for Microsoft Excel. The use of Andrews Plots is demonstrated by using data from the 2001 Parliamentary General Election in the United Kingdom.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A computational investigation into maladaptive aggression</title>
	<abstract>Maladaptive aggression is a serious, growing, and ill-understood problem for today's society. This is due, at least in part, to a lack of knowledge regarding how economic, social, environmental, and/or psychiatric factors influence the incidence of maladaptive aggression at the individual patient level. Standard statistics have teased out the etiological factors that correlate with the incidence of maladaptive aggression in the population as a whole, but have proven ineffective at predicting which patients will display maladaptive aggression and which will not. This failing is likely due to the high number of interactions implicated in the development of maladaptive aggression, the heterogeneous nature of maladaptive aggression, a distinct lack of adequate data sets, or some combination thereof. Thus, the most comprehensive data set on maladaptive aggression available to date was examined with a variety of techniques to overcome some of the difficulties inherent in predicting maladaptive aggression. The techniques employed were: adapted standard statistics, statistical pattern recognition, machine learning, and a suite of novel predictive analysis tools developed during the process of this dissertation. The results of this investigation provide a method capable of illuminating the complex causes and correlates of maladaptive aggression with both expected and unexpected factors implicated by the current data set. Notably, this method is easily adapted for use with other data sets and a broad range of predictive problems, especially the investigation of mental illnesses.</abstract>
	<search_task_number>13</search_task_number>
	<query>complex data set display</query>
	<relevance>0</relevance>
  </item>




  <!-- 1-10 -->
  <item>
    <title>Integration of Vision and Inertial Sensors for 3D Arm Motion Tracking in Home-based Rehabilitation</title>
	<abstract>The integration of visual and inertial sensors for human motion tracking has attracted significant attention recently, due to its robust performance and wide potential application. This paper introduces a real-time hybrid solution to articulated 3D arm motion tracking for home-based rehabilitation by combining visual and inertial sensors. Data fusion is a key issue in this hybrid system and two different data fusion methods are proposed. The first is a deterministic method based on arm structure and geometry information, which is suitable for simple rehabilitation motions. The second is a probabilistic method based on an Extended Kalman Filter (EKF) in which data from two sensors is fused in a predict-correct manner in order to deal with sensor noise and model inaccuracy. Experimental results are presented and compared with commercial marker-based systems, CODA and Qualysis. They show good performance for the proposed solution.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Detection and Tracking of Human Motion with a View-Based Representation</title>
	<abstract>This paper proposes a solution for the automatic detection and tracking of human motion in image sequences. Due to the complexity of the human body and its motion, automatic detection of 3D human motion remains an open, and important, problem. Existing approaches for automatic detection and tracking focus on 2D cues and typically exploit object appearance (color distribution, shape) or knowledge of a static background. In contrast, we exploit 2D optical flow information which provides rich descriptive cues, while being independent of object and background appearance. To represent the optical flow patterns of people from arbitrary viewpoints, we develop a novel representation of human motion using low-dimensional spatio-temporal models that are learned using motion capture data of human subjects. In addition to human motion (the foreground) we probabilistically model the motion of generic scenes (the background); these statistical models are defined as Gibbsian fields specified from the first-order derivatives of motion observations. Detection and tracking are posed in a principled Bayesian framework which involves the computation of a posterior probability distribution over the model parameters (i.e., the location and the type of the human motion) given a sequence of optical flow observations. Particle filtering is used to represent and predict this non-Gaussian posterior distribution over time. The model parameters of samples from this distribution are related to the pose parameters of a 3D articulated model (e.g. the approximate joint angles and movement direction). Thus the approach proves suitable for initializing more complex probabilistic models of human motion. As shown by experiments on real image sequences, our method is able to detect and track people under different viewpoints with complex backgrounds.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A Low Power Architecture for HASM Motion Tracking</title>
	<abstract>This paper proposes low power VLSI architecture for motion tracking that can be used in online video applications such as in MPEG and VRML. The proposed architecture uses a hierarchical adaptive structured mesh (HASM) concept that generates a content-based video representation. The developed architecture shows the significant reducing of power consumption that is inherited in the HASM concept. The proposed architecture consists of two units: a motion estimation and motion compensation units.

The motion estimation (ME) architecture generates a progressive mesh code that represents a mesh topology and its motion vectors. ME reduces the power consumption since it (1) implements a successive splitting strategy to generate the mesh topology. The successive split allows the pipelined implementation of the processing elements. (2) It approximates the mesh nodes motion vector by using the three step search algorithm. (3) and it uses parallel units that reduce the power consumption at a fixed throughput.

The motion compensation (MC) architecture processes a reference frame, mesh nodes and motion vectors to predict a video frame using affine transformation to warp the texture with different mesh patches. The MC reduces the power consumption since it uses (1) a multiplication-free algorithm for affine transformation. (2) It uses parallel threads in which each thread implements a pipelined chain of scalable affine units to compute the affine transformation of each patch.

The architecture has been prototyped using top-down low-power design methodology. The performance of the architecture has been analyzed in terms of video construction quality, power and delay.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Effective Multi-Model Motion Tracking using Action Models</title>
	<abstract>We consider tasks where robots act on the target that is visually tracked, such as kicking a ball or pushing an object. We introduce a principled approach to incorporate models of the robot-object interaction into the tracking algorithm to effectively improve the performance of the tracker. We first present the integration of a single robot behavioral model with multiple actions into our dynamic Bayesian probabilistic tracking algorithm. We then extend to multiple motion tracking models corresponding to known multi-robot coordination plans or from multi-robot communication. We evaluate our resulting informed-tracking approach empirically in simulation and using a setup Segway robot soccer task. The input of the multiple single and multi-robot behavioral models allows a robot to visually track mobile targets with dynamic trajectories more effectively.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A novel fitting algorithm using the ICP and the particle filters for robust 3d human body motion tracking</title>
	<abstract>This paper proposes a novel fitting algorithm using the iterative closest point (ICP) registration algorithm and the particle filters for robust 3D human body motion tracking. We use the ICP registration algorithm that fits the 3D human body model to the 3D articulation data in a hierarchical manner. However, it often can not fit under the rapidly moving human body motion. To solve this problem, we combine the modified particle filter with the ICP algorithm. It can search the most appropriate motion parameters by using the observation model based on the surface normal vector and the binary valued function and the state transitional model based on the motion history information. Experimental results show that the proposed combined fitting algorithm provides accurate fitting performance and high convergence rate.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Self-occlusion handling for human body motion tracking from 3D ToF image sequence</title>
	<abstract>A 3D Time-of-flight (ToF) image is very useful to accurately track the human body motion due to its precision. However, the ToF image can not provide occluded 3D data because it also has a limitation of camera viewpoint. This paper proposes a self-occlusion handling scheme for human body motion tracking from 3D ToF image sequence. The proposed self-occlusion handling scheme consists of two steps: detect whether the body part is occluded or not and then estimate its motion from estimating the motion of non-occluded its adjacent body parts. Occlusion can be easily detected by using the eigenvalue analysis of 3D ToF data gathered from the joint point of each body part, and their motions can be estimated by calculating the rotation of the occluded body part. To apply it to the human body motion tracking, we use the Iterative closest point (ICP) algorithm and particle filter to track even the motion of fast moving body parts. Experimental results show that the human body motion tracking with the proposed self-occlusion handling scheme can correctly estimate even the motion of the self-occluded body part by comparing the estimated joint points with the manually marked joint points.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>3D human motion tracking based on a progressive particle filter</title>
	<abstract>Human body tracking has received increasing attention in recent years due to its broad applicability. Among these tracking algorithms, the particle filter is considered an effective approach for human motion tracking. However, it suffers from the degeneracy problem and considerable computational burden. This paper presents a novel 3D model-based tracking algorithm called the progressive particle filter to decrease the computational cost in high degrees of freedom by employing hierarchical searching. In the proposed approach, likelihood measure functions involving four different features are presented to enhance the performance of model fitting. Moreover, embedded mean shift trackers are adopted to increase accuracy by moving each particle toward the location with the highest probability of posture through the estimated mean shift vector. Experimental results demonstrate that the progressive particle filter requires lower computational cost and delivers higher accuracy than the standard particle filter.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>High Resolution Tracking of Non-Rigid Motion of Densely Sampled 3D Data Using Harmonic Maps</title>
	<abstract>We present a novel automatic method for high resolution, non-rigid dense 3D point tracking. High quality dense point clouds of non-rigid geometry moving at video speeds are acquired using a phase-shifting structured light ranging technique. To use such data for the temporal study of subtle motions such as those seen in facial expressions, an efficient non-rigid 3D motion tracking algorithm is needed to establish inter-frame correspondences. The novelty of this paper is the development of an algorithmic framework for 3D tracking that unifies tracking of intensity and geometric features, using harmonic maps with added feature correspondence constraints. While the previous uses of harmonic maps provided only global alignment, the proposed introduction of interior feature constraints allows to track non-rigid deformations accurately as well. The harmonic map between two topological disks is a diffeomorphism with minimal stretching energy and bounded angle distortion. The map is stable, insensitive to resolution changes and is robust to noise. Due to the strong implicit and explicit smoothness constraints imposed by the algorithm and the high-resolution data, the resulting registration/deformation field is smooth, continuous and gives dense one-to-one inter-frame correspondences. Our method is validated through a series of experiments demonstrating its accuracy and efficiency.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Maneuvering head motion tracking by coarse-to-fine particle filter</title>
	<abstract>Tracking a very actively maneuvering object is challenging due to the lack of state transition dynamics to describe the system's evolution. In this paper, a coarse-to-fine particle filter algorithm is proposed for such tracking, whereby one loop of the traditional particle filtering approach is divided into two stages. In the coarse stage, the particles adopt a uniform distribution which is parameterized by the limited motion range within each time step. In the following fine stage, the particles are resampled using the results of the coarse stage as the proposal distribution, which incorporates the most present observation. The weighting scheme is implemented using a partitioned color cue that implicitly embeds geometric information to enhance robustness. The system is tested by a publicly available dataset for tracking an intentionally erratic moving human head. The results demonstrate that the proposed system is capable of handling random motion dynamics with a relatively small number of particles.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A convex penalty method for optical human motion tracking</title>
	<abstract>Human motion tracking from monocular image sequences has been explored widely. However, there is a lack of a framework addressing the variety of sensing conditions. In this paper, we present a simple, efficient, and robust method for recovering plausible 3D motion from a video without knowledge of the camera's parameters. Our method transforms the motion capture problem into a convex problem and employs a hierarchical geometrical solver for the minimization. This algorithm was applied to synthetic and real image sequences with very encouraging results. Specifically, our results indicate that it can handle challenges posed by variation of lighting, partial self-occlusion, and rapid motion.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  
  
  <!-- 11-20 -->
  <item>
    <title>Contour cue based particle filter for monocular human motion tracking</title>
	<abstract>Particle filter is widely used in human motion tracking but its efficiency is low. A contour cue based particle filter algorithm is proposed in this paper for the human motion tracking in a markerless monocular video. The likelihood of sampled particles is measured by chamfer distance between two contours. One contour is extracted from the video image. The other is transformed from the sampled particle. The value of likelihood is the weight of corresponding particle. Then weighted particles are optimized by Levenberg-Marquardt method to make the final estimation closer to the posterior distribution of motion state. Apart from this, the skin part of human body is detected to constrain the sampled particles when the contour feature points are not sufficient with large occlusion. The experiment result shows that the contour cue based method is more efficient than the edge method.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Motion tracking and strain map computation for quasi-static magnetic resonance elastography</title>
	<abstract>This paper presents a new imaging method for quasi-static magnetic resonance elastography (MRE). Tagged magnetic resonance (MR) imaging of human lower leg was acquired with probe indentation using a MR-compatible actuation system. Indentation force was recorded for soft tissue elasticity reconstruction. Motion tracking and strain map of human lower leg are calculated using a harmonic phase (HARP)-based method. Simulated tagged MR images were constructed and analyzed to validate the HARP-based method. Our results show that the proposed imaging method can be used to generate accurate motion distribution and strain maps of the targeted soft tissue.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Geodesic active regions and level set methods for motion estimation and tracking</title>
	<abstract>Motion analysis in computer vision is a well-studied problem with numerous applications. In particular, the tasks of optical flow estimation and tracking are of increasing interest. In this paper, we propose a level set approach to address both aspects of motion analysis. Our approach relies on the propagation of smooth interfaces to perform tracking while using an incremental estimation of the motion models. Implicit representations are used to represent moving objects, and capture their motion parameters. Information from different sources like a boundary attraction term, a background subtraction component and a visual consistency constraint are considered. The Euler-Lagrange equations within a gradient descent method lead to a flow that deforms a set of initial curve towards the object boundaries as well an incremental robust estimator of their apparent motion. Partial extension of the proposed framework to address dense motion estimation and the case of moving observer is also presented. Promising results demonstrate the performance of the method.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A Variational Technique for Time Consistent Tracking of Curves and Motion</title>
	<abstract>In this paper, a new framework for the tracking of closed curves and their associated motion fields is described. The proposed method enables a continuous tracking along an image sequence of both a deformable curve and its velocity field. Such an approach is formalized through the minimization of a global spatio-temporal continuous cost functional, w.r.t a set of variables representing the curve and its related motion field. The resulting minimization process relies on optimal control approach and consists in a forward integration of an evolution law followed by a backward integration of an adjoint evolution model. This latter pde includes a term related to the discrepancy between the current estimation of the state variable and discrete noisy measurements of the system. The closed curves are represented through implicit surface modeling, whereas the motion is described either by a vector field or through vorticity and divergence maps depending on the kind of targeted applications. The efficiency of the approach is demonstrated on two types of image sequences showing deformable objects and fluid motions.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Implicit Probabilistic Models of Human Motion for Synthesis and Tracking</title>
	<abstract>This paper addresses the problem of probabilistically modeling 3D human motion for synthesis and tracking. Given the high dimensional nature of human motion, learning an explicit probabilistic model from available training data is currently impractical. Instead we exploit methods from texture synthesis that treat images as representing an implicit empirical distribution. These methods replace the problem of representing the probability of a texture pattern with that of searching the training data for similar instances of that pattern. We extend this idea to temporal data representing 3D human motion with a large database of example motions. To make the method useful in practice, we must address the problem of efficient search in a large training set; efficiency is particularly important for tracking. Towards that end, we learn a low dimensional linear model of human motion that is used to structure the example motion database into a binary tree. An approximate probabilistic tree search method exploits the coefficients of this low-dimensional representation and runs in sub-linear time. This probabilistic tree search returns a particular sample human motion with probability approximating the true distribution of human motions in the database. This sampling method is suitable for use with particle filtering techniques and is applied to articulated 3D tracking of humans within a Bayesian framework. Successful tracking results are presented, along with examples of synthesizing human motion using the model.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Shape-From-Silhouette Across Time Part II: Applications to Human Modeling and Markerless Motion Tracking</title>
	<abstract>In Part I of this paper we developed the theory and algorithms for performing Shape-From-Silhouette (SFS) across time. In this second part, we show how our temporal SFS algorithms can be used in the applications of human modeling and markerless motion tracking. First we build a system to acquire human kinematic models consisting of precise shape (constructed using the temporal SFS algorithm for rigid objects), joint locations, and body part segmentation (estimated using the temporal SFS algorithm for articulated objects). Once the kinematic models have been built, we show how they can be used to track the motion of the person in new video sequences. This marker-less tracking algorithm is based on the Visual Hull alignment algorithm used in both temporal SFS algorithms and utilizes both geometric (silhouette) and photometric (color) information.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Human Motion Tracking with a Kinematic Parameterization of Extremal Contours</title>
	<abstract>This paper addresses the problem of human motion tracking from multiple image sequences. The human body is described by five articulated mechanical chains and human body-parts are described by volumetric primitives with curved surfaces. If such a surface is observed with a camera, an extremal contour appears in the image whenever the surface turns smoothly away from the viewer. We describe a method that recovers human motion through a kinematic parameterization of these extremal contours. The method exploits the fact that the observed image motion of these contours is a function of both the rigid displacement of the surface and of the relative position and orientation between the viewer and the curved surface. First, we describe a parameterization of an extremal-contour point velocity for the case of developable surfaces. Second, we use the zero-reference kinematic representation and we derive an explicit formula that links extremal contour velocities to the angular velocities associated with the kinematic model. Third, we show how the chamfer-distance may be used to measure the discrepancy between predicted extremal contours and observed image contours; moreover we show how the chamfer distance can be used as a differentiable multi-valued function and how the tracker based on this distance can be cast into a continuous non-linear optimization framework. Fourth, we describe implementation issues associated with a practical human-body tracker that may use an arbitrary number of cameras. One great methodological and practical advantage of our method is that it relies neither on model-to-image, nor on image-to-image point matches. In practice we model people with 5 kinematic chains, 19 volumetric primitives, and 54 degrees of freedom; We observe silhouettes in images gathered with several synchronized and calibrated cameras. The tracker has been successfully applied to several complex motions gathered at 30 frames/second.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Dimensionality reduction using a Gaussian Process Annealed Particle Filter for tracking and classification of articulated body motions</title>
	<abstract>This paper presents a framework for 3D articulated human body tracking and action classification. The method is based on nonlinear dimensionality reduction of high-dimensional data space to low dimensional latent space. Human body motion is described by concatenation of low-dimensional manifolds that characterize different motion types. We introduce a body pose tracker thats uses the learned mapping function from latent space to body pose space. The trajectories in the latent space provide low dimensional representations of body pose sequences representing a specific action type. These trajectories are used to classify human actions. The approach is illustrated on the HumanEvaI and HumanEvaII datasets, as well as on other datasets that include scenarios of interactions between people. A comparison to other methods is presented. The tracker is shown to be robust when classifying individual actions and is also capable of the harder task of classifying interactions between people.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Nonlinear dynamic shape and appearance models for facial motion tracking</title>
	<abstract>We present a framework for tracking large facial deformations using nonlinear dynamic shape and appearance model based upon local motion estimation. Local facial deformation estimation based on a given single template fails to track large facial deformations due to significant appearance variations. A nonlinear generative model that uses low dimensional manifold representation provides adaptive facial appearance templates depending upon the movement of the facial motion state and the expression type. The proposed model provides a generative model for Bayesian tracking of facial motions using particle filtering with simultaneous estimation of the expression type. We estimate the geometric transformation and the global deformation using the generative model. The appearance templates from the global model then estimate local deformation based on thin-plate spline parameters.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Three-dimensional Motion Tracking for Beating Heart Surgery Using a Thin-plate Spline Deformable Model</title>
	<abstract>Minimally invasive cardiac surgery offers important benefits for the patient but it also imposes several challenges for the surgeon. Robotic assistance has been proposed to overcome many of the difficulties inherent to the minimally invasive procedure, but so far no solutions for compensating physiological motion are present in the existing surgical robotic platforms. In beating heart surgery, cardiac and respiratory motions are important sources of disturbance, hindering the surgeon's gestures and limiting the types of procedures that can be performed in a minimally invasive fashion. In this context, computer vision techniques can be used for retrieving the heart motion for active motion stabilization, which improves the precision and repeatability of the surgical gestures. However, efficient tracking of the heart surface is a challenging problem due to the heart surface characteristics, large deformations and the complex illumination conditions. In this article, we present an efficient method for active cancellation of cardiac motion where we combine an efficient algorithm for 3D tracking of the heart surface based on a thin-plate spline deformable model and an illumination compensation algorithm able to cope with arbitrary illumination changes. The proposed method has two novelties: the thin-plate spline model for representing the heart surface deformations and an efficient parametrization for 3D tracking of the beating heart using stereo images from a calibrated stereo endoscope. The proposed tracking method has been evaluated offline on in vivo images acquired by a DaVinci surgical robotic platform.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  
  <!-- 21-30 -->
  <item>
    <title>History: The use of the kalman filter for human motion tracking in virtual reality</title>
	<abstract>In 1960 Rudolph E. Kalman published his now famous article describing a recursive solution to the discrete-data linear filtering problem (Kalman, “A new approach to linear filtering and prediction problems,” Transactions of the ASME---Journal of Basic Engineering, 82 (D), 35--45, 1960). Since that time, due in large part to advances in digital computing, the Kalman filter has been the subject of extensive research and applications, particularly in the area of autonomous or assisted navigation. The purpose of this paper is to acknowledge the approaching 50th anniversary of the Kalman filter with a look back at the use of the filter for human motion tracking in virtual reality (VR) and augmented reality (AR).

In recent years there has been an explosion in the use of the Kalman filter in VR/AR. In fact, at technical conferences related to VR these days, it would be unusual to see a paper on tracking that did not use some form of a Kalman filter, or draw comparisons to those that do. As such, rather than attempt a comprehensive survey of all uses of the Kalman filter to date, what follows focuses primarily on the early discovery and subsequent period of evolution of the Kalman filter in VR, along with a few examples of modern commercial systems that use the Kalman filter.

This paper begins with a very brief introduction to the Kalman filter, a brief look at the origins of VR, a little about tracking in VR---in particular the work and conditions that gave rise to the use of the filter, and then the evolution of the use of the filter in VR.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Efficient tracking and ego-motion recovery using gait analysis</title>
	<abstract>We present a strategy based on human gait to achieve efficient tracking, recovery of ego-motion and 3-D reconstruction from an image sequence acquired by a single camera attached to a pedestrian. In the first phase, the parameters of the human gait are established by a classical frame-by-frame analysis, using an generalized least squares (GLS) technique. The gait model is non-linear, represented by a truncated Fourier series. In the second phase, this gait model is employed within a ''predict-correct'' framework using a maximum a posteriori, expectation-maximization (MAP-EM) strategy to obtain robust estimates of the ego-motion and scene structure, while continuously refining the gait model. Experiments on synthetic and real image sequences show that the use of the gait model results in more efficient tracking. This is demonstrated by improved matching and retention of features, and a reduction in execution time, when processing video sequences.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>3D Human Motion Tracking with a Coordinated Mixture of Factor Analyzers</title>
	<abstract>A major challenge in applying Bayesian tracking methods for tracking 3D human body pose is the high dimensionality of the pose state space. It has been observed that the 3D human body pose parameters typically can be assumed to lie on a low-dimensional manifold embedded in the high-dimensional space. The goal of this work is to approximate the low-dimensional manifold so that a low-dimensional state vector can be obtained for efficient and effective Bayesian tracking. To achieve this goal, a globally coordinated mixture of factor analyzers is learned from motion capture data. Each factor analyzer in the mixture is a "locally linear dimensionality reducer" that approximates a part of the manifold. The global parametrization of the manifold is obtained by aligning these locally linear pieces in a global coordinate system. To enable automatic and optimal selection of the number of factor analyzers and the dimensionality of the manifold, a variational Bayesian formulation of the globally coordinated mixture of factor analyzers is proposed. The advantages of the proposed model are demonstrated in a multiple hypothesis tracker for tracking 3D human body pose. Quantitative comparisons on benchmark datasets show that the proposed method produces more accurate 3D pose estimates over time than those obtained from two previously proposed Bayesian tracking methods.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Effective team-driven multi-model motion tracking</title>
	<abstract>Autonomous robots use sensors to perceive and track objects in the world. Tracking algorithms use object motion models to estimate the position of a moving object. Tracking efficiency completely depends on the accuracy of the motion model and of the sensory information. Interestingly, when the robots can actuate the object being tracked, the motion can become highly discontinuous and nonlinear. We have previously developed a successful tracking approach that effectively switches among object motion models as a function of the robot's actions. If the object to be tracked is actuated by a team, the set of motion models is quite more complex. In this paper, we report on a tracking approach that can use a dynamic multiple motion model based on a team coordination plan. We present the multi-model probabilistic tracking algorithms in detail and present empirical results both in simulation and real robot test. Our physical team is composed of a robot and a human in a real Segway soccer game scenario. We show how the coordinated plan allows the robot to better track a mobile object through the effective interaction with its human teammate.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Space-variant motion detection for active visual target tracking</title>
	<abstract>A biologically inspired approach to active visual target tracking is presented. The approach makes use of three strategies found in biological systems: space-variant sensing, a spatio-temporal-frequency-based model of motion detection and the alignment of sensory-motor maps. Space-variant imaging is used to create a 1D array of elementary motion detectors (EMDs) that are tuned in such a way as to make it possible to detect motion over a wide range of velocities while still being able to detect motion precisely. The array is incorporated into an active visual tracking system. A method of analysis and design for such a tracking system is proposed. It makes use of a sensory-motor map which consists of a phase-plane plot of the continuous-time dynamics of the tracking system overlaid onto a map of the detection capabilities of the array of EMDs. This sensory-motor map is used to design a simple 1D tracking system and several simulations show how the method can be used to control tracking performance using such metrics as overshoot and settling time. A complete 1D active vision system is implemented and a set of simple target tracking experiments are performed to demonstrate the effectiveness of the approach.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Multi-model motion tracking under multiple team member actuators</title>
	<abstract>Autonomous robots need to track objects. Object tracking relies on predefined robot motion and sensory models. Tracking is particularly challenging if the robots can actuate on the object to be tracked, as the motion can become highly discontinuous and nonlinear. We have previously developed a successful tracking approach that switches among target motion models as a function of one robot's actions. In this paper, we consider the object to be effected by a team of agents. We contribute on our team-based tracking method that can use a dynamic multi-motion model based on a team coordination plan. We present the multi-target multi-model probabilistic tracking algorithm in detail and present empirical results both in simulation and in a human-robot Segway soccer team. The team coordination plan allows the robot to much more effectively track mobile targets.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Multi-camera tracking of articulated human motion using motion and shape cues</title>
	<abstract>We present a framework and algorithm for tracking articulated motion for humans. We use multiple calibrated cameras and an articulated human shape model. Tracking is performed using motion cues as well as image-based cues (such as silhouettes and “motion residues” hereafter referred to as spatial cues,) as opposed to constructing a 3D volume image or visual hulls. Our algorithm consists of a predictor and corrector: the predictor estimates the pose at the t + 1 using motion information between images at t and t + 1. The error in the estimated pose is then corrected using spatial cues from images at t + 1. In our predictor, we use robust multi-scale parametric optimisation to estimate the pixel displacement for each body segment. We then use an iterative procedure to estimate the change in pose from the pixel displacement of points on the individual body segments. We present a method for fusing information from different spatial cues such as silhouettes and “motion residues” into a single energy function. We then express this energy function in terms of the pose parameters, and find the optimum pose for which the energy is minimised.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Monocular 3D tracking of articulated human motion in silhouette and pose manifolds</title>
	<abstract>This paper presents a robust computational framework for monocular 3D tracking of human movement. The main innovation of the proposed framework is to explore the underlying data structures of the body silhouette and pose spaces by constructing low-dimensional silhouettes and poses manifolds, establishing intermanifold mappings, and performing tracking in such manifolds using a particle filter. In addition, a novel vectorized silhouette descriptor is introduced to achieve low-dimensional, noise-resilient silhouette representation. The proposed articulated motion tracker is view-independent, self-initializing, and capable of maintaining multiple kinematic trajectories. By using the learned mapping from the silhouette manifold to the pose manifold, particle sampling is informed by the current image observation, resulting in improved sample efficiency. Decent tracking results have been obtained using synthetic and real videos.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Human body shape and motion tracking by hierarchical weighted ICP</title>
	<abstract>We present a new approach for tracking both the human body shape and the whole body motion with complete six DOF of each body limb without imposing rotation or translation constraints. First, a surface mesh with highly improved quality is obtained by using our new silhouette-based visual hull reconstruction method for each frame of multi-view videos. Then, a skinned mesh model is fitted to the data using hierarchical weighted ICP (HWICP) algorithm, where an easy-toadjust strategy for selecting the set of ICP registration points is given based on the weights of the skinned model and the Approximate Nearest Neighbors (ANN) method is applied for fast searching nearest neighbors. By comparing HWICP with the general hierarchical ICP (Iterative Closest Point) method based on synthetic data, we demonstrate the power of weighting corresponding point pairs in HWICP, especially when adjacent body segments of target are near 'cylindrical-shaped'.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An Object-Oriented Approach to Simulating Human Gait Motion Based on Motion Tracking</title>
	<abstract>Accurate bone motion reconstruction from marker tracking is still an open and challenging issue in biomechanics. Presented in this paper is a novel approach to gait motion reconstruction based on kinematical loops and functional skeleton features extracted from segmented Magnetic Resonance Imaging (MRI) data. The method uses an alternative path for concatenating relative motion starting at the feet and closing at the hip joints. From the evaluation of discrepancies between predicted and geometrically identified functional data, such as hip joint centers, a cost function is generated with which the prediction model can be optimized. The method is based on the object-oriented multibody library MOBILE, which has already been successfully applied to the development of industrial virtual design environments. The approach has been implemented in a general gait visualization environment termed Mobile Body.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  
  
  <!-- 31-40 -->
  <item>
    <title>Real-time multi-view human motion tracking using 3D model and latency tolerant parallel particle swarm optimization</title>
	<abstract>This paper demonstrates how latency tolerant parallel particle swarm optimization can be used to achieve real-time full-body motion tracking. The tracking is realized using multi-view images and articulated 3D model with a truncated cones-based representation of the body. Each CPU core computes fitness score for a single camera. On each node the algorithm uses the current temporary best fitness value without waiting for the global best one from cooperating sub-swarms. The algorithm runs at 10 Hz on eight PC nodes connected by 1 GigE.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>StrokeTrack: wireless inertial motion tracking of human arms for stroke telerehabilitation</title>
	<abstract>Stroke is a leading cause of disability in the United States and yet little technology is currently available for individuals with stroke to practice rehabilitation therapy progress in their homes. This paper presents StrokeTrack, an efficient, wearable upper limb motion tracking method for stroke rehabilitation therapy at home. StrokeTrack consists of two inertial measurement unit(IMU)s that are placed on the wrist and the elbow. Each IMU consists of a 3-axis accelerometer and a 3-axis gyroscope. In order to track the motion of the upper limb, StrokeTrack estimates the position of the forearm and upper arm by using an inertial tracking algorithm and a kinematic model. In the next step, StrokeTrack corrects the positions of the joints inferred from the inherent integration drift, and updates them. Finally, dynamic time warping (DTW) is adopted in order to check the accuracy of the patient's motions by matching them to the reference motions.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A real-time model-based human motion tracking and analysis for human computer interface systems</title>
	<abstract>This paper introduces a real-time model-based human motion tracking and analysis method for human computer interface (HCI). This method tracks and analyzes the human motion from two orthogonal views without using any markers. The motion parameters are estimated by pattern matching between the extracted human silhouette and the human model. First, the human silhouette is extracted and then the body definition parameters (BDPs) can be obtained. Second, the body animation parameters (BAPs) are estimated by a hierarchical tritree overlapping searching algorithm. To verify the performance of our method, we demonstrate different human posture sequences and use hidden Markov model (HMM) for posture recognition testing.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Video retrieval of human interactions using model-based motion tracking and multi-layer finite state automata</title>
	<abstract>Recognition of human interactions in a video is useful for video annotation, automated surveillance, and content-based video retrieval. This paper presents a model-based approach to motion tracking and recognition of human interactions using multi-layer finite state automata (FA). The system is used for widely-available, static-background monocular surveillance videos. A three-dimensional human body model is built using a sphere and cylinders and is projected on a two-dimensional image plane to fit the foreground image silhouette. We convert the human motion tracking problem into a parameter optimization problem without the need to compute inverse kinematics. A cost functional is used to estimate the degree of the overlap between the foreground input image silhouette and a projected three-dimensional body model silhouette. Motion data obtained from the tracker is analyzed in terms of feet, torso, and hands by a behavior recognition system. The recognition model represents human behavior as a sequence of states that register the configuration of individual body parts in space and time. In order to overcome the exponential growth of the number of states that usually occurs in single-level FA, we propose a multi-layer FA that abstracts states and events from motion data at multiple levels: low-level FA analyzes body parts only, and high-level FA analyzes the human interaction. Motion tracking results from video sequences are presented. Our recognition framework successfully recognizes various human interactions such as approaching, departing, pushing, pointing, and handshaking.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>An adaptive real-time skin detector based on Hue thresholding: A comparison on two motion tracking methods</title>
	<abstract>Various applications like face and hand tracking and image retrieval have made skin detection an important area of research. However, currently available algorithms are based on static features of the skin colour, or require a significant amount of computation. On the other hand, skin detection algorithms are not robust to deal with real-world conditions, like background noise, change of intensity and lighting effects. This situation can be improved by using dynamic features of the skin colour in a sequence of images. This article proposes a skin detection algorithm based on adaptive Hue thresholding and its evaluation using two motion detection technique. The skin classifier is based on the Hue histogram of skin pixels, and adapts itself to the colour of the skin of the persons in the video sequence. This algorithm has demonstrated improvement in comparison to the static skin detection method.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Nonparametric density estimation with adaptive, anisotropic kernels for human motion tracking</title>
	<abstract>In this paper, we suggest to model priors on human motion by means of nonparametric kernel densities. Kernel densities avoid assumptions on the shape of the underlying distribution and let the data speak for themselves. In general, kernel density estimators suffer from the problem known as the curse of dimensionality, i.e., the amount of data required to cover the whole input space grows exponentially with the dimension of this space. In many applications, such as human motion tracking, though, this problem turns out to be less severe, since the relevant data concentrate in a much smaller subspace than the original high-dimensional space. As we demonstrate in this paper, the concentration of human motion data on lower-dimensional manifolds, approves kernel density estimation as a transparent tool that is able to model priors on arbitrary mixtures of human motions. Further, we propose to support the ability of kernel estimators to capture distributions on low-dimensional manifolds by replacing the standard isotropic kernel by an adaptive, anisotropic one.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Human motion tracking by combining view-based and model-based methods for monocular video sequences</title>
	<abstract>Reliable tracking of moving humans is essential to motion estimation, video surveillance and human-computer interface. This paper presents a new approach to human motion tracking that combines view-based and model-based techniques. Monocular color video is processed at both pixel level and object level. At the pixel level, a Gaussian mixture model is used to train and classify individual pixel colors. At the object level, a 3D human body model projected on a 2D image plane is used to fit the image data. Our method does not use inverse kinematics due to the singularity problem. While many others use stochastic sampling for model-based motion tracking, our method is purely dependent on parameter optimization. We convert the human motion tracking problem into a parameter optimization problem. A cost function for parameter optimization is used to estimate the degree of the overlapping between the foreground input image silhouette and a projected 3D model body silhouette. The overlapping is computed using computational geometry by converting a set of pixels from the image domain to a polygon in the real projection plane domain. Our method is used to recognize various human motions. Motion tracking results from video sequences are very encouraging.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A general method for comparing the expected performance of tracking and motion capture systems</title>
	<abstract>We introduce a general method for evaluating and comparing the expected performance of sensing systems for interactive computer graphics. Example applications include head tracking systems for virtual environments, motion capture systems for movies, and even multi-camera 3D vision systems for image-based visual hulls.Our approach is to estimate the asymptotic position and/or orientation uncertainty at many points throughout the desired working volume, and to visualize the results graphically. This global performance estimation can provide both a quantitative assessment of the expected performance, and intuition about the type and arrangement of sources and sensors, in the context of the desired working volume and expected scene dynamics.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Fast normalized cross correlation for motion tracking using basis functions</title>
	<abstract>Digital image-based elasto-tomography (DIET) is an emerging method for non-invasive breast cancer screening. Effective clinical application of the DIET system requires highly accurate motion tracking of the surface of an actuated breast with minimal computation. Normalized cross correlation (NCC) is the most robust correlation measure for determining similarity between points in two or more images providing an accurate foundation for motion tracking. However, even using fast Fourier transform (FFT) methods, it is too computationally intense for rapidly managing several large images. A significantly faster method of calculating the NCC is presented that uses rectangular approximations in place of randomly placed landmark points or the natural marks on the breast. These approximations serve as an optimal set of basis functions that are automatically detected, dramatically reducing computational requirements. To prove the concept, the method is shown to be 37-150 times faster than the FFT-based NCC with the same accuracy for simulated data, a visco-elastic breast phantom experiment and human skin. Clinically, this approach enables thousands of randomly placed points to be rapidly and accurately tracked providing high resolution for the DIET system.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Posture constraints for bayesian human motion tracking</title>
	<abstract>One of the most used techniques for full-body human tracking consists of estimating the probability of the parameters of a human body model over time by means of a particle filter. However, given the high-dimensionality of the models to be tracked, the number of required particles to properly populate the space of solutions makes the problem computationally very expensive. To overcome this, we present an efficient scheme which makes use of an action-specific model of human postures to guide the prediction step of the particle filter, so only feasible human postures are considered. As a result, the prediction step of this model-based tracking approach samples from a first order motion model only those postures which are accepted by our action-specific model. In this manner, particles are propagated to locations in the search space with most a posteriori information avoiding particle wastage. We show that this scheme improves the efficiency and accuracy of the overall tracking approach</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>

  
  <!-- 41-50 -->
  <item>
    <title>Automatic reconstruction of 3D human motion pose from uncalibrated monocular video sequences based on markerless human motion tracking</title>
	<abstract>We present a method to reconstruct human motion pose from uncalibrated monocular video sequences based on the morphing appearance model matching. The human pose estimation is made by integrated human joint tracking with pose reconstruction in depth-first order. Firstly, the Euler angles of joint are estimated by inverse kinematics based on human skeleton constrain. Then, the coordinates of pixels in the body segments in the scene are determined by forward kinematics, by projecting these pixels in the scene onto the image plane under the assumption of perspective projection to obtain the region of morphing appearance model in the image. Finally, the human motion pose can be reconstructed by histogram matching. The experimental results show that this method can obtain favorable reconstruction results on a number of complex human motion sequences.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Tracking and classifying of human motions with Gaussian process annealed particle filter</title>
	<abstract>This paper presents a framework for 3D articulated human body tracking and action classification. The method is based on nonlinear dimensionality reduction of high dimensional data space to low dimensional latent space. Motion of human body is described by concatenation of low dimensional manifolds which characterize different motion types. We introduce a body pose tracker, which uses the learned mapping function from low dimensional latent space to high dimensional body pose space. The trajectories in the latent space provide low dimensional representations of body poses performed during motion. They are used to classify human actions. The approach was checked on HumanEva dataset as well as on our own one. The results and the comparison to other methods are presented.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>3D markerless motion tracking in real-time using a single camera</title>
	<abstract>We present a novel motion tracking system that estimates the three-dimensional position of a moving object in real time by analyzing the image stream from a single lowest-end camera. Tracking is achieved without the need of any markers, calibration, or previous knowledge about the object. Our proposal can be applied given there is enough brightness contrast of the object with its surroundings. Such a technique allows for new Human-Computer Interaction solutions to be implemented in already running systems without a significative deployment expense.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Swarm intelligence based searching schemes for articulated 3D body motion tracking</title>
	<abstract>We investigate swarm intelligence based searching schemes for effective articulated human body tracking. The fitness function is smoothed in an annealing scheme and then quantized. This allows us to extract a pool of candidate best particles. The algorithm selects a global best from such a pool. We propose a global-local annealed particle swarm optimization to alleviate the inconsistencies between the observed human pose and the estimated configuration of the 3D model. At the beginning of each optimization cycle, estimation of the pose of the whole body takes place and then the limb poses are refined locally using smaller number of particles. The investigated searching schemes were compared by analyses carried out both through qualitative visual evaluations as well as quantitatively through the use of the motion capture data as ground truth. The experimental results show that our algorithm outperforms the other swarm intelligence searching schemes. The images were captured using multi-camera system consisting of calibrated and synchronized cameras.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>ENMIM: energetic normalized mutual information model for online multiple object tracking with unlearned motions</title>
	<abstract>In multiple-object tracking, the lack in prior information limits the association performance. Furthermore, to improve tracking, dynamic models are needed in order to determine the settings of the estimation algorithm. In case of complex motions, the dynamic cannot be learned and the task of tracking becomes difficult. That is why on-line spatio-temporal motion estimation is of crucial importance. In this paper, we propose a new model for multiple target online tracking: the Energetic Normalized Mutual Information Model (ENMIM). ENMIM combines two algorithms: (i) Quadtree Normalized Mutual Information, QNMI, a recursive partitioning methodology involving a region motion extraction; (ii) an energy minimization approach for data association adapted to the constraint of lack in prior information about motion and based on geometric properties. ENMIM is able to handle typical problems such as large inter-frame displacements, unlearned motions and noisy images with low contrast. The main advantage of ENMIM is its parameterless and its capacity to handle noisy multi-modal images without exploiting any pre-processing step.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A Method for Tracking the Camera Motion of Real Endoscope by Epipolar Geometry Analysis and Virtual Endoscopy System</title>
	<abstract>This paper describes a method for tracking the camera motion of a real endoscope by epipolar geometry analysis and image-based registration. In an endoscope navigation system, which provides navigation information to a medical doctor during an endoscopic examination, tracking the camera motion of the endoscopic camera is one of the fundamental functions. With a flexible endoscope, it is hard to directly sense the position of the camera, since we cannot attach a positional sensor at the tip of the endoscope. The proposed method consists of three parts: (1) calculation of corresponding point-pairs of two time-adjacent frames, (2) coarse estimation of the camera motion by solving the epipolar equation, and (3) fine estimation by executing image-based registration between real and virtual endoscopic views. In the method, virtual endoscopic views are obtained from X-ray CT images of real endoscopic images of the same patient. To evaluate the method, we applied it a real endoscopic video camera and X-ray CT images. The experimental results showed that the method could track the motion of the camera satisfactorily.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Bronchoscope tracking based on image registration using multiple initial starting points estimated by motion prediction</title>
	<abstract>This paper presents a method for tracking a bronchoscope based on motion prediction and image registration from multiple initial starting points as a function of a bronchoscope navigation system. We try to improve performance of bronchoscope tracking based on image registration using multiple initial guesses estimated using motion prediction. This method basically tracks a bronchoscopic camera by image registration between real bronchoscopic images and virtual ones derived from CT images taken prior to the bronchoscopic examinations. As an initial guess for image registration, we use multiple starting points to avoid falling into local minima. These initial guesses are computed using the motion prediction results obtained from the Kalman filter’s output. We applied the proposed method to nine pairs of X-ray CT images and real bronchoscopic video images. The experimental results showed significant performance in continuous tracking without using any positional sensors.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Articulated body motion tracking by combined particle swarm optimization and particle filtering</title>
	<abstract>This paper proposes the use of a particle filter with embedded particle swarm optimization as an efficient and effective way of dealing with 3d model-based human body tracking. A particle swarm optimization algorithm is utilized in the particle filter to shift the particles toward more promising configurations of the human model. The algorithm is shown to be able of tracking full articulated body motion efficiently. It outperforms the annealed particle filter, kernel particle filter as well as a tracker based on particle swarm optimization. Experiments on real video sequences as well as a qualitative analysis demonstrate the strength of the approach.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Robust motion tracking in video sequences using particle filter</title>
	<abstract>A robust motion tracking algorithm based on color and motion information was presented. Color is an effective feature in visual object tracking because of its robustness against rotation and scale variation. Nevertheless, the color of an object may change with varying illuminations, different image capture devices and different visual positions. Here, the color and motion information were fused in our visual tracking applications. Particle filter was employed as the essential framework because of its capacity of dealing with Non-linear/Non-Gaussian models by randomly sampling in state space. A particle filter can generate several hypotheses simultaneously in state space by randomly sampling and evaluate the states by weighing them respectively. The similarity between prediction data and observation information depends on the integration of Bhattacharyya distance and spacial Euclidean distance. Experimental results show the effectiveness of the proposed approach.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Articulated object registration using simulated physical force/moment for 3D human motion tracking</title>
	<abstract>In this paper, we present a 3D registration algorithm based on simulated physical force/moment for articulated human motion tracking. Provided with sparsely reconstructed 3D human surface points from multiple synchronized cameras, the tracking problem is equivalent to fitting the 3D model to the scene points. The simulated physical force/ moment generated by the displacement between the model and the scene points is used to align the model with the scene points in an Iterative Closest Points (ICP) [1] approach. We further introduce a hierarchical scheme for model state updating, which automatically incorporates human kinematic constraints. Experimental results on both synthetic and real data from several unconstrained motion sequences demonstrate the efficiency and robustness of our proposed method.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion tracking</query>
	<relevance>0</relevance>
  </item>



  <item>
    <title>Toward trustworthy recommender systems: An analysis of attack models and algorithm robustness</title>
	<abstract>Publicly accessible adaptive systems such as collaborative recommender systems present a security problem. Attackers, who cannot be readily distinguished from ordinary users, may inject biased profiles in an attempt to force a system to "adapt" in a manner advantageous to them. Such attacks may lead to a degradation of user trust in the objectivity and accuracy of the system. Recent research has begun to examine the vulnerabilities and robustness of different collaborative recommendation techniques in the face of "profile injection" attacks. In this article, we outline some of the major issues in building secure recommender systems, concentrating in particular on the modeling of attacks and their impact on various recommendation algorithms. We introduce several new attack models and perform extensive simulation-based evaluations to show which attacks are most successful and practical against common recommendation techniques. Our study shows that both user-based and item-based algorithms are highly vulnerable to specific attack models, but that hybrid algorithms may provide a higher degree of robustness. Using our formal characterization of attack models, we also introduce a novel classification-based approach for detecting attack profiles and evaluate its effectiveness in neutralizing attacks.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Taxonomy-driven computation of product recommendations</title>
	<abstract>Recommender systems have been subject to an enormous rise in popularity and research interest over the last ten years. At the same time, very large taxonomies for product classification are becoming increasingly prominent among e-commerce systems for diverse domains, rendering detailed machine-readable content descriptions feasible. Amazon.com makes use of an entire plethora of hand-crafted taxonomies classifying books, movies, apparel, and various other goods. We exploit such taxonomic background knowledge for the computation of personalized recommendations. Hereby, relationships between super-concepts and sub-concepts constitute an important cornerstone of our novel approach, providing powerful inference opportunities for profile generation based upon the classification of products that customers have chosen. Ample empirical analysis, both offline and online, demonstrates our proposal's superiority over common existing approaches when user information is sparse and implicit ratings prevail.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Classification features for attack detection in collaborative recommender systems</title>
	<abstract>Collaborative recommender systems are highly vulnerable to attack. Attackers can use automated means to inject a large number of biased profiles into such a system, resulting in recommendations that favor or disfavor given items. Since collaborative recommender systems must be open to user input, it is difficult to design a system that cannot be so attacked. Researchers studying robust recommendation have therefore begun to identify types of attacks and study mechanisms for recognizing and defeating them. In this paper, we propose and study different attributes derived from user profiles for their utility in attack detection. We show that a machine learning classification approach that includes attributes derived from attack models is more successful than more generalized detection algorithms previously studied.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Conceptual recommender system for CiteSeerX</title>
	<abstract>Short search engine queries do not provide contextual information, making it difficult for traditional search engines to understand what users are really requesting. One approach to this problem is to use recommender systems that identify user interests through various methods in order to provide information specific to the user's needs. However, many current recommender systems use a collaborative model based on a network of users to provide the recommendations, leading to problems in environments where network relationships are sparse or unknown. Content-based recommenders can avoid the sparsity problem but they may be inefficient for large document collections. In this paper, we propose a concept-based recommender system that recommends papers to general users of the CiteSeerx digital library of Computer Science research publications. We also represent a novel way of classifying documents and creating user profiles based on the ACM (Association for Computer Machinery) classification tree. Based on these user profiles which are built using past click histories, relevant papers in the domain are recommended to users. Experiments with a set of users on the CiteSeerX database show that our concept-based method provides accurate recommendations even with limited user profile histories.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>SVR-based music mood classification and context-based music recommendation</title>
	<abstract>With the advent of the ubiquitous era, context-based music recommendation has become one of rapidly emerging applications. Context-based music recommendation requires multidisciplinary efforts including low level feature extraction, music mood classification and human emotion prediction. Especially, in this paper, we focus on the implementation issues of context-based mood classification and music recommendation. For mood classification, we reformulate it into a regression problem based on support vector regression (SVR). Through the use of the SVR-based mood classifier, we achieved 87.8% accuracy. For music recommendation, we reason about the user's mood and situation using both collaborative filtering and ontology technology. We implement a prototype music recommendation system based on this scheme and report some of the results that we obtained.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Personalized mobile e-commerce system using DISC psychological model</title>
	<abstract>Today, mobile e-commerce has become widespread due to technological advances and the dissemination of mobile devices that make business transactions possible anytime, anywhere. However, factors such as slow processing speed, limited RAM, small, low-resolution screens, limited battery life, somewhat unreliable data connections, and extensive product information can hinder mobile e-commerce. Therefore, it is necessary to develop a method to promote customer spending by selectively providing only product information that is necessary and helpful to customers. In this paper, we propose a recommendation system that utilizes other data, such as information on latent or unconscious customers' psychological patterns. Toward this end, we apply the psychological pattern called DISC behavioral style classification model, which is commonly used in psychology and sociology, to the e-commerce context. A virtual second-hand goods transaction application for the Android platform was used for the experiment, and the performance of the recommendation system was compared to that of other methods based on the results obtained through actual distribution.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Tensor-CUR decompositions for tensor-based data</title>
	<abstract>Motivated by numerous applications in which the data may be modeled by a variable subscripted by three or more indices, we develop a tensor-based extension of the matrix CUR decomposition. The tensor-CUR decomposition is most relevant as a data analysis tool when the data consist of one mode that is qualitatively different than the others. In this case, the tensor-CUR decomposition approximately expresses the original data tensor in terms of a basis consisting of underlying subtensors that are actual data elements and thus that have natural interpretation in terms ofthe processes generating the data. In order to demonstrate the general applicability of this tensor decomposition, we apply it to problems in two diverse domains of data analysis: hyperspectral medical image analysis and consumer recommendation system analysis. In the hyperspectral data application, the tensor-CUR decomposition is used to compress the data, and we show that classification quality is not substantially reduced even after substantial data compression. In the recommendation system application, the tensor-CUR decomposition is used to reconstruct missing entries in a user-product-product preference tensor, and we show that high quality recommendations can be made on the basis of a small number of basis users and a small number of product-product comparisons from a new user.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A design framework for recommender system by incorporating sequential information</title>
	<abstract>Recommender Systems are used for generating recommendations for users with respect to various products and applications. Currently, recommender systems are widely used in e- commerce applications to suggest the appropriate products and services to the users. Sequential information plays an important role for deciding the interests of the user. The proposed system happens to be a collaborative-model based recommendation system and considers the sequential information present in web logs for generation of the recommendations. The model is a combination of clustering, classification and recommendation engine. Clustering has been performed to group users on the basis of sequential and content similarity present in their web page visit sequences. Each cluster represents an interest area or category. Singular value decomposition (SVD) has been used for classification and generating the recommendations for new users.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Determining Mood for a Blog by Combining Multiple Sources of Evidence</title>
	<abstract>Mood classification for blogs is useful in helping user-to-agent interaction for a variety of applications involving the web, such as user modeling, recommendation systems, and user interface fields. It is challenging at the same time because of the diversity of the characteristics of bloggers, their experiences, and the way moods are expressed. As an attempt to handle the diversity, we combine multiple sources of evidence for a mood type. Support Vector Machine based Mood Classifier (SVMMC) is integrated with Mood Flow Analyzer (MFA) that incorporates commonsense knowledge obtained from the general public (i.e. ConceptNet), the Affective Norms English Words (ANEW) list, and mood transitions. In combining the two different approaches, we employ a statistically weighted voting scheme based on the Support Vector Machine (SVM). For evaluation, we have built a mood corpus consisting of manually annotated blogs, which amounts to over 4000 blogs. Our proposed method outperforms SVMMC by 5.68% in precision. The improvement is attributed to the strategy of choosing more trustable classification results in an interleaving fashion between the SVMMC and our MFA.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Predicting WWW surfing using multiple evidence combination</title>
	<abstract>The improvement of many applications such as web search, latency reduction, and personalization/ recommendation systems depends on surfing prediction. Predicting user surfing paths involves tradeoffs between model complexity and predictive accuracy. In this paper, we combine two classification techniques, namely, the Markov model and Support Vector Machines (SVM), to resolve prediction using Dempster's rule. Such fusion overcomes the inability of the Markov model in predicting the unseen data as well as overcoming the problem of multiclassification in the case of SVM, especially when dealing with large number of classes. We apply feature extraction to increase the power of discrimination of SVM. In addition, during prediction we employ domain knowledge to reduce the number of classifiers for the improvement of accuracy and the reduction of prediction time. We demonstrate the effectiveness of our hybrid approach by comparing our results with widely used techniques, namely, SVM, the Markov model, and association rule mining.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A privacy preserving web recommender system</title>
	<abstract>In this paper we propose a recommender system that helps users to navigate though the Web by providing dynamically generated links to pages that have not yet been visited and are of potential interest. To this end, traditional recommender systems use Web Usage Mining (WUM) techniques in order to automatically extract knowledge from Web usage data. Thanks to WUM techniques we are able to classify users and adaptively provide useful recommendations. The drawback of a user classification approach is that it makes the system prone to privacy breaches.Our contribution here is πSUGGEST, a privacy enhanced recommender system that allows for creating serendipity recommendations without breaching users privacy. We will show that our system does not provide malicious users with any mean to track or detect users activity or preferences.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>SheepDog: group and tag recommendation for flickr photos by automatic search-based learning</title>
	<abstract>Online photo albums have been prevalent in recent years and have resulted in more and more applications developed to provide convenient functionalities for photo sharing. In this paper, we propose a system named SheepDog to automatically add photos into appropriate groups and recommend suitable tags for users on Flickr. We adopt concept detection to predict relevant concepts of a photo and probe into the issue about training data collection for concept classification. From the perspective of gathering training data by web searching, we introduce two mechanisms and investigate their performances of concept detection. Based on some existing information from Flickr, a ranking-based method is applied not only to obtain reliable training data, but also to provide reasonable group/tag recommendations for input photos. We evaluate this system with a rich set of photos and the results demonstrate the effectiveness of our work.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A behavioral study of emotions in south indian classical music andits implications in music recommendation systems</title>
	<abstract>In order to model a culture-specific content-based music recommendatio/n system, a total of 750 subjective emotional responses to tunes composed in popular raagas of South Indian classical (Carnatic) music are empirically investigated to find out the long speculated relation between raagas (indian music scales) and rasas (emotion clusters). We discuss the results from analysis of this survey, which show that raagas are quite useful as a first step in a different direction towards content-based music recommendation. Along the way, we discriminate Carnatic and North-Indian classical (Hindustani) music traditions. We also convey the definition of rasa, which is different from being a single emotional state. We use a classification based on a novel approach in conceptualization of emotions based on navarasa, which is a emotion classification given by Bharata, that suits behavioral studies with Indian arts. Pitch-class profiles which were previously shown to give high accuracies in Hindustani raaga recognition are tested in a preliminary experiment to automatically recognize Carnatic raagas. The results are discussed and additional challenges in dealing with melodies in Carnatic tradition are highlighted.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Modeling and predicting personal information dissemination behavior</title>
	<abstract>In this paper, we propose a new way to automatically model and predict human behavior of receiving and disseminating information by analyzing the contact and content of personal communications. A personal profile, called CommunityNet, is established for each individual based on a novel algorithm incorporating contact, content, and time information simultaneously. It can be used for personal social capital management. Clusters of CommunityNets provide a view of informal networks for organization management. Our new algorithm is developed based on the combination of dynamic algorithms in the social network field and the semantic content classification methods in the natural language processing and machine learning literatures. We tested CommunityNets on the Enron Email corpus and report experimental results including filtering, prediction, and recommendation capabilities. We show that the personal behavior and intention are somewhat predictable based on these models. For instance, "to whom a person is going to send a specific email" can be predicted by one's personal social network and content analysis. Experimental results show the prediction accuracy of the proposed adaptive algorithm is 58% better than the social network-based predictions, and is 75% better than an aggregated model based on Latent Dirichlet Allocation with social network enhancement. Two online demo systems we developed that allow interactive exploration of CommunityNet are also discussed.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Intelligent email: reply and attachment prediction</title>
	<abstract>We present two prediction problems under the rubric of Intelligent Email that are designed to support enhanced email interfaces that relieve the stress of email overload. Reply prediction alerts users when an email requires a response and facilitates email response management. Attachment prediction alerts users when they are about to send an email missing an attachment or triggers a document recommendation system, which can catch missing attachment emails before they are sent. Both problems use the same underlying email classification system and task specific features. Each task is evaluated for both single-user and cross-user settings.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Machine learning for information retrieval</title>
	<abstract>In recent years, we have witnessed successful application of machine learning techniques to a wide range of information retrieval problems, including Web search engines, recommendation systems, online advertising, etc. It is thus critical for researchers in the information retrieval community to understand the core machine learning techniques. In order to accommodate audiences with different levels of understanding of machine learning, we divide this tutorial into two sessions: the first session will focus on basic machine learning concepts and tools; in the second session, we will introduce more advanced topics in machine learning, and will present recent developments in machine learning and its application to information retrieval. Each season is self-contained.

Session 1: Core Learning Technologies for Information Retrieval. This session of the tutorial will cover the core machine learning methods, basic optimization techniques and key information retrieval applications. In particular, it includes: 1). Core concepts in machine learning, such as supervised learning/unsupervised learning, bias and variance trade off, and probabilistic models; 2). Useful concepts and algorithms in optimization including the first and second order gradient methods, and Expectation and Maximization; 3). The application of machine learning methods to key information retrieval problems including text classification, collaborative filtering, clustering and learning to rank;

Session 2: Emerging Learning Technologies for Information Retrieval. This session will cover more advanced machine learning techniques that have started to be utilized in information retrieval applications. In particular, it will cover: 1). Advanced Optimization Techniques including stochastic optimization and smooth minimization; 2). Emerging Learning Techniques such as Multiple-Instance Learning, Active Learning and Semi-supervised Learning.

The tutorial will benefit a large body of audience in the information retrieval community, ranging from students who are new to machine learning to the seasoned researchers who would like to understand the recent advance in machine learning for information retrieval research. This tutorial will also benefit the practitioners who apply learning techniques to real-world information retrieval systems.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Machine learning for query-document matching in search</title>
	<abstract>In web search, relevance is one of the most important factors to meet users' satisfaction, and the success of a web search engine heavily depends on its performance on relevance. It has been observed that many hard cases in search relevance are due to term mismatch between query and documnt (e.g., query 'ny times' does not match well with document only containing 'new york times'), and thus it is not exaggerated to say that dealing with mismatch between query and document is one of the most critical research problems in web search. Recently researchers have spent significant effort to address the grand challenge. The major approach is to conduct more query and document understanding, and perform better matching between enriched query and document representations. With the availability of large amount of log data and advanced machine learning techniques, this becomes more feasible and significant progress has been made recently.

In this tutorial, we will give a systematic and detailed presentation on newly developed machine learning technologies for query document matching in search. We will focus on the fundamental problems, as well as the novel solutions for query document matching at word form level, word sense level, topic level, and structure level. We will talk about novel technologies about query spelling error correction [3, 13], query rewriting [1, 4, 6, 7], query classification [2], topic modeling of documents [5, 9], query document matching [8, 10, 11, 12], and query document-title translation. The ideas and solutions introduced in this tutorial may motivate industrial practitioners to turn the research fruits into product reality. The summary of the state-of-the-art methods and the discussions on the technical issues in this tutorial may stimulate academic researchers to find new research directions and solutions.

Matching between query and document is not limited to search, and similar problems can be observed at online advertisement, recommendation system, and other applications, as matching between objects from two spaces. The technologies we introduce can be generalized into more general machine learning techniques, which we call learning to match.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>An integrated approach to discover tag semantics</title>
	<abstract>Tag-based systems have become very common for online classification thanks to their intrinsic advantages such as self-organization and rapid evolution. However, they are still affected by some issues that limit their utility, mainly due to the inherent ambiguity in the semantics of tags. Synonyms, homonyms, and polysemous words, while not harmful for the casual user, strongly affect the quality of search results and the performances of tag-based recommendation systems.

In this paper we rely on the concept of tag relatedness in order to study small groups of similar tags and detect relationships between them. This approach is grounded on a model that builds upon an edge-colored multigraph of users, tags, and resources. To put our thoughts in practice, we present a modular and extensible framework of analysis for discovering synonyms, homonyms and hierarchical relationships amongst sets of tags. Some initial results of its application to the delicious database are presented, showing that such an approach could be useful to solve some of the well known problems of folksonomies.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Taking advice from intelligent systems: the double-edged sword of explanations</title>
	<abstract>Research on intelligent systems has emphasized the benefits of providing explanations along with recommendations. But can explanations lead users to make incorrect decisions? We explored this question in a controlled experimental study with 18 professional network security analysts doing an incident classification task using a prototype cybersecurity system. The system provided three recommendations on each trial. The recommendations were displayed with explanations (called "justifications") or without. On half the trials, one of the recommendations was correct; in the other half none of the recommendations was correct. Users were more accurate with correct recommendations. Although there was no benefit overall of explanation, we found that a segment of the analysts were more accurate with explanations when a correct choice was available but were less accurate with explanations in the absence of a correct choice. We discuss implications of these results for the design of intelligent systems.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Adapting the user context in realtime: tailoring online machine learning algorithms to ambient computing</title>
	<abstract>Ambient systems weave computing and communication aspects into everyday life. To provide self-adaptive services, it is necessary to acquire context information using sensors and to leverage the collected information for reasoning and classification of situations. To enable self-learning systems, we propose to depart from static rule-based decisions and first-order logic to define situations from basic context, but to build on machine-learning techniques. However, existing learning algorithms show substantial weaknesses if applied in highly dynamic environments, where we expect accurate decisions in realtime while the user is in-the-loop to give feedback to the system's recommendations. To address ambient and pervasive computing environments, we propose the FLORA-multiple classification (FLORAMC) online learning algorithm. In particular, we enhance the FLORA algorithm to allow for (1) multiple classification and (2) numerical input values, while improving its concept drift handling capabilities; thus, making it an excellent choice for use in the area of ambient computing. The multiple classification allows context-aware systems to differentiate between multiple categories instead of taking binary decisions. Support for numerical input values enables the processing of arbitrary sensor inputs beyond nominal data. To provide the capability of concept drift handling, we propose the use of an advanced window adjustment heuristic, which allows FLORA-MC to continuously adapt to the user's behavior, even if her/his preferences change abruptly over time. In combination with the inherent characteristics of online learning algorithms, our scheme is very well suited for realtime application in the area of ambient and pervasive computing. We describe the design and implementation of FLORA-MC and evaluate its performance vs. state-of-the-art learning algorithms. We are able to show the superior performance of our algorithm with respect to reaction time and concept drift handling, while maintaining an excellent accuracy. Our implementation is available to the research community as a WEKA module.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Learning to recommend helpful hotel reviews</title>
	<abstract>User-generated reviews are a common and valuable source of product information, yet little attention has been paid as to how best to present them to end-users. In this paper, we describe a classification-based recommender system that is designed to recommend the most helpful reviews for a given product. We present a large-scale evaluation of our approach using TripAdvisor hotel reviews, and we show that our approach is capable of suggesting superior reviews compared to a number of alternative recommendation benchmarks.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Social recommender systems</title>
	<abstract>The goal of this tutorial is to expose participants to the current research on social recommender systems (i.e., recommender systems for the social web). Participants will become familiar with state-of-the-art recommendation methods, their classifications according to various criteria, common evaluation methodologies, and potential applications that can utilize social recommender systems. Additionally, open issues and challenges in the field will be discussed.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Notes toward a politics of personalization</title>
	<abstract>A recommender system is an information organization tool which extracts knowledge of individual users of a specific (online) resource based on their activity within that domain, and uses this knowledge to generate for them individual recommendations. These recommendations are made based on the broad assumption that people who have agreed on some things in the past will likely agree on things in the future. [1] Because these systems classify content based on how it is engaged with by previous users, they have emerged as an effective (and profitable) way to organize content on the web, the web itself being resistant, almost by its very nature, to the imposition of top-down, ontological classificatory control. [2]

This paper interrogates some of the assumptions and biases involved in the personalized organization and presentation of digital media content, in an effort to devise a more critical analysis of the economic, technical, and social forces that contribute to the growing popularity of the personalized recommender systems. It pays particular attention to how personalized content filtering finds expression in the recommendation engines of taste-coordinating websites like Amazon and Pandora, in the self-organization of information through social classification sites like LibraryThing and Delicious, in the adaptive capabilities of next generation search engines (what Michael Zimmer refers to as "Search 2.0"), and finally within locational media software like FourSquare. Discussion of these recommender systems will refer to their mechanics as well as their accompanying rhetoric, which often associates the personalized delivery of content with a more empowered individual user.

Promises of individual empowerment attached to emerging expressions of personalized media are generally made in opposition to the broadcast media model of the mass market. The concluding section of this paper will examine the rhetoric of these promises in a discussion that considers such novel adaptations of content production and delivery as a market response to current media configurations. These adaptations, it will be argued, serve in large part to define differences to be "commercially exploited." [3] The nurturing of differentiated markets represents a potential challenge to some of the very characteristics that have traditionally been associated with the internet's social and political potential.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Query-independent evidence in home page finding</title>
	<abstract>Hyperlink recommendation evidence, that is, evidence based on the structure of a web's link graph, is widely exploited by commercial Web search systems. However there is little published work to support its popularity. Another form of query-independent evidence, URL-type, has been shown to be beneficial on a home page finding task. We compared the usefulness of these types of evidence on the home page finding task, combined with both content and anchor text baselines. Our experiments made use of five query sets spanning three corpora---one enterprise crawl, and the WT10g and VLC2 Web test collections.We found that, in optimal conditions, all of the query-independent methods studied (in-degree, URL-type, and two variants of PageRank) offered a better than random improvement on a content-only baseline. However, only URL-type offered a better than random improvement on an anchor text baseline. In realistic settings, for either baseline, only URL-type offered consistent gains. In combination with URL-type the anchor text baseline was more useful for finding popular home pages, but URL-type with content was more useful for finding randomly selected home pages. We conclude that a general home page finding system should combine evidence from document content, anchor text, and URL-type classification.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Usability recommendations in the design of mixed interactive systems</title>
	<abstract>Mixed Interactive Systems (MIS) are systems allowing several interaction forms resulting from the fusion between physical and digital worlds. Such systems being relatively new, the underlying design process leading to their design is not entirely defined, particularly in terms of user-centered design. The goal of this paper is to present an approach that attempts to identify, model and integrate available usability knowledge into a user-centered approach for the design of MIS. The approach consisted of: systematic review of the literature on MIS; selection and deciphering of usability recommendations under a common format; classification of the 141 usability recommendations obtained; and application of the recommendations to the design of a MIS case study (museum application).</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Recommendation algorithm combining the user-based classified regression and the item-based filtering</title>
	<abstract>With the expansion of the Internet services, providing personalized product recommendations has become one of the most important ways to attract customers. Especially, collaborative recommender systems have achieved widespread success on the web. Information of products is recommended to the users based on their nearest "neighbors" who have similar interests. It is widely known that there is a sparsity problem in such systems. However, according to our research, there are other problems: one is that the typical collaborative algorithm loses some important parameter when it predicts the ratings, because there might be a strong similarity between the users who give very different ratings. Another is that the classification information of resources is not used. To solve these problems, we have proposed a recommendation algorithm combining the user-based classified regression and the item-based filtering. The experiment results show that performance is improved after applying the new algorithm.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An intelligent method for computer-aided trauma decision making system</title>
	<abstract>Based on the National Vital Statistics Reports 2001, nearly 115,200 lives are cut short annually because of traumatic injuries and many patients who survive traumatic events have to face the future with life-long disabilities that negatively affect them and their families [29]. Moreover, U.S. trauma center reports suggest that the annual death rate, as well as permanent disabilities, can be reduced when medical treatments are performed rapidly. To provide a certain kinds of medical treatment, a decision has to be made based on the outcomes (discharge status and intensive care unit days). Moreover, predicting the patient's outcome is useful when determining the most reliable transport method: ambulance or helicopter.

In this paper, we focus on creating an appropriate decision making process on helicopter transport. Although helicopter transport provides a high level of medical care as well as rapid transport service to trauma patients, the cost of helicopter transport is highly expensive. So there is a question still remains that the use of helicopter transport for trauma patients makes any change in patients' survival and the transport cost. To answer this question, we designed a computer aided decision making system for trauma patients' helicopter transport. The decision making is accomplished by using Decision Tree (DT). Especially, Nonlinear Regression and Classification Tree (CART) is used to create reliable rules. The rules is designed by using a large number of features captured from all available patients' informative attributes. The quality of the extracted rules is also evaluated. Therefore, the computer aided system support making predictions/and recommendations on the outcomes of trauma patients.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The anti-forensics challenge</title>
	<abstract>Computer and Network Forensics has emerged as a new field in IT that is aimed at acquiring and analyzing digital evidence for the purpose of solving cases that involve the use, or more accurately misuse, of computer systems. Many scientific techniques, procedures, and technological tools have been evolved and effectively applied in this field. On the opposite side, Anti-Forensics has recently surfaced as a field that aims at circumventing the efforts and objectives of the field of computer and network forensics. The purpose of this paper is to highlight the challenges introduced by Anti-Forensics, explore the various Anti-Forensics mechanisms, tools and techniques, provide a coherent classification for them, and discuss thoroughly their effectiveness. Moreover, this paper will highlight the challenges seen in implementing effective countermeasures against these techniques. Finally, a set of recommendations are presented with further seen research opportunities.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Proceedings of the nineteenth ACM conference on Hypertext and hypermedia</title>
	<abstract>Welcome to the 19th ACM Conference on Hypertext and Hypermedia -- Hypertext 2008. One of the great joys of the Hypertext conference series has always been the diversity of the topics that the conference encompasses. Regular attendees have always known and valued this diversity, but the conference steering committee have been aware that it can be confusing to newcomers, and at times difficult for reviewers who have to decide whether a paper is 'in-scope' for the conference. This year we have refined the approach that we began last year, whereby we have had four separate tracks, each with their own chairs and committees; these committees have been responsible for selecting and briefing their reviewers and advertising the call for papers into their communities. We believe that this approach has made the conference more accessible to newcomers, and this has been reflected in the increased quantity and quality of papers submitted.

The conference theme this year is "linking people and places" and celebrates the power and importance of the link, in its widest sense.

The first track, Information Linking and Organization, specifically targets the formal study of scholarly, structural, sculptural, spatial, open, dynamic and adaptive or any other type of hypertext (or Web-based Information System). In this track, researchers discuss models, architecture, interfaces, properties, or theory in general, about hypertext and hypermedia.

The second track, Applications of Hypertext, has been for many years the most difficult for people to scope; it targets descriptions of applications of hypertext, for example in healthcare, cultural heritage, education or industry, where the affordance of the link has in some way enabled a novel application.

The third track, Hypertext, Culture, and Communication, examines the creation and reception of literary machines ranging from literary fiction to creative nonfiction and scholarly argumentation. As hypertext reading and writing become ever more pervasive in society, the rhetoric of links continues to offer frequent surprises and unexpected opportunities. This year's papers integrate fresh insights into spontaneous forms and ephemeral social media with considered reflection on carefully crafted hypermedia.

Social Linking track expands the remit of the Social Hypertext track introduced in 2007. This track focuses on one of the most exciting recent developments in Web science, social annotation, by which users can easily markup other authors' resources via collaborative mechanisms such as tagging, filtering, voting, editing, classification, and rating. These social processes lead to the emergence of many types of links between texts, users, concepts, pages, articles, and media. The social linking track has immediately established itself as the most popular venue for this year's Hypertext technical submissions, covering several aspects of design, analysis, and modeling of information systems driven by social linking. The accepted papers represent a high-quality sample of research covering a broad range of social linking topics, which include the co-evolution of social, information, and semantic networks; formal models of social annotation and its behavioral patterns; link inference from blogs and social networks; applications to search, retrieval, recommendation, navigation, and scalability issues; information-theoretic aspects of socially-induced semantic networks; structure and dynamics of social information networks; and evaluation of mechanisms and interfaces for social linking systems.

This is the third time the Hypertext Conference has been held in Pittsburgh. We are very fortunate, this time, to have excellent keynote presentations from Bernardo Huberman of HP Labs and Prof. Jon Kleinberg of Cornell University, and an excellent technical and social program to look forward to.

In addition to the contributions featured in this volume Hypertext 2008 offered two workshops: ;)

Web Science: Collaboration and Collective Intelligence organized by Weigang Wang (University of Manchester, UK) and David Millard (University of Southampton, UK)
Creating out of the Machine: Hypertext, Hypermedia, and Web Artists Explore the Craft organized by Stephen Ersinghaus (Tunxis Community College, Connecticut, USA)
)</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Personalized, interactive tag recommendation for flickr</title>
	<abstract>We study the problem of personalized, interactive tag recommendation for Flickr: While a user enters/selects new tags for a particular picture, the system suggests related tags to her, based on the tags that she or other people have used in the past along with (some of) the tags already entered. The suggested tags are dynamically updated with every additional tag entered/selected. We describe a new algorithm, called Hybrid, which can be applied to this problem, and show that it outperforms previous algorithms. It has only a single tunable parameter, which we found to be very robust.

Apart from this new algorithm and its detailed analysis, our main contributions are (i) a clean methodology which leads to conservative performance estimates, (ii) showing how classical classification algorithms can be applied to this problem, (iii) introducing a new cost measure, which captures the effort of the whole tagging process, (iv) clearly identifying, when purely local schemes (using only a user's tagging history) can or cannot be improved by global schemes (using everybody's tagging history).</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The evolution of design patterns in HCI: from pattern languages to pattern-oriented design</title>
	<abstract>User interface design patterns also called HCI or interaction or usability patterns have been introduced first as a medium to capture and represent solutions to users' problems. Patterns have been used also as a medium for transferring the expertise of HCI designers and usability professionals to software engineers, who are usually unfamiliar with UI design and usability principles. Design patterns have been considered also as a lingua franca for crossing cultural and professional barriers between different stakeholders. Several HCI professionals have introduced their own pattern languages with specific terminology, classification and meanings. Patterns have also been presented as building reusable blocks at different levels of granularity, which can be combined to compose new interactive systems. Despite the obvious and acclaimed potential of these pattern-driven design approaches, patterns usage has not achieved the acceptance and widespread applicability envisaged by pattern pioneers such as Christopher Alexander. This paper provides an analysis of the facts about patterns usages, pattern languages and pattern-based design approaches. Some shortcomings in the presentation and application of HCI patterns are identified and discussed under the prevailing fallacies. Based on the analysis of how patterns have used so far, we draw some recommendations and future perspectives on what can be done to address the existing shortcomings. Making patterns more accessible, easily understandable, comparable and integratable in software and HCI design tools can promote HCI patterns to claim the usability, usefulness and importance originally envisaged for the pattern-oriented design approach.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Medical search and classification tools for recommendation</title>
	<abstract>their patients' records from paper to computer, enormous amounts of electronic medical records (EMR) have become available for medical research. Some of the EMR data are well-structured, for which traditional database management systems can provide effective retrieval and management functions. However, most of the EMR data (such as progress notes and consultation letters) are in free text formats. How to effectively and efficiently retrieve and discover useful information from the vast amount of such semi-structured data is a challenge faced by medical professionals. Without proper tools, the rich information and knowledge buried in the medical health records are unavailable for clinical research and decision-making. The objective of our research is to develop text analytics tools that are capable of parsing clinical medical data so that predefined search subjects that correspond to a list of medical diagnoses can be extracted. In addition to this particular core functionality, it is also desired that several important assets should be present within the text-analytics tools in order to improve its overall ability to be used as recommendation tools. In this research, we work with research scientists at the Institute for Clinical Evaluative Sciences (ICES) in Toronto and examine a number of techniques for structuring and processing free text documents in order to effectively and efficiently search and analyze vast amount of medical records. We implement several powerful medical text analytics tools for clinical data searching and classification. For data classification, our tools sort through a great amount of patientrecords to identify the likelihood of a patient having myocardial infarction (MI) or hypertension (HTN), and classify the patients accordingly. Our tools can also identify the likelihood of a patient being a smoker, previous smoker or non-smoker based on the text data of medical records.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A multimodal sensing system for detection of traumatic brain injury</title>
	<abstract>We propose to develop a portable, handheld, noninvasive solution for accurate screening and real-time monitoring of traumatic brain injury (TBI) in ambulatory/emergency response scenarios. A layered sensing concept that unifies modalities such as a) ultrasound (US) (B-mode, Doppler flow), b) tonometry and c) pulse oximeter to predict TBI, its severity and mode of recommendations for emergency medical service (EMS) personnel is currently investigated. Specifically, we aim to determine novel 3D morphometric parameters of optic nerve sheath (ONS) that can predict elevated intracranial pressure (EICP) from US data. These parameters when combined with intraocular pressure (IOP), blood oxygen saturation (SaO2) and Doppler flow readings of the carotid artery can improve the overall classification accuracy. In addition, we have also developed a preliminary decision-support system (DSS) to provide an automated analysis of subject's brain health status and thereby, recommend further screening, etc. In the demo, we would show the chain of processing starting from capture of our desired signals from a volunteer, pre-processing (reformatting, de-noising) of US data, post-processing of features extracted from the 3D US model and finally, the classification output of the DSS.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Software fault tolerance in real-time embedded systems</title>
	<abstract>Many critical embedded systems which have very high reliability requirements operate in real time. Considering the significance of their applications, design of highly reliable software is a very important research area. In the development of reliable real-time embedded applications, fault-tolerance is a very important aspect. Embedded systems which do not include every facility to enhance reliability are unacceptable in the critical applications being considered. Particularly this research is important for systems where the consequences of any noncompliance of software with its requirements may be disastrous. Even though some techniques have been proposed to increase reliability, it is almost always that software still contains residual errors. These errors may have extremely serious consequences especially for critical applications such as military command and control, missile guidance and control system, nuclear power reactor control, transportation, patient monitoring system, etc. Software fault-tolerance techniques are useful in handling these residual errors. Research is being conducted for developing a systematic design methodology with fault-tolerant capabilities for the design of reliable software in real-time embedded applications. In particular, we are currently investigating effective error detection and recovery techniques to provide the continued service required by the embedded system. We will also investigate methodologies to design modules with good error-resistant and self-recovery capabilities. The value and nature of the research project is such that its result will be directly applicable to any practical real-time embedded systems.

The serious consequences of faulty software occur in real-time systems, where either human life can be endangered or large economic losses can be incurred due to a failure. While techniques are being perfected to cope with known sources of errors in software, the growth of software complexity continues with a resultant increase in its vulnerability to errors. Although improvements in software development methodologies can undoubtedly reduce the incidence of software faults, the final line of defense rests on testing and verification. However, it is clear that neither formal verification techniques, nor extensive testing can be depended upon to provide the degree of reliability that is required for software in critical applications, especially in real-time embedded systems. Considering the lack of design methodologies which can guarantee that a complex software system does not contain residual design faults, it is surprising that fault tolerance has rarely been considered necessary. In order to achieve fault-tolerance in software, the software must possess the capabilities to detect errors and to recover from the errors.

This research is primarily concerned with the development of effective techniques for achieving fault-tolerance in the large-scale embedded applications providing users with continued services in real-time operation in spite of the occurrence of partial malfunctions of the systems. Techniques are being developed using the structural properties of a software so that the techniques will be useful for programs written in different programming languages and operating in different environments. To do this, it is important for us to represent a program by a suitable structural model for analysis so that its structural properties can be conveniently exploited. We have found that the simple directed graph model is most suitable for the project. In this model, the program is represented by a directed graph consisting of a set of nodes and a set of directed branches. A node can represent a computational task, whereas a directed branch can represent possible transfer of control from a node to another. The graph model can represent a program at any level of abstraction in a way easily interpretable by the programmer. Therefore, the real-time software systems are modeled as a set of cooperating sequential processes with constraints on their execution time. Each corresponds to the execution of program which is part of the system, and provides some subset of the necessary system outputs.

The next step is to develop error-resistant programming techniques using the self-checking approach. Self-checking techniques can be implemented to check the function, the control sequence and the data of a program. There are three specific aspects in this part of the proposed research. First, an error classification scheme is being investigated. Types of errors we consider include control flow and data flow errors, module level errors, program level errors, etc.

Based on the error classification scheme, developing techniques for error detection and checking data integrity is underway. A Fault-Handler is being developed and utilized to monitor the behavior of the software system for detecting errors. In order to classify errors with reasonable accuracy, it may be necessary for the Fault-Handler to retain information concerning the error history of processes in the system.

Effective techniques for recovery is being investigated by making good use of the information retained in the erroneous state and in the Fault-Handler. In order to achieve resistance to errors, the program should contain enough data or information to handle all error conditions intelligently. The techniques for program design using the criteria of error-resistant modularization are also being investigated and suitable recommendations made along with the recovery techniques. Error-resistant modularization seems to involve the identification and localization of recovery data and building a module around them.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The European community and information technology</title>
	<abstract>The world has watched Eastern Europe erupt into such political turmoil that historians are expected to call this period the Revolutions of 1989. Economic evolution was also underway as the Continent progressed toward a single European market. The goal—a market without national borders or barriers to the movement of goods, services, capital and people—was first outlined over 30 years ago by the 12 countries which became members of the Common Market. In the mid 1980s, the effort was renewed when these same countries approved an ambitious plan outlining hundreds of legislative directives and policies that would harmonize and re-regulate those of the member states. The measures are drafted by the European Commission, voted on by the Council of Ministers, amended if necessary, and then assigned budgets by the Parliament. They include competition law, labor law, product regulation and standardization, taxation and subsidies, and quota and tariff guidelines. In 1987, the Single European Act created a timetable for the passage of legislation with a formal deadline for the removal of barriers by December 31, 1992, hence the term Europe '92 (EC '92). But many have described EC '92 as a process that will continue throughout the 1990s. The ouster of communist leaderships throughout Eastern Europe, however, has raised unexpected questions about the participation of the Eastern countries, and this could alter or delay the process. Nevertheless, the changes have begun and are taking place during the Information Revolution. It is therefore natural to ask what impact EC '92 will have on the computer industry. Inevitably, several of the directives and policies relate primarily, and many secondarily, to information technology. Table 2 lists the policies in effect and those being proposed. In the following pages, Communications presents several points of view regarding the impact of EC '92 on the information technology market in Europe. As of July 1988, the European information systems market was estimated at $90 billion by Datamation magazine and is expected by many to be the fastest growing market this decade. But during the last ten years, European-based computer companies have had difficulty keeping pace with American and Japanese firms. In 1988, European companies managed only a 20 percent market share on their own turf, according to market researcher International Data Corporation. Not much had changed since 1982 when their market share was 21 percent. As reported in the Wall Street Journal last January, European computer companies have been hindered by lack of economies of scale, narrow focus on national markets, and difficulty in keeping pace with Japanese and IJ.S. product innovations. But the occasion for the Journal article was the news that Germany's Siemens AG was merging with the ailing Nixdorf Computer AG. The result would possibly be the largest computer company based in Europe, and the sixth or seventh largest in the world. And in October of 1989, France's Groupe Bull announced the purchase of Zenith Electronics Corporation's personal computer unit. Bull claimed that it would become the sixth largest information service company in the world. Such restructurings have been predicted with the approach of EC '92, as corporate strategies would begin to take into account directives and trade rules regarding the computer and telecommunications industries. Smaller European and American computer companies are anticipating battle with giants like IBM and DEC, which have long-established European divisions or subsidiaries. IBM has been the leader in mainframes, minicomputers, and personal computers, but it is expected that all computer companies, European-based or not, will face greater competition in Europe. The Netherlands' NV Philips, the largest European semiconductor and consumer electronics company, says it has been preparing for EC '92 since the 1970s. And North American Philips Chairman Gerrit Jeelof has claimed company credit for initiating the 1987 European Act. In a speech delivered at a Business Week and Foreign Policy Association Seminar last May, Jeelof said that while American companies had forsaken consumer electronics, Philips and France's Thompson have held their own against the Japanese. But he indicated that American dominance of the European semiconductor market was a major impetus for EC '92. Jeelof said: . . . because of the lack of European strength in the field of computers, the integrated circuits business in Europe is dominated by Americans. Europe consumes about 34 percent of all ICs in the world and only 18 percent are made in Europe by European companies. The rest are made by American companies or are imported. It is not a surprise then that in 1984 we at Philips took the initiative to stimulate a more unified European market. At the time, we called it Europe 1990. Brussels thought that 1990 was a bit too early and made it 1992. But it has been the electronics industry in Europe together with other major companies, that have been pushing for Europe 1992. Why did we want it? We wanted a more homogeneous total market in Europe and, based on that, we wanted to become more competitive. The process is on its way and obviously we see some reactions. If you take action, you get reaction. One reaction has been concern on the part of non-European companies and their governments that the EC is creating a protectionist environment, a "Fortress Europe." As walls between nations are coming down, some fear that other more impenetrable ones are going up on the Continent's edges. Jeelof argues against this perception in another speech, "Europe 1992—Fraternity or Fortress," reprinted in this issue in its entirety. Communications also presents an analysis of several trade rules relating to semi-conductors in "The Semiconductor Market in the European Community: Implications of Recent Rules and Regulations," by Roger Chiarodo and Judee Mussehl, both analysts in the Department of Commerce Office of Microelectronics and Instruments. The authors outline the consequences of Europe's Rules of Origin, anti-dumping measures that are supposed to prevent companies from using assembly operations in an importing country to circumvent duty on imported products. In the United States, if the difference between the value of parts or components from the dumping country and the value of the final product is small, then duty will be placed on those parts or components used in U.S. assembly operations. By contrast, the EC rule says that if the value of parts or components exceeds 60 percent of the value of all parts and materials, then duty will be placed on those parts and materials upon assembly in Europe. Since 1968, origin was also determined according to "the last substantial process or operation" resulting in the manufacture of a new product. In the case of printed circuit boards, some countries interpreted this as assembly and testing, while others thought it meant diffusion. In 1982, the EC began harmonizing these interpretations, and as of 1989, the last substantial operation was considered diffusion: the selective introduction of chemical dopants on a semiconductor substrate. As a result, American and Japanese semi-conductor manufacturers have spent millions building foundries on European soil. To reveal the Japanese interpretation of such changes, Japanese Commerce Minister Eiichi Ono, with the Japanese Embassy in Washington, DC, expresses his country's impressions of EC '92 in this issue. In his speech, "Japan's View of EC '92," delivered at an Armed Forces Communications and Electronics Association (AFCEA) conference on Europe '92, Ono states that while the EC's intentions might not be protectionist, they could become so upon implementation. His discussion focuses on semi-conductors and technology transfer issues. Although not a formal directive, in July 1988, the European Council decided to promote an internal information services market (the last "L" document in Table 2). To present the reasoning and objectives behind this initiative, we reprint the Communication from the Commission to the Council of Ministers, "The Establishment at Community Level of a Policy and a Plan of Priority Actions for the Development of an Information Services Market," and the resulting July 1988 "Council Decision" itself. Funds allocated for 1989 and 1990 are approximately $36 million, $23 million of which was slated for a pilot/demonstration program called IMPACT, for Information Market Policy Actions. This may seem a pittance in comparison to the programs of other governments, but this Decision and other EC legislation are the first steps toward an EC industrial policy. Recognizing that Europe's non-profit organizations and the public sector play a very important role in providing database services, in contrast to the U.S. where the private sector is now seeding the production of such database services, IMPACT has prepared guidelines to help the public sector cooperate with the private sector in marketing information. These guidelines would also allow private information providers to use public data and add value to it to create commercial products. IMPACT is providing incentives to accelerate innovative services for users by paying 25 percent of a project's cost. After the first call for proposals, 16 of 167 projects proposed by teams composed of 300 organizations were funded. American-based companies can apply for funds if they are registered in Europe. Unlike the U.S., the EC allows registration regardless of who owns a company's capital. Projects funded are to develop databases that would be accessible to all members of the Community either on CD-ROM or eventually on a digital network, an ISDN for all Europe, as planned by the fifth recommendation listed in Table 2. One project in the works is a library of pharmaceutical patents on CD-ROM that will enable users to locate digitized documents. Users will also have direct access to on-line hosts for all kinds of patents. A tourism information database and a multi-media image bank of atlases are other pilot projects chosen, and another project will provide information on standards. Eventually, audiotext might be used to retrieve data by telephone instead of a computer terminal. When the initial projects have been completed, the Commission will inform the market place about the results of the implementation. Plans for a five-year follow-up program, IMPACT-2 are also under discussion. These projects depend to some extent on the implementation and passage of directives or the success of larger and better funded projects. On-line access to databases depends on the recommendation for an ISDN as well as on the standardization directive for information technology and telecommunications. The certification, quality assurance, and conformity assessment issues involved in that directive are too numerous and important to just touch on here and will be covered in a later issue of Communications. To make these databases accessible not only technically, but also linguistically, the EC has funded two automatic language translation projects called Systran and Eurotra. Systran is also the name of the American company in La Jolla, CA, known for its pioneering work in translation. In conjunction with the EC, Systran Translation Systems, Inc., has completed a translation system for 24 language pairs (English—French, French—English, for example, are two language pairs) for the translation of IMPACT- funded databases. The system resides on an EC mainframe; there will be on-line access by subscription; and it will also be available on IBM PS/2s modified to run VMS DOS. It is already on France's widespread Minitel videotext network. As this practical, market-oriented approach to technology implementation is beginning, Europe's cooperative research effort, ESPRIT, is also starting to transfer its results. Last year, the second phase, ESPRIT II, set up a special office for technology transfer. Its mission is to ensure the exploitation, for the benefit of European industry, of the fruits of the $1.5 billion ESPRIT I program that began in 1984, as well as the current $3.2 billion program (funding through 1992). The EC contributes half of the total cost, which is matched by consortia comprised of university and industry researchers from more than one country. About 40 percent of ESPRIT II's funds will be devoted to computer related-technologies. Every November, ESPRIT holds a week-long conference. Last year for the first time it devoted a day to technology transfer. Several successful technology transfers have occurred either from one member of the program to another or out of the program to a member of industry that had not participated in the research. An electronic scanner that detects and eradicates faults on chips, for example, was developed by a consortium and the patents licensed by a small company. This automatic design validation scanner was co-developed by CSELT, Italy, British Telecom, CNET, another telecom company in France, IMAG, France, and Trinity College, Dublin. The company that will bring it to market is ICT, Gmbh, a relatively small German company. It seems that in Europe, as in the United States, small companies and spin-offs like those found in the Silicon Valley here, are better at running quickly with innovative ideas, says an EC administrator. Another technology transfer success is the Supernode computer. This hardware and software parallel processing project resulted in an unexpected product from transputer research. The Royal Signal Radar Establishment, Inmos, Telmat, and Thorn EMI, all of the UK, APTOR of France, and South Hampton University and the University of Grenoble, all participated in the research and now Inmos has put the product on the market. Three companies and two universities participated in developing the Dragon Project (for Distribution and Reusability of ADA Real-time Applications through Graceful On-line Operations). This was an effort to provide effective support for software reuse in real-time for distributed and dynamically reconfigurable systems. The researchers say they have resolved the problems of distribution in real-time performance and are developing a library and classification scheme now. One of the companies, TXT, in Milan, will bring it to market. Several other software projects are also ready for market. One is Meteor, which is aimed at integrating a formal approach to industrial software development, particularly in telecommunications. The participants have defined several languages, called ASF, COLD, ERAE, PLUSS, and PSF for requirements engineering and algebraic methods. Another project is QUICK, the design and experimentation of a knowledge-based system development tool kit for real-time process control applications. The tool kit consists of a general system architecture, a set of building modules, support tools for construction, and knowledge-based system analysis of design methodology. The tool kit will also contain a rule-based component based on fuzzy logic. During the next two years, more attention and funds will be indirectly devoted to technology transfer, and the intention to transfer is also likely to be one of the guides in evaluating project proposals. Some industry experts maintain that high technology and the flow of information made the upheaval in Eastern Europe inevitable. Leonard R. Sussman, author of Power, the Press, and the Technology of Freedom: The Coming Age of ISDN (Freedom House, 1990), predicted that technology and globally linked networks would result in the breakdown of censorious and suppressive political systems. He says the massive underground information flow due to books, copiers, software, hardware, and fax machines, in Poland for example, indicates that technology can mobilize society. Knowing that computers are essential to an industrial society, he says, Gorbachev faced a dilemma as decentralized computers loosened the government's control over the people running them. Glasnost evolved out of that dilemma, says Sussman. Last fall, a general draft trade and economic cooperation accord was signed by the European Commission and the Soviet Union. And both American and Western European business interests are calling for the Coordinating Committee on Multilateral Export Controls (COCOM) to relax high technology export rules to the Eastern Bloc and the Soviet Union. The passage of that proposal could allow huge computer and telecommunications markets to open up. And perhaps the Revolutions of 1989 will reveal themselves to have been revolutions in communication and the flow of information due in part to high technology and the hunger for it.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Prediction of preferences through optimizing users and reducing dimension in collaborative filtering system</title>
	<abstract>Collaborative filtering systems employ statistical techniques to find a set of users known as neighbors, who have a history of agreeing with the target user. However, the problems associated with high dimensionality in the recommender systems have been discussed in several studies. In addition, the degree of correlation is computed between only two users. Although the preference correlation of two users may not be very high, their preferences can serve as useful data for preference prediction. The preference information of the two users, however, cannot be used to give a recommendation because the degree of their mutual correlation is low. The users preferences in collaborative filtering systems are not necessarily accurate information. Carelessly entered information stored in the collaborative filtering database must be excluded. In this paper, Entropy is used for optimizing users. Bayesian classification shall be used to classify items to lower the dimension of the user-item matrix in this paper. To extract the features of items this paper uses association word mining. Since this method is representing document as a set of association words, it prevents users from being confused by word sense disambiguation. Also, the users can be put into clusters by using the genetic algorithm on the classified items to solve the problems of previous clustering algorithm such as the need to predefine the number of clusters, the high sensitivity to noise in data, and the possibility their resulting data would converge with the region's optimal solution. Finally, it predicts typical preferences by using entropy to solve the problem depending on the correlation match between only two users. The typical preferences mean pseudo preferences that represent preferences of items within the group. A dynamic recommendation for an item is made using the typical preference. To give dynamic recommendations to users, the process of classifying users receiving recommendations into the most suitable group takes precedence.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Recommendation on the social web: diversification and personalization</title>
	<abstract>The social Web has led to a flood of user-generated content in the last years being both a blessing and a curse. On the one hand, the individual user is totally overwhelmed by the endless stream of data and ubiquitous information. On the other hand, this huge amount of data can be explored and analyzed automatically to enable accurate predictions and behavioral studies. We will look into a particular social web application: tagging systems annd investigate how personal and diverse tag recommendations can be realized exploiting the wisdom of the crowd.

Tagging systems provide a platform for users to share, retrieve, and organize their social annotations. These tags fulfill different purposes and can be used in different ways. In the first part, we will investigate the different tag types and how they can be exploited for different task. Based on the work by Bischoff et al. [1] we look at a classification scheme for social annotations and analyse different tagging behavior.

In the second part, we will look into tag recommendation and how we can achieve good results with respect to the whole system and with respect to individual users. Collective tag recommendation using topic modeling techniques [3] and the combination with a language modeling approach [2] will be presented. Besides balancing diversity and personal preferences, we will also look into evaluation methods for tag recommendation and discuss possible problems and dependencies.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Large scale multi-label classification via metalabeler</title>
	<abstract>The explosion of online content has made the management of such content non-trivial. Web-related tasks such as web page categorization, news filtering, query categorization, tag recommendation, etc. often involve the construction of multi-label categorization systems on a large scale. Existing multi-label classification methods either do not scale or have unsatisfactory performance. In this work, we propose MetaLabeler to automatically determine the relevant set of labels for each instance without intensive human involvement or expensive cross-validation. Extensive experiments conducted on benchmark data show that the MetaLabeler tends to outperform existing methods. Moreover, MetaLabeler scales to millions of multi-labeled instances and can be deployed easily. This enables us to apply the MetaLabeler to a large scale query categorization problem in Yahoo!, yielding a significant improvement in performance.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A framework for choosing a database query language</title>
	<abstract>This paper presents a systematic approach to matching categories of query language interfaces with the requirements of certain user types. The method is based on a trend model of query language development on the dimensions of functional capabilities and usability. From the trend model the following are derived: a classification scheme for query languages, a criterion hierarchy for query language evaluation, a comprehensive classification scheme of query language users and their requirements, and preliminary recommendations for allocating language classes to user types. The method integrates the results of existing human factors studies and provides a structured framework for future research in this area. Current and expected developments are exemplified by the description of "new generation" database query languages. In a practical query language selection problem, the results of this paper can be used for preselecting suitable query language types; the final selection decision will also depend on organization-specific factors, such as the available database management system, hardware and software strategies, and financial system costs.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Text mining for product attribute extraction</title>
	<abstract>We describe our work on extracting attribute and value pairs from textual product descriptions. The goal is to augment databases of products by representing each product as a set of attribute-value pairs. Such a representation is beneficial for tasks where treating the product as a set of attribute-value pairs is more useful than as an atomic entity. Examples of such applications include demand forecasting, assortment optimization, product recommendations, and assortment comparison across retailers and manufacturers. We deal with both implicit and explicit attributes and formulate both kinds of extractions as classification problems. Using single-view and multi-view semi-supervised learning algorithms, we are able to exploit large amounts of unlabeled data present in this domain while reducing the need for initial labeled data that is expensive to obtain. We present promising results on apparel and sporting goods products and show that our system can accurately extract attribute-value pairs from product descriptions. We describe a variety of application that are built on top of the results obtained by the attribute extraction system.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Incorporating contextual information in recommender systems using a multidimensional approach</title>
	<abstract>The article presents a multidimensional (MD) approach to recommender systems that can provide recommendations based on additional contextual information besides the typical information on users and items used in most of the current recommender systems. This approach supports multiple dimensions, profiling information, and hierarchical aggregation of recommendations. The article also presents a multidimensional rating estimation method capable of selecting two-dimensional segments of ratings pertinent to the recommendation context and applying standard collaborative filtering or other traditional two-dimensional rating estimation techniques to these segments. A comparison of the multidimensional and two-dimensional rating estimation approaches is made, and the tradeoffs between the two are studied. Moreover, the article introduces a combined rating estimation method, which identifies the situations where the MD approach outperforms the standard two-dimensional approach and uses the MD approach in those situations and the standard two-dimensional approach elsewhere. Finally, the article presents a pilot empirical study of the combined approach, using a multidimensional movie recommender system that was developed for implementing this approach and testing its performance.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Item-based top-N recommendation algorithms</title>
	<abstract>The explosive growth of the world-wide-web and the emergence of e-commerce has led to the development of recommender systems---a personalized information filtering technology used to identify a set of items that will be of interest to a certain user. User-based collaborative filtering is the most successful technology for building recommender systems to date and is extensively used in many commercial recommender systems. Unfortunately, the computational complexity of these methods grows linearly with the number of customers, which in typical commercial applications can be several millions. To address these scalability concerns model-based recommendation techniques have been developed. These techniques analyze the user--item matrix to discover relations between the different items and use these relations to compute the list of recommendations.In this article, we present one such class of model-based recommendation algorithms that first determines the similarities between the various items and then uses them to identify the set of items to be recommended. The key steps in this class of algorithms are (i) the method used to compute the similarity between the items, and (ii) the method used to combine these similarities in order to compute the similarity between a basket of items and a candidate recommender item. Our experimental evaluation on eight real datasets shows that these item-based algorithms are up to two orders of magnitude faster than the traditional user-neighborhood based recommender systems and provide recommendations with comparable or better quality.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Recommending friends and locations based on individual location history</title>
	<abstract>The increasing availability of location-acquisition technologies (GPS, GSM networks, etc.) enables people to log the location histories with spatio-temporal data. Such real-world location histories imply, to some extent, users' interests in places, and bring us opportunities to understand the correlation between users and locations. In this article, we move towards this direction and report on a personalized friend and location recommender for the geographical information systems (GIS) on the Web. First, in this recommender system, a particular individual's visits to a geospatial region in the real world are used as their implicit ratings on that region. Second, we measure the similarity between users in terms of their location histories and recommend to each user a group of potential friends in a GIS community. Third, we estimate an individual's interests in a set of unvisited regions by involving his/her location history and those of other users. Some unvisited locations that might match their tastes can be recommended to the individual. A framework, referred to as a hierarchical-graph-based similarity measurement (HGSM), is proposed to uniformly model each individual's location history, and effectively measure the similarity among users. In this framework, we take into account three factors: 1) the sequence property of people's outdoor movements, 2) the visited popularity of a geospatial region, and 3) the hierarchical property of geographic spaces. Further, we incorporated a content-based method into a user-based collaborative filtering algorithm, which uses HGSM as the user similarity measure, to estimate the rating of a user on an item. We evaluated this recommender system based on the GPS data collected by 75 subjects over a period of 1 year in the real world. As a result, HGSM outperforms related similarity measures, namely similarity-by-count, cosine similarity, and Pearson similarity measures. Moreover, beyond the item-based CF method and random recommendations, our system provides users with more attractive locations and better user experiences of recommendation.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Stacking recommendation engines with additional meta-features</title>
	<abstract>In this paper, we apply stacking, an ensemble learning method, to the problem of building hybrid recommendation systems. We also introduce the novel idea of using runtime metrics which represent properties of the input users/items as additional meta-features, allowing us to combine component recommendation engines at runtime based on user/item characteristics. In our system, component engines are level-1 predictors, and a level-2 predictor is learned to generate the final prediction of the hybrid system. The input features of the level-2 predictor are predictions from component engines and the runtime metrics. Experimental results show that our system outperforms each single component engine as well as a static hybrid system. Our method has the additional advantage of removing restrictions on component engines that can be employed; any engine applicable to the target recommendation task can be easily plugged into the system.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>AIMED: a personalized TV recommendation system</title>
	<abstract>Previous personalized DTV recommendation systems focus only on viewers' historical viewing records or demographic data. This study proposes a new recommending mechanism from a user oriented perspective. The recommending mechanism is based on user properties such as Activities, Interests, Moods, Experiences, and Demographic information--AIMED. The AIMED data is fed into a neural network model to predict TV viewers' program preferences. Evaluation results indicate that the AIMED model significantly increases recommendation accuracy and decreases prediction errors compared to the conventional model.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Latent semantic models for collaborative filtering</title>
	<abstract>Collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, that is, a database of available user preferences. In this article, we describe a new family of model-based algorithms designed for this task. These algorithms rely on a statistical modelling technique that introduces latent class variables in a mixture model setting to discover user communities and prototypical interest profiles. We investigate several variations to deal with discrete and continuous response variables as well as with different objective functions. The main advantages of this technique over standard memory-based methods are higher accuracy, constant time prediction, and an explicit and compact model representation. The latter can also be used to mine for user communitites. The experimental evaluation shows that substantial improvements in accucracy over existing methods and published results can be obtained.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Research on the Recommending Method Used in C2C Online Trading</title>
	<abstract>Electronic commerce becomes more and more frequent and the online trading bases on C2C context are more common. However, the ever-increasing user size and commodities cause the problem of information overload. Collaborative filtering technology is the most popular and successful method to overcome the problem in E-commerce recommender systems. Since there is much difference between B2C and C2C context where not only the buyer preference but also the seller preference is taken into account .This paper analyzes the user behaviors on the website and constructs the user preference model under the C2C context. And the author defines trust vector in this paper. Based on this definition, a new recommend trust model is proposed. The simulation shows that compared with the current recommending method, the proposed one is more effective.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The wisdom of the few: a collaborative filtering approach based on expert opinions from the web</title>
	<abstract>Nearest-neighbor collaborative filtering provides a successful means of generating recommendations for web users. However, this approach suffers from several shortcomings, including data sparsity and noise, the cold-start problem, and scalability. In this work, we present a novel method for recommending items to users based on expert opinions. Our method is a variation of traditional collaborative filtering: rather than applying a nearest neighbor algorithm to the user-rating data, predictions are computed using a set of expert neighbors from an independent dataset, whose opinions are weighted according to their similarity to the user. This method promises to address some of the weaknesses in traditional collaborative filtering, while maintaining comparable accuracy. We validate our approach by predicting a subset of the Netflix data set. We use ratings crawled from a web portal of expert reviews, measuring results both in terms of prediction accuracy and recommendation list precision. Finally, we explore the ability of our method to generate useful recommendations, by reporting the results of a user-study where users prefer the recommendations generated by our approach.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Support vector machines for collaborative filtering</title>
	<abstract>Support Vector Machines (SVMs) have successfully shown efficiencies in many areas such as text categorization. Although recommendation systems share many similarities with text categorization, the performance of SVMs in recommendation systems is not acceptable due to the sparsity of the user-item matrix. In this paper, we propose a heuristic method to improve the predictive accuracy of SVMs by repeatedly correcting the missing values in the user-item matrix. The performance comparison to other algorithms has been conducted. The experimental studies show that the accurate rates of our heuristic method are the highest.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Collaborative filtering via gaussian probabilistic latent semantic analysis</title>
	<abstract>Collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, i.e. a database of available user preferences. In this paper, we describe a new model-based algorithm designed for this task, which is based on a generalization of probabilistic latent semantic analysis to continuous-valued response variables. More specifically, we assume that the observed user ratings can be modeled as a mixture of user communities or interest groups, where users may participate probabilistically in one or more groups. Each community is characterized by a Gaussian distribution on the normalized ratings for each item. The normalization of ratings is performed in a user-specific manner to account for variations in absolute shift and variance of ratings. Experiments on the EachMovie data set show that the proposed approach compares favorably with other collaborative filtering techniques.</abstract>
	<search_task_number>2</search_task_number>
	<query>recommendation systems classification</query>
	<relevance>0</relevance>
  </item>




  <item>
    <title>Visualization of large data sets by mixing Tcl and C++ interfaces to the VTK library</title>
    <abstract>This article describes a technique to build applications for the visualization of large data sets using VTK library and components written in Tcl and C++. VTK is a software system for computer graphics, visualization and image processing. VTK library is written in C++ however it provides interfaces to the scripting languages Tcl, Python and Java. Though it is possible to write a whole VTK application in a scripting language like Tcl, it is more suitable, for efficiency reasons, to implement some functionality in a compiled language like C/C++ for instance. This article presents in through details how to access VTK objects from different languages and how to mix Tcl and C++ components in one application.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Ontology supported semantic simplification of large data sets of industrial plant CAD models for design review visualization</title>
    <abstract>We present in this article a semantic compression system for design review visualization of large data sets in the domain of industrial plant design. This system introduces semantic aspects that improve traditional Computer Graphics techniques used for interactive walkthroughs. We complement previous works introducing new modules and algorithms for the automatic categorization, simplification and user-oriented adaptation of engineering components in the model, and base directly our work on a full Ontology based on international standards for product data in this domain (ISO-STEP 10303-AP227).</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Pixel bar charts: a visualization technique for very large multi-attribute data sets</title>
    <abstract>Simple presentation graphics are intuitive and easy-to-use, but show only highly aggregated data presenting only a very small number of data values (as in the case of bar charts) and may have a high degree of overlap occluding a significant portion of the data values (as in the case of the x-y plots). In this article, the authors therefore propose a generalization of traditional bar charts and x-y plots, which allows the visualization of large amounts of data. The basic idea is to use the pixels within the bars to present detailed information of the data records. The so-called pixel bar charts retain the intuitiveness of traditional bar charts while allowing very large data sets to be visualized in an effective way. It is shown that, for an effective pixel placement, a complex optimization problem has to be solved. The authors then present an algorithm which efficiently solves the problem. The application to a number of real-world e-commerce data sets shows the wide applicability and usefulness of this new idea, and a comparison to other well-known visualization techniques (parallel coordinates and spiral techniques) shows a number of clear advantages.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automated Hierarchical Density Shaving: A Robust Automated Clustering and Visualization Framework for Large Biological Data Sets</title>
    <abstract>A key application of clustering data obtained from sources such as microarrays, protein mass spectroscopy, and phylogenetic profiles is the detection of functionally related genes. Typically, only a small number of functionally related genes cluster into one or more groups, and the rest need to be ignored. For such situations, we present Automated Hierarchical Density Shaving (Auto-HDS), a framework that consists of a fast hierarchical density-based clustering algorithm and an unsupervised model selection strategy. Auto-HDS can automatically select clusters of different densities, present them in a compact hierarchy, and rank individual clusters using an innovative stability criteria. Our framework also provides a simple yet powerful 2D visualization of the hierarchy of clusters that is useful for further interactive exploration. We present results on Gasch and Lee microarray data sets to show the effectiveness of our methods. Additional results on other biological data are included in the supplemental material.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Interactive visualization of large scale time varying data sets</title>
    <abstract>Visual analysis of time varying scientific data can be divided into four different categories with an increasing degree of user interaction.
1) Production of static images representing scientific data at selected times.
2) Production of video sequences in which graphical representation, time line and viewpoints are predefined.
3) Interactive streaming of logged data sets, allowing the user to alter graphical representation, filtering, time lines and viewpoints.
4) Real time interaction with the simulation or experiment that produces the data, allowing the user to alter parameters, graphical representation, filtering, time lines and viewpoints.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Visualization of Large Data Sets with the Active Data Repository</title>
    <abstract>The authors present implementations of ray-casting-based volume rendering andisosurface rendering methods using the Active Data Repository (ADR)for visualizing out-of-core datasets, and describe experimental performance results. ADR is anobject-oriented framework designed to provide support for applications that analyze, explore, and visualize very large multidimensional data sets. It targets distributed memory parallel machines with one ormore disks attached to each node.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Distributed and collaborative visualization of large data sets using high-speed networks</title>
    <abstract>We describe an architecture for distributed collaborative visualization that integrates video conferencing, distributed data management and grid technologies as well as tangible interaction devices for visualization. High-speed, low-latency optical networks support high-quality collaborative interaction and remote visualization of large data.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Visualization of large scientific data sets on a parallel system with a tiled display</title>
    <abstract>As data collection technologies advance, almost every field of scientific research sees an increase in the amount and frequency of data that can be collected. In many cases, the first best method for preprocessing two and three dimensional data sets is to examine them visually, using some sort of plotting software. Although display technology is advancing and monitors are becoming available with increasing resolutions, this increase in available resolution does not necessarily match the rapid increase in data collection. This mismatch leaves researchers with only three choices for visual processing: to view the data in many small chunks, to view only a representative subset of the data, or to create a custom display for viewing much larger data sets at higher resolutions. This paper discusses one method of fullfilling the third option. The process used here is to create a customized display using nine consumer level monitors mounted in a three by three configuration resulting in a display with nine times the resolution of a single monitor. A custom viewing environment is also developed within this project that is designed to lend itself better to the tiled display than would an existing three dimensional data visualization application. The data used for this project consists of a large set of Oceanographic sensor readings taken from several deployments throughout the Gulf of Maine.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Visualization of large web access data sets</title>
    <abstract>Many real-world e-service applications require analyzing large volumes of transaction data to extract web access information. This paper describes Web Access Visualization (WAV) a system that visually associates the affinities and relationships of clients and URLs for large volumes of web transaction data. To date, many practical research projects have shown the usefulness of a physics-based mass-spring technique to layout data items with close relationships onto a graph. The WAV system: (1) maps transaction data items (clients, URLs) and their relationships to vertices, edges, and positions on a 3D spherical surface; (2) encapsulates a physics-based engine in a visual data analysis platform; and (3) employs various content sensitive visual techniques - linked multiple views, layered drill-down, and fade in/out - for interactive data analysis. We have applied this system to a web application to analyze web access patterns and trends. The web service quality has been greatly benefited from using the information provided by WAV.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <!-- 10 -->
  <item>
    <title>Viz3D: Effective Exploratory Visualization of Large Multidimensional Data Sets</title>
    <abstract>We propose a multidimensional visualization technique, named Viz3D, that creates a 3D representation of n-dimensional data that may be interactively manipulated by users to handle visual cluttering and object occlusion. The projection performed in Viz3D is comparable in quality with the 3D projections obtained with well-known dimensionality reduction techniques, at a lower complexity cost. While a 3D projection conveys more information, giving the user more control of the visual representation and an additional dimension, as compared to 2D, visual cluttering and object occlusion are still a problem in handling large multidimensional data sets. To produce more effective visualizations, two strategies are introduced. Dimensionality is handled with a similarity clustering of attributes prior to projection. Data set size is handled with a new strategy of visualizing data densities, rather than individual data records. Both the direct and density Viz3D visualizations provide the basis for a user driven visual clustering approach applicable to high-dimensional data sets that is very simple, intuitive and effective. </abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>On the Interactive Visualization of Very Large Image Data Sets</title>
    <abstract>This paper presents a system for real-time visualization of very large image data sets using on- demand loading and dynamic view prediction. We use a robust image representation scheme for efficient adaptive rendering and a perspective view generation module to extend the applicability of the system to panoramic images. We demonstrate the effectiveness of the system by applying it both to imagery that does not require perspective correction and to very large panoramic data sets requiring perspective view generation. The system permits smooth, real-time interactive navigation of very large panoramic and non-panoramic image data sets on average personal computers without the use of specialized hardware. </abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Visualization of Diversity in Large Multivariate Data Sets</title>
    <abstract>Understanding the diversity of a set of multivariate objects is an important problem in many domains, including ecology, college admissions, investing, machine learning, and others. However, to date, very little work has been done to help users achieve this kind of understanding. Visual representation is especially appealing for this task because it offers the potential to allow users to efficiently observe the objects of interest in a direct and holistic way. Thus, in this paper, we attempt to formalize the problem of visualizing the diversity of a large (more than 1000 objects), multivariate (more than 5 attributes) data set as one worth deeper investigation by the information visualization community. In doing so, we contribute a precise definition of diversity, a set of requirements for diversity visualizations based on this definition, and a formal user study design intended to evaluate the capacity of a visual representation for communicating diversity information. Our primary contribution, however, is a visual representation, called the Diversity Map, for visualizing diversity. An evaluation of the Diversity Map using our study design shows that users can judge elements of diversity consistently and as or more accurately than when using the only other representation specifically designed to visualize diversity.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Algorithms for the visualization of large and multivariate data sets</title>
    <abstract>In this chapter we discuss algorithms for clustering and visualization of large and multivariate data. We describe an algorithm for exploratory data analysis which combines adaptive c-means clustering and multi-dimensional scaling (ACMDS). ACMDS is an algorithm for the online visualization of clustering processes and may be considered as an alternative approach to Kohonen's self organizing feature map (SOM). Whereas SOM is a heuristic neural network algorithm, ACMDS is derived from multivariate statistical algorithms. The implications of ACMMDS are illustrated through five different data sets.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>3D Visualization of Large Digital Elevation Model (DEM) Data Set</title>
    <abstract>With the rapid development of Earth observation technologies, raster data has become the main Geographic Information System (GIS) data source. GIS should be able to manage and process huge raster data sets. In this paper, the problem of 3-dimensional visualization of huge DEM data sets and its corresponding images data sets in GIS was addressed. A real-time delamination method based on raster characteristics of DEM and image data has been developed. A simple geometry algorithm was used to implement dynamic Level of Division (LOD) delamination of DEM and image data and to realize real-time 3-dimensional visualization of huge DEM and image data set based on RDBMS management.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Time and Streak Surfaces for Flow Visualization in Large Time-Varying Data Sets</title>
    <abstract>Time and streak surfaces are ideal tools to illustrate time-varying vector fields since they directly appeal to the intuition about coherently moving particles. However, efficient generation of high-quality time and streak surfaces for complex, large and time-varying vector field data has been elusive due to the computational effort involved. In this work, we propose a novel algorithm for computing such surfaces. Our approach is based on a decoupling of surface advection and surface adaptation and yields improved efficiency over other surface tracking methods, and allows us to leverage inherent parallelization opportunities in the surface advection, resulting in more rapid parallel computation. Moreover, we obtain as a result of our algorithm the entire evolution of a time or streak surface in a compact representation, allowing for interactive, high-quality rendering, visualization and exploration of the evolving surface. Finally, we discuss a number of ways to improve surface depiction through advanced rendering and texturing, while preserving interactivity, and provide a number of examples for real-world datasets and analyze the behavior of our algorithm on them.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Ssecrett and NeuroTrace: Interactive Visualization and Analysis Tools for Large-Scale Neuroscience Data Sets</title>
    <abstract>Recent advances in optical and electron microscopy let scientists acquire extremely high-resolution images for neuroscience research. Data sets imaged with modern electron microscopes can range between tens of terabytes to about one petabyte. These large data sizes and the high complexity of the underlying neural structures make it very challenging to handle the data at reasonably interactive rates. To provide neuroscientists flexible, interactive tools, the authors introduce Ssecrett and NeuroTrace, two tools they designed for interactive exploration and analysis of large-scale optical- and electron-microscopy images to reconstruct complex neural circuits of the mammalian nervous system.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Efficient Point-Based Isosurface Exploration Using the Span-Triangle</title>
    <abstract>We introduce a novel span-triangle data structure, based on the span-space representation for isosurfaces. It stores all necessary cell information for dynamic manipulation of the isovalue in an efficient way. We have found that using our data structure in combination with point-based techniques, implemented on graphics hardware, effects in real-time rendering and exploration. Our extraction algorithm utilizes an incremental and progressive update scheme, enabling smooth interaction without significant latency. Moreover, the corresponding visualization pipeline is capable of processing large data sets by utilizing all three levels of memory: disk, system and graphics. We address practical usability in actual medical applications, achieving a new level of interactivity.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Fast implicit KD-trees: accelerated isosurface ray tracing and maximum intensity projection for large scalar fields</title>
    <abstract>Many scientific data sets are 3D or 4D scalar fields, for which typically isosurface- and volume visualization methods are used to extract information. These data sets are either massively complex (e.g., seismic data sets), or steadily increasing in size due to the permanently improving resolutions of different 3D scanners (e.g., CT- and MRT-scanners) or calculation results (e.g., CFD-simulations). Only algorithms that scale well to data set complexity are suited to visualize those increasing data sets.
Isosurface ray tracing and maximum intensity projection (MIP) accelerated through implicit KD-trees have a logarithmic dependency between visualization time and scene size, making them ideal algorithms for the visualization of massively complex scalar fields. Furthermore is ray tracing efficiently parallelized on the more and more commonly used shared memory machines (e.g., desktop machines with several multicore processors) and may be used to realize advanced shading effects like shadows and reflections. We introduce new optimized implicit KD-trees which allow on today's desktop computers interactive isosurfacing and MIP of data sets that are bigger than one half of the machine's main memory.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Adaptive 4-8 Texture Hierarchies</title>
    <abstract>We address the texture level-of-detail problem for extremely large surfaces such as terrain during realtime, view-dependent rendering. A novel texture hierarchy is introduced based on 4-8 refinement of raster tiles, in which the texture grids in effect rotate 45 degrees for each level of refinement. This hierarchy provides twice as many levels of detail as conventional quadtree-style refinement schemes such as mipmaps, and thus provides per-pixel view-dependent filtering that is twice as close to the ideal cutoff frequency for an average pixel. Because of this more gradual change in low-pass filtering, and due to the more precise emulation of the ideal cutoff frequency, we find in practice that the transitions between texture levels of detail are not perceptible. This allows rendering systems to avoid the complexity and performance costs of per-pixel blending between texture levels of detail. The 4-8 texturing scheme is integrated into a variant of the Real-time Optimally Adapting Meshes (ROAM) algorithm for viewdependent multiresolution mesh generation. Improvements to ROAM included here are: the diamond data structure as a stream-lined replacement for the triangle bintree elements, the use of low-pass- filtered geometry patches in place of individual triangles, integration of 4-8 textures, and a simple out-of-core data access mechanism for texture and geometry tiles.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <!-- 20 -->
  <item>
    <title>Visualization and Analysis of Large Data Collections: a Case Study Applied to Confocal Microscopy Data</title>
    <abstract>In this paper we propose an approach in which interactive visualization and analysis are combined with batch tools for the processing of large data collections. Large and heterogeneous data collections are difficult to analyze and pose specific problems to interactive visualization. Application of the traditional interactive processing and visualization approaches as well as batch processing encounter considerable drawbacks for such large and heterogeneous data collections due to the amount and type of data. Computing resources are not sufficient for interactive exploration of the data and automated analysis has the disadvantage that the user has only limited control and feedback on the analysis process.In our approach, an analysis procedure with features and attributes of interest for the analysis is defined interactively. This procedure is used for off-line processing of large collections of data sets. The results of the batch process along with ``visual summaries'' are used for further analysis. Visualization is not only used for the presentation of the result, but also as a tool to monitor the validity and quality of the operations performed during the batch process. Operations such as feature extraction and attribute calculation of the collected data sets are validated by visual inspection. This approach is illustrated by an extensive case study, in which a collection of confocal microscopy data sets is analyzed.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Scalable Data Servers for Large Multivariate Volume Visualization</title>
    <abstract>Volumetric datasets with multiple variables on each voxel over multiple time steps are often complex, especially when considering the exponentially large attribute space formed by the variables in combination with the spatial and temporal dimensions. It is intuitive, practical, and thus often desirable, to interactively select a subset of the data from within that high-dimensional value space for efficient visualization. This approach is straightforward to implement if the dataset is small enough to be stored entirely in-core. However, to handle datasets sized at hundreds of gigabytes and beyond, this simplistic approach becomes infeasible and thus, more sophisticated solutions are needed. In this work, we developed a system that supports efficient visualization of an arbitrary subset, selected by range-queries, of a large multivariate time-varying dataset. By employing specialized data structures and schemes of data distribution, our system can leverage a large number of networked computers as parallel data servers, and guarantees a near optimal load-balance. We demonstrate our system of scalable data servers using two large time-varying simulation datasets.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Real-Time Optimal Adaptation for Planetary Geometry and Texture: 4-8 Tile Hierarchies</title>
    <abstract>The real-time display of huge geometry and imagery databases involves view-dependent approximations, typically through the use of precomputed hierarchies that are selectively refined at runtime. A classic motivating problem is terrain visualization in which planetary databases involving billions of elevation and color values are displayed on PC graphics hardware at high frame rates. This paper introduces a new diamond data structure for the basic selective-refinement processing, which is a streamlined method of representing the well-known hierarchies of right triangles that have enjoyed much success in real-time, view-dependent terrain display. Regular-grid tiles are proposed as the payload data per diamond for both geometry and texture. The use of 4-8 grid refinement and coarsening schemes allows level-of-detail transitions that are twice as gradual as traditional quadtree-based hierarchies, as well as very high-quality low-pass filtering compared to subsampling-based hierarchies. An out-of-core storage organization is introduced based on Sierpinski indices per diamond, along with a tile preprocessing framework based on fine-to-coarse, same-level, and coarse-to-fine gathering operations. To attain optimal frame-to-frame coherence and processing-order priorities, dual split and merge queues are developed similar to the Realtime Optimally Adapting Meshes (ROAM) Algorithm, as well as an adaptation of the ROAM frustum culling technique. Example applications of lake-detection and procedural terrain generation demonstrate the flexibility of the tile processing framework.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Designing Pixel-Oriented Visualization Techniques: Theory and Applications</title>
    <abstract>Visualization techniques are of increasing importance in exploring and analyzing large amounts of multidimensional information. One important class of visualization techniques which is particularly interesting for visualizing very large multidimensional data sets is the class of pixel-oriented techniques. The basic idea of pixel-oriented visualization techniques is to represent as many data objects as possible on the screen at the same time by mapping each data value to a pixel of the screen and arranging the pixels adequately. A number of different pixel-oriented visualization techniques have been proposed in recent years and it has been shown that the techniques are useful for visual data exploration in a number of different application contexts. In this paper, we discuss a number of issues which are of high importance in developing pixel-oriented visualization techniques. The major goal of this article is to provide a formal basis of pixel-oriented visualization techniques and show that the design decisions in developing them can be seen as solutions of well-defined optimization problems. This is true for the mapping of the data values to colors, the arrangement of pixels inside the subwindows, the shape of the subwindows, and the ordering of the dimension subwindows. The paper also discusses the design issues of special variants of pixel-oriented techniques for visualizing large spatial data sets. The optimization functions for the mentioned design decisions are important for the effectiveness of the resulting visualizations. We show this by evaluating the optimization functions and comparing the results to the visualizations obtained in a number of different application.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Recursive Pattern: A Technique for Visualizing Very Large Amounts of Data</title>
    <abstract>An important goal of visualization technology is to support the exploration and analysis of very large amounts of data. In this paper, we propose a new visualization technique called `recursive pattern' which has been developed for visualizing large amounts of multidimensional data. The technique is based on a generic recursive scheme which generalizes a wide range of pixel-oriented arrangements for displaying large data sets. By instantiating the technique with adequate data- and application-dependent parameters, the user may largely influence the structure of the resulting visualizations. Since the technique uses one pixel for presenting each data value, the amount of data which can be displayed is only limited by the resolution of current display technology and by the limitations of human perceptibility. Beside describing the basic idea of the `recursive pattern' technique, we provide several examples of useful parameter settings for the various recursion levels. We further show that our `recursive pattern ' technique is particularly advantageous for the large class of data sets which have a natural order according to one dimension (e.g. time series data). We demonstrate the usefulness of our technique by using a stock market application.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Histographs: Interactive Clustering of Stacked Graphs</title>
    <abstract>Visualization systems must intuitively display and allow interaction with large multivariate data on low-dimensional displays. One problem often encountered in the process is occlusion: the ambiguity that occurs when records from different data sets are mapped to the same display location. For example, because of occlusion summarizing 1000 graphs by simply stacking them one over another is pointless. We solve this problem by adapting the solution to a similar problem in the Information Murals system [2]: mapping the number of data elements at a location to display luminance. Inspired by histograms, which map data frequency to space, we call our solution histographs. By treating a histograph as a digital image, we can blur and highlight edges to emphasize data features. We also support interactive clustering of the data with data zooming and shape-based selection. We are currently investigating alternative occlusion blending schemes. </abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Extreme Scaling of Production Visualization Software on Diverse Architectures</title>
    <abstract>A series of experiments studied how visualization software scales to massive data sets. Although several paradigms exist for processing large data, the experiments focused on pure parallelism, the dominant approach for production software. The experiments used multiple visualization algorithms and ran on multiple architectures. They focused on massive-scale processing (16,000 or more cores and one trillion or more cells) and weak scaling. These experiments employed the largest data set sizes published to date in the visualization literature. The findings on scaling characteristics and bottlenecks will help researchers understand how pure parallelism performs at high levels of concurrency with very large data sets.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A series of experiments studied how visualization software scales to massive data sets. Although several paradigms exist for processing large data, the experiments focused on pure parallelism, the dominant approach for production software. The experiments used multiple visualization algorithms and ran on multiple architectures. They focused on massive-scale processing (16,000 or more cores and one trillion or more cells) and weak scaling. These experiments employed the largest data set sizes published to date in the visualization literature. The findings on scaling characteristics and bottlenecks will help researchers understand how pure parallelism performs at high levels of concurrency with very large data sets.</title>
    <abstract>Recently, multiresolution visualization methods have become an indispensable ingredient of real-time interactive postprocessing. The enormous databases, typically coming along with some hierarchical structure, are locally resolved on different levels of detail to achieve a significant savings of CPU and rendering time. Here, the method of adaptive projection and the corresponding operators on data functions, respectively, are introduced. They are defined and discussed as mathematically rigorous foundations for multiresolution data analysis. Keeping in mind data from efficient numerical multigrid methods, this approach applies to hierarchical nested grids consisting of elements which are any tensor product of simplices, generated recursively by an arbitrary, finite set of refinement rules from some coarse grid. The corresponding visualization algorithms, e.g., color shading on slices or isosurface rendering, are confined to an appropriate depth-first traversal of the grid hierarchy. A continuous projection of the data onto an adaptive, extracted subgrid is thereby calculated recursively. The presented concept covers different methods of local error measurement, time-dependent data which have to be interpolated from a sequence of key frames, and a tool for local data focusing. Furthermore, it allows for a continuous level of detail.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Hierarchical Pixel Bar Charts</title>
    <abstract>Simple presentation graphics are intuitive and easy-to-use, but only show highly aggregated data. Bar charts, for example, only show a rather small number of data values and x-y-plots often have a high degree of overlap. Presentation techniques are often chosen depending on the considered data type--bar charts, for example, are used for categorical data and x-y plots are used for numerical data. In this article, we propose a combination of traditional bar charts and x-y-plots, which allows the visualization of large amounts of data with categorical and numerical data. The categorical data dimensions are used for the partitioning into the bars and the numerical data dimensions are used for the ordering arrangement within the bars. The basic idea is to use the pixels within the bars to present the detailed information of the data records. Our so-called pixel bar charts retain the intuitiveness of traditional bar charts while applying the principle of x-y charts within the bars. In many applications, a natural hierarchy is defined on the categorical data dimensions such as time, region, or product type. In hierarchical pixel bar charts, the hierarchy is exploited to split the bars for selected portions of the hierarchy. Our application to a number of real-world e-business and Web services data sets shows the wide applicability and usefulness of our new idea.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A Coherent Grid Traversal Approach to Visualizing Particle-Based Simulation Data</title>
    <abstract>We present an approach to visualizing particle-based simulation data using interactive ray tracing and describe an algorithmic enhancement that exploits the properties of these data sets to provide highly interactive performance and reduced storage requirements. This algorithm for fast packet-based ray tracing of multilevel grids enables the interactive visualization of large time-varying data sets with millions of particles and incorporates advanced features like soft shadows. We compare the performance of our approach with two recent particle visualization systems: one based on an optimized single ray grid traversal algorithm and the other on programmable graphics hardware. This comparison demonstrates that the new algorithm offers an attractive alternative for interactive particle visualization.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <!-- 30 -->
  <item>
    <title>Visualization of the Energy-Containing Turbulent Scales</title>
    <abstract>In this study, we explore a novel approach for visualizing the energetic turbulent structures in a flow field. The flow field is generated by a direct numerical simulation (DNS) of a stratified turbulent shear layer instigated by the Kelvin-Helmholtz instability. The use of so-called structure-based tensors combined with volume rendering seems to be a very promising tool to gain new insight into the dynamically most important part of the turbulence. Rendering of these tensors depicts the large-scale structures that carry most of the turbulence energy. This is in distinct contrast to traditional methods based on derivatives of the velocity field, such as those based on the velocity gradient tensor and vorticity. These methods only capture the smaller-scale structures of the flow. Traditionally, statistical measures are used to handle the enormous amount of data generated by DNS, whereby a lot of detailed information is inevitably lost. The rapid increase in computer performance combined with advanced visualization techniques makes it possible to use a non-statistical or deterministical approach to study the kinematic and dynamic properties of turbulent flows. This paper presents a promising first attempt to render structure-based tensors to see how faithfully they can describe the large-scale structures in a stratified turbulent shear flow.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Accelerating Large Data Analysis By Exploiting Regularities</title>
    <abstract>We present techniques for discovering and exploiting regularity in large curvilinear data sets. The data can be based on a single mesh or a mesh composed of multiple submeshes (also known as zones). Multi-zone data are typical in Computational Fluid Dynamics (CFD) simulations. Regularities include axis-aligned rectilinear and cylindrical meshes as well as cases where one zone is equivalent to a rigid-body transformation of another. Our algorithms can also discover rigid-body motion of meshes in time-series data. Next, we describe a data model where we can utilize the results from the discovery process in order to accelerate large data visualizations. Where possible, we replace general curvilinear zones with rectilinear or cylindrical zones. In rigid-body motion cases we replace a time-series of meshes with a transformed mesh object where a reference mesh is dynamically transformed based on a given time value in order to satisfy geometry requests, on demand. The data model enables us to make these substitutions and dynamic transformations transparently with respect to the visualization algorithms. We present results with large data sets where we combine our mesh replacement and transformation techniques with out-of-core paging in order to achieve analysis speedups ranging from 1.5 to 2.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>VIRACOCHA: An Efficient Parallelization Framework for Large-Scale CFD Post-Processing in Virtual Environments</title>
    <abstract>One recommended strategy for the analysis of CFD-data is the interactive exploration within virtual environments. Common visualization systems are unable to process large data sets while carrying out real-time interaction and visualization at the same time. The obvious idea is to decouple flow feature extraction from visualization. This paper covers the functionality of the parallel CFD post-processing toolkit Viracocha. Two aspects are discussed in more detail. The first approach covers strategies to reduce the loading time. Data caching and prefetching are employed to reduce access time. The second aspect concerns an approach called streaming that minimizes the time a user has to wait for first results. Viracocha already sends coarse intermediate data back to the virtual environment before the final result is available. Different streaming and data handling strategies are described. In order to emphasize the benefit of our implementation efforts, some strategies are applied to multi-block CFD data sets.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Interactive Exploration of Large Remote Micro-CT Scans</title>
    <abstract>Datasets of tens of gigabytes are becoming common in computational and experimental science. This development is driven by advances in imaging technology, producing detectors with growing resolutions, as well as availability of cheap processing power and memory capacity in commodity-based computing clusters. In this article we describe the design of a visualization system that allows scientists to interactively explore large remote data sets in an efficient and flexible way. The system is broadly applicable and currently used by medical scientists conducting an osteoporosis research project. Human vertebral bodies are scanned using a high resolution micro-CT scanner producing scans of roughly 8 GB size each. All participating research groups require access to the centrally stored data. Due to the rich internal bone structure, scientists need to interactively explore the full dataset at coarse levels, as well as visualize subvolumes of interest at the highest resolution. Our solution is based on HDF5 and GridFTP. When accessing data remotely, the HDF5 data processing pipeline is modified to support efficient retrieval of subvolumes. We reduce the overall latency and optimize throughput by executing high-level operations on the remote side. The GridFTP protocol is used to pass the HDF5 requests to a customized server. The approach takes full advantage of local graphics hardware for rendering. Interactive visualization is accomplished using a background thread to access the datasets stored in a multi-resolution format. A hierarchical volume renderer provides seamless integration of high resolution details with low resolution overviews.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Representing unit test data for large scale software development</title>
    <abstract>Large scale software projects rely on routine, automated testing to gauge progress towards its goals. The diversity and quantity of these tests grow as time and project scope increase. This is as a consequence of both experience and expanding audience. It becomes increasingly difficult to interpret testing results as the testing suites multiply and diversify. If interpretation becomes too difficult, testings results could become ignored all together. Visualization has proven to be an effective tool to aid the interpretation of large amounts of data. We have adapted visualization techniques based on small multiples to communicate the health of the software project across several levels of abstraction. The collective set of techniques we refer to as the SeeTest visualization schema. We applied this visualization technique to the Open MPI test results in order to assist developers in the software release cycle. Through the visualizations, developers found a variety of surprising mismatches between their data and their intuitions. This exploration did not involve collecting any data not already being collected, merely presenting it in manner that better supported their needs. In this paper, we detail the development of the representation we used and give more particular analysis of the insights gained by the Open MPI community. The techniques presented in this paper can be applied to other software projects.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Giga-scale multiresolution volume rendering on distributed display clusters</title>
    <abstract>Visualizing the enormous level of detail comprised in many of today's data sets is a challenging task and demands special processing techniques as well as a presentation on appropriate display devices. Desktop computers and laptops are often not suited for this task because data sets are simply too large and the limited screen size of these devices prevents users from perceiving the entire data set and severely restricts collaboration. Large high-resolution displays that combine the images of multiple smaller devices to form one large display area have proven to be an adequate solution to the ever-growing quantity of available data. The displays offer enough screen real estate to visualize such data sets entirely and facilitate collaboration, since multiple users are able to perceive the information at the same time. For an interactive visualization, the CPUs on the cluster driving the GPUs can be used to split up the computation of a scene into different areas, where each area is computed by a different rendering node.
In this paper we focus on volumetric data sets and introduce a dynamic subdivision scheme incorporating multi-resolution wavelet representation to visualize data sets with several gigabytes of voxel data interactively on distributed rendering clusters. The approach makes efficient use of the resources available on modern graphics cards which mainly limit the amount of data that can be visualized. The implementation was successfully tested on a tiled display comprised of 25 compute nodes driving 50 LCD panels.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Large data visualization on distributed memory multi-GPU clusters</title>
    <abstract>Data sets of immense size are regularly generated on large scale computing resources. Even among more traditional methods for acquisition of volume data, such as MRI and CT scanners, data which is too large to be effectively visualized on standard workstations is now commonplace.
One solution to this problem is to employ a 'visualization cluster,' a small to medium scale cluster dedicated to performing visualization and analysis of massive data sets generated on larger scale supercomputers. These clusters are designed to fit a different need than traditional supercomputers, and therefore their design mandates different hardware choices, such as increased memory, and more recently, graphics processing units (GPUs). While there has been much previous work on distributed memory visualization as well as GPU visualization, there is a relative dearth of algorithms which effectively use GPUs at a large scale in a distributed memory environment. In this work, we study a common visualization technique in a GPU-accelerated, distributed memory setting, and present performance characteristics when scaling to extremely large data sets.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Equalizer: a scalable parallel rendering framework</title>
    <abstract>Continuing improvements in CPU and GPU performances as well as increasing multi-core processor and cluster-based parallelism demand for flexible and scalable parallel rendering solutions that can exploit multipipe hardware accelerated graphics. In fact, to achieve interactive visualization, scalable rendering systems are essential to cope with the rapid growth of data sets. However, parallel rendering systems are non-trivial to develop and often only application specific implementations have been proposed. The task of developing a scalable parallel rendering framework is even more difficult if it should be generic to support various types of data and visualization applications, and at the same time work efficiently on a cluster with distributed graphics cards.
In this paper we introduce a novel system called Equalizer, a toolkit for scalable parallel rendering based on OpenGL which provides an application programming interface (API) to develop scalable graphics applications for a wide range of systems ranging from large distributed visualization clusters and multi-processor multipipe graphics systems to single-processor single-pipe desktop machines. We describe the system architecture, the basic API, discuss its advantadges over previous approaches, present example configurations and usage scenarios as well as scalability results.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Visualization of high-resolution image collections on large tiled display walls</title>
    <abstract>This paper introduces an approach to interactively visualize a large collection of high-resolution images in large-scale tiled display environments. Our approach fully utilizes the distributed computing and rendering capabilities of visualization clusters that are driving tiled display walls. A seamless and unified workspace spanning across the entire wall is provided, while view dependent resource management strategies support smooth and fully interactive data analysis of a collection of images. Interactive image filtering and graphing techniques are presented, allowing features in individual images and patterns in image sets to be visualized on the fly. The provided case studies show that the presented approach can scale well beyond terapixel-scale visualization while providing highly responsive interactive environments. </abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Parallel Cell Projection Rendering of Adaptive Mesh Refinement Data</title>
    <abstract>Adaptive Mesh Refinement (AMR) is a technique used in numerical simulations to automatically refine (or de-refine) certain regions of the physical domain in a finite difference calculation. AMR data consists of nested hierarchies of data grids. As AMR visualization is still a relatively unexplored topic, our work is motivated by the need to perform efficient visualization of large AMR data sets. We present a software algorithm for parallel direct volume rendering of AMR data using a cell-projection technique on several different parallel platforms. Our algorithm can use one of several different distribution methods, and we present performance results for each of these alternative approaches. By partitioning an AMR data set into blocks of constant resolution and estimating rendering costs of individual blocks using an application specific benchmark, it is possible to achieve even load balancing.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>0</relevance>
  </item>
  <!-- 40 -->
  <item>
    <title>Special Section: Parallel Graphics and Visualization: Practical global illumination for interactive particle visualization</title>
    <abstract>Particle-based simulation methods are used to model a wide range of complex phenomena and to solve time-dependent problems of various scales. Effective visualizations of the resulting state will communicate subtle changes in the three-dimensional structure, spatial organization, and qualitative trends within a simulation as it evolves. We present two algorithms targeting upcoming, highly parallel multicore desktop systems to enable interactive navigation and exploration of large particle data sets with global illumination effects. Monte Carlo path tracing and texture mapping are used to capture computationally expensive illumination effects such as soft shadows and diffuse interreflection. The first approach is based on precomputation of luminance textures and removes expensive illumination calculations from the interactive rendering pipeline. The second approach is based on dynamic luminance texture generation and decouples interactive rendering from the computation of global illumination effects. These algorithms provide visual cues that enhance the ability to perform analysis and feature detection tasks while interrogating the data at interactive rates. We explore the performance of these algorithms and demonstrate their effectiveness using several large data sets. </abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Dimensionality reduction techniques for blog visualization</title>
    <abstract>Exploratory data analysis often relies heavily on visual methods because of the power of the human eye to detect structures. For large, multidimensional data sets which cannot be easily visualized, the number of dimensions of the data can be reduced by applying dimensionality reduction techniques. This paper reviews current linear and nonlinear dimensionality reduction techniques in the context of data visualization. The dimensionality reduction techniques were used in our case study of business blogs. The superior techniques were able to discriminate the various categories of blogs quite accurately. To our knowledge, this is the first study using dimensionality reduction techniques for visualization of blogs. In summary, we have applied dimensionality reduction for visualization of real-world blog data, with potential applications in the ever-growing digital realm of social media.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Interactive ray tracing of time varying data</title>
    <abstract>We present a simple and effective algorithm for ray tracing iso-surfaces of time varying data sets. Each time step is partitioned into separate ranges of potentional iso-surface values. This creates a large number of relatively small files. Out-of-core rendering is implemented by reading for each time step the relevant iso-surface file, which contains its own spatial subdivision as well as the volumetric data. Since any of these data partitions is smaller than a single time step, the I/O bottleneck is overcome. Our method capitalizes on the ability of modern architectures to stream data off disk without interference of the operating system. Additionally, only a fraction of a time-step is held in memory at any moment during the visualization, which significantly reduces the required amount of internal memory.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Real-time rendering of 3D medical data sets</title>
    <abstract>Interactive exploration of three dimension (3D) medical data sets is required by many applications, but the huge amount of computational time and storage space needed for rendering do not allow the visualization of large medical data sets by now. In this paper we present a new algorithm for rendering large medical data sets at interactive frame rates on standard PC hardware. The input data is converted into a compressed hierarchical wavelet representation in a preprocessing step. During rendering, the wavelet representation is decompressed on-the-fly and rendered using hardware texture mapping. The level of detail used for rendering is adapted to the local frequency spectrum of the data and its position relative to the viewer. Using a prototype implementation of the algorithm we were able to perform an interactive walkthrough of large medical data sets on a single of-the-shelf PC. </abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Constructing streak surfaces for 3D unsteady vector fields</title>
    <abstract>Visualization of 3D, unsteady flow (4D) is very difficult due to both perceptual challenges and the large size of 4D vector field data. One approach to this challenge is to use integral surfaces to visualize the 4D properties of the field. However the construction of streak surfaces has remained elusive due to problems stemming from expensive computation and complex meshing schemes. We present a novel streak surface construction algorithm that generates the surface using a quadrangular mesh. In contrast to previous approaches the algorithm offers a combination of speed for exploration of 3D unsteady flow, high precision, and places less restriction on data or mesh size due to its CPU-based implementation compared to a GPU-based method. The algorithm can be applied to large data sets because it is based on local operations performed on the quad primitives. We demonstrate the technique on a variety of 3D, unsteady simulation data sets to show its speed and robustness. We also present both a detailed implementation and a performance evaluation. We show that a technique based on quad meshes handles large data sets and can achieve interactive frame rates.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Visualization of Steep Breaking Waves and Thin Spray Sheets Around a Ship</title>
    <abstract>The simulation of breaking of waves, the formation of thin spray sheets, and the entrainment of air around the next generation of naval surface combatants is an ongoing 3-year Department of Defense (DoD) Challenge Project. The goal of this project is a validated computation capability to model the full hydrodynamics around a surface combatant including all of the processes that affect mission and performance. Visualization of these large-scale simulations is paramount to understanding the complex physics involved. These simulations produce enormous data sets with both surface and volumetric qualities. Wave breaking, spray sheets, and air entrainment can be visualized using isosurfaces of scalar data. Visualization of quantities such as the vorticity field also provides insight into the dynamics of droplet and bubble formation. This paper documents the techniques used, results obtained, and lessons learned from the visualization of the hydrodynamics of naval vessels.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Perceptually-motivated graphics, visualization and 3D displays</title>
    <abstract>This course presents timely, relevant examples on how researchers have leveraged perceptual information for optimization of rendering algorithms, to better guide design and presentation in (3D stereoscopic) display media, and for improved visualization of complex or large data sets. Each presentation will provide references and short overviews of cutting-edge current research pertaining to that area. We will ensure that the most up-to-date research examples are presented by sourcing information from recent perception and graphics conferences and journals such as ACM Transactions on Perception, paying particular attention work presented at the 2010 Symposium on Applied Perception in Graphics and Visualization.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>An optimization-based approach to dynamic data transformation for smart visualization</title>
    <abstract>We are building a smart visual dialog system that aids users in investigating large and complex data sets. Given a user's data request, we automate the generation of a visual response that is tailored to the user's context. In this paper, we focus on the problem of data transformation, which is the process of preparing the raw data (e.g., cleaning and scaling) for effective visualization. Specifically, we develop an optimization-based approach to data transformation. Compared to existing approaches, which normally focus on specific transformation techniques, our work addresses how to dynamically determine proper data transformations for a wide variety of visualization situations. As a result, our work offers two unique contributions. First, we provide a general computational framework that can dynamically derive a set of data transformations to help optimize the quality of the target visualization. Second, we provide an extensible, feature-based model to uniformly represent various data transformation operations and visualization quality metrics. Our evaluation shows that our work significantly improves visualization quality and helps users to better perform their tasks.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>A geoscience perspective on immersive 3D gridded data visualization</title>
    <abstract>We describe visualization software, Visualizer, that was developed specifically for interactive, visual exploration in immersive virtual reality (VR) environments. Visualizer uses carefully optimized algorithms and data structures to support the high frame rates required for immersion and the real-time feedback required for interactivity. As an application developed for VR from the ground up, Visualizer realizes benefits that usually cannot be achieved by software initially developed for the desktop and later ported to VR. However, Visualizer can also be used on desktop systems (unix/linux-based operating systems including Mac OS X) with a similar level of real-time interactivity, bridging the ''software gap'' between desktop and VR that has been an obstacle for the adoption of VR methods in the Geosciences. While many of the capabilities of Visualizer are already available in other software packages used in a desktop environment, the features that distinguish Visualizer are: (1) Visualizer can be used in any VR environment including the desktop, GeoWall, or CAVE, (2) in non-desktop environments the user interacts with the data set directly using a wand or other input devices instead of working indirectly via dialog boxes or text input, (3) on the desktop, Visualizer provides real-time interaction with very large data sets that cannot easily be viewed or manipulated in other software packages. Three case studies are presented that illustrate the direct scientific benefits realized by analyzing data or simulation results with Visualizer in a VR environment. We also address some of the main obstacles to widespread use of VR environments in scientific research with a user study that shows Visualizer is easy to learn and to use in a VR environment and can be as effective on desktop systems as native desktop applications.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Exploratory spatio-temporal data mining and visualization</title>
    <abstract>Spatio-temporal data sets are often very large and difficult to analyze and display. Since they are fundamental for decision support in many application contexts, recently a lot of interest has arisen toward data-mining techniques to filter out relevant subsets of very large data repositories as well as visualization tools to effectively display the results. In this paper we propose a data-mining system to deal with very large spatio-temporal data sets. Within this system, new techniques have been developed to efficiently support the data-mining process, address the spatial and temporal dimensions of the data set, and visualize and interpret results. In particular, two complementary 3D visualization environments have been implemented. One exploits Google Earth to display the mining outcomes combined with a map and other geographical layers, while the other is a Java3D-based tool for providing advanced interactions with the data set in a non-geo-referenced space, such as displaying association rules and variable distributions.</abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Patch clustering for massive data sets</title>
    <abstract>The presence of huge data sets poses new problems to popular clustering and visualization algorithms such as neural gas (NG) and the self-organising-map (SOM) due to memory and time constraints. In such situations, it is no longer possible to store all data points in the main memory at once and only a few, ideally only one run over the whole data set is still affordable to achieve a feasible training time. In this contribution we propose single pass extensions of the classical clustering algorithms NG and SOM which are based on a simple patch decomposition of the data set and fast batch optimization schemes of the underlying cost function. The algorithms only require a fixed memory space. They maintain the benefits of the original ones including easy implementation and interpretation as well as large flexibility and adaptability. We demonstrate that parallelization of the methods becomes easily possible and we show the efficiency of the approach in a variety of experiments. </abstract>
    <search_task_number>13</search_task_number>
    <query>large data set visualization</query>
    <relevance>0</relevance>
  </item>



  <item>
	  <title>Analysis and reconstruction of the tiling of Alcazar in Seville using computer vision tools</title>
    <abstract>This paper describes different tools, developed by the authors, for the comprehensive analysis and cataloguing of Islamic design patterns from digital images, in the context of the Plane Symmetry Groups theory. For validation of the results, was chosen the ceramic tiles of the Real Alcazar of Seville (Spain), a palace built in the 14th century, highly decorated with ceramic mosaics whose patterns were designed using a specific technique called "lacerías" (interweaving manufacturing technique in tiling, where the pieces seem to be interweaved). This fact together with its handcraft nature has required the development of specific stages for design pattern analysis.</abstract>
	<search_task_number>4</search_task_number>
	<query>computer vision analysis</query>
	<relevance>1</relevance>
  </item>
  <item>
	  <title>A computer vision system for automated corn seed purity analysis</title>
	  <abstract>Electrophoresis gel analysis is a viable technique for determining the purity of hybrid corn seeds. Visually analyzing the electrophoretic gel images is a very tedious and time-consuming task. In this paper, a computer vision system integrating image processing and pattern recognition techniques with domain-specific structural information to automate the electrophoresis gel scoring procedure is presented. A set of image processing algorithms are developed to perform extraction of the region of interest, segmentation of samples, identification of bands within samples, and final classification of different types of seeds. The image processing algorithms utilize the structural information and operator expertise to achieve high classification rate with fuzzy and incomplete information contained on the electrophoresis gels. The developed technique clearly demonstrates the potential of using computer vision in automating the gel scoring procedure. The developed technology may also be extended to other areas such as general one-dimensional electrophoresis and high performance thin layer chromatography. </abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>Towards context-aware face recognition</title>
	  <abstract>In this paper, we focus on the use of context-aware, collaborative filtering, machine-learning techniques that leverage automatically sensed and inferred contextual metadata together with computer vision analysis of image content to make accurate predictions about the human subjects depicted in cameraphone photos. We apply Sparse-Factor Analysis (SFA) to both the contextual metadata gathered in the MMM2 system and the results of PCA (Principal Components Analysis) of the photo content to achieve a 60% face recognition accuracy of people depicted in our cameraphone photos, which is 40% better than media analysis alone. In short, we use context-aware media analysis to solve the face recognition problem for cameraphone photos.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>Human activity analysis: A review</title>
	  <abstract>Human activity recognition is an important area of computer vision research. Its applications include surveillance systems, patient monitoring systems, and a variety of systems that involve interactions between persons and electronic devices such as human-computer interfaces. Most of these applications require an automated recognition of high-level activities, composed of multiple simple (or atomic) actions of persons. This article provides a detailed overview of various state-of-the-art research papers on human activity recognition. We discuss both the methodologies developed for simple human actions and those for high-level activities. An approach-based taxonomy is chosen that compares the advantages and limitations of each approach.
		  
Recognition methodologies for an analysis of the simple actions of a single person are first presented in the article. Space-time volume approaches and sequential approaches that represent and recognize activities directly from input images are discussed. Next, hierarchical recognition methodologies for high-level activities are presented and compared. Statistical approaches, syntactic approaches, and description-based approaches for hierarchical recognition are discussed in the article. In addition, we further discuss the papers on the recognition of human-object interactions and group activities. Public datasets designed for the evaluation of the recognition methodologies are illustrated in our article as well, comparing the methodologies' performances. This review will provide the impetus for future research in more productive areas.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>Enabling technologies on hybrid camera networks for behavioral analysis of unattended indoor environments and their surroundings</title>
	  <abstract>This paper presents a layered network architecture and the enabling technologies for accomplishing vision-based behavioral analysis of unattended environments. Specifically the vision network covers both the attended environment and its surroundings by means of hybrid cameras. The layer overlooking at the surroundings is laid outdoor and tracks people, monitoring entrance/exit points. It recovers the geometry of the site under surveillance and communicates people positions to a higher level layer. The layer monitoring the unattended environment undertakes similar goals, with the addition of maintaining a global mosaic of the observed scene for further understanding. Moreover, it merges information coming from sensors beyond the vision to deepen the understanding or increase the reliability of the system. The behavioral analysis is demanded to a third layer that merges the information received from the two other layers and infers knowledge about what happened, happens and will be likely happening in the environment. The paper also describes a case study that was implemented in the Engineering Campus of the University of Modena and Reggio Emilia, where our surveillance system has been deployed in a computer laboratory which was often unaccessible due to lack of attendance.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Social signals, their function, and automatic analysis: a survey</title>
	  <abstract>Social Signal Processing (SSP) aims at the analysis of social behaviour in both Human-Human and Human-Computer interactions. SSP revolves around automatic sensing and interpretation of social signals, complex aggregates of nonverbal behaviours through which individuals express their attitudes towards other human (and virtual) participants in the current social context. As such, SSP integrates both engineering (speech analysis, computer vision, etc.) and human sciences (social psychology, anthropology, etc.) as it requires multimodal and multidisciplinary approaches. As of today, SSP is still in its early infancy, but the domain is quickly developing, and a growing number of works is appearing in the literature. This paper provides an introduction to nonverbal behaviour involved in social signals and a survey of the main results obtained so far in SSP. It also outlines possibilities and challenges that SSP is expected to face in the next years if it is to reach its full maturity.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Event detection in video using motion analysis</title>
	  <abstract>Digital video is being used widely in a variety of applications such as entertainment, surveillance and security. Large amount of video in surveillance and security requires systems capable of processing video to automatically detect and recognize events to alleviate the load on humans and enable preventive actions when events are detected. The main objective of this work is the analysis of computer vision techniques and algorithms to perform automatic detection of specific events in video sequences. This paper presents a surveillance system based on motion analysis and introduces the idea of event probability zones. Advantages, limitations, capabilities and possible solution alternatives are also discussed. The result is a system capable of detecting events of objects moving in opposing direction in a predefined context or running in the scene; the results showed precision greater than 50% and recall greater than 80%.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>Phase space for face pose estimation</title>
	  <abstract>Face pose estimation from standard imagery remains a complex computer vision problemthat requires identifying the primary modes of variance directly corresponding to pose variation, while ignoring variance due to face identity and other noise factors. Conventional methods either fail to extract the salient pose defining features, or require complex embedding operations. We propose a new method for pose estimation that exploits oriented Phase Congruency (PC) features and Canonical Correlation Analysis (CCA) to define a latent pose-sensitive subspace. The oriented PC features serve to mitigate illumination and identity features present in the imagery, while highlighting alignment and pose features necessary for estimation. The proposed system is tested using the Pointing'04 face database and is shown to provide better estimation accuracy than similar methods including Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and conventional CCA.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>Self shadow elimination algorithm for surveillance videos using ANOVA F test</title>
	  <abstract>Identifying moving objects from a video sequence is a fundamental and critical task in many computer vision applications and a robust segmentation of motion objects from the static background is generally required. Segmented foreground objects generally include their self shadows as foreground objects since the shadow intensity differs and gradually changes from the background in a video sequence. Moreover, self shadows are vague in nature and have no clear boundaries. To eliminate such shadows from motion segmented video sequences, we propose an algorithm based on inferential statistical one way ANalysis Of VAriance (ANOVA) F test. This statistical model can deal scenes with complex and time varying illuminations without restrictions on the number of light sources and surface orientations. Results obtained with different indoor and outdoor sequences show that algorithm can effectively and robustly detect associated self shadows from segmented frames.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>Are current monocular computer vision systems for human action recognition suitable for visual surveillance applications?</title>
	  <abstract>Since video recording devices have become ubiquitous, the automated analysis of human activity from a single uncalibrated video has become an essential area of research in visual surveillance. Despite variability in terms of human appearance and motion styles, in the last couple of years, a few computer vision systems have reported very encouraging results. Would these methods be already suitable for visual surveillance applications? Alas, few of them have been evaluated in the two most challenging scenarios for an action recognition system: view independence and human interactions. Here, first a review of monocular human action recognition methods that could be suitable for visual surveillance is presented. Then, the most promising frameworks, i.e. methods based on advanced dimensionality reduction, bag of words and random forest, are described and evaluated on IXMAS and UT-Interaction datasets. Finally, suitability of these systems for visual surveillance applications is discussed.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>Shape analysis of planar objects with arbitrary topologies using conformal geometry</title>
	  <abstract>The study of 2D shapes is a central problem in the field of computer vision. In 2D shape analysis, classification and recognition of objects from their observed silhouettes are extremely crucial and yet difficult. It usually involves an efficient representation of 2D shape space with natural metric, so that its mathematical structure can be used for further analysis. Although significant progress has been made for the study of 2D simply-connected shapes, very few works have been done on the study of 2D objects with arbitrary topologies. In this work, we propose a representation of general 2D domains with arbitrary topologies using conformal geometry. A natural metric can be defined on the proposed representation space, which gives a metric to measure dissimilarities between objects. The main idea is to map the exterior and interior of the domain conformally to unit disks and circle domains, using holomorphic 1-forms. Aset of diffeomorphisms from the unit circle S1 to itself can be obtained, which together with the conformal modules are used to define the shape signature. We prove mathematically that our proposed signature uniquely represents shapes with arbitrary topologies. We also introduce a reconstruction algorithm to obtain shapes from their signatures. This completes our framework and allows us tomove back and forth between shapes and signatures. Experiments show the efficacy of our proposed algorithm as a stable shape representation scheme.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Accurate extraction of reciprocal space information from transmission electron microscopy images</title>
	  <abstract>As the study of complex systems has become dominant in physics the link between computational and physical science has become ever more important. In particular, with the rising popularity of imaging techniques in physis, the development and application of cutting edge computer vision techniques has become vital. Here we present novel image analysis methods which can be used to extract the position of features in diffraction patterns (reciprocal space) with unprecedented accuracy.
		  
		  The first contribution we have developed is a method for calculating the nonlinear response of photographic film by using the noise in the image enabling the extraction of accurate intensity information. This allows high-resolution (but non-linear) film to be used in place of low-resolution (but linear) CCD cameras. The second contribution is a method for accurately localising very faint features in diffraction patterns by modelling the features and using the expectation maximization algorithm directly on the image to fit them. The accuracy of this technique has been verified by testing it on synthetic data.
		  
These methods have been applied to transmission electron microscopy data, and have already enabled discoveries which would have been impossible using previously available techniques.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Proscenium: a framework for spatio-temporal video editing</title>
	  <abstract>We present an approach to video editing where movie sequences are treated as spatio-temporal volumes that can be sheered and warped under user control. This simple capability enables new video editing operations that support complex postproduction modifications, such as object removal and/or changes in camera motion. Our methods do not rely on complicated and error-prone image analysis or computer vision methods. Moreover, they facilitate an editing approach to video that is similar to standard image-editing tasks. Central to our system is a movie representation framework called Proscenium that supports efficient queries and operations on spatio-temporal volumes while maintaining the original source content. We have adopted a graph-based lazy-evaluation model in order to support interactive visualizations, complex data modifications, and efficient processing of large spatio-temporal volumes.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>A texture manifold for curve-based morphometry of the cerebral cortex</title>
	  <abstract>The cortical surface of the human brain is composed of folds that are juxtaposed alongside one another. Several methods have been proposed to study the shape of these folds, e.g., by first segmenting them on the cortical surface or by analysis via a continuous deformation of a common template. A major disadvantage of these methods is that, while they can localize shape differences, they cannot easily identify the directions in which they occur. The type of deformation that causes a fold to change in length is quite different from that which causes it to change in width. Furthermore, these two deformations may have a completely different biological interpretation. In this article we propose a method to analyze such deformations using directional filters locally adapted to the geometry of the folding pattern. Motivated by the texture flow literature in computer vision we recover flow fields that maintain a fixed angle with the orientation of folds, over a significant spatial extent. We then trace the flow fields to determine which correspond to the shape changes that are the most salient. Using the OASIS database, we demonstrate that in addition to known regions of atrophy, our method can find subtle but statistically significant shape deformations.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>A3: HCI Coding Guideline for Research Using Video Annotation to Assess Behavior of Nonverbal Subjects with Computer-Based Intervention</title>
	  <abstract>HCI studies assessing nonverbal individuals (especially those who do not communicate through traditional linguistic means: spoken, written, or sign) are a daunting undertaking. Without the use of directed tasks, interviews, questionnaires, or question-answer sessions, researchers must rely fully upon observation of behavior, and the categorization and quantification of the participant’s actions. This problem is compounded further by the lack of metrics to quantify the behavior of nonverbal subjects in computer-based intervention contexts. We present a set of dependent variables called A3 (pronounced A-Cubed) or Annotation for ASD Analysis, to assess the behavior of this demographic of users, specifically focusing on engagement and vocalization. This paper demonstrates how theory from multiple disciplines can be brought together to create a set of dependent variables, as well as demonstration of these variables, in an experimental context. Through an examination of the existing literature, and a detailed analysis of the current state of computer vision and speech detection, we present how computer automation may be integrated with the A3 guidelines to reduce coding time and potentially increase accuracy. We conclude by presenting how and where these variables can be used in multiple research areas and with varied target populations.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Survey and analysis of multimodal sensor planning and integration for wide area surveillance</title>
	  <abstract>Although sensor planning in computer vision has been a subject of research for over two decades, a vast majority of the research seems to concentrate on two particular applications in a rather limited context of laboratory and industrial workbenches, namely 3D object reconstruction and robotic arm manipulation. Recently, increasing interest is engaged in research to come up with solutions that provide wide-area autonomous surveillance systems for object characterization and situation awareness, which involves portable, wireless, and/or Internet connected radar, digital video, and/or infrared sensors. The prominent research problems associated with multisensor integration for wide-area surveillance are modality selection, sensor planning, data fusion, and data exchange (communication) among multiple sensors. Thus, the requirements and constraints to be addressed include far-field view, wide coverage, high resolution, cooperative sensors, adaptive sensing modalities, dynamic objects, and uncontrolled environments. This article summarizes a new survey and analysis conducted in light of these challenging requirements and constraints. It involves techniques and strategies from work done in the areas of sensor fusion, sensor networks, smart sensing, Geographic Information Systems (GIS), photogrammetry, and other intelligent systems where finding optimal solutions to the placement and deployment of multimodal sensors covering a wide area is important. While techniques covered in this survey are applicable to many wide-area environments such as traffic monitoring, airport terminal surveillance, parking lot surveillance, etc., our examples will be drawn mainly from such applications as harbor security and long-range face recognition.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Image-based multi-view scene analysis using 'conexels'</title>
	  <abstract>Multi-camera environments allow constructing volumetric models of the scene to improve the analysis performance of computer vision algorithms (e.g. disambiguating occlusion). When representing volumetric results of image-based multi-camera analysis, a direct approach is to scan the 3D space with regular voxels. Regular voxelization is good at high spatial resolutions for applications such as volume visualization and rendering of synthetic scenes generated by geometric models, or to represent data resulting from direct 3D data capture (e.g. MRI). However, regular voxelization shows a number of drawbacks for visual scene analysis, where direct measurements on 3D voxels are not usually available. In this case, voxel values are computed rather as a result of the analysis on 'projected' image data.
		  
In this paper, we first provide some statistics to show how voxels project to 'unbalanced' sets of image data in common multi-view analysis settings. Then, we propose a 3D geometry for multi-view scene analysis providing a better balance in terms of the number of pixels used to analyse each elementary volumetric unit. The proposed geometry is non-regular in 3D space, but becomes regular once projected onto camera images, adapting the sampling to the images. The aim is to better exploit multi-view image data by balancing its usage across multiple cameras instead of focusing in regular sampling of 3D space, from which we do not have direct measurements. An efficient recursive algorithm using the proposed geometry is outlined. Experimental results reflect better balance and higher accuracy for multi-view analysis than regular voxelization with equivalent restrictions.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Spectral mesh processing</title>
	  <abstract>Spectral mesh processing is an idea that was proposed at the beginning of the 90's, to port the "signal processing toolbox" to the setting of 3D mesh models. Recent advances in both computer horsepower and numerical software make it possible to fully implement this vision. In the more classical context of sound and image processing, Fourier analysis was a corner stone in the development of a wide spectrum of techniques, such as filtering, compression, and recognition. In this course, attendees will learn how to transfer the underlying concepts to the setting of a mesh model, how to implement the "spectral mesh processing" toolbox and use it for real applications, including filtering, shape matching, remeshing, segmentation, and parameterization.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>A signal-processing framework for reflection</title>
	  <abstract>We present a signal-processing framework for analyzing the reflected light field from a homogeneous convex curved surface under distant illumination. This analysis is of theoretical interest in both graphics and vision and is also of practical importance in many computer graphics problems---for instance, in determining lighting distributions and bidirectional reflectance distribution functions (BRDFs), in rendering with environment maps, and in image-based rendering. It is well known that under our assumptions, the reflection operator behaves qualitatively like a convolution. In this paper, we formalize these notions, showing that the reflected light field can be thought of in a precise quantitative way as obtained by convolving the lighting and BRDF, i.e. by filtering the incident illumination using the BRDF. Mathematically, we are able to express the frequency-space coefficients of the reflected light field as a product of the spherical harmonic coefficients of the illumination and the BRDF. These results are of practical importance in determining the well-posedness and conditioning of problems in inverse rendering---estimation of BRDF and lighting parameters from real photographs. Furthermore, we are able to derive analytic formulae for the spherical harmonic coefficients of many common BRDF and lighting models. From this formal analysis, we are able to determine precise conditions under which estimation of BRDFs and lighting distributions are well posed and well-conditioned. Our mathematical analysis also has implications for forward rendering---especially the efficient rendering of objects under complex lighting conditions specified by environment maps. The results, especially the analytic formulae derived for Lambertian surfaces, are also relevant in <i>computer vision</i> in the areas of recognition, photometric stereo and structure from motion.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Video quality for face detection, recognition, and tracking</title>
	  <abstract>Many distributed multimedia applications rely on video analysis algorithms for automated video and image processing. Little is known, however, about the minimum video quality required to ensure an accurate performance of these algorithms. In an attempt to understand these requirements, we focus on a set of commonly used face analysis algorithms. Using standard datasets and live videos, we conducted experiments demonstrating that the algorithms show almost no decrease in accuracy until the input video is reduced to a certain critical quality, which amounts to significantly lower bitrate compared to the quality commonly acceptable for human vision. Since computer vision percepts video differently than human vision, existing video quality metrics, designed for human perception, cannot be used to reason about the effects of video quality reduction on accuracy of video analysis algorithms. We therefore investigate two alternate video quality metrics, blockiness and mutual information, and show how they can be used to estimate the critical video qualities for face analysis algorithms.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>OpenVIDIA: parallel GPU computer vision</title>
	  <abstract>Graphics and vision are approximate inverses of each other: ordinarily Graphics Processing Units (GPUs) are used to convert "numbers into pictures" (i.e. computer graphics). In this paper, we propose using GPUs in approximately the reverse way: to assist in "converting pictures into numbers" (i.e. computer vision). The OpenVIDIA project uses single or multiple graphics cards to accelerate image analysis and computer vision. It is a library and API aimed at providing a graphics hardware accelerated processing framework for image processing and computer vision. OpenVIDIA explores the creation of a parallel computer architecture consisting of multiple Graphics Processing Units (GPUs) built entirely from commodity hardware. OpenVIDIA uses multiple Graphic.Processing Units in parallel to operate as a general-purpose parallel computer architecture. It provides a simple API which implements some common computer vision algorithms. Many components can be used immediately and because the project is Open Source, the code is intended to serve as templates and examples for how similar algorithms are mapped onto graphics hardware. Implemented are image processing techniques (Canny edge detection, filtering), image feature handling (identifying and matching features) and image registration, to name a few.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>Real-time monocular tracking of view frustum for large screen human-computer interaction</title>
	  <abstract>This paper introduces a novel approach towards direct interaction with large display systems. Monocular computer vision is utilised to avoid restraints imposed by input devices. Tracking the user's head and determining the view frustum in real-time is one of the key processes in our proposed human-computer interaction system. We also proposed using a view frustum to model the user's interaction volume allowing flexible interaction with the display. Finally, we demonstrate the feasibility of this new concept and provide an accuracy analysis of our prototype system.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Categorization of natural scenes: Local versus global information and the role of color</title>
	  <abstract>Categorization of scenes is a fundamental process of human vision that allows us to efficiently and rapidly analyze our surroundings. Several studies have explored the processes underlying human scene categorization, but they have focused on processing global image information. In this study, we present both psychophysical and computational experiments that investigate the role of local versus global image information in scene categorization. In a first set of human experiments, categorization performance is tested when only local or only global image information is present. Our results suggest that humans rely on local, region-based information as much as on global, configural information. In addition, humans seem to integrate both types of information for intact scene categorization. In a set of computational experiments, human performance is compared to two state-of-the-art computer vision approaches that have been shown to be psychophysically plausible and that model either local or global information. In addition to the influence of local versus global information, in a second series of experiments, we investigated the effect of color on the categorization performance of both the human observers and the computational model. Analysis of the human data suggests that color is an additional channel of perceptual information that leads to higher categorization results at the expense of increased reaction times in the intact condition. However, it does not affect reaction times when only local information is present. When color is removed, the employed computational model follows the relative performance decrease of human observers for each scene category and can thus be seen as a perceptually plausible model for human scene categorization based on local image information.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Can computers learn from humans to see better?: inferring scene semantics from viewers' eye movements</title>
	  <abstract>This paper describes an attempt to bridge the semantic gap between computer vision and scene understanding employing eye movements. Even as computer vision algorithms can efficiently detect scene objects, discovering semantic relationships between these objects is as essential for scene understanding. Humans understand complex scenes by rapidly moving their eyes (saccades) to selectively focus on salient entities (fixations). For 110 social scenes, we compared verbal descriptions provided by observers against eye movements recorded during a free-viewing task. Data analysis confirms (i) a strong correlation between task-explicit linguistic descriptions and task-implicit eye movements, both of which are influenced by underlying scene semantics and (ii) the ability of eye movements in the form of fixations and saccades to indicate salient entities and entity relationships mentioned in scene descriptions.
		  
We demonstrate how eye movements are useful for inferring the meaning of social (everyday scenes depicting human activities) and affective (emotion-evoking content like expressive faces, nudes) scenes. While saliency has always been studied through the prism of fixations, we show that saccades are particularly useful for (i) distinguishing mild and high-intensity facial expressions and (ii) discovering interactive actions between scene entities.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Proceedings of the international workshop on Educational multimedia and multimedia education</title>
	  <abstract>Advances in multimedia capture, analysis and delivery, combined with the rapid adoption of broadband communication, have resulted in multimedia systems that have advanced traditional forms of education. Research in these areas has achieved impressive results in the last few years and many actual working systems and commercial products are now routinely used by a growing number of people. However, the various web sites and lecture videos produced as part of the "e-learning hype" generally do not exploit the full potential of multimedia. The question of how multimedia can really make learning more exploratory and enjoyable is as yet unanswered, and we are just beginning to understand the real contribution of multimedia to education. In addition, new trends in multimedia technology - such as multimedia on handheld devices or advanced approaches for the automatic analysis of multimodal signals -- offer novel and exciting opportunities for teaching and learning.
		  
		  The growing pervasiveness of multimedia on computing devices also increases the relevance of knowledge about multimedia for computer scientists and software engineers. The significance of multimedia for the future of computing, however, is generally not reflected in current curricula. For example, few universities offer dedicated courses, and multimedia is often only taught as part of other courses such as computer vision or machine learning. In addition, multimedia is a very active and rapidly changing field. New and emerging technologies may not only influence how we teach but also have an impact on what we teach.
		  
		  Against this background, we organized the ACM Workshop on Educational Multimedia and Multimedia Education (EMME) 2007. The goal of the workshop is to identify current and evolving trends, specify open problems, and discover challenges and prospects for new research in the broad topic of multimedia-based education. By bringing together researchers working on educational multimedia with multimedia educators, we want to establish an open discussion of these issues and create a reference for future research in this area.
		  
The call for papers attracted 25 submissions from Asia, the Middle East, Canada, Europe, Australia, and the United States. The program committee accepted 14 papers -- 9 full papers for oral presentation and 5 poster presentations -- resulting in an acceptance rate of 36% for oral presentations and 56% overall. The submissions truly reflect the diversity of the research currently done in the field. In addition to the presentations on current trends in educational multimedia, we are happy to welcome Susanne Boll, Ramesh Jain, Max Mühlhäuser, and Timothy K. Shih, who will discuss teaching multimedia in the workshop's closing panel on "The Future of Educational Multimedia.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Proceedings of the thirty-first Australasian conference on Computer science - Volume 74</title>
	  <abstract>The Australasian Computer Science Conference (ACSC) series is an annual forum, bringing together research sub-disciplines in Computer Science. The meeting allows academics and researchers to discuss research topics as well as progress in the field, and policies to stimulate its growth. This volume contains papers presented at the Thirty First ACSC in Wollongong, NSW, Australia. ACSC 2008 is part of the Australasian Computer Science Week which ran from Jan 22nd to 25th, 2008.
		  
		  The ACSC 2008 call for papers solicited contributions in all areas of computer science research. This years conference received submissions from Australia, New Zealand, China, France, India, Iran, Jamaica, Jordon, Malaysia, Pakistan, South Africa, Turkey, UK, and Taiwan. The topics addressed by the submitted papers illustrate the broadness of the discipline. The authors categorised their submissions into one or more of the following topics:
		  
		  - Algorithms (9 papers)
		  
		  - Artificial Intelligence (7 papers)
		  
		  - Communications and Networks (4 papers)
		  
		  - Computer Architecture (2 paper)
		  
		  - Computer Vision (4 papers)
		  
		  - Databases (5 papers)
		  
		  - Distributed Systems (6 papers)
		  
		  - E-Commerce (4 papers)
		  
		  - Formal Methods (6 papers)
		  
		  - Graphics (6 papers)
		  
		  - High Performance Computing (7 papers)
		  
		  - Human-Computer Interaction (8 papers)
		  
		  - Mobile Computing (6 papers)
		  
		  - Multimedia (1 paper)
		  
		  - Object Oriented Systems (3 papers)
		  
		  - Ontologies (1 paper)
		  
		  - Operating Systems (5 papers)
		  
		  - Programming Languages (4 papers)
		  
		  - Robotics (1 paper)
		  
		  - Scientific Computing (5 papers)
		  
		  - Security and Trusted Systems (5 papers)
		  
		  - Simulation (6 papers)
		  
		  - Software Engineering (5 papers)
		  
		  - Speech (1 paper)
		  
		  - Theory (3 papers)
		  
		  - Visualization (6 papers)
		  
		  - Web Services (3 papers)
		  
		  The programme committee consisted of 28 highly regarded academics from around the globe, including Australia, Brazil, Canada, Japan, New Zealand, Singapore and USA. All papers were sent to at least three programme committee members for review and every effort was made to obtain at least three reviews. Of the 47 papers submitted, 16 were selected for presentation at the conference.
		  
The programme committee invited Professor Joxan Jaffar, to give a keynote on Constraint Logic Programming for Program Analysis. Professor Jaffar has recently completed a stint as Dean of the School of Computing from 2001-2007 at the National University of Singapore. His interests are in programming languages and applications, with emphasis on the logic and constraint programming paradigms. Amongst his main contributions are the principles of constraint logic programming, and the widely-used CLP(R) system. The committee also invited Dr Benjamin Burton and Associate Professor Ewan Tempero to give invited talks. Dr Burtons talk was entitled Informatics Olympiads:Challenges in Programming and Algorithm Design. Associate Professor Temperos talk is entitled On Measuring Java Software.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Feasibility of the living canvas: restricting projection to a performer on stage</title>
	  <abstract>The Living Canvas initiative aims to use a performer on stage as a dynamic projection surface. Using machine vision in the near-infrared spectrum enables the system to follow and adapt to the performer, restricting projection to the silhouette. Ultimately, the system aims to create the illusion of a completely dynamic costume. This paper introduces the concept and presents an implementation and analysis of the performance-critical stages of the projection pipeline, proving the feasibility of the idea as well as analysing the limitations introduced by current digital projection technology.
		  
Bringing together the research from computer graphics and machine vision with the artistic vision and guidance from Cryptic, the initiative aims to create and explore a new expressive medium by taking projection systems on stage to a highly interactive level and providing a powerful new tool for live video artists.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Shape distributions</title>
	  <abstract>Measuring the similarity between 3D shapes is a fundamental problem, with applications in computer graphics, computer vision, molecular biology, and a variety of other fields. A challenging aspect of this problem is to find a suitable shape signature that can be constructed and compared quickly, while still discriminating between similar and dissimilar shapes.In this paper, we propose and analyze a method for computing shape signatures for arbitrary (possibly degenerate) 3D polygonal models. The key idea is to represent the signature of an object as a shape distribution sampled from a shape function measuring global geometric properties of an object. The primary motivation for this approach is to reduce the shape matching problem to the comparison of probability distributions, which is simpler than traditional shape matching methods that require pose registration, feature correspondence, or model fitting.We find that the dissimilarities between sampled distributions of simple shape functions (e.g., the distance between two random points on a surface) provide a robust method for discriminating between classes of objects (e.g., cars versus airplanes) in a moderately sized database, despite the presence of arbitrary translations, rotations, scales, mirrors, tessellations, simplifications, and model degeneracies. They can be evaluated quickly, and thus the proposed method could be applied as a pre-classifier in a complete shape-based retrieval or analysis system concerned with finding similar whole objects. The paper describes our early experiences using shape distributions for object classification and for interactive web-based retrieval of 3D models.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>A platform for storing, visualizing, and interpreting collections of noisy documents</title>
	  <abstract>The goal of document image analysis is to produce interpretations that match those of a fluent and knowledgeable human when viewing the same input. Because computer vision techniques are not perfect, the text that results when processing scanned pages is frequently noisy. Building on previous work, we propose a new paradigm for handling the inevitable incomplete, partial, erroneous, or slightly orthogonal interpretations that commonly arise in document datasets. Starting from the observation that interpretations are dependent on application context or user viewpoint, we describe a platform now under development that is capable of managing multiple interpretations for a document and offers an unprecedented level of interaction so that users can freely build upon, extend, or correct existing interpretations. In this way, the system supports the creation of a continuously expanding and improving document analysis repository which can be used to support research in the field.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>Inferring body pose using speech content</title>
	  <abstract>Untethered multimodal interfaces are more attractive than tethered ones because they are more natural and expressive for interaction. Such interfaces usually require robust vision-based body pose estimation and gesture recognition. In interfaces where a user is interacting with a computer using speech and arm gestures, the user's spoken keywords can be recognized in conjuction with a hypothesis of body poses. This co-occurence can reduce the number of body pose hypothesis for the vision based tracker. In this paper we show that incorporating speech-based body pose constraints can increase the robustness and accuracy of vision-based tracking systems.Next, we describe an approach for gesture recognition. We show how Linear Discriminant Analysis (LDA), can be employed to estimate 'good features' that can be used in a standard HMM-based gesture recognition system. We show that, by applying our LDA scheme, recognition errors can be significantly reduced over a standard HMM-based technique.We applied both techniques in a Virtual Home Desktop scenario. Experiments where the users controlled a desktop system using gestures and speech were conducted and the results show that the speech recognised in conjunction with body poses has increased the accuracy of the vision-based tracking system.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>A statistical model for synthesis of detailed facial geometry</title>
	  <abstract>Detailed surface geometry contributes greatly to the visual realism of 3D face models. However, acquiring high-resolution face geometry is often tedious and expensive. Consequently, most face models used in games, virtual reality, or computer vision look unrealistically smooth. In this paper, we introduce a new statistical technique for the analysis and synthesis of small three-dimensional facial features, such as wrinkles and pores. We acquire high-resolution face geometry for people across a wide range of ages, genders, and races. For each scan, we separate the skin surface details from a smooth base mesh using displaced subdivision surfaces. Then, we analyze the resulting displacement maps using the texture analysis/synthesis framework of Heeger and Bergen, adapted to capture statistics that vary spatially across a face. Finally, we use the extracted statistics to synthesize plausible detail on face meshes of arbitrary subjects. We demonstrate the effectiveness of this method in several applications, including analysis of facial texture in subjects with different ages and genders, interpolation between high-resolution face scans, adding detail to low-resolution face scans, and adjusting the apparent age of faces. In all cases, we are able to re-produce fine geometric details consistent with those observed in high resolution scans.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Practical error analysis of cross-ratio-based planar localization</title>
	  <abstract>Recently, more and more computer vision researchers are paying attention to error analysis so as to fulfill various accuracy requirements arising from different applications. As a geometric invariant under projective transformations, cross-ratio is the basis of many recognition and reconstruction algorithms which are based on projective geometry. We propose an efficient way of analyzing localization error for computer vision systems which use cross-ratios in planar localization. By studying the inaccuracy associated with cross-ratio-based computations, we inspect the possibility of using linear transformation to approximate localization error due to 2-D noises of image extraction for reference points. Based on such a computationally efficient analysis, a practical way of choosing point features in an image so as to establish the probabilistically most accurate planar location system using crossratios is developed.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>Design and impressions of a multi-user tabletop interaction device</title>
	  <abstract>TableMouse is a cursor manipulation device designed specifically for multiple users interacting on large tabletop surface. TableMouse tracks position, height, orientation, button state, and unique identification. It is designed using infrared light emitting diodes and computer vision to perform device tracking and identification. This paper explores the functional design of such a device. Insights into the inherent features enabled by this functionality - out of arms reach interaction, collaborative interaction - are described. The architecture, vision analysis process, and issues to consider are described. Finally two example applications utilising the TableMouse are described.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Finite sample bias of robust scale estimators in computer vision problems</title>
	  <abstract>In computer vision applications of robust estimation techniques, it is usually assumed that a large number of data samples are available. As a result, the finite sample bias of estimation processes has been overlooked. This is despite the fact that many asymptotically unbiased estimators have substantial bias in cases where a moderate number of data samples are available. Such cases are frequently encountered in computer vision practice, therefore, it is important to choose the right estimator for a given task by virtue of knowing its finite sample bias. This paper investigates the finite sample bias of robust scale estimation and analyses the finite sample performance of three modern robust scale estimators (Modified Statistical Scale Estimator, Residual Consensus estimator and Two-Step Scale Estimator) that have been used in computer vision applications. Simulations and real data experiments are used to verify the results.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>Distributed vision with smart pixels</title>
	  <abstract>We study a problem related to computer vision: How can a field of sensors compute higher-level properties of observed objects deterministically in sublinear time, without accessing a central authority? This issue is not only important for real-time processing of images, but lies at the very heart of understanding how a brain may be able to function. In particular, we consider a quadratic field of n "smart pixels" on a video chip that observe a B/W image. Each pixel can exchange low-level information with its immediate neighbors. We show that it is possible to compute the centers of gravity along with a principal component analysis of all connected components of the black grid graph in time O(sqrt(n)), by developing appropriate distributed protocols that are modeled after sweepline methods. Our method is not only interesting from a philosophical and theoretical point of view, it is also useful for actual applications for controling a robot arm that has to seize objects on a moving belt. We describe details of an implementation on an FPGA; the code has also been turned into a hardware design for an application-specific integrated circuit (ASIC).</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Finding canonical behaviors in user protocols</title>
	  <abstract>While the collection of behavioral protocols has been common practice in human-computer interaction research for many years, the analysis of large protocol data sets is often extremely tedious and time-consuming, and automated analysis methods have been slow to develop. This paper proposes an automated method of protocol analysis to find canonical behaviors --- a small subset of protocols that is most representative of the full data set, providing a reasonable "big picture" view of the data with as few protocols as possible. The automated method takes advantage of recent algorithmic developments in computational vision, modifying them to allow for distance measures between behavioral protocols. The paper includes an application of the method to web-browsing protocols, showing how the canonical behaviors found by the method match well to sets of behaviors identified by expert human coders.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>An interactive interface for remote administration of clinical tests based on eye tracking</title>
	  <abstract>A challenging goal today is the use of computer networking and advanced monitoring technologies to extend human intellectual capabilities in medical decision making. Modern commercial eye trackers are used in many of research fields, but the improvement of eye tracking technology, in terms of precision on the eye movements capture, has led to consider the eye tracker as a tool for vision analysis, so that its application in medical research, e.g. in ophthalmology, cognitive psychology and in neuroscience has grown considerably. The improvements of the human eye tracker interface become more and more important to allow medical doctors to increase their diagnosis capacity, especially if the interface allows them to remotely administer the clinical tests more appropriate for the problem at hand. In this paper, we propose a client/server eye tracking system that provides an interactive system for monitoring patients eye movements depending on the clinical test administered by the medical doctors. The system supports the retrieval of the gaze information and provides statistics to both medical research and disease diagnosis.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Combining environmental cues and head gestures to interact with wearable devices</title>
	  <abstract>As wearable sensors and computing hardware are becoming a reality, new and unorthodox approaches to seamless human-computer interaction can be explored. This paper presents the prototype of a wearable, head-mounted device for advanced human-machine interaction that integrates speech recognition and computer vision with head gesture analysis based on inertial sensor data. We will focus on the innovative idea of integrating visual and inertial data processing for interaction. Fusing head gestures with results from visual analysis of the environment provides rich vocabularies for human-machine communication because it renders the environment into an interface: if objects or items in the surroundings are being associated with system activities, head gestures can trigger commands if the corresponding object is being looked at. We will explain the algorithmic approaches applied in our prototype and present experiments that highlight its potential for assistive technology. Apart from pointing out a new direction for seamless interaction in general, our approach provides a new and easy to use interface for disabled and paralyzed users in particular.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>Top-down, bottom-up multivalued default reasoning for identity maintenance</title>
	  <abstract>Persistent tracking systems require the capacity to track individuals by maintaining identity across visibility gaps caused by occlusion events. In traditional computer vision systems, the flow of information is typically bottom-up. The low level image processing modules take video input, perform early vision tasks such as background subtraction and object detection,and pass this information to the high level reasoning module. This paper describes the architecture of a system that uses top-down information flow to perform identity maintenance across occlusion events. This system uses the high level reasoning module to provide control feedback to the low level image processing module to perform forensic analysis of archival video and actively acquire information required to arrive at identity decisions. This functionality is in addition to traditional bottom-up reasoning about identity, employing contextual cues and appearance matching, within the multivalued default logic framework proposed in [18]. This framework, in addition to bestowing upon the system the property of nonmonotonicity, also allows for it to qualitatively encode its confidence in the identity decisions it takes.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Automatically identifying targets users interact with during real world tasks</title>
	  <abstract>Information about the location and size of the targets that users interact with in real world settings can enable new innovations in human performance assessment and soft-ware usability analysis. Accessibility APIs provide some information about the size and location of targets. How-ever this information is incomplete because it does not sup-port all targets found in modern interfaces and the reported sizes can be inaccurate. These accessibility APIs access the size and location of targets through low-level hooks to the operating system or an application. We have developed an alternative solution for target identification that leverages visual affordances in the interface, and the visual cues produced as users interact with targets. We have used our novel target identification technique in a hybrid solution that combines machine learning, computer vision, and accessibility API data to find the size and location of targets users select with 89% accuracy. Our hybrid approach is superior to the performance of the accessibility API alone: in our dataset of 1355 targets covering 8 popular applications, only 74% of the targets were correctly identified by the API alone.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>Incorporating social entropy for crowd behavior detection using SVM</title>
	  <abstract>Crowd behavior analysis is a challenging task for computer vision. In this paper, we present a novel approach for crowd behavior analysis and anomaly detection in coherent and incoherent crowded scenes. Two main aspects describe the novelty of the proposed approach: first, modeling the observed flow field in each non-overlapping block through social entropy to measure the concerning uncertainty of underlying field. Each block serves as an independent social system and social entropy determine the optimality criteria. The resulted in distributions of the flow field in respective blocks are accumulated statistically and the flow feature vectors are computed. Second, Support Vector Machines are used to train and classify the flow feature vectors as normal and abnormal. Experiments are conducted on two benchmark datasets PETS 2009 and University of Minnesota to characterize the specific and overall behaviors of crowded scenes. Our experiments show promising results with 95.6% recognition rate for both the normal and abnormal behavior in coherent and incoherent crowded scenes. Additionally, the similar method is tested using flow feature vectors without incorporating social entropy for comparative analysis and the detection results indicate the dominating performance of the proposed approach.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Ontology-driven image analysis for histopathological images</title>
	  <abstract>Ontology-based software and image processing engine must cooperate in new fields of computer vision like microscopy acquisition wherein the amount of data, concepts and processing to be handled must be properly controlled. Within our own platform, we need to extract biological objects of interest in huge size and high-content microscopy images. In addition to specific low-level image analysis procedures, we used knowledge formalization tools and high-level reasoning ability of ontology-based software. This methodology made it possible to improve the expressiveness of the clinical models, the usability of the platform for the pathologist and the sensitivity or sensibility of the low-level image analysis algorithms.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>what is the chance of happening: a new way to predict where people look</title>
	  <abstract>Visual attention is an important issue in image and video analysis and keeps being an open problem in the computer vision field. Motivated by the famous Helmholtz principle, a new approach of visual attention analysis is proposed in this paper based on the low level feature statistics of natural images and the Bayesian framework. Firstly, two priors, i.e., Surrounding Feature Prior (SFP) and Single Feature Probability Distribution (SFPD) are learned and integrated by a Bayesian framework to compute the chance of happening (CoH) of each pixel in an image. Then another prior, i.e., Center Bias Prior (CBP), is learned and applied to the CoH to compute the saliency map of the image. The experimental results demonstrate that the proposed approach is both effective and efficient by providing more accurate and quick visual attention location. We make three major contributions in this paper: (1) A set of simple but powerful priors, SFP, SFPD and CBP, are presented in an intuitive way; (2) A computational model of CoH based on Bayesian framework is given to integrate SFP and SFPD together; (3) A computationally plausible way to obtain the saliency map of natural images based on CoH and CBP.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>Manifold valued statistics, exact principal, geodesic analysis and the effect of linear, approximations</title>
	  <abstract>Manifolds are widely used to model non-linearity arising in a range of computer vision applications. This paper treats statistics on manifolds and the loss of accuracy occurring when linearizing the manifold prior to performing statistical operations. Using recent advances in manifold computations, we present a comparison between the non-linear analog of Principal Component Analysis, Principal Geodesic Analysis, in its linearized form and its exact counterpart that uses true intrinsic distances. We give examples of datasets for which the linearized version provides good approximations and for which it does not. Indicators for the differences between the two versions are then developed and applied to two examples of manifold valued data: outlines of vertebrae from a study of vertebral fractures and spacial coordinates of human skeleton end-effectors acquired using a stereo camera and tracking software.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>A novel visual organization based on topological perception</title>
	  <abstract>What are the primitives of visual perception? The early feature-analysis theory insists on it being a local-to-global process which has acted as the foundation of most computer vision applications for the past 30 years. The early holistic registration theory, however, considers it as a global-to-local process, of which Chen’s theory of topological perceptual organization (TPO) has been strongly supported by psychological and physiological proofs. In this paper, inspired by Chen’s theory, we propose a novel visual organization, termed computational topological perceptual organization (CTPO), which pioneers the early holistic registration in computational vision. Empirical studies on synthetic datasets prove that CTPO is invariant to global transformation such as translation, scaling, rotation and insensitive to topological deformation. We also extend it to other applications by integrating it with local features. Experiments show that our algorithm achieves competitive performance compared with some popular algorithms.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>Beyond shape: incorporating color invariance into a biologically inspired feedforward model of category recognition</title>
	  <abstract>Being lack of theoretical support from biological cues in computer vision, current computational and learning approaches of object categorization mostly aim at better performances neglecting analysis on framework in human brain for visual information processing materially which cause little-marginal improvement and more complexity. Focusing on the uncertainty of color mechanism in visual cortex and motivating from biological issues on shape information, we present the model incorporating color invariant descriptors and plausible shape feature biologically to formulate the robust representation of each category with only simple SVM classifier to achieve the amazing performance. Our model has the characteristics of illumination, scale, position, orientation, viewpoint invariance, and competitive with current algorithms on only a few training examples from several data sets, including Caltech 101 and GRAZ for category recognition. Also, experimental results show the robustness when challenged by noisy or blurred images.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>A streakline representation of flow in crowded scenes</title>
	  <abstract>Based on the Lagrangian framework for fluid dynamics, a streakline representation of flowis presented to solve computer vision problems involving crowd and traffic flow. Streaklines are traced in a fluid flow by injecting color material, such as smoke or dye, which is transported with the flow and used for visualization. In the context of computer vision, streaklines may be used in a similar way to transport information about a scene, and they are obtained by repeatedly initializing a fixed grid of particles at each frame, then moving both current and past particles using optical flow. Streaklines are the locus of points that connect particles which originated from the same initial position. In this paper, a streakline technique is developed to compute several important aspects of a scene, such as flow and potential functions using the Helmholtz decomposition theorem. This leads to a representation of the flow that more accurately recognizes spatial and temporal changes in the scene, compared with other commonly used flow representations. Applications of the technique to segmentation and behavior analysis provide comparison to previously employed techniques, showing that the streakline method outperforms the state-of-theart in segmentation, and opening a new domain of application for crowd analysis based on potentials.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>A visio-haptic wearable system for assisting individuals who are blind</title>
	  <abstract>Computer vision algorithms for visio-haptic information analysis, i.e., the conversion of visual data into haptic (tangible) features, can be utilized in wearable assistive devices for individuals who are blind. Touch is an important modality for individuals who are blind, but it is limited to the extent of one's reach. By estimating how an object feels from its visual image, we are able to overcome this limitation. This paper proposes a wearable assistive device to estimate haptic features from visual data to enable users to feel objects from a distance.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  <item>
	  <title>Image understanding as a second course in AI: preparing students for research</title>
	  <abstract>This paper describes the development and structure of a second course in artificial intelligence that was developed to meet the needs of upper-division undergraduate and graduate computer science and computer engineering students. These students already have a background in either computer vision or artificial intelligence, and desire to apply that knowledge to the design of algorithms that are able to automate the process of extracting semantic content from either static or dynamic imagery. Theory and methodology from diverse areas were incorporated into the course, including techniques from image processing, statistical pattern recognition, knowledge representation, multivariate analysis, cognitive modeling, and probabilistic inference. Students read selected current literature from the field, took turns presenting the selected literature to the class, and participated in discussions about the literature. Programming projects were required of all students, and in addition, graduate students were required to propose, design, implement, and defend an image understanding project of their own choosing. The course served as preparation for and an incubator of an active research group.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>0</relevance>
  </item>
  <item>
	  <title>A novel human computer interface based on hand gesture recognition using computer vision techniques</title>
	  <abstract>In daily life, human beings communicate with each other and use broad range of gestures in the process of interaction. Apart of the interpersonal communication, many hours are spent in the interaction with the electronic devices. In the last decade, new classes of devices for accessing information have emerged along with increased connectivity. In parallel to the proliferation of these devices, new interaction styles have been explored. The objective of this paper is to provide a gesture based interface for controlling applications like media player using computer vision techniques. The human computer interface application consists of a central computational module which uses the Principal Component Analysis for gesture images and finds the feature vectors of the gesture and save it into a XML file. The Recognition of the gesture is done by K Nearest Neighbour algorithm. The Training Images are made by cropping the hand gesture from static background by detecting the hand motion using Lucas Kanade Pyramidical Optical Flow algorithm. This hand gesture recognition technique will not only replace the use of mouse to control the media player but also provide different gesture commands which will be useful in controlling the application.</abstract>
	  <search_task_number>4</search_task_number>
	  <query>computer vision analysis</query>
	  <relevance>1</relevance>
  </item>
  


  <item>
    <title>A speech recognition ic with an efficient mfcc extraction algorithm and multi-mixture models</title>
	<abstract>Automatic speech recognition (ASR) by machine has received a great deal of attention in past decades. Speech recognition algorithms based on the Mel frequency cepstrum coefficient (MFCC) and the hidden Markov model (HMM) have a better recognition performance compared with other speech recognition algorithms and are widely used in many applications. In this thesis a speech recognition system with an efficient MFCC extraction algorithm and multi-mixture models is presented. It is composed of two parts: a MFCC feature extractor and a HMM-based speech decoder.

In the conventional MFCC feature extraction algorithm, speech is separated into some short overlapped frames. The existing extraction algorithm requires a lot of computations and is not suitable for hardware implementation. We have developed a hardware efficient MFCC feature extraction algorithm in our work. The new algorithm reduces the computational power by 54% compared to the conventional algorithm with only 1.7% reduction in recognition accuracy.

For the HMM-based decoder of the speech recognition system, it is advantageous to use models with multi mixtures, but with more mixtures the calculation becomes more complicated. Using a table look-up method proposed in this thesis the new design can handle up to 16 states and 8 mixtures. This new design can be easily extended to handle models which have more states and mixtures. We have implemented the new algorithm with an Altera FPGA chip using fix-point calculation and tested the FPGA chip with the speech data from the AURORA 2 database, which is a well known database designed to evaluate the performance of speech recognition algorithms in noisy conditions [27]. The recognition accuracy of the new system is 91.01%. A conventional software recognition system running on PC using 32-bit floating point calculation has a recognition accuracy of 94.65%.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Robust automatic speech recognition algorithms for dealing with noise and accent</title>
	<abstract>Although there has been significant progress in automatic speech recognition (ASR) systems over the past five decades, many challenging problems still remain. In addition to the intrinsic confusability between speech units, the environment, speaker, and speaking styles all contribute to variations in speech signals, which pose one of the most challenging issues facing ASR research. Variability in speech requires both the signal processing and pattern modeling components of an ASR system to adapt. The focus of this dissertation is on developing algorithms that improve the performance of speech recognition systems when dealing with variability in speech signals. Specifically, the focus is on variabilities due to environmental noise and to accent. For example, environmental noise contributes to significant speech variability that depends on the type of noise and signal-to-noise ratio (SNR). A noise robust feature extraction technique is necessary in order for ASR to deal with noisy speech. Variations in speech signals due to certain pronunciations can also result in degraded ASR performance. An ASR system needs to compensate for these pronunciation variations.

In terms of noise robustness, we explore feature extraction and frame selection algorithms that can enhance the signal processing component to handle variability caused by noise. Algorithms are then tested on several bench-mark databases to compare their performance with state-of-the-art noise robust ASR systems. Improved recognition accuracy is observed. In terms of speaker accent robustness, we focus on pronunciation modeling. We propose algorithms to analyze pronunciation variations for Spanish-accented speech at the pronunciation lexical level. Since speech recognition systems rely heavily on Hidden Markov Models (HMM), a confusability measure for HMMs is important. We propose a distance measurement between HMMs which improves upon existing HMM confusability metrics in terms of ASR performance prediction, confusion pattern prediction and pronunciation modeling.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>DHMM speech recognition algorithm based on immune particle swarm vector quantization</title>
	<abstract>This article presents a novel Immune Particle Swarm Optimization (IPSO), which combines the artificial immune system methods like immunologic memory, immunologic selection and vaccination together, by making reference to the self adjusting mechanism derived from biological immune system. IPSO as a method of Vector Quantization applied to the Discrete Hidden Markov Model (DHMM) and proposes IPSO-DHMM speech recognition algorithm. Each particle represents a codebook in the algorithm. The experiments using IPSO vector quantization algorithm get optimal codebook. Finally it enters the DHMM speech recognition system to train and recognize. The experimental results show that the IPSO-DHMM speech recognition system has faster convergence, higher recognition ratio and better robustness than the PSO-DHMM algorithm.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Accelerating Speech Recognition Algorithm with Synergic Hidden Markov Model and Genetic Algorithm Based on Cellular Automata</title>
	<abstract>Abstract - One of the best current methods for modeling dynamic speech signal is using of HMM model. The speech recognition systems based on HMM can be able to compute the best likelihood measure between unknown input pattern and reference models by using Viterbi algorithm. Whereas such algorithm is based on dynamic programming, it consists of many computations with increasing number of reference words. In this paper, we will present a new evolutionary methodology based on synergic HMM and GA that will be able to compute likelihood measurement between unknown input pattern and reference patterns in the parallel form and based on cellular automata. We introduce this algorithm as HGC. The HGC algorithm will be compared with the Viterbi algorithm from the "recognition accuracy" and "recognition speed" viewpoints.Obtained results show that the HGC and Viterbi algorithms are close from "recognition accuracy" viewpoint, but HGCisso faster than the Viterbi</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A large-vocabulary continuous speech recognition algorithm and its application to a multi-modal telephone directory assistance system</title>
	<abstract>This paper describes an accurate and efficient algorithm for very-large-vocabulary continuous speech recognition based on an HMM-LR algorithm. The HMM-LR algorithm uses a generalized LR parser as a language model and hidden Markov models (HMMs) as phoneme models. To reduce the search space without pruning the correct candidate, we use forward and backward trellis likelihoods, an adjusting window for choosing only the probable part of the trellis for each predicted phoneme, and an algorithm for merging candidates that have the same allophonic phoneme sequences and the same context-free grammar states. Candidates are also merged at the meaning level. This algorithm is applied to a telephone directory assistance system that recognizes spontaneous speech containing the names and addresses of more than 70,000 subscribers (vocabulary size is about 80,000). The experimental results show that the system performs well in spite of the large perplexity. This algorithm was also applied to a multi-modal telephone directory assistance system, and the system was evaluated from the human-interface point of view. To cope with the problem of background noise, an HMM composition technique which combines a noise-source HMM and a clean phoneme HMM into a noise-added phoneme HMM was investigated and incorporated into the system.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An A* algorithm for very large vocabulary continuous speech recognition</title>
	<abstract>We present a new search algorithm for very large vocabulary continuous speech recognition. Continuous speech recognition with this algorithm is only about 10 times more computationally expensive than isolated word recognition. We report preliminary recognition results obtained by testing our recognizer on "books on tape" using a 60,000 word dictionary.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A hardware accelerator for speech recognition algorithms</title>
	<abstract>This paper describes two custom architectures tailored to a speech recognition beam search algorithm. Both architectures have been simulated using real data and the results of the simulation are presented. The paper also describes the design process of the custom architectures and presents a number of ideas on the automatic design of custom systems for data dependent computations.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Research on the Algorithm of Noisy-Robust Tibetan Speech Recognition Based on RBF</title>
	<abstract>Aiming at the problem of Tibetan speech recognition under the condition of resistance from noise, a kind of Tibetan speech recognition algorithm, combining RBF network with auditory feature was presented in this paper. The description for the Tibetan speech signals was carried out with Mel Frequency Cepstrum Constant (MFCC), and the recognition classifier was designed based on RBF network with the property of supervised learning based on gradient descent. The simulation on the presented algorithm was run, and the results illustrates that the presented algorithm is valid.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Large-Vocabulary Speech Recognition Algorithms</title>
	<abstract>Providing the computer with a natural interface, including the ability to understand human speech, has been a research goal for almost 40 years. Practical versions of such systems have become moderately usable and commercially successful only in the past few years, however. To date, statistical modeling techniques trained from hundreds of hours of speech have provided most speech recognition advancements. Although it may appear that we have far to go before these systems can match human performance, if researchers maintain the cur-rent rate of yearly progress in reducing word error rates, that objective should be within reach in less than a decade.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A computationally efficient mel-filter bank VAD algorithm for distributed speech recognition systems</title>
	<abstract>This paper presents a novel computationally efficient voice activity detection (VAD) algorithm and emphasizes the importance of such algorithms in distributed speech recognition (DSR) systems. When using VAD algorithms in telecommunication systems, the required capacity of the speech transmission channel can be reduced if only the speech parts of the signal are transmitted. A similar objective can be adopted in DSR systems, where the nonspeech parameters are not sent over the transmission channel. A novel approach is proposed for VAD decisions based on mel-filter bank (MFB) outputs with the so-called Hangover criterion. Comparative tests are presented between the presented MFB VAD algorithm and three VAD algorithms used in the G.729, G.723.1, and DSR (advanced front-end) Standards. These tests were made on the Aurora 2 database, with different signal-to-noise (SNRs) ratios. In the speech recognition tests, the proposed MFB VAD outperformed all the three VAD algorithms used in the standards by 14.19% relative (G.723.1 VAD), by 12.84% relative (G.729 VAD), and by 4.17% relative (DSR VAD) in all SNRs.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Evaluation of hands-free large vocabulary continuous speech recognition by blind dereverberation based on spectral subtraction by multi-channel LMS algorithm</title>
	<abstract>Previously, Wang et al. [1] proposed a blind dereverberation method based on spectral subtraction using a multi-channel leastmean squares (MCLMS) algorithm for distant-talking speech recognition. Preliminary experiments showed that this method is effective for isolated word recognition in a reverberant environment. However, robustness and effect factors of the dereverberation method based on spectral subtraction were not investigated. In this paper, we analyze the effect factors of compensation parameter estimation for the dereverberation method based on spectral subtraction, such as the number of channels (the number of microphones), the length of reverberation to be suppressed, and the length of the utterance used for parameter estimation, and evaluate these on large vocabulary continuous speech recognition (LVCSR).We conducted speech recognition experiments on a distorted speech signal simulated by convolving multi-channel impulse responses with clean speech. The proposed method with beamforming achieves a relative word error reduction rate of 19.2% relative to conventional cepstral mean normalization with beamforming for LVCSR. The experimental results also show that our proposed method is robust in a variety of reverberant environments for both isolated and continuous speech recognition and under various parameter estimation conditions.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A clustering algorithm for the fast match of acoustic conditions in continuous speech recognition</title>
	<abstract>In practical speech recognition applications, channel/environment conditions may not match those of the corpus used to estimate the acoustic models. A straightforward methodology is proposed in this paper by which the speech recognizer can match the acoustic conditions of input utterances, thus allowing instantaneous adaptation schemes. First a number of clusters is determined in the training material in a fully unsupervised way, using a dissimilarity measure based on shallow acoustic models. Then accurate acoustic models are estimated for each cluster, and finally a fast match strategy, based on the shallow models, is used to choose the most likely acoustic condition for each input utterance. The performance of the clustering algorithm was tested on two speech databases in Spanish: SENGLAR (read speech) and CORLEC-EHU-1 (spontaneous human-human dialogues). In both cases, speech utterances were consistently grouped by gender, by recording conditions or by background/channel noise. Furthermore, the fast match methodology led to noticeable improvements in preliminary phonetic recognition experiments, at 20-50% of the computational cost of the ML match.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Biomimetic spike-based algorithms and hardware for sound classification, localization, and speech recognition</title>
	<abstract>The objective of the thesis work is to design real-time, spike-based algorithms and implementation for biomimetic sound processing systems. An acoustic direction finding (ADF) system was designed and implemented in hardware to process transient sounds. Additionally, a top-down attentional mechanism model based on a study of mammalian brain activity was designed and explored to improve speech recognition. The front-end of the ADF system, which mimics a mammalian peripheral auditory system, generates spiking neuron firings as its output. The back-end algorithm was developed in MATLAB and an FPGA-based neural network was its final embodiment. The algorithm accomplishes sound detection, classification, direction finding, and localization of various kinds of audio data under noisy conditions and from reverberant environments. The attentional model was integrated with the front-end processing to help segregate a target sound source from masker sound sources as well as improving the classification accuracy of the target source.

The neural-network-based sound classification and localization algorithm was first developed and tested using weaponry sound data obtained in the field. The algorithm is able to differentiate and trace various gunfire acoustic signatures in the presence of high background noise. The algorithm can locate the sound source by using single or multiple microphone array sites. Compared to a least square time difference of arrival algorithm, the neural-network-based algorithm has higher detection rate and more accurate localization. The complete back-end processing system was implemented on a single Xilinx Virtex-5 FPGA chip. The neural-network-based algorithm was also modified for a frog habitat monitoring application to demonstrate that the algorithm can be useful for applications other than weaponry classification and localization.

Literature was reviewed and a functional, biologically-based, top-down attentional model was formulated, coded, and tested using speech signals with varying target masker ratios. The model improves the correctness of word identification of target speech by up to 50% in a noisy environment when the masker source is either a white noise signal or a speech- like signal.

The thesis work presents the first spike-based transient sound classification and localization algorithm using neural networks, the first spike-based frog habitat monitoring algorithm, and a novel top-down, biologically-based attentional model.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Compression algorithms for distributed classification with applications to distributed speech recognition</title>
	<abstract>With the wide proliferation of mobile devices coupled with the explosion of new multimedia applications, there is a need for adopting a client-server architecture to enable clients with low complexity/memory to support complex multimedia applications. In these client-server systems compression is vital to minimize the communication channel bandwidth requirements by compressing the transmitted data. Traditionally, compression techniques have been designed to minimize perceptual distortion, i.e., the compressed data was intended to be heard/viewed by humans. However, recently there has been an emergence of applications in which the compressed data is processed by an algorithm. Examples include distributed estimation or classification. In these applications, for best system performance, rather than minimizing perceptual distortion, the compression algorithm should be optimized to have the least effect on the estimation/classification capability of the processing algorithm. In this work novel compression techniques optimized for classification are proposed.

The first application considered is remote speech recognition, where the speech recognizer uses compressed data to recognize the spoken utterance. For this application, a scalable encoder designed to maximize recognition performance is proposed. The scalable encoder is shown to have superior rate-recognition performance compared to conventional speech encoders. Additionally, a scalable recognition system capable of trading off recognition performance for reduced complexity is also proposed. These are useful in distributed speech recognition systems where several clients are accessing a single server and efficient server design becomes important to both reduce the computational complexity and the bandwidth requirement at the server.

The second application considered is distributed classification, where the classifier operates on the compressed and transmitted data to make the class decision. A novel algorithm is proposed which is shown to significant reduce the misclassification penalty with a small sacrifice in distortion performance. The generality of this algorithm is demonstrated by extending it to improve the performance of table-lookup encoders. It is shown that by designing product vector quantizers (PVQ) to approximate a higher dimension vector quantizer (VQ), a significant improvement in PSNR performance over conventional PVQ design is possible while not increasing the encoding time significantly over conventional table-lookup encoding.

Finally, a new distortion metric, mutual information (MI) loss , is proposed for designing quantizers in distributed classification applications. It is shown that the MI loss optimized quantizers are able to provide significant improvements in classification performance when compared to mean square error optimized quantizers. Empirical quantizer design and rate allocation algorithms are provided to optimize quantizers for minimizing MI loss. Additionally, it is shown that the MI loss metric can be used to design quantizers operating on low dimension vectors. This is a vital requirement in classification systems employing high dimension classifiers as it enables design of optimal and practical minimum MI loss quantizers implementable on low complexity/memory clients.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Performance analysis of speech enhancement algorithm for robust speech recognition system</title>
	<abstract>Widely Speech Signal Processing has not been used much in the field of electronics and computers due to the complexity and variety of speech signals and sounds with the advent of new technology. However, with modern processes, algorithms, and methods which can process speech signals easily and also recognize the text. Demand for speech recognition technology is expected to raise dramatically over the next few years as people use their mobile phones as all purpose lifestyle devices. In this paper, an implementation of a speech-to-text system using isolated word recognition with a vocabulary of ten words (digits 0 to 9 with each 100 samples) and statistical modeling (Hidden Markov Model - HMM) for machine speech recognition was undertaken. In the training phase, the uttered digits are recorded using 8-bit Pulse Code Modulation (PCM) with a sampling rate of 8 KHz and saved as a wave file using sound recorder software. The system performs speech analysis using the Linear Predictive Coding (LPC) method of degree. From the LPC coefficients, the weighted cepstral coefficients and cepstral time derivatives are derived. From these variables the feature vector for a frame is arrived. Then, the system performs Vector Quantization (VQ) utilizing a vector codebook which result vecttor for a frame is arrived. Then, the system performs given word in the vocabulary, the system builds an HMM model and trains the model during the training phase. The training steps, from Speech Enhancement to HMM model building, are performed using PC-based Matlab programs. Our current framework uses a speech processing module includes Speech Enhancement algorithm with Hidden Markov Model (HMM)-based classification and noise language modeling to achieve effective noise knowledge estimation.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Advanced narrow speech channeling algorithm for robot speech recognition</title>
	<abstract>We propose a source extraction algorithm that provides robust speech recognition of intelligent service robots. This system consists of two stages. First, the target speech from a target direction is extracted from a mixture inputs using a binary time-frequency masking method. This stage can work alone. The next stage re-estimates the range of the target direction calculating the energy of the extracted speech by ? histogram. This algorithm was mounted on the speech recognition system of a service robot as a preprocessing step. In section 6, we show that our algorithm has excellent performances.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>On the use of evolutionary algorithms to improve the robustness of continuous speech recognition systems in adverse conditions</title>
	<abstract>Limiting the decrease in performance due to acoustic environment changes remains a major challenge for continuous speech recognition (CSR) systems. We propose a novel approach which combines the Karhunen-Loeve transform (KLT) in the mel-frequency domain with a genetic algorithm (GA) to enhance the data representing corrupted speech. The idea consists of projecting noisy speech parameters onto the space generated by the genetically optimized principal axis issued from the KLT. The enhanced parameters increase the recognition rate for highly interfering noise environments. The proposed hybrid technique, when included in the front-end of an HTK-based CSR system, outperforms that of the conventional recognition process in severe interfering car noise environments for a wide range of signal-to-noise ratios (SNRs) varying from 16 dB to -4 dB. We also showed the effectiveness of the KLT-GA method in recognizing speech subject to telephone channel degradations.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Non-stationary environment compensation using sequential EM algorithm for robust speech recognition</title>
	<abstract>The paper presents a non-stationary environment compensation using sequential EM estimation for tracking the complicated environment. All of the noisy features used in the recognition system are effectively compensated. The speech corruption in the log domain such as the 24 log-filterbank coefficients and the log-energy feature can be modeled as a nonlinear model. For efficient estimating noise parameter using the subsequent sequential Expectation-Maximization (EM) algorithm, the nonlinear environment model is linearized by the truncated first-order vector Taylor series (VTS) approximation. Due to the cepstral features are nearly independence, we train the clean speech using cepstral features and the log-energy feature, and then obtain a diagonal Gaussian mixture model in the log domain by taking inverse discrete cosine transform (IDCT). The experiments are conducted on the large vocabulary continuous speech recognition (LVCSR) system. Results demonstrate that it achieves attractive improvements when compared with CMN (cepstral mean normalization) and the batch-EM based compensation approach.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>HMM parameter adaptation using the truncated first-order VTS and EM algorithm for robust speech recognition</title>
	<abstract>This paper presents a framework of HMM parameter adaptation technique for improving automatic speech recognition (ASR) performance in the noisy environments, which online combines the clean hidden Markov models (HMMs) with the noise model. Based on the given composite HMM corresponding to the initial recognition pass result and truncated vector Taylor series, the noise model in the cepstral domain is updated and refined using iterative Expectation-Maximization (EM) algorithm under maximum likelihood (ML) criterion. Experiments results show that the presented approach in this paper is found to greatly improve recognition performance under mismatched conditions.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Phoneme Lattice Based A* Search Algorithm for Speech Recognition</title>
	<abstract>This paper presents the Speeral continuous speech recognition system developed in the LIA. Speeral uses a modified A* algorithm to find in the search graph the best path taking into account acoustic and linguistic constraints. Rather than words by words, the A* used in Speeral is based on a phoneme lattice previously generated. To avoid the backtraking problems, the system keeps for each frame the deepest nodes of the partially explored lexical tree starting at this frame. If a new hypothesis to explore is ended by a word and the lexicon starting where this word finishes has already been developed, then the next hypothesis will "jump" directly to the deepest nodes. Decoding performances of Speeral are evaluated on the test set of the ARC B1 campaign of AUPELF '97. The experiments on this French database show the efficiency of the search strategy described in this paper.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A comparison using different speech parameters in the automatic emotion recognition using feature subset selection based on evolutionary algorithms</title>
	<abstract>Study of emotions in human-computer interaction is a growing research area. Focusing on automatic emotion recognition, work is being performed in order to achieve good results particularly in speech and facial gesture recognition. This paper presents a study where, using a wide range of speech parameters, improvement in emotion recognition rates is analyzed. Using an emotional multimodal bilingual database for Spanish and Basque, emotion recognition rates in speech have significantly improved for both languages comparing with previous studies. In this particular case, as in previous studies, machine learning techniques based on evolutive algorithms (EDA) have proven to be the best emotion recognition rate optimizers.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Uniform Speech Recognition Platform for Evaluation of New Algorithms</title>
	<abstract>This paper presents the development of a speech recognition platform. Its main area of use would be the evaluation of different new and improved algorithms for speech recognition (noise reduction, feature extraction, language model generation, training of acoustic models, ...). To enable wide usage of the platform, different test configurations were added - from alphabet spelling to large vocabulary continuous speech recognition. At the moment, this speech recognition platform is implemented and evaluated using a studio (SNABI) and a fixed telephone (SpeechDat(II)) speech database.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An Application of Extended Simulated Annealing Algorithm to Generate the Learning Data Set for Speech Recognition System</title>
	<abstract>In this paper, we suggest a method of data extraction for constructing the speech recognition system. The proposed algorithm is based on the Extended Simulated Annealing (ESA) algorithm. We have used Korean text data, drawn randomly from the internet. The Korean LDS built by the proposed algorithm has the equiprobable distribution among Korean alphabets.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Continuous speech recognition with modified learning vector quantization algorithm and two-level DP-matching</title>
	<abstract>This paper proposes a new phoneme recognition method based on the Learning Vector Quantization (LVQ2) algorithm. We propose three kinds of modified training algorithms for the LVQ2 algorithm. In the recognition stage, the likelihood matrix is computed using the reference vectors and then the optimum phoneme sequence is computed from the matrix using the 2-level DP-matching with duration constraints. The recognition score of phonemes in isolated spoken words was 89.1% for the test set. The phoneme recognition scores obtained by the modified LVQ2 algorithms were higher than that obtained by the original LVQ2 algorithm. We applied this method to a multi-speaker-dependent phoneme recognition task for continuous speech uttered Bunsetsu-by-Bunsetsu. The phoneme recognition score was 85.5% for the test speech samples in continuous speech.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Speech Recognition Enhancement Using Beamforming and a Genetic Algorithm</title>
	<abstract>This paper proposes a genetic algorithm (GA) based beamformer to optimize speech recognition accuracy for a pretrained speech recognizer. The proposed beamformer is designed to tackle the non-differentiable and non-linear natures of speech recognition by employing the GA algorithm to search for the optimal beamformer weights. Specifically, a population of beamformer weights is reproduced by crossover and mutation until the optimal beamformer weights are obtained. Results show that the speech recognition accuracies can be greatly improved even in noisy environments.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Feature extraction and sentence recognition algorithm in speech input system</title>
	<abstract>A feature extraction method for speech waves and an algorithm for sentence recognition are studied. The feature extraction is based on an articulatory model constructed from the statistical analysis of X-ray data. The model holds implicitly the physiological constraints and made possible to estimate the state of the articulatory mechanism. The estimated articulatory parameters provide a set of good features for the speech recognition. The sentence recognition problem is mathematically formulated as an optimization problem with constraints by introducing sentence structures from the syntactic and semantic considerations. The algorithm presents an optimal solution in the Bayesian sense.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Improving Robustness of Connectionist Speech Recognition Systems by Genetic Algorithms</title>
	<abstract>In this paper, we present an approach which limits significantly the drop of performances related to Automatic Speech Recognition Systems (ASRSs) caused by acoustic environment changes. We propose to combine a Principal Component Analysis (PCA) and Genetic Algorithms (GA) in order to transform the noisy acoustic environment into a predefined and well-known (canonical) environment. The idea consists in projecting the noisy speech parameters onto the optimal subspace generated by the genetically modified principal components of the canonical environment. The results show that in noisy and changing environments, the proposed PCA/GA optimized system achieves high recognition rate compared to the baseline system. </abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>All-path decoding algorithm for segmental based speech recognition</title>
	<abstract>In conventional speech processing, researchers adopt a dividable assumption, that the speech utterance can be divided into non-overlapping feature sequences and each segment represents an acoustic event or a label. And the probability of a label sequence on an utterance approximates to the probability of the best utterance segmentation for this label sequence. But in the real case, feature sequences of acoustic events may be overlapped partially, especially for the neighboring phonemes within a syllable. And the best segmentation approximation even reinforces the distortion by the dividable assumption. In this paper, we propose an all-path decoding algorithm, which can fuse the information obtained by different segmentations (or paths) without paying obvious computation load, so the weakness of the dividable assumption could be alleviated. Our experiments show, the new decoding algorithm can improve the system performance effectively in tasks with heavy insertion and deletion errors.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>On Time Alignment and Metric Algorithms for Speech Recognition</title>
	<abstract>In this paper, a new algorithm for comparing speech waveforms to decide if the spoken utterance is part of a given vocabulary of word waveforms or not, and if it is part of the vocabulary, to choose the matching word is presented. Our algorithm has been implemented in connection with our own Vector Interpolation alignment algorithm which is faster than dynamic time warping and yet as accurate as dynamic time warping. Our algorithm is faster than the Eucledian distance presently used in speech recognition.This new algorithm, Vector Interpolation, has a classification rate comparable to that of Dynamic Time Warping. While Vector Interpolation is able to match Dynamic Time Warping for recognition accuracy, it requires significantly less computation, making it much faster than DTW based algorithms. Both algorithms are presented and a comparison of the two is made. Also an alternate algorithm, where the number of intervals of the two utterances to be compared is the same, where the length of the intervals in one utterance is different than the length of the intervals in the other utterance, has been investigated.When appropriate adjustments are made so that the beginning and end of the two utterances match, this algorithm has a classification rate comparable to that of Dynamic Time Warping. Furthermore an alternate to LPC analysis for utterance recognition is presented here. Unlike LPC which is an extrapolation algorithm, our algorithm is an interpolation algorithm. Theoretically our algorithm has smaller variance and smaller mean square error than the LPC algorithm. Preliminary results show that our algorithm provides high probability of correct classification.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Deterministic Annealing EM Algorithm in Acoustic Modeling for Speaker and Speech Recognition</title>
	<abstract>This paper investigates the effectiveness of the DAEM (Deterministic Annealing EM) algorithm in acoustic modeling for speaker and speech recognition. Although the EM algorithm has been widely used to approximate the ML estimates, it has the problem of initialization dependence. To relax this problem, the DAEM algorithm has been proposed and confirmed the effectiveness in artificial small tasks. In this paper, we applied the DAEM algorithm to practical speech recognition tasks: speaker recognition based on GMMs and continuous speech recognition based on HMMs. Experimental results show that the DAEM algorithm can improve the recognition performance as compared to the standard EM algorithm with conventional initialization algorithms, especially in the flat start training for continuous speech recognition.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Using Genetic Algorithm to Improve the Performance of Speech Recognition Based on Artificial Neural Network</title>
	<abstract>The goal of this paper is to apply artificial neural network (ANN) to recognize speech. We use Genetic algorithm (GA) to replace the Steepest Descent Method (SDM) for the training of BPNN such that a global search of optimal weight in neural network can be. Thus, the performance of speech recognition was improved by the proposed method in this paper. The non-specific speaker recognition, which is trained by SDM, The recognition rate achieve up to 91% in this experiment. This paper will show that if BPNN is trained by genetic algorithm, higher recognition rate will be attained. Key words: back-propagation neural network, genetic algorithm and non-specific speaker speech recognition</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A Robust Feature Normalization Algorithm for Automatic Speech Recognition</title>
	<abstract>In this paper, we present an effective feature normalization algorithm to improve the robustness of automatic speech recognition systems. At front-end, minimum mean square error log-spectral amplitude estimation speech enhancement is adopted to suppress noise from noisy speech. Then, at back-end, the histogram equalization feature normalization is used to deal with the residual mismatch between enhanced speech and clean speech. We have evaluated recognition performance under noisy environments using NOISEX-92 database and recorded speech signals in continuous speech recognition task. Experimental results show that our approach exhibits considerable improvements in the degraded environment.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Speech recognition method based on linear descending inertia weight PSO algorithm optimizing SVM Kernel parameters</title>
	<abstract>An important factor that influences the performance of support vector machine is how to select the parameters. Particle swarm optimization is an efficient algorithm and it is broadly used in many research areas like pattern recognition. In order to improve the learning and generalization ability of support vector machine and enhance the speech recognition system accuracy, a method for searching the Gaussian kernel support vector machine optimal parameters(C, ?) based on particle swarm optimization is proposed and a speech recognition system based on support vector machine using the optimal parameters is constructed in this paper. The inertia weight, a crucial parameter of the particle swarm optimization, is adopted in linear descending adjusting method. The speech data is isolated, non-specific and middle vocabulary words. The speech feature we used is modified MFCC feature. Experiments indicate that it is an efficient approach for parameters selection of support vector machine and has higher correct speech recognition rates than default parameters of the support vector machine open source software.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Research on the Algorithm of Tibetan Speech Recognition Based on DBN</title>
	<abstract>The research on Tibetan speech recognition is in its initial stage. It is significant to research on recognition algorithm adapted for Tibetan speech. A kind of algorithm of Tibetan speech recognition, based on Dynamic Bayesian Network (DBN), would be investigated in this paper. The simulation on the given algorithm would be carried out, and through the comparing with the recognizing algorithm based on Hidden Markov Model (HMM), the results illustrated that the proposed one would be owned better in these respects of improving the ability of recognizing rate and that of resisting noise.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A syllable-synchronous network search algorithm for word decoding in Chinese speech recognition</title>
	<abstract>The Chinese language is syllabic in nature with frequent homonym phenomena and severe word boundary uncertainty problem. This makes the Chinese continuous speech recognition (CSR) slightly difficult. In order to solve these problems, a Chinese syllable-synchronous network search (SSNS) algorithm is proposed. Together with the vocabulary word search tree and the N-gram based language model, the syllable-synchronous network search algorithm gives a good solution to the Chinese syllable-to-word conversion. In addition, this algorithm is a good method for the accent Chinese speech recognition. The experimental results have showed that the SSNS algorithm can achieve a good overall continuous Chinese speech recognition system performance. </abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The Application of Improved Genetic Algorithm on the Training of Neural Network for Speech Recognition</title>
	<abstract>This paper adopted Artificial Neural Network (ANN) to recognize Mandarin digit speech. The nonspecific speaker speech recognition was the target of this paper. Genetic algorithm (GA) was first used to replace Steepest Descent Method (SDM) and make a global search of optimal weight in neural network. The improved GA is then used to train the ANN. We can find that the performance of speech recognition was improved by the later method. The experiment in this paper will show that if BPNN is trained by GAs, higher recognition rate will be attained. Key words: neural network, improved genetic algorithm, non-specific speaker speech recognition</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A Hybrid Speech Recognition Training Method for HMM Based on Genetic Algorithm and Baum Welch Algorithm</title>
	<abstract>HMM describes the time-domain feature of speech signal by statistical modeling method. Classical training method Baum Welch algorithm could only obtains locally optimal solution, which might decrease the recognition rate, while an important character of genetic algorithm is global search, so we can get a globally optimal solution or at least sub-optimal solution. In this paper genetic algorithm was applied to the optimization of the initial value of B in Baum Welch algorithm. A hybrid training method that combined the traditional method with genetic algorithm was proposed. Experimental results showed that the method had both qualities of global search and rapid convergence and the resulting models were superior to those obtained with traditional methods. </abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An improved noise compensation algorithm for speech recognition in noise</title>
	<abstract>When moving a speech recognition system whose models were trained in a clean laboratory condition to real environments, one of most important issues is how to modify the models according to the changing environments. Using an HMM composition technique we present an algorithm to compensate the dynamic cepstral coefficients for HMM based speech recognition systems in noise environments. Noise compensation for acceleration parameters and for dynamic parameters which are calculated using longer linear regression are discussed. The experimental results show a clear improvement when the algorithm was applied to a speech database recorded in a car. A noise compensation system based realtime speech recognizer using the TMS320C40 was implemented and achieves a good performance in noisy environments.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A FPGA-based Viterbi algorithm implementation for speech recognition systems</title>
	<abstract>This work proposes a speech recognition system based on a hardware/software co-design implementation approach. The main advantage in this approach is an expressive processing time reduction in speech recognition, because part of the system is implemented by dedicated hardware. This work also discuss another way to implement "hidden Markov models" (HMM), a probabilistic model extensively used in speech recognition systems. In this new approach, the Viterbi algorithm, used to compute the HMM likelihood score, will be "built in" together with the HMM structure designed in hardware, and implementing probabilistic state machines that will run as parallel processes one for each word in the vocabulary handled by the system. So far, we have a dramatic speed up performance, getting measures around 500 times faster than a classic implementation with the correctness comparable with others isolated word recognition systems.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A Genetic Algorithm-aided Hidden Markov Model Topology Estimation for Phoneme Recognition of Thai Continuous Speech</title>
	<abstract>The use of Hidden Markov Models (HMM) in many pattern recognition tasks is now very common. Like other pattern recognitions, most Automatic Speech Recognition systems rely on HMM acoustic models. In such systems, recognition performances are significantly affected by their topologies. In this paper,we propose an HMM topology estimation approach for Thai phoneme recognition tasks whose process is divided into 2 stages. First, a set of suitable topologies are constructed by combinations of different objective functions and topology generation methods. Second, a Genetic Algorithm is deployed as the topology selection algorithm which considers global fitness and selects the most suitable topology from the candidates proposed in the previous stage for each phoneme. As aresult, the well-trained topology yields a maximum of4.36% error reduction over predefined left-to-right models. The estimated topologies still work well when the topology estimation was performed on speech utterances whose recording environments differ from the ones recognized.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A Novel Acoustic Feature Extraction Algorithm Based on Root Cepstrum Coefficients and CCBC for Robust Speech Recognition</title>
	<abstract>Studies have shown that depending on speaker task and environmental conditions, recognizers are sensitive to noisy stressful environments. The focus of this study is to achieve robust recognition in diverse environmental conditions through extracting robust features. Central to the technique is Root Cepstrum Coefficients (RCC) method, instead of logarithm amplitude spectrum and discrete cosine transform of the conventional Mel Frequency Cepstral Coefficients (MFCC), but also using Two-dimensional Root Cepstrum Coefficients (TDRCC). This feature is called TDRCC-MFCC. And then, we consider incorporating Canonical Correlation Based Compensation (CCBC) to cope with the mismatch between training and test set. The mismatch between training and test conditions can be simply clustered into three classes: differences of speakers, changes of recording channel and effects of noisy environment. We evaluate the technique using Back-Propagation Neural Networks (BPNN) on two different tasks: one is in-car speech recognition task, another is different SNR speech recognition. The experimental results show that the novel feature has very good robustness and effectiveness relative to MFCC feature and the CCBC algorithm can make speech recognition system greatly robust to all three kinds of mismatch between training set and test set.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Generalized Baum-Welch algorithm for discriminative training on large vocabulary continuous speech recognition system</title>
	<abstract>We propose a new optimization algorithm called Generalized Baum Welch (GBW) algorithm for discriminative training on hidden Markov model (HMM). GBW is based on Lagrange relaxation on a transformed optimization problem. We show that both Baum-Welch (BW) algorithm for ML estimate ofHMMparameters, and the popular extended Baum-Welch (EBW) algorithm for discriminative training are special cases of GBW.We compare the performance of GBW and EBW for Farsi large vocabulary continuous speech recognition (LVCSR).</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Application of a modified neural fuzzy network and an improved genetic algorithm to speech recognition</title>
	<abstract>This paper presents the recognition of speech commands using a modified neural fuzzy network (NFN). By introducing associative memory (the tuner NFN) into the classification process (the classifier NFN), the network parameters could be made adaptive to changing input data. Then, the search space of the classification network could be enlarged by a single network. To train the parameters of the modified NFN, an improved genetic algorithm is proposed. As an application example, the proposed speech recognition approach is implemented in an eBook experimentally to illustrate the design and its merits.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A New Wavelet Threshold Denoising Algorithm in Speech Recognition</title>
	<abstract>To obtain a high robust of speech recognition for noisy conditions, a new pre-processing stage based on wavelet thresholding algorithm is proposed in this paper. The purpose of using the DWT is to benefit from its localization property in the time and frequency domains. Compromise function is proposed compared with hard and soft thresholding function. A new thresholding value, Neyman-Pearson criterion is proposed compared with the commonly used Sqtwolog, Rigrsure, minimaxi criterion. MSE and SNR are given to evaluate the improvement of noisy speech recognition performance. The result shows that the Neyman-Pearson criterion can get a better performance especially at adverse conditions.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Progressive-search algorithms for large-vocabulary speech recognition</title>
	<abstract>We describe a technique we call Progressive Search which is useful for developing and implementing speech recognition systems with high computational requirements. The scheme iteratively uses more and more complex recognition schemes, where each iteration constrains the search space of the next. An algorithm, the Forward-Backward Word-Life Algorithm, is described. It can generate a word lattice in a progressive search that would be used as a language model embedded in a succeeding recognition pass to reduce computation requirements. We show that speed-ups of more than an order of magnitude are achievable with only minor costs in accuracy.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Belarussian Speech Recognition Using Genetic Algorithms</title>
	<abstract>One of the factors complicating activity with speech signals is its large degree of acoustic variability. To decrease influence of acoustic variability of speech signals, it is offered to use genetic algorithms in speech processing systems. We constructed a program model which implements the technology of speech recognition using genetic algorithms. We made experiments on our program model with a database of separated Belarussian words and achieve optimal results.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An efficient A* stack decoder algorithm for continuous speech recognition with a stochastic language model</title>
	<abstract>The stack decoder is an attractive algorithm for controlling the acoustic and language model matching in a continuous speech recognizer. A previous paper described a near-optimal admissible Viterbi A * search algorithm for use with noncrossword acoustic models and no-grammar language models [16]. This paper extends this algorithm to include unigram language models and describes a modified version of the algorithm which includes the full (forward) decoder, cross-word acoustic models and longer-span language models. The resultant algorithm is not admissible, but has been demonstrated to have a low probability of search error and to be very efficient.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Continuous speech recognition by context-dependent phonetic HMM and an efficient algorithm for finding N-Best sentence hypotheses</title>
	<abstract>In this paper, a continuous speech recognition system, "niNja" (Natural language INterface in JApanese), is presented. Efficient search algorithms are proposed to get high accuracy and to reduce the required computations. First, an LR parsing algorithm with context-dependent phone models is proposed. Second, scores of the same phone models in different hypotheses at the phone-level are represented by the single score of the best hypothesis. The system is tested for the task with 113 word vocabulary, word perplexity 4.1. It produces sentence accuracy of 97.3% for the 10 open speakers's 110 sentences and the error reduction is as much as 77% comparing with the case using context independent phone models.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An improved search algorithm using incremental knowledge for continuous speech recognition</title>
	<abstract>In this paper, we propose a search algorithm that incrementally makes effective use of detailed sources of knowledge. The proposed algorithm incrementally applies all available acoustic and linguistic infomation in three search phases. Phase one is a left to right Viterbi beam search that produces word end times and scores using right context between-word models with a bigram language model. Phase two, guided by results from phase one, is a right to left Viterbi beam search that produces word begin times and scores based on left context between-word models. Phase three is an A* search that combines the results of phases one and two with a long distance language model. Our objective is to maximize the recognition accuracy with a minimal increase in computational cost. With our decomposed, incremental, search algorithm, we show that early use of detailed acoustic models can significantly reduce the recognition error rate with a negligible increase in computational cost.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Segment-based acoustic models with multi-level search algorithms for continuous speech recognition</title>
	<abstract>The goal of this project is to develop improved acoustic models for speaker-independent recognition of continuous speech, together with efficient search algorithms appropriate for use with these models. The current work on acoustic modelling is focussed on stochastic, segment-based models that capture the time correlation of a sequence of observations (feature vectors) that correspond to a phoneme. Since the use of segment models is computationally complex, we will also investigate multi-level, iterative algorithms to achieve a more efficient search. Furthermore, these algorithms will provide a formalism for incorporating higher-order information. This research is jointly sponsored by DARPA and NSF.</abstract>
	<search_task_number>9</search_task_number>
	<query>speech recognition algorithm</query>
	<relevance>1</relevance>
  </item>



	<item>
		<title>Application of Artificial Neural Networks to Medical Image Pattern Recognition: Detection of Clustered Microcalcifications on Mammograms and Lung
			Cancer on Chest Radiographs
		</title>
		<abstract>Three neural network models were employed to evaluate their performances in the recognition of medical image patterns associated with lung cancer
			and breast cancer in radiography. The first method was a pattern match neural network. The second was a conventional backpropagation neural network. The
			third method was a backpropagation trained neocognitron in which the signal propagation is operated with the convolution calculation from one layer to the
			next. In the convolution neural network (CNN) experiment, several output association methods and trainer imposed driving functions in conjunction with the
			convolution neural network are proposed for general medical image pattern recognition. An unconventional method of applying rotation and shift invariance is
			also used to enhance the performance of the neural nets.

			We have tested these methods for the detection of microcalcifications on mammograms and lung nodules
			on chest radiographs. Pre-scan methods were previously
			described in our early publications. The artificial neural networks act as final detection classifiers
			to determine if a disease pattern is presented on the
			suspected image area. We found that the convolution neural network, which internally performs feature
			extraction and classification, achieves the best
			performance among the three neural network models. These results show that some processing associated with
			disease feature extraction is a necessary step
			before a classifier can make an accurate determination.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Image pattern recognition with separable trade-off correlation filters</title>
		<abstract>In this paper, a method to design separable trade-off correlation filters for optical pattern recognition is developed. The proposed method not only
			is able to include the information about de desirable peak correlation value but also is able to minimize both the average correlation energy and the effect
			of additive noise on the correlation output. These optimization criteria are achieved by employing multiple training objects. The main advantage of the
			method is based on using multiple information for improving the optical pattern recognition work on images with various objects. The separable Trade-off
			filter is experimentally tested by using both digital and optical pattern recognition.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Image pattern recognition using phase-based local features and their flexible spatial configuration</title>
		<abstract>We propose a new image pattern recognition system that is applicable to several computer vision tasks, such as long range motion matching and object
			recognition. The main strength of our system is its ability to handle substantial image deformations without significantly sacrificing the expressiveness of
			the model representation. This system is divided into three steps, namely: (a) feature extraction, (b) similarity search, and (c) hypothesis verification.
			The phase-based local feature proposed for step (a) is shown to be distinctive and robust to 2-D rigid deformations and severe brightness changes. The step
			(b) pairs similar model and test image features, producing the correspondence set, which is usually densely populated with outliers. Hence, the rejection of
			outliers from this set is necessary to reduce the number of hypotheses to be verified in step (c). We propose two methods to reject outliers that are robust
			to rigid and non-rigid deformations. Quantitative evaluations for both the local feature extractor and the outlier rejection methods are also provided.
			Comparison results produced by these evaluations show that our feature is more robust and distinctive than state-of-the-art features proposed in the
			literature, and our methods to reject outliers are more robust to 3-D rigid and non-rigid deformations than the Hough transform, which is a common method
			used to reject outliers. Finally, our last contribution is a probabilistic verification for step (c) that uses local and semi-local similarities between test
			and model images. The effectiveness of our system is tested in several recognition problems.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>The identification of mammalian species through the classification of hair patterns using image pattern recognition</title>
		<abstract>The identification of mammals through the use of their hair is important in the fields of forensics and ecology. The application of computer pattern
			recognition techniques to this process provides a means of reducing the subjectivity found in the process, as manual techniques rely on the interpretation of
			a human expert rather than quantitative measures. The first application of image pattern recognition techniques to the classification of African mammalian
			species using hair patterns is presented. This application uses a 2D Gabor filter-bank and motivates the use of moments to classify hair scale patterns.
			Application of a 2D Gabor filter-bank to hair scale processing provides results of 52% accuracy when using a filter-bank of size four and 72% accuracy when
			using a filter-bank of size eight. These initial results indicate that 2D Gabor filters produce information that may be successfully used to classify hair
			according to images of its patterns.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>PASM: A Partitionable SIMD/MIMD System for Image Processing and Pattern Recognition</title>
		<abstract>PASM, a large-scale multimicroprocessor system being designed at Purdue University for image processing and pattern recognition, is described. This
			system can be dynamically reconfigured to operate as one or more independent SIMD and/or MIMD machines. PASM consists of a parallel computation unit, which
			contains N processors, N memories, and an interconnection network; Q microcontrollers, each of which controls N/Q processors; N/Q parallel secondary storage
			devices; a distributed memory management system; and a system control unit, to coordinate the other system components. Possible values for N and Q are 1024
			and 16, respectively. The control schemes and memory management in PASM are explored. Examples of how PASM can be used to perform image processing tasks are
			given.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Fast, robust and efficient 2D pattern recognition for re-assembling fragmented images</title>
		<abstract>We discuss the realization of a fast, robust and accurate pattern matching algorithm for comparison of digital images implemented by discrete
			Circular Harmonic expansions based on sampling theory. The algorithm and its performance for re-assembling fragmented digital images are described in detail
			and illustrated by examples and data from the experimentation on an art fresco real problem. Because of the huge database of patterns and the large-scale
			dimension, the results of the experimentation are relevant to describe the power of discrimination and the efficiency of such method.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Image Pattern Recognition Using Near Sets</title>
		<abstract>The problem considered in this paper is how to recognize similar objects based on the detection of patterns in pairs of images. This article
			introduces a new form of classifier based on approximation spaces in the context of near sets for use in pattern recognition. By way of introducing the basic
			approach, nonlinear diffusion is used for edge detection and object contour extraction. This form of image transformation makes it possible to compare the
			contours of objects in pairs of images. Once the contour of an image has been identified, it is then possible to construct approximation spaces based on
			vectors of probe function measurements associated with selected image features. In this article, the only feature considered is &lt;em&gt;contour&lt;/em&gt;,
			which leads to many contour probe functions. The contribution of this article is a new form of classifier, based on approximation spaces, for use in image
			pattern recognition.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Large-Scale Simulation Studies in Image Pattern Recognition</title>
		<abstract>Many obstacles to progress in image pattern recognition result from the fact that per-class distributions are often too irregular to be
			well-approximated by simple analytical functions. Simulation studies offer one way to circumvent these obstacles. We present three closely related studies of
			machine-printed character recognition that rely on synthetic data generated pseudorandomly in accordance with an explicit stochastic model of document image
			degradations. The unusually large scale of experiments¿involving several million samples¿that this methodology makes possible has allowed us to compute sharp
			estimates of the intrinsic difficulty (Bayes risk) of concrete image recognition problems, as well as the asymptotic accuracy and domain of competency of
			classifiers.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Image Pattern Recognition Based on Examples - A Combined Statistical and Structural-Syntactic Approach</title>
		<abstract>An application of combined statistical and structural-syntactic approach in Chinese character recognition is presented. The algorithm adopts a
			structural representation for Chinese characters, but in the classification and training process, the structural matching and parameter adjustment is
			conducted in a statistical way. Different from the conventional structural approaches, in this system, only a few predefined &quot;knowledge&quot; is
			required. In most cases, knowledge acquisition is simplified to &quot;memorization&quot; of examples, and the parameters for classification can be refined
			using statistical training. In this way it avoids the main difficulties inherent in the implementation of classification systems based on structural
			features. Compared with conventional statistical algorithms, the algorithm is based on a structural model of image patterns, so it has approximately all the
			advantages of structural pattern recognition algorithms. A prototype system has been realized based on this strategy, and the effectiveness of the method is
			verified by the experimental results.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>An efficient image pattern recognition system using an evolutionary search strategy</title>
		<abstract>A mechanism involving evolutionary genetic programming (GP) and the expectation maximization algorithm (EM) is proposed to generate feature
			functions automatically, based on the primitive features, for an image pattern recognition system on the diagnosis of the disease OPMD. Prior to the feature
			function generation, we introduce a novel technique of the primitive texture feature extraction, which deals with non-uniform images, from the histogram
			region of interest by thresholds (HROIT). Compared with the performance achieved by support vector machine (SVM) using the whole primitive texture features,
			the GP-EM methodology, as a whole, achieves a better performance of 90.20% recognition rate on diagnosis, while projecting the hyperspace of the primitive
			features onto the space of a single generated feature.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Artificial neural network-based image pattern recognition</title>
		<abstract>This paper addresses the use of a multi-layer fully-connected perceptron neural network for implementing a pattern recognizer. The input of the
			neural network is a set of seven standardized invariant moments in both the training procedure and recognition procedure. This standardization results in
			significantly increasing the accuracy of recognition. The neural network in this paper can recognize the shape of patterns regardless of the size, location
			or brightness. Images are captured and transformed to binary images through a global threshold technique. The weights of the network are computed using the
			back propagation algorithm. An example is given to demonstrate this neural network pattern recognizer.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Image Pattern Recognition in Natural Environment Using Morphological Feature Extraction</title>
		<abstract>The gray-scale morphological Hit-or-Miss transform is theoretically invariant to vertical translation of the input function, which is analogous to
			gray-value shift of the input images. Designing optimal structuring elements for the Hit-or-Miss transform operator is achieved by neural network learning
			methodology using a shared-weight neural network (SWNN) architecture. Early stage of the neural network system performs feature extraction using the
			operator, while the late stage does classification. In experimental studies, this morphological feature-based neural network (MFNN) system is applied to
			location of human face and automatic recognition of vehicle license plate to examine the property of the operator. The results of the experimental studies
			show that the gray-scale morphological Hit-or-Miss transform operator is reducing the effects of lighting variation.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Prototype Setting for Elastic Matching-Based Image Pattern Recognition</title>
		<abstract>The purpose of this paper is to emphasize the importance the consistency between the distance measures on prototype setting and discrimination in
			elastic matching (EM)-based recognition. Specifically, this paper focuses on the following points: (i) confirmation of performance degradation when Euclidean
			distance is used on prototype setting whereas EM-distance is used on discrimination, and (ii) proposal of new prototype setting algorithm where this
			inconsistency is avoided. Through an experiment of handwritten character recognition, the effectiveness of the proposed algorithm was quantified.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>A system for pattern recognition and pattern summarization in multi-band satellite images</title>
		<abstract>A new system (C.T.R.F.'s LSGENSYS--Linguistic Summary Generation System) that has been developed for pattern recognition and summarization of
			patterns in multiband (RGB) satellite images is described in this paper. The system design is described in some detail. The system has been tested
			successfully with SPOT MS and LANDSAT images. It extracts, analyzes, and summarizes patterns such as land, island, water body, river, fire, and urban
			settlements from these images. The results are presented by allowing the system to automatically classify and interpret these images. Some elements of
			supervised classification are also introduced, and a comparison is made between the results in each case.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Minimum description length principle in the field of image analysis and pattern recognition</title>
		<abstract>Problems of decision criterion in the tasks of image analysis and pattern recognition are considered. Overlearning as a practical consequence of
			fundamental paradoxes in inductive inference is illustrated with examples. Theoretical (on the base of algorithmic complexity) and practical formulations of
			the minimum description length (MDL) principle are given. Decrease of the overlearning effect is shown in the examples of modern recognition, grouping, and
			segmentation methods modified with the MDL principle. Novel possibilities of construction of learnable image analysis algorithms by representation
			optimization on the base of the MDL principle are described.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Functional Pattern Recognition of 3D Laser Scanned Images of Wood-Pulp Chips</title>
		<abstract>We evaluate the appropriateness of applying a functional rather than the typical vectorial approach to a pattern recognition problem. The problem to
			be resolved was to construct an online system for controlling wood-pulp chip granulometry quality for implementation in a wood-pulp factory. A functional
			linear model and a functional logistic model were used to classify the hourly empirical distributions of wood-chip thicknesses estimated on the basis of
			images produced by a 3D laser scanner. The results obtained using these functional techniques were compared to the results of their vectorial counterparts
			and support vector machines, whose input consisted of several statistics of the hourly empirical distribution. We conclude that the empirical distributions
			have sufficiently rich functional traits so as to permit the pattern recognition process to benefit from the functional representation.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Efficient face recognition fusing dynamic morphological quotient image with local binary pattern</title>
		<abstract>In this paper, we propose a novel illumination normalized Local Binary Pattern (LBP)-based algorithm for face recognition under varying illumination
			conditions. The proposed DMQI-LBP algorithm fuses illumination normalization, using the Dynamic Morphological Quotient Image (DMQI), into the current
			LBP-based face recognition system. So it makes full use of advantages of illumination compensation offered by the quotient image, estimated with a dynamic
			morphological close operation, as well as the powerful discrimination ability provided by the LBP descriptor. Evaluation results on the Yale face database B
			indicate that the proposed DMQI-LBP algorithm significantly improve the recognition performance (by 5% for the first rank) of the original raw LBP-based
			system for face recognition with severe lighting variations. Furthermore, our algorithm is efficient and simple to implement, which makes it very suitable
			for real-time face recognition.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Pattern recognition in AVHRR images by means of hibryd and neuro-fuzzy systems</title>
		<abstract>The main goal of this work is to improve the automatic interpretation of ocean satellite images. We present a comparative study of different
			classifiers: Graphic Expert System (GES), ANN-based Symbolic Processing Element (SPE), Hybrid System (ANN – Radial Base Function &amp; Fuzzy System),
			Neuro-Fuzzy System and Bayesian Network.. We wish to show the utility of hybrid and neuro-fuzzy system in recongnition of oceanic structures. On the other
			hand, other objective is the feature selection, which is considered a fundamental step for pattern recognition. This paper reports a study of learning
			Bayesian Network for feature selection [1] in the recognition of oceanic structures in satellite images.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>A study of computer vision and pattern recognition in medical image analysis: digital microscopy and optical coherent tomography</title>
		<abstract>Computer vision and pattern recognition techniques have been fostered to solve many practical problems of diverse areas. Medical image analysis
			using machine vision and learning intelligence is one of the most sought-after fields. Computer vision addresses problems of the use of computers to detect,
			partition, represent, group, track, and interpret crucial primitives from given visual inputs. By contrast, pattern recognition is the study of
			distinguishing and recognizing different patterns represented with quantitative measurements. As a result, both of these two components usually present
			themselves in medical image analysis research work.In this dissertation, two parts of new theoretical work and one new segmentation method are proposed.
			Additionally, three medical image analysis problems are solved with intensive use of computer vision and pattern recognition techniques. In the first
			theoretical work, we demonstrate a new paradigm of modelling the optimal geometry fitting problem with deformable statistical models. Two novel
			ellipse-specific fitting algorithms are developed from this schema, along with an algorithm specifically devised to allow for an appropriate choice of
			methods optimally matched with the input data distribution. The second theoretical analysis work is related to connections between the generalized and the
			ordinary eigenvalue decomposition problems often presented in Linear Discriminant Analysis (LDA). The third contribution is illustrated by the work aimed at
			the development of a whole-slide microscopy image analysis system. A complete work flow of analysis, including establishment of image hierarchy,
			segmentation, feature representation, feature selection, feature extraction, classification, classifier combination, and decision evaluation, is developed.
			With this work, a new segmentation approach using the Fisher-Rao criterion, embedded in the generic Expectation-Maximization algorithm, is proposed for
			segmenting images. As an extension, a combined microscopy image analysis system is next developed to integrate into the computerized decision making process
			two prognosis components (stromal detection and differentiation grading) clinically crucial for pathologists. Both linear and non-linear methods are
			investigated for reducing the dimensionality of the feature space where data are classified by the classification and regression tree (CART) boosted with a
			Multi-Class Gentle Boosting method. Finally, the work of detecting multiple layer boundaries within human retina OCT images is presented. The developed
			algorithm is a hybrid approach consisting of a segmented model fitting process and the probabilistic relaxation labeling procedure. In the initial model
			fitting step, a geometrical model concatenating three parabolas with downward openings is developed. In the second stage, the fine tuning problem is
			formulated as the probabilistic relaxation labeling process that transforms the image to a relational graph consisting of attributed nodes. Experimental
			results demonstrate a good correlation between the sub-layer locations identified by the automated process and those manual markers.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Advances in Digital Image Processing and Information Technology: First International Conference on Digital Image Processing and Pattern Recognition,
			... in Computer and Information Science)
		</title>
		<abstract>This book constitutes the refereed proceedings of the First International Conference on Digital Image Processing and Pattern Recognition, DPPR 2011,
			held in Tirunelveli, India, in September 2011. The 48 revised full papers were carefully reviewed and selected from about 400 submissions. The conference
			brought together leading researchers, engineers and scientists in the domain of Digital Image Processing and Pattern Recognition. The papers cover all
			theoretical and practical aspects of the field and present new advances and current research results in two tracks, namely: digital image processing and
			pattern recognition, and computer science, engineering and information technology.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Image Analysis and Recognition: 8th International Conference, ICIAR 2011, Burnaby, BC, Canada, June 22-24, 2011. Proceedings, Part I (Lecture Notes in
			... Vision, Pattern Recognition, and Graphics)
		</title>
		<abstract>The two-volume set LNCS 6753/6754 constitutes the refereed proceedings of the 8th International Conference on Image and Recognition, ICIAR 2011,
			held in Burnaby, Canada, in June 2011. The 84 revised full papers presented were carefully reviewed and selected from 147 submissions. The papers are
			organized in topical sections on image and video processing; feature extraction and pattern recognition; computer vision; color, texture, motion and shape;
			tracking; biomedical image analysis; biometrics; face recognition; image coding, compression and encryption; and applications.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Image Analysis and Processing -- ICIAP 2011: 16th International Conference, Ravenna, Italy, September 14-16, 2011, Proceedings, Part I (Lecture Notes
			... Vision, Pattern Recognition, and Graphics)
		</title>
		<abstract>The two-volume set LNCS 6978 + LNCS 6979 constitutes the proceedings of the 16th International Conference on Image Analysis and Processing, ICIAP
			2011, held in Ravenna, Italy, in September 2011. The total of 121 papers presented was carefully reviewed and selected from 175 submissions. The papers are
			divided into 10 oral sessions, comprising 44 papers, and three post sessions, comprising 77 papers. They deal with the following topics: image analysis and
			representation; image segmentation; pattern analysis and classification; forensics, security and document analysis; video analysis and processing; biometry;
			shape analysis; low-level color image processing and its applications; medical imaging; image analysis and pattern recognition; image and video analysis and
			processing and its applications.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Automatic solar feature detection using image processing and pattern recognition techniques</title>
		<abstract>The objective of the research in this dissertation is to develop a software system to automatically detect and characterize solar flares, filaments
			and Corona Mass Ejections (CMEs), the core of so-called solar activity. These tools will assist us to predict space weather caused by violent solar activity.
			Image processing and pattern recognition techniques are applied to this system.For automatic flare detection, the advanced pattern recognition techniques
			such as Multi-Layer Perceptron (MLP), Radial Basis Function (RBF), and Support Vector Machine (SVM) are used. By tracking the entire process of flares, the
			motion properties of two-ribbon flares are derived automatically. In the applications of the solar filament detection, the Stabilized Inverse Diffusion
			Equation (SIDE) is used to enhance and sharpen filaments; a new method for automatic threshold selection is proposed to extract filaments from background; an
			SVM classifier with nine input features is used to differentiate between sunspots and filaments. Once a filament is identified, morphological thinning,
			pruning, and adaptive edge linking methods are applied to determine filament properties. Furthermore, a filament matching method is proposed to detect
			filament disappearance. The automatic detection and characterization of flares and filaments have been successfully applied on Hα full-disk images that are
			continuously obtained at Big Bear Solar Observatory (BBSO). For automatically detecting and classifying CMEs, the image enhancement, segmentation, and
			pattern recognition techniques are applied to Large Angle Spectrometric Coronagraph (LASCO) C2 and C3 images.The processed LASCO and BBSO images are saved to
			file archive, and the physical properties of detected solar features such as intensity and speed are recorded in our database. Researchers are able to access
			the solar feature database and analyze the solar data efficiently and effectively. The detection and characterization system greatly improves the ability to
			monitor the evolution of solar events and has potential to be used to predict the space weather.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Subpixel pattern recognition by image histograms</title>
		<abstract>Recognition of small patterns covering only a few pixels in an image cannot be done by conventional recognition methods. A theoretically new pattern
			recognition method has been developed for under-sampled objects which are (much) smaller than the window-size of a picture element (pixel), i.e. these
			objects have subpixel size. The proposed statistical technique compares the gray-level histogram of the patterns of a set of scanned objects to be examined
			with the (calculated) gray-level densities of different (in shape or size) possible objects, and the recognition is based on this comparison. This method
			does not need high-precision movement of scanning sensors or any additional hardware. Moreover, the examined patterns should be randomly distributed on the
			screen, or a random movement of camera is (or target or both are) needed. Effects of noise are analyzed, and filtering processes are suggested in the
			histogram domain. Several examples of different object shapes (triangle, rectangle, square, circle, curving lines, etc.) are presented through simulations
			and experiments. A number of possible application areas are suggested, including astronomy, line-drawing analysis and industrial laser measurements.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Bio-Inspired Hybrid Intelligent Systems for Image Analysis and Pattern Recognition</title>
		<abstract>Bio-Inspired Hybrid Intelligent Systems for Image Analysis and Pattern Recognition comprises papers on diverse aspects of bio-inspired models, soft
			computing and hybrid intelligent systems. The articles are divided into four main parts. The first one consists of papers that propose new fuzzy and
			bio-inspired models to solve general problems. The second part deals with the main theme of modular neural networks in pattern recognition, which are
			basically papers using bio-inspired techniques. The third part contains papers that apply hybrid intelligent systems to the problem of time series analysis
			and prediction, while the fourth one shows papers dealing with bio-inspired models in optimization and robotics applications. An edited book in which both
			theoretical and application aspects are covered.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Computer Analysis of Images and Patterns: 14th International Conference, CAIP 2011, Seville, Spain, August 29-31, 2011, Proceedings, Part I (Lecture
			... Vision, Pattern Recognition, and Graphics)
		</title>
		<abstract>The two volume set LNCS 6854/6855 constitutes the refereed proceedings of the International Conference on Computer Analysis of Images and Patterns,
			CAIP 2011, which took place in Seville, Spain, August 29-31, 2011. The 138 papers presented together with 2 invited talks were carefully reviewed and
			selected from 286 submissions. The papers are organized in topical section on: motion analysis, image and shape models, segmentation and grouping, shape
			recovery, kernel methods, medical imaging, structural pattern recognition, Biometrics, image and video processing, calibration; and tracking and stereo
			vision.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Method for image informational properties exploitation in pattern recognition environment</title>
		<abstract>The problem of finding optimal image transformation according to informational image properties is considered. During the research, image
			informational properties that distinguish images from all other recognition objects were investigated. The notion of image equivalence was introduced. Notion
			of image equivalence as identity of images with respect to some transformation set was explored. The proposed equivalence definition allows decomposition of
			image set into subsets of a certain type and establishes correspondence between image equivalence classes and some subsets of operations. On this basis, the
			method for selecting efficient image recognition algorithm according to image informational nature was elaborated. The proposed method provides a possibility
			of improving efficiency of selecting image analysis algorithms and automation (partial or complete) of image processing. It allows taking into consideration
			internal information that image conveys, syntactic and semantic.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>CMOS-based radiation movie and still image pickup system with a phototimer using smart pattern recognition</title>
		<abstract>Radiation imaging systems are used in biology, medicine, industry, high-energy physics, and et ceteras application. However, it has been highly
			difficult to realize a radiation digital image pickup device that is adapted to show a moving image with a high resolution if it is made to show reduced
			dimensions at low cost. CMOSs can advantageously be used for the photoelectric converters. CMOSs are adapted to show a moving image with a high resolution
			and, at the same time, can be made to show reduced dimensions at low cost. Therefore, we propose a CMOS-based radiation movie and still image pickup device.
			Next we propose a phototimer for the device. The radiation image pickup system includes a phototimer using pattern recognition for X-ray emission. The system
			is able to recognize a pattern image of interest by allowing radiation from an X-ray source to pass through a object. The phototimer signal can be obtained
			from the image recognition unit by non-destructive reading in the image sensing operation. The radiation image pickup system can easily and accurately
			perform optimal exposure. Finally the system could obtain a high-quality image without any deterioration in S/N characteristics.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>New Methods to Generate Neutral Images for Spatial Pattern Recognition</title>
		<abstract>Three new methods are developed to generate neutral spatial models for pattern recognition on raster data. The first method employs Genetic
			Programming (GP), the second Sequential Gaussian Simulation (SGS), and the third Conditional Pixel Swapping (CPS) in order to produce sets of &quot;neutral
			images&quot; that provide a probabilistic assessment of how unlikely an observed spatial pattern on a target image is under the null hypothesis. The sets of
			neutral images generated by the three methods are found to preserve different aspects of spatial autocorrelation on the target image. This preliminary
			research demonstrates the feasibility of using neutral image generation in spatial pattern recognition.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Predicate Logic Based Image Grammars for Complex Pattern Recognition</title>
		<abstract>Predicate logic based reasoning approaches provide a means of formally specifying domain knowledge and manipulating symbolic information to
			explicitly reason about different concepts of interest. Extension of traditional binary predicate logics with the bilattice formalism permits the handling of
			uncertainty in reasoning, thereby facilitating their application to computer vision problems. In this paper, we propose using first order predicate logics,
			extended with a bilattice based uncertainty handling formalism, as a means of formally encoding pattern grammars, to parse a set of image features, and
			detect the presence of different patterns of interest. Detections from low level feature detectors are treated as logical facts and, in conjunction with
			logical rules, used to drive the reasoning. Positive and negative information from different sources, as well as uncertainties from detections, are
			integrated within the bilattice framework. We show that this approach can also generate proofs or justifications (in the form of parse trees) for each
			hypothesis it proposes thus permitting direct analysis of the final solution in linguistic form. Automated logical rule weight learning is an important
			aspect of the application of such systems in the computer vision domain. We propose a rule weight optimization method which casts the instantiated inference
			tree as a knowledge-based neural network, interprets rule uncertainties as link weights in the network, and applies a constrained, back-propagation algorithm
			to converge upon a set of rule weights that give optimal performance within the bilattice framework. Finally, we evaluate the proposed predicate logic based
			pattern grammar formulation via application to the problems of (a) detecting the presence of humans under partial occlusions and (b) detecting large complex
			man made structures as viewed in satellite imagery. We also evaluate the optimization approach on real as well as simulated data and show favorable results.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Fuzzy spatial relationships for model-based pattern recognition in images and spatial reasoning under imprecision</title>
		<abstract>We show in this paper that mathematical morphology provides a unified and consistent framework to express different types of spatial relationships
			and to answer different questions about them, with good properties. We show then how to use these fuzzy relationships in model-based pattern recognition and
			spatial reasoning under imprecision. Two examples are presented, one where recognition of face features is expressed as non bijective correspondence between
			graphs representing regions and spatial relations, and one where anatomical expert knowledge involving spatial relationships is used to guide the recognition
			of brain structures.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Research on automatic classification technology of web image based on fuzzy pattern recognition</title>
		<abstract>The automatic classification technology of web image is important for control the web content. For handy image classification, it is difficult to
			satisfy people's demands because of the period, fee and effect. This paper presents a kind of web image automatic classification based on fuzzy pattern
			recognition, HSV color space and complexion model. The membership function is obtained to extract complexion characteristics firstly, and then the
			classification determinant rule of sexy image is given. The experiment results show the classification algorithm is effective for severity sexy image and
			attains anticipated target. This algorithm improved the currency character of classification algorithm because the hypothesis of classification model is a
			little.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Pattern recognition in histopathological images: an ICPR 2010 contest</title>
		<abstract>The advent of digital whole-slide scanners in recent years has spurred a revolution in imaging technology for histopathology. In order to encourage
			further interest in histopathological image analysis, we have organized a contest called &quot;Pattern Recognition in Histopathological Image Analysis.&quot;
			This contest aims to bring some of the pressing issues facing the advance of the rapidly emerging field of digital histology image analysis to the attention
			of the wider pattern recognition and medical image analysis communities. Two sample histopathological problems are explored: counting lymphocytes and
			centroblasts. The background to these problems and the evaluation methodology are discussed.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Affine-invariant pattern recognition using momentums in log-polar images</title>
		<abstract>Log-polar images are useful in the sense that their image data size is reduced dramatically comparing with conventional images and it is possible to
			develop faster pattern recognition algorithms. Especially, the log-polar image is very similar with the structure of human eyes and is used in a face
			detection and tracking. However, there are almost no researches on pattern recognition using the log-polar images while a number of researches on visual
			tracking have been executed. In this paper, an affine-invariant pattern recognition technique for log-polar images using momentums. We handle basic
			distortions of a pattern including translation, rotation, scaling, and skew in a log-polar image. The algorithm is experimented in a PC-based real-time
			vision system successfully.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Pattern Recognition and Image Processing</title>
		<abstract>Extensive research and development has taken place over the last 20 years in the areas of pattern recognition and image processing. Areas to which
			these disciplines have been applied include business (e. g., character recognition), medicine (diagnosis, abnormality detection), automation (robot vision),
			military intelligence, communications (data compression, speech recognition), and many others. This paper presents a very brief survey of recent developments
			in basic pattern recognition and image processing techniques.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Multispectral Image Recognition Research Based on Biomimetic Pattern Recognition</title>
		<abstract>Studies on learning problems from geometry perspective have attracted an ever increasing attention in machine learning, leaded by Biomimetic Pattern
			Recognition on information geometry. Biomimetic Pattern Recognition is a new model of Pattern Recognition based on “matter cognition” instead of “matter
			classification”. This new model is much closer to the function of human being, than traditional statistical Pattern Recognition using “optimal separating” as
			its main principle.This paper proposes a high-dimensional descriptive way about multispectral image. Geometrical properties of high-dimensional structures
			underlying a set of samples are learned via successive projections from the higher dimension to the lower dimension until Clifford space, under guidance of
			the established properties and theorems in high-dimensional descriptive way. Specifically, we introduce a principle of homology continuity based on Clifford
			Algebra and provides a geometrical learning algorithm for specifying Clifford Algebra shapes, which is then applied to biomimetic pattern recognition.
			Experimental results are presented to show the efficiency of our theory.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Bimimetic Pattern Recognition for Multispectral Image</title>
		<abstract>Biomimetic Pattern Recognition is a new model of Pattern Recognition based on &quot;matter cognition&quot; instead of &quot;matter
			classification&quot;. This new model is much closer to the function of human being, than traditional statistical Pattern Recognition using &quot;optimal
			separating&quot; as its main principle. But it has been investigated in 2D sample space, In this paper, we extend Multispectral Image sample space by
			Clifford Algebra, and study its property. The experiment result proves the efficiency of our theory.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Pattern recognition techniques for image and video post-processing: specific application to image interpolation</title>
		<abstract>Video and image enhancement relate to the age old problem of constructing new image detail to effect a more pleasurable viewing experience in video
			sequences and images. The image processing procedure described in this thesis is image and video interpolation, but the discussed algorithms are easily
			applicable to other areas in a variety of fields. The motivations behind resolution enhancement span the full spectrum of government to commercial to medical
			applications. For example, video quality from on-demand sites such as YouTube, Daily Motion, and Veoh, where hit counts sometimes reach 4.7 million viewers
			per day, have traded off quality for bandwidth; travelers who use Google Map's Street View cannot read signs, see landmarks from the provided low-resolution
			panoramic; Google Earth® and government contractors in image intelligence demand for increasing resolution in satellite imagery. The long list of
			applications goes on, including applications such as web-photo editing, MRI scan feature enhancements, on-demand creation of HDTV content, etc.The image
			interpolation problem is an inherently ill-posed problem, and quality assessments of the resulting images are often driven by human decision rather than
			numerical analysis. By computer learning, the best viewing performance can be achieved by mimicking the human decision-making process. Therefore, this thesis
			is concerned with modeling machine and statistical learning techniques to fit the interpolation framework, while easily modified to accommodate other video
			and image processing problems in general. Specifically, we learn properties in a training set by using regression to map a relationship between known and
			unknown information. The known information by our definitions are low-resolution images while unknown information are high-resolution images.Machine learning
			is a particularly broad topic, and we can approach the image interpolation problems in several different ways. Our input space is the image patch domain,
			where we process fixed-size contiguous subsets of the image independently. Consequently, we first discuss properties of the feature space and propose a
			multivariate probability distribution function to describe the image patch domain. Knowledge of the distribution and properties of the feature space is
			especially conducive to both parametric and nonparametric estimation techniques.Because k-nearest neighbors is a relatively simple and commonly used
			nonparametric estimation technique, we propose a nearest neighbor algorithm that adaptively finds the appropriate number of neighbors to use and then
			performs the necessary regression steps. The algorithm also imposes global constraints through a heavily approximated Markov Random Field. Due to runtime
			considerations, we explore quicker ways to search for the k nearest neighbors of a given input.Next, we investigate the use of kernel-based methods by
			employing support vector regression. We improve the generalization potential for nonlinear relationships by proposing convex optimization problems focused in
			kernel learning. There are various interpolation frameworks that can include the newly proposed regression technique, and we experiment with several.
			Ultimately, we propose a mixture of experts framework to describe the relationships in the training set.Finally, we propose a single, general, zero-phase
			MMSE interpolation filter to address computational complexity concerns of all learning algorithms. The idea arises from image processing analysis of machine
			learning techniques rather than the application of machine learning to image processing. In the development of the final filter, we analyze a general
			classification-based filtering scheme using polyphase representation. Because there are inherent similarities and considerable overlap between each class in
			such an approach, one zero-phase filter for all image content seems to logically follow as an adequate approximation that reduces the total number of
			computations to that of bicubic interpolation. Analyzing the frequency response, we can generate filters on-the-fly for arbitrary scaling factors.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>The Pattern Recognition of Inscriptions on Oracle Bones by Esthetics Analysis in Computer Image</title>
		<abstract>The pattern recognition of inscriptions on oracle bones by esthetics analysis in image have been studied in the paper. Samples of inscriptions on
			oracle bones were scanned into computer images. By esthetics analysis of inscriptions on oracle bones, a new arithmetic was chosed and a standard inflexion
			curve of inscriptions on oracle bones by the arithmetic can be gotten. There are curves by the arithmetic in images. The standard inflexion curve of
			inscriptions on oracle bones and curves in images were compared. If both of curves look very similar, there are inscriptions on oracle bones in the image.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Image representation and pattern recognition in brains and machines</title>
		<abstract>This dissertation explores two different topics. In Part I we present data analysis and modeling studies characterizing how simple cell receptive
			fields in the mammalian primary visual cortex collectively represent visual information. We build on standard models which treat simple cell responses as the
			outputs of linear spatio-temporal filters. We characterize a widely assumed but previously unquantified linear trend relating simple cell preferred spatial
			frequencies and spatial frequency bandwidths. In contrast with popular assumptions, we discover that receptive field shapes are not scale invariant on
			average, but evolve systematically with preferred spatial frequency. We present a new parametric model for simple cell space-time receptive fields, and a new
			index for directional-tuning strength. Examination of this new index suggests that strongly directionally-selective simple cells are more common than
			previously believed. Finally, we present theoretical arguments to account for the way in which simple cell populations represent time-varying imagery. These
			arguments predict a novel constraint between a receptive field's temporal bandwidth and its preferred spatial and temporal frequencies and spatial frequency
			bandwidth. We find this constraint is obeyed by cat simple cell receptive fields.In Part II we present an information theoretic analysis of pattern
			recognition. Biological and machine pattern recognition systems face a common challenge: Given sensory data about an unknown object, classify the object by
			comparing the sensory data with a library of internal representations stored in memory. In real-world environments, the number of possible objects to be
			recognized and the richness of the raw sensory data often force recognition systems to internally represent memory and sensory information in a compressed
			format. However, these representations must preserve information in the original data with some minimum level of fidelity to support reliable pattern
			recognition. Thus, there is an intrinsic tradeoff between the amount of resources devoted to data representation and the complexity of the environment in
			which a recognition system may reliably operate. We propose a general mathematical model for pattern recognition problems subject to resource constraints,
			and prove single-letter information theoretic bounds governing the aforementioned tradeoff. We apply our theoretical results to the example of recognizing
			sequences of binary symbols.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Image management using pattern recognition systems</title>
		<abstract>With the popular usage of personal image devices and the continued increase of computing power, casual users need to handle a large number of images
			on computers. Image management is challenging because in addition to searching and browsing textual metadata, we also need to address two additional
			challenges. First, thumbnails, which are representative forms of original images, require significant screen space to be represented meaningfully. Second,
			while image metadata is crucial for managing images, creating metadata for images is expensive. My research on these issues is composed of three components
			which address these problems.First, I explore a new way of browsing a large number of images. I redesign and implement a zoomable image browser, PhotoMesa,
			which is capable of showing thousands of images clustered by metadata. Combined with its simple navigation strategy, the zoomable image environment allows
			users to scale up the size of an image collection they can comfortably browse. Second, I examine tradeoffs of displaying thumbnails in limited screen space.
			While bigger thumbnails use mare screen space, smaller thumbnails are hard to recognize. I introduce an automatic thumbnail cropping algorithm based on a
			computer vision saliency model. The cropped thumbnails keep the core informative part and remove the less informative periphery. My user study shows that
			users performed visual searches mare than 18% faster with cropped thumbnails.Finally, I explore semi-automatic annotation techniques to help users make
			accurate annotations with low effort. Automatic metadata extraction is typically fast but inaccurate while manual annotation is slow but accurate. I
			investigate techniques to combine these two approaches. My semi-automatic annotation prototype, SAPHARI, generates image clusters which facilitate efficient
			bulk annotation. For automatic clustering, I present hierarchical event clustering and clothing based human recognition. Experimental results demonstrate the
			effectiveness of the semi-automatic annotation when applied on personal photo collections. Users were able to make annotation 49% and 6% faster with the
			semi-automatic annotation interface on event and face tasks, respectively.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Interval Type-2 Fuzzy Logic Applications in Image Processing and Pattern Recognition</title>
		<abstract>Interval type-2 fuzzy logic can be applied to perform image processing and pattern recognition. In this work a new type-2 fuzzy logic method is
			applied for edge detection in images and the results are compared with three different traditional techniques for the same goal with the type-2 edge
			detection outperforming the other techniques.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Medical Image Computing and Computer-Assisted Intervention - MICCAI 2011: 14th International Conference, Toronto, Canada, September 18-22, 2011, ...
			Vision, Pattern Recognition, and Graphics)
		</title>
		<abstract>The three-volume set LNCS 6891, 6892 and 6893 constitutes the refereed proceedings of the 14th International Conference on Medical Image Computing
			and Computer-Assisted Intervention, MICCAI 2011, held in Toronto, Canada, in September 2011. Based on rigorous peer reviews, the program committee carefully
			selected 251 revised papers from 819 submissions for presentation in three volumes. The third volume includes 82 papers organized in topical sections on
			computer-aided diagnosis and machine learning, and segmentation.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>A compact, efficient preprocessing scheme for inputting any binary image to novel neural network used in robust, accurate pattern recognition</title>
		<abstract>The curves and lines in an edge-detected binary image can be analyzed using the adaptive-window detection technique. This window at first moves from
			the top-left corner of the image frame, and then scans horizontally and downward until it hits the starting point S of a “continuous” line or curve. Then it
			will automatically track the direction of the curve until it hits an end point E or a branch point B. The coordinates of the starting point S, the end point
			E or the branch point B will be automatically recorded in a data file, and so are the coordinates of all continuous points between S, E or S, B. For the
			branch point, the adaptive window will detect how many branches are connected to point B, and it will track automatically each branch until another end point
			or another branch point is hit. The coordinates of all continuous points between any pair of (cusp) points S, B; S, E; B, B; or B, E, will be automatically
			recorded in a different data file. Each data file then represents a single curve between 2 cusp points. These data file can then be used to find the
			analytical expression for each curve. We use polynomial, least square curve fitting techniques to get a very compact set of analytical data for representing
			or reconstructing the original binary image.This dissertation reports the image-processing steps, the programming algorithm, and the experimental results on
			this novel feature extraction technique. It will be verified in each experiment by the reconstruction of the original image from the compactly extracted
			analog data lines. These data lines can then be used very efficiently for inputting to a specially designed neural network for carrying out a very accurate
			and very robust pattern identification task.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Multimodal Brain Image Analysis: First International Workshop, MBIA 2011, Held in Conjunction with MICCAI 2011, Toronto, Canada, September 18, 2011,
			... Vision, Pattern Recognition, and Graphics)
		</title>
		<abstract>This book constitutes the refereed proceedings of the First International Workshop on Multimodal Brain Image Analysis, held in conjunction with
			MICCAI 2011, in Toronto, Canada, in September 2011. The 15 revised full papers presented together with 4 poster papers were carefully reviewed and selected
			from 24 submissions. The objective of this workshop is to facilitate advancements in the multimodal brain image analysis field, in terms of analysis
			methodologies, algorithms, software systems, validation approaches, benchmark datasets, neuroscience, and clinical applications.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Mathematical Morphology and Its Application to Signal and Image Processing: 9th International Symposium on Mathematical Morphology, ISMM 2009
			Groningen, ... Vision, Pattern Recognition, and Graphics
		</title>
		<abstract>This book constitutes the refereed proceedings of the 9th International Symposium on Mathematical Morphology, ISMM 2009 held in Groningen, The
			Netherlands in August 2009. The 27 revised full papers presented together with one invited paper were carefully reviewed and selected from numerous
			submissions. The papers are organized in topical sections on theory, connectivity and connected filters, adaptive morphology, graphs and topology,
			segmentation, shape, morphology of multi-valued images, and algorithms.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Rotationally invariant filter bank for pattern recognition of noisy images</title>
		<abstract>We propose new rotation moment invariants based on multiresolution filter bank techniques. The multiresolution pyramid motivates our simple but
			efficient feature selection procedure based on fuzzy C-mean clustering, combined with the Mahalanobis distance. The procedure verifies an impact of random
			noise as well as an interesting and less known impact of noise due to spatial transformations. The recognition accuracy of the proposed techniques has been
			tested with the Zernike moments, the Fourier-Mellin moments as well as with some wavelet based schemes. The numerical experiments, with more than 30,000
			images, demonstrate a tangible accuracy increase of about 3% for low noise, 8% for the average noise and 15% for high-level noise.</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Design of correlation filters for pattern recognition using a noisy training image</title>
		<abstract>Correlation filters for object detection and location estimation are commonly designed assuming the shape and graylevel structure of the object of
			interest are explicitly available. In this work we propose the design of correlation filters when the appearance of the target is given in a single training
			image. The target is assumed to be embedded in a cluttered background and the image is assumed to be corrupted by additive sensor noise. The designed filters
			are used to detect the target in an input scene modeled by the nonoverlapping signal model. An optimal correlation filter, with respect to the peak-to-output
			energy ratio criterion, is proposed for object detection and location estimation. We also present estimation techniques for the required parameters. Computer
			simulation results obtained with the proposed filters are presented and compared with those of common correlation filters.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>A weak structure model for regular pattern recognition applied to facade images</title>
		<abstract>We propose a novel method for recognition of structured images and demonstrate it on detection of windows in facade images. Given an ability to
			obtain local low-level data evidence on primitive elements of a structure (like window in a facade image), we determine their most probable number, attribute
			values (location, size) and neighborhood relation. The embedded structure is weakly modeled by pair-wise attribute constraints, which allow structure and
			attribute constraints to mutually support each other. We use a very general framework of reversible jump MCMC, which allows simple implementation of a
			specific structure model and plug-in of almost arbitrary element classifiers. The MC controls the classifier by prescribing it &quot;where to look&quot;,
			without wasting too much time on unpromising locations.We have chosen the domain of window recognition in facade images to demonstrate that the result is an
			efficient algorithm achieving performance of other strongly informed methods for regular structures like grids, while our general model covers loosely
			regular configurations as well.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>New approach for segmentation and pattern recognition of jacquard images</title>
		<abstract>Phase field models provide a well-established framework for the mathematical description of free boundary problems for image segmentation. In phase
			field models interfaces represent edges of jacquard images and the determination of the edges of jacquard images is the main goal of image segmentation. In
			this paper, the phase field model was applied to segment and recognize pattern structures of jacquard images. The segmentation was performed in two major
			steps. Firstly, a pattern extraction and representation was performed by an adaptive mesh generation scheme. For the conjugate gradient method has been
			successfully used in solving the symmetric and positive definite systems obtained by the finite element approximation of energy functionals, a novel
			conjugate gradient algorithm was adapted to the minimization of energy functional of discrete phase model. Experimental results show efficiency of our
			approach.
		</abstract>
		<search_task_number>4</search_task_number>
		<query>image pattern recognition</query>
		<relevance>1</relevance>
	</item>



  <item>
    <title>A conceptual design for augmented reality games using motion detection as user interface and interaction</title>
	<abstract>Augmented Reality (AR) is a technology that provides the seamless interaction between real and virtual environment. Since its emergence, AR has been gaining wide acceptance in various fields of application including the gaming industry. Its unique interface allows for integration of difference user interface and interaction techniques to create immersing and fun gaming experience. Motion detection which is related with visual user interface (UI) and gesture recognition is one such example. With the introduction of devices such as Microsoft Kinect and Nintendo Wii Remote, motion based technique has been fast gaining popularity as a more natural mean of user interface, which can be referred as natural user interface (NUI). This paper presents a conceptual design framework for motion detection based AR games. A physical prop is introduced as a reference point to enhance the game experience. A case scenario of a simple game will be presented to illustrate the concept. Potential design and implementation challenges will be highlighted and discussed.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>User Interface Development for a Computer-Based User Study: The Universal Model Approach</title>
	<abstract>The Universal Model is an Human Computer Interaction model claimed to be applicable to any interactive system or product. In a computer-based user study, what users do and how fast they can do their tasks are the critical factors that can influence research results. Therefore, having a friendly and easy-to-use interface is crucial in a computer-based user study applications. In this paper, we present our approach in developing a pre-prepared software, ViSigStudy using Universal Model. ViSigStudy is designed to evaluate the effectiveness of four types of visual representations using visual signatures that represent video motions in two-dimensional form. It contains a data entry page and test pages for displaying the visual signatures with questions, to evaluate the effectiveness of these representations. We added a preliminary tier into the Universal Model to introduce the visual signatures (which participants have no prior knowledge of) before participants enter the test application. The added preliminary tier is important in this scientific experiment as the understanding of the visual representation introduced as stimuli, determines the success of the experiment. Results of the experiment have been very encouraging, showing that the experiment was a success.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Intuitive map navigation on mobile devices</title>
	<abstract>
	In this paper, we propose intuitive motion-based interfaces for map navigation on mobile devices with built-in cameras. The interfaces are based on the visual detection of the devices self-motion. This gives people the experience of navigating maps with a virtual looking glass. We conducted a user study to evaluate the accuracy, sensitivity and responsiveness of our proposed system. Results show that users appreciate our motion-based user interface and find it more intuitive than traditional key-based controls, even though there is a learning curve.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Automatic detection of users' skill levels using high-frequency user interface events</title>
	<abstract>
	Computer users have different levels of system skills. Moreover, each user has different levels of skill across different applications and even in different portions of the same application. Additionally, users' skill levels change dynamically as users gain more experience in a user interface. In order to adapt user interfaces to the different needs of user groups with different levels of skills, automatic methods of skill detection are required. In this paper, we present our experiments and methods, which are used to build automatic skill classifiers for desktop applications. Machine learning algorithms were used to build statistical predictive models of skill. Attribute values were extracted from high frequency user interface events, such as mouse motions and menu interactions, and were used as inputs to our models. We have built both task-independent and task-dependent classifiers with promising results.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Multisensor based security robot system for intelligent building</title>
	<abstract>
	Intelligent building can provide safety, convenience, efficiency and entertainment for life in the 21st century. The most importance role of the intelligent building is the security system. We develop a multi sensor-based intelligent security robot (ISR) that is widely employed in intelligent buildings. The intelligent security robot can detect abnormal and dangerous situations and notify users. The robot has the shape of cylinder and its diameter, height, and weight are 50 cm, 130 cm and 100 kg respectively. The function of the ISR contains six parts. There is the software development system; avoiding obstacle and motion planning system, image system, sensor system, remote supervise system and other systems. We develop a multi sensor-based sensor system in the ISR. We use multiple multisensor fusion algorithms to get an exact decision in the detection subsystem of the sensor system. There is an adaptive fusion method, a rule based method, and a statistical signal method. We demonstrate the remote supervisory system to control the ISR using a direct control mode and a behavior control mode. We think that the man-machine interface in a security robot system must have mobility and convenience. Therefore, we use a touch screen to display the system state, and design a general user interface (GUI) to service the user and visitors. The user can remotely control the appliance using a cell phone through a GSM modem, too. The appliance module can feedback reaction results to the user through a cell phone. Finally, we implement the fire detection system in the intelligent security robot (Chung-Cheng-I). If a fire occurs, the intelligent security robot can find out the fire source using the fire detection system. In intruder detection, we program the same scenario to detect the intruder using the intelligent security robot. The intelligent security robot transmits the message of the detection result to the user using a GSM modem for a fire event or intruder, and transmits the detection result to a client computer through the internet. 
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Intuitive application-specific user interfaces for mobile devices</title>
	<abstract>
The user interfaces on current-day mobile devices are limited owing to their small form factor. In this paper, we propose intuitive user interfaces for two applications on mobile phones with built-in cameras, namely, a car-racing game and a map navigation application. These user interfaces are based on the visual detection of the device's self-motion. We discuss three different methodologies to implement same and conduct user studies to analyze people's acceptance of the interfaces in comparison to controlling the applications with keys on the phone. Results show that our proposed interfaces are well appreciated for their intuitiveness, even though there is a learning curve.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
  <item>
  <title>Novel interfaces for digital cameras and camera phones</title>
	<abstract>
Camera phones are now very common but there are some usability issues that affect their use. These can occur because the users look through the LCD to frame the image and can often miss the icons displayed around the edges that present important information about the status of the camera. This may lead to shots being missed or poorly exposed. Most camera phones do not take full advantage of the features of the underlying phone platform to enhance their interfaces. We created a camera application for the Nokia N95 that featured novel interface elements and made use of the features of the platform to provide a rich variety of information in more usable forms, such as: sonifications of the luminance histogram to ensure better exposure before a picture is taken; phone orientation to give a level indicator to ensure the camera is straight; measuring phone movement to ensure the phone is being held steady; and the detection of image motion to support panning We also present a scenario for how these features could be used in conjunction with each other during the photo taking process.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
  <item>
  <title>An assistive bi-modal user interface integrating multi-channel speech recognition and computer vision</title>
	<abstract>
In this paper, we present a bi-modal user interface aimed both for assistance to persons without hands or with physical disabilities of hands/arms, and for contactless HCI with able-bodied users as well. Human being can manipulate a virtual mouse pointer moving his/her head and verbally communicate with a computer, giving speech commands instead of computer input devices. Speech is a very useful modality to reference objects and actions on objects, whereas head pointing gesture/motion is a powerful modality to indicate spatial locations. The bi-modal interface integrates a tri-lingual system for multi-channel audio signal processing and automatic recognition of voice commands in English, French and Russian as well as a vision-based head detection/tracking system. It processes natural speech and head pointing movements in parallel and fuses both informational streams in a united multimodal command, where each modality transmits own semantic information: head position indicates 2D head/pointer coordinates, while speech signal yields control commands. Testing of the bi-modal user interface and comparison with contact-based pointing interfaces was made by the methodology of ISO 9241-9.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
  <item>
  <title>MOCA: a low-power, low-cost motion capture system based on integrated accelerometers</title>
	<abstract>
Human-computer interaction (HCI) and virtual reality applications pose the challenge of enabling real-time interfaces for natural interaction. Gesture recognition based on body-mounted accelerometers has been proposed as a viable solution to translate patterns of movements that are associated with user commands, thus substituting point-and-click methods or other cumbersome input devices. On the other hand, cost and power constraints make the implementation of a natural and efficient interface suitable for consumer applications a critical task. Even though several gesture recognition solutions exist, their use in HCI context has been poorly characterized. For this reason, in this paper, we consider a low-cost/low-power wearable motion tracking system based on integrated accelerometers called motion capture with accelerometers (MOCA) that we evaluated for navigation in virtual spaces. Recognition is based on a geometric algorithm that enables efficient and robust detection of rotational movements. Our objective is to demonstrate that such a low-cost and a low-power implementation is suitable for HCI applications. To this purpose, we characterized the system from both a quantitative point of view and a qualitative point of view. First, we performed static and dynamic assessment of movement recognition accuracy. Second, we evaluated the effectiveness of user experience using a 3D game application as a test bed.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
  <item>
  <title>Human gesture recognition using 3.5-dimensional trajectory features for hands-free user interface</title>
	<abstract>
We present a new human motion recognition technique for a hands-free user interface. Although many motion recognition technologies for video sequences have been reported, no man-machine interface that recognizes enough variety of motions has been developed. The difficulty was the lack of spatial information that could be acquired from video sequences captured by a normal camera. The proposed system uses a depth image in addition to a normal grayscale image from a time-of-flight camera that measures the depth to objects, so various motions are accurately recognized. The main functions of this system are gesture recognition and posture measurement. The former is performed using the bag-of-words approach. The trajectories of tracked key points around the human body are used as features in this approach. The main technical contribution of the proposed method is the use of 3.5D spatiotemporal trajectory features, which contain horizontal, vertical, time, and depth information. The latter is obtained through face detection and object tracking technology. The proposed user interface is useful and natural because it does not require any contact-type devices, such as a motion sensor controller. The effectiveness of the proposed 3.5D spatiotemporal features was confirmed through a comparative experiment with conventional 3.0D spatiotemporal features. The generality of the system was proven by an experiment with multiple people. The usefulness of the system as a pointing device was also proven by a practical simulation.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
 <item>
  <title>A neural-based remote eye gaze tracker under natural head motion</title>
	<abstract>
A novel approach to view-based eye gaze tracking for human computer interface (HCI) is presented. The proposed method combines different techniques to address the problems of head motion, illumination and usability in the framework of low cost applications. Feature detection and tracking algorithms have been designed to obtain an automatic setup and strengthen the robustness to light conditions. An extensive analysis of neural solutions has been performed to deal with the non-linearity associated with gaze mapping under free-head conditions. No specific hardware, such as infrared illumination or high-resolution cameras, is needed, rather a simple commercial webcam working in visible light spectrum suffices. The system is able to classify the gaze direction of the user over a 15-zone graphical interface, with a success rate of 95% and a global accuracy of around 2^o, comparable with the vast majority of existing remote gaze trackers. 
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
 <item>
  <title>Computer interface using eye tracking for handicapped people</title>
	<abstract>
In this paper, a computer interface for handicapped people is proposed, where input signals are given by eye movement of the handicapped people. Eye movement is detected by neural network (NN)-based texture classifier, which enables our system to be not obliged to constrained environment. To be robust the natural motion of a user, we first detect a user’s face using skin-color information, and then detect her or his eyes using neural network (NN)-based texture classifier. After detection of eye movements, the tracking is performed using mean-shift algorithms. We use this eye-tracking system as an interface to control the surrounding system such as audio, TV, light, phone, and so on. The experimental results verify the feasibility and validity of the proposed eye-tracking system to be applicable as an interface for the handicapped people.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
 <item>
  <title>Towards hands-free interfaces based on real-time robust facial gesture recognition
</title>
	<abstract>
Perceptual user interfaces are becoming important nowadays, because they offer a more natural interaction with the computer via speech recognition, haptics, computer vision techniques and so on. In this paper we present a visual-based interface (VBI) that analyzes users' facial gestures and motion. This interface works in real-time and gets the images from a conventional webcam. Due to this, it has to be robust recognizing gestures in webcam standard quality images. The system automatically finds the user's face and tracks it through time for recognizing the gestures within the face region. Then, a new information fusion procedure has been proposed to acquire data from computer vision algorithms and its results are used to carry out a robust recognition process. Finally, we show how the system is used to replace a conventional mouse for human computer interaction. We use the head's motion for controlling the mouse's motion and eyes winks detection to execute the mouse's events
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
 <item>
  <title>Motion planning for a landmine-detection robot</title>
	<abstract>
This article describes a landmine-detection system that contains a landmine-detection mobile robot and a following mobile robot. In this system, the landmine-detection robot goes ahead, and uses a landmine detector and a GPS module to find a landmine, records the coordinates of its location, and transmits these coordinates to the following mobile robot via a wireless RF interface. The following robot can record the location and orientation of the landmine-detection robot and all the landmines in the region. The following robot moves close to the landmine, and programs a path to avoid obstacles and landmines automatically. The driving system of the landmine-detection mobile robot uses a microprocessor dsPIC 30F4011 as the core, and controls two DC servomotors to program the motion path. The user interface of the landmine-detection robot and the following robot uses Borland C++ Builder language to receive the location data. In the experimental results, the landmine-detection robot records the location of landmines using a GPS module, and transmits the locations to the following robot via a wireless RF interface. The following robot avoids the landmines, and improves the safety of people or materials being carried through the landmine area.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
 <item>
  <title>Visual recognition of 3D emblematic gestures in an HMM framework</title>
	<abstract>
In human-machine interaction, gestures play an important role as input modality for natural and intuitive interfaces. While the usage of sign languages or crafted command gestures typically requires special user training, the class of gestural actions called “emblems” represents more intuitive yet expressive signs that seem well suited for the task. Following this, an approach for the visual recognition of 3D emblematic arm gestures in a realistic smart room scenario is presented. Hand and head positions are extracted in multiple unsynchronized monocular camera streams, combined to spatiotemporal 3D gesture trajectories and classified in an Hidden Markov Model (HMM) classification and detection framework. The contributions within this article are threefold: Firstly, a solution for the 3D combination of trajectories obtained from unsynchronized cameras and with varying frame rates is proposed. Secondly, the suitability of different alternative feature representations derived from a hand trajectory is assessed, and it is shown that intuitive gestures can be represented by projection on their principal plane of motion. Thirdly, it is demonstrated that a rejection model for gesture spotting and segmentation can be constructed using out-of-domain data. The approach is evaluated on a challenging realistic data set.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
 <item>
  <title>Designing in virtual reality (DesIRe): a gesture-based interface</title>
	<abstract>
In this paper, our purpose is to focus on detection of dynamic gestures and find the thresholds to detect gestures for a gesture-based system, DesIRe (DESigning In virtual REality). DesIRe treats virtual reality as a platform of virtual clay and allows realtime dynamic interaction with an object to perform a number of design operations. We developed an interface using a stereoscopic display environment and combining elements of gesture recognition and motion capture. The system architecture of DesIRe include following components: Vizard Virtual Reality Toolkit, an immersive projection system (VISOR), an optical tracking system (specifically the PPT system and the in-house gesture recognition software), and Data Input System (a data glove). In DesIRe, the user has control of a hand-shaped cursor which is guided by a single infrared LED and linked to the data glove finger states and orientation in order to provide an immediate, visual feedback. In this paper, we present an approach to represent the finger states using binary digits in gesture detection and demonstrate the thresholds. We found that thresholds are glove specific.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
 <item>
  <title>Realistic virtual hand modeling with applications for virtual grasping</title>
	<abstract>
In virtual environments, virtual hand interactions play key roles in the human-computer interface. Specifically, the virtual grasping of 3D objects provides an intuitive way for users to interact with virtual objects. This paper demonstrates the creation of a sophisticated virtual hand model simulating natural anatomy in its appearance and motion. To achieve good visual realism, the virtual hand is modeled with metaball modeling, and visually enhanced by applying texture mapping. For realistic kinematics modeling, a three-layer model (skeleton, muscle and skin layers) is adopted to handle the motion as well as the deformation of the virtual hand. We also present an approach for virtual grasping of 3D objects with the realistic virtual hand driven by a CyberGlove dataglove. Grasping heuristics are proposed based on the classification with the shapes of objects, and simplified proxies for the virtual hand are used for the purpose of real-time collision detection between the virtual hand and 3D objects.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
 <item>
  <title>Surprise Grabber: a co-located tangible social game using phone hand gesture</title>
	<abstract>
Social network games (SNGs) are among the most popular games recently. Different from the asynchronous and online based SNGs, we present Surprise Grabber to see how tangible gesture interface could benefit the synchronous co-located social game. In Surprise Grabber, users control a virtual grabber's moving in 3D game to catch the gifts by using their camera phone. An efficient code running on the phone detects hand motion, delivers results to Serve PC and provides feedbacks in real time. Distinguished from online SNGS, all players stand together in front of a public display. The results of the pilot user studies showed that: 1) Gesture interface was easy to catch up and made the game more immersive; 2) Occasionally inaccuracy in hand motion detection made the game more competitive instead of frustrating players; 3) Players' performances were obviously influenced by the social atmosphere; 4) In most cases, players' performances became better or worse at the same time.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
  <item>
  <title>LightWave: using compact fluorescent lights as sensors</title>
	<abstract>
In this paper, we describe LightWave, a sensing approach that turns ordinary compact fluorescent light (CFL) bulbs into sensors of human proximity. Unmodified CFL bulbs are shown to be sensitive proximity transducers when they are illuminated. This approach utilizes predictable variations in electromagnetic noise resulting from the change in impedance due to the proximity of a human body to the bulb. The electromagnetic noise can be sensed from any point along a home's electrical wiring. This allows users to perform gestures near any CFL lighting fixture, even when multiple lamps are operational. Gestures can be sensed using a single interface device plugged into any electrical outlet. We experimentally show that we can reliably detect hover gestures (waving a hand close to a lamp), touches on lampshades, and touches on the glass part of the bulb itself. Additionally, we show that touches anywhere along the body of a metal lamp can be detected. These basic detectable signals can then be combined to form complex gesture sequences for a variety of applications. We also show that CFLs can function as more general-purpose sensors for distributed human motion detection and ambient temperature sensing.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
 <item>
  <title>Gestures for mixed-initiative news video browsing on mobile devices</title>
	<abstract>
Although small screen sizes and limited input methods are challenges for mobile devices, there are affordances like gestures and motion detection through on-device sensors. A prototype system has been implemented on a HTC Dream (T-Mobile G1) mobile phone demonstrating an "adaptive TV-channel" like mixed-initiative interface used to solicit user relevance feedback, provide recommendations and facilitate news video watching. The user is provided on-screen finger gesture operations to vote-up, vote-down or skip a video. Shaking the device resets the video sequence. This creates a cognitively palatable stream of videos and a seamless lower-latency user experience easily operated with one hand.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
 <item>
  <title>Ad-Smell: advertising movie with a simple olfactory display</title>
	<abstract>
The idea of adding the smell to the advertising movie for describing the scent of each event in the advertising movie has been proposed to improve the current way of advertising movies, which can perceive only pictures and sound. Using the Ad-Smell, the customer can smell the advertising movie. The users need an Ad-Smell application to read a smell and an olfactory display in order to release scents.

There are two main parts in an Ad-Smell system, which are an olfactory display and a release smell application. An olfactory display consists of a control box and four smell boxes with four fans. Fans will be turned on and release scents when receiving the signal from an Ad-Smell application. The Ad-Smell application is designed to detect a customer movement from a web camera and then show an advertising movie. During a display of advertising movie, Ad-Smell reads control signals in the movie and decides to send signals to an olfactory display. The Ad-Smell application consists of six major components: 1) Webcam Controlling, 2) Motion Detection, 3) Movie Controlling, 4) SubSmell Reading, 5) Scent Releasing, and 6) Olfactory Display Monitoring. We use Microsoft Visual Basic 6.0 to develop the user interface and the Ad-Smell components.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
 <item>
  <title>iPhone application development</title>
	<abstract>
Apple's mobile handheld devices have been a huge success; beginning with the launching of the iPhone in 2007 and that has been maintained until present days with the launching of the iPad in 2010. There are now more than 85,000 apps available for the more than 50 million iPhone and iPod touch customers worldwide and over 125,000 developers in Apple's iPhone Developer Program. That is why iPhone is a new and widely extended platform to develop object-oriented applications.

iPhone platform involves several and amazing technologies that makes programming it a cool activity for experienced programmers willing to know the goodness of a mobile device, and a great option for students or novices to learn programming. Both of them will find the power of the iPhone libraries and development tools amazing to quickly start building powerful iPhone apps. We will be using Xcode under Mac OS X as our IDE, and Objective-C and iPhone API as our programming tools to create iPhone applications (from basic to medium level), which finally can be loaded into the iPhone device. We'll start with the classical "Hello World" and continue to develop applications using graphical user interfaces, handling multi-touch and motion detection, communication interfaces, and different media.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
 <item>
  <title>Natural user interfaces</title>
	<abstract>
Recent developments in user-input technologies are changing the way we interact with digital screens. The mouse and the keyboard are being replaced by touch and motion based interfaces, increasingly known as Natural User Interfaces (NUI). YDreams has developed Yvision, a platform that enables the development of natural user interfaces. YVision has a modular architecture matching YDreams technologies with the best of open source third party libraries. Our technologies emphasize the creation of smart interfaces using autonomous agents that go beyond the traditional reactive systems. Yvision also includes computer vision algorithms for motion detection and the application of 3D depth sensing in rendering engines. NUI applications involve: data acquisition using various sensors that detect the user's motion and gestures; interpretation of sensor data; and presentation, the end visualization layer. YVision includes augmented reality capabilities as a visualization component, where images are captured from the real world and enhanced in real-time with contextual information. Natural user interface applications, developed for both 2D and 3D depth sensing, will be presented for illustrative purposes. Applications include projects developed for clients such as Orange, Coca-Cola, Santander and Nike. Ongoing research projects focusing on digital signage and serious games will be also discussed.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
  <item>
  <title>Distributed sensory systems and developer platforms from Crossbow technology</title>
	<abstract>
This demonstration will show two distributed sensory systems. One is an environmental monitoring system that can be used outdoor weather and habitat studies or for controlled environment monitoring (Figure 1). This system is composed of two types of wireless sensor. One is a simple relative humidity and temperature sensor and the other has total solar radiation, photosynthetic radiation, and barometric pressure (in addition to relative humidity and temperature).The second system is a platform for distributed motion detection that can be used for perimeter or area activity monitoring or for building occupancy analysis (Figure 2). The system is composed of a node which has multiple sensors including four passive infrared, two-axis magnetic field, and acoustic microphone.The data from both systems will be displayed on a user interface called MOTE-VIEW. This is a PC application which is used to view incoming data, chart sensor information over time, and visualize the network topology and sensor data on a map of the deployment.The demo will also provide information on the software and hardware building blocks for sensory systems. The software building blocks include the embedded firmware in the sensor nodes ("XMesh-enabled applications"), the data aggregation and dissemination layer ("XServe"), and finally the client layer ("MOTE-VIEW"). The hardware building blocks include the wireless platform modules ("Motes") and sensor suites and interfaces. These building blocks are packaged as development platform and will be on display.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
  <title>Distributed sensory systems and developer platforms from Crossbow technology</title>
	<abstract>
This demonstration will show two distributed sensory systems. One is an environmental monitoring system that can be used outdoor weather and habitat studies or for controlled environment monitoring (Figure 1). This system is composed of two types of wireless sensor. One is a simple relative humidity and temperature sensor and the other has total solar radiation, photosynthetic radiation, and barometric pressure (in addition to relative humidity and temperature).The second system is a platform for distributed motion detection that can be used for perimeter or area activity monitoring or for building occupancy analysis (Figure 2). The system is composed of a node which has multiple sensors including four passive infrared, two-axis magnetic field, and acoustic microphone.The data from both systems will be displayed on a user interface called MOTE-VIEW. This is a PC application which is used to view incoming data, chart sensor information over time, and visualize the network topology and sensor data on a map of the deployment.The demo will also provide information on the software and hardware building blocks for sensory systems. The software building blocks include the embedded firmware in the sensor nodes ("XMesh-enabled applications"), the data aggregation and dissemination layer ("XServe"), and finally the client layer ("MOTE-VIEW"). The hardware building blocks include the wireless platform modules ("Motes") and sensor suites and interfaces. These building blocks are packaged as development platform and will be on display.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
  <title>Smart Motion Detection Surveillance System</title>
	<abstract>
Motion detection surveillance technology came about as a relief for the generally time-consuming reviewing process that a normal video surveillance system offers. It has gained a lot of interests over the past few years. In this paper, we propose a motion detection surveillance system, consisting of its Graphic User Interface (GUI) and its method for motion detection, through the study and evaluation of currently available products and methods. The proposed system is efficient and convenient for both office and home uses. 
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
   <item>
  <title>hMouse: Head Tracking Driven Virtual Computer Mouse</title>
	<abstract>
A novel head tracking driven camera mouse system, called "hMouse", is developed for manipulating hand-free perceptual user interfaces. The system consists of a robust real-time head tracker, a head pose/motion estimator, and a virtual mouse control module. For the hMouse tracker, we propose a 2D detection/tracking complementary switching strategy with an interactive loop. Based on the reliable tracking results, hMouse calculates the user's head roll, tilt, yaw, scaling, horizontal, and vertical motion for further mouse control. Cursor position is navigated and fine tuned by calculating the relative position of tracking window in image space and the user's head tilt or yaw rotation. After mouse cursor is navigated to the desired location, head roll rotation triggers virtual mouse button clicks. Experimental results demonstrate that hMouse succeeds under the circumstances of user jumping, extreme movement, large degree rotation, turning around, hand/object occlusion, part face out of camera shooting region, and multi-user occlusion. It provides alternative solutions for convenient device control, which encourages the application of interactive computer games, machine guidance, robot control, and machine access for disabilities and elders. 
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
   <item>
  <title>Interactive Type Synthesis of Mechanisms</title>
	<abstract>
As a step towards CAD systems that model function as well as form, we have developed an interactive tool for qualitative mechanism design. Called the mechanism editor, it enables the user to quickly sketch abstract planar mechanisms made up of polygonal links connected by revolute and prismatic joints. The user may interactively control the parameter of a joint, and the editor will compute and display the resulting motion of the entire mechanism. When the user requests an animation, the editor builds a plan to be executed repeatedly as the user flexes the input joint. The graph of the mechanisms topology, in which each link is represented by a node and each joint by an edge, is used for counting degrees of freedom including the detection of overconstrained submechanisms. The graph is modified during the planning stage to represent the portion of the mechanism that remains to be solved. Each step in the plan solves a small subgraph that matches the graph of one of the available base cases. This step computes the relative positions of the links in the subgraph. This subgraph is then contracted to a single node, leaving a smaller graph to be solved. After a number of steps, the graph is reduced to one node, and the positions of every link are known relative to ground. There are some mechanism whose graphs cannot be solved in this fashion. A set of base cases, composed of the RRR, PRR, and RPR mechanisms, is described in detail. Simple trigonometry yield closed-form solutions for these base cases. Therefore the whole plan represents a closed-form solution. Each base case typically has either two possible solutions, in which case the plan step must choose one of them, or it has zero solutions, in which case the mechanism breaks. A number of possible policies for these situations are presented. Several earlier systems, most of them descendants of Sutherland''s Sketchpad. Are described and compared to the mechanism editor. Some implementation details are presented, including control structures for the user interface and for the planner.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
  <title>Mobile Robot Based Online Chinese Chess Game</title>
	<abstract>
We present the game tree search techniques based on Chinese chess game, and use mobile robots to present the scenario. We design a mobile robot to be applied in many fields, such as entertainment, education and security. The mobile robot has the shape of cylinder and its diameter, height and weight is 8cm, 15cm and 1.5kg. The power of the mobile robot is three Li batteries, and connects with parallel arrangement. It has three IR sensors to detect obstacle. The controller of the mobile robot is MCS-51 chip, and can acquire the detection signal from sensors through I/O pins, and receives the command from the supervised computer and the remote controller via wireless RF interface. The controller of the mobile robot can transmits the detection result to the supervised computer via wireless RF interface. The mobile robot can speak Chinese language using voice driver module. We develop the user interface of mobile robots according to the basic rules of Chinese chess game, and program the motion trajectories for the mobile robot to play the Chinese chess game. In the experimental results, mobile robots can receive the command from the supervised computer, and move the next position according to the attribute of the chess piece.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
  <title>Laser Range Finder Applying in Motion Control System of Mobile Robots</title>
	<abstract>
We have been designed a module based mobile robot applying in the intelligent life space. The mobile robot is constructed using aluminum frame. The mobile robot has the shape of cylinder and its diameter, height and weight is 40 cm, 80cm and 40kg. There are six systems in the mobile robot, including structure, avoidance obstacle and driver system, software development system, detection module system, remote supervise system and others. In the avoidance obstacle and driver system, we use NI motion control card and MAXON drivers to control two DC servomotors of the mobile robot, and detect obstacle using laser range finder, sixteen reflective IR sensors and sixteen ultrasonic sensors. The mobile robot measure the range of obstacles using decision tree method, and find out the free space to move autonomously. We develop the user interface of the avoidance obstacle for the mobile robot. Finally, we control the mobile robot to avoid obstacles according to the programmed trajectory. The mobile robot can avoid obstacle using laser range finder, and follow the programmed trajectory.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
  <title>Attribute-based vehicle search in crowded surveillance videos</title>
	<abstract>
We present a novel application for searching for vehicles in surveillance videos based on semantic attributes. At the interface, the user specifies a set of vehicle characteristics (such as color, direction of travel, speed, length, height, etc.) and the system automatically retrieves video events that match the provided description. A key differentiating aspect of our system is the ability to handle challenging urban conditions such as high volumes of activity and environmental factors. This is achieved through a novel multi-view vehicle detection approach which relies on what we call motionlet classifiers, i.e. classifiers that are learned with vehicle samples clustered in the motion configuration space. We employ massively parallel feature selection to learn compact and accurate motionlet detectors. Moreover, in order to deal with different vehicle types (buses, trucks, SUVs, cars), we learn the motionlet detectors in a shape-free appearance space, where all training samples are resized to the same aspect ratio, and then during test time the aspect ratio of the sliding window is changed to allow the detection of different vehicle types. Once a vehicle is detected and tracked over the video, fine-grained attributes are extracted and ingested into a database to allow future search queries such as "Show me all blue trucks larger than 7ft length traveling at high speed northbound last Saturday, from 2pm to 5pm".
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
  <title>Plenary lecture 1: assisted movement of visually impaired in outdoor environments - work directions and new results</title>
	<abstract>
There are approximately 45 million blind individuals world-wide according to the World Health Report. Vision loss limits their access to the educational opportunities, social events, public transportation and leads to a higher rate of unemployment than that of individuals with no functional limitations (58% and 18% respectively, according to the American Foundation for the Blind (AFB)). Many efforts have been invested in the last years, based on ingenious devices and information technology, to help people to overcome these barriers and to integrate them in the social and productive life.

In this talk, research efforts are presented to develop electronic travel aids (ETA) that increase the visual impaired people's independence in their working and living environment. These devices, based on sensor technology and signal processing, are capable to improve the mobility of blind users (in terms of safety and speed), in unknown or dynamically changing environment. In particular, an integrated environment that improves the mobility of blind persons in to a limited area is presented. The proposed solution includes wearable equipment, placed on the subject, who guides the blind user to navigate autonomous with obstacles avoidance and stationary, monitoring equipment, which supervises the motion in order to avoid some unexpected events. The 3D obstacles detection system, included in the wearable equipment is bio-inspired, i.e. the system detects obstacles in a similar way as a subject with normal sight is looking for obstacles in front of him. The monitoring equipment, based on a GPS and a GSM/GPRS communication system, is capable to track the movement of a group of visually impaired, each of them moving on a specified pathway, in order to reach the desired target. The man-machine interface exploits the remarkable abilities of the human hearing system in identifying sound source positions in 3D space. The proposed solution relies on the Acoustic Virtual Reality (AVR) concept, which can be considered as a substitute for the lost sight of blind and visually impaired individuals. According to the AVR concept, the presence of obstacles in the surrounding environment and the path to the target will be signalized to the subject by burst of sounds, whose virtual source position suggests the position of the real obstacles and the direction of movement, respectively.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
  <title>Plenary lecture 4: work directions and new results in electronic travel aids for blind and visually impaired people</title>
	<abstract>
There are approximately 45 million blind &amp; visually impaired people world-wide according to the World Health Report. Vision loss limits the access of these individuals to the educational opportunities, social events, public transportation and leads to a higher rate of unemployment.

Many efforts have been invested in the last years, based on sensor technology and signal processing, to develop electronic travel aids (ETA) capable to improve the mobility of blind users in unknown or dynamically changing environment. In spite of these efforts, the already proposed ETAs do not meet the requirements of the blind community and the traditional tools (white cane and guiding dogs) are still the only used by visually impaired to navigate in their working and living environment.

In this paper, research efforts to improve the main two components of an ETA tool: the Obstacles Detection System (ODS) and the Man-machine Interface (MMI) are presented. Now, for the first time, the ODS under development is bioinspired from the visual system of insects, particularly from the Lobula Giant Motion Detector (LGMD) found in locusts. LGMD is a large neuron found in optical lobule of the locust, which mainly responds at the approaching objects. Starting from the mathematical model of the LGMD, known in the literature, it has been developed an ODS that can be used by visually impaired to navigate autonomously with obstacles avoidance. The already obtained results are very promising, but some improvements are also possible. We are developing now preprocessing algorithms for the visual information applied to the input of the LGMD neuron, in order to improve the response of the ODS. In the proposed solution, the position of the detected obstacles is correlated with the attitude parameters of the subject's head. In this way, the visually impaired person detects obstacles in a similar way as a subject with normal sight is looking for obstacles in front of him.

The man-machine interface developed in the present research exploits the remarkable abilities of the human hearing system in identifying sound source positions in 3D space. The proposed solution relies on the Acoustic Virtual Reality (AVR) concept, which can be considered as a substitute for the lost sight of blind and visually impaired individuals. According to the AVR concept, the presence of obstacles in the surrounding environment and the path to the target will be signalized to the subject by burst of sounds, whose virtual source position suggests the position of the real obstacles and the direction of movement, respectively. The practical implementation of this method encounters some difficulties due to the Head Related Transfer Functions (HRTF) which should be known for each individual and for a limited number of points in the 3D space. These functions can be determined using a quite complex procedure, which requires many experimental measurements. The proposed solution in our research avoids these difficulties by generating the HRTF's coefficients using an Artificial Neural Network (ANN). The ANN has been trained using a public data base, available for the whole scientific community and which contains HRTF's coefficients for a limited number of individuals and a limited number of points in 3D space for each individual.

The ODS and the MMI presented in the above have been implemented on a specific hardware build around an ARM-based microcontroller system. The obtained results and some conclusions are also presented.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
  <title>2nd Workshop on Smart Surveillance System Applications</title>
	<abstract>
Motivation and Justification:

Automatic recognition of people and their activities has very important social implications, because it is related to the extremely sensitive topic of civil liberties. Society needs to address this issue of automatic recognition and find a balanced solution that is able to meet its various needs and concerns. In the post 9/11 period, population security and safety considerations have given rise to research needs for identification of threatening human activities and emotional behaviours.

Timely identification of human intent is one of the most challenging areas of "all-hazards" risk assessment in the protection of critical infrastructure, business continuity planning and community safety. The "all-hazards" approach is used extensively by the public and private sector, including Public Safety Canada (PS Canada -- formerly PSEPC), Emergency Management Ontario (EMO), US Federal Emergency Management Agency (FEMA) and US Department of Homeland Security (DHS).

There is a clear need for industry and the research community to addresses fundamental issues involved in the prevention of human-made disasters, namely the variable context-dependent, real-time detection/identification of potential threatening behaviour of humans, acting individually or in crowded environments.

Such an industry and academia forum will have to discuss development and commercialization of new multimodal (video and infrared, voice and sound, RFID and perimeter intrusion) intelligent sensor technologies for location and socio-cultural context-aware security risk assessment and decision support in human-crowd surveillance applications in environments such as school campuses, hospitals, shopping centers, subways or railway stations, airports, sports and artistic arenas etc. Due to the complexity of the surveillance task there is a clear need for the development of a distributed intelligent surveillance system architecture, which combines visual and audio surveillance based on wireless sensor nodes equipped with video or infrared (IR) cameras, audio detectors, or other object detection and motion sensors with location aware wireless sensor network solutions. The integration of visual, sound and radio tracking methods results in a highly intelligent, proactive, and adaptive surveillance and security solution sensor networks. Task-directed sensor data collection and observation planning algorithms need to be developed to allow for a more elastic and efficient use of the inherently limited sensing and processing capabilities. Each task a sensor has to carry out determines the nature and the level of the information that is actually needed. There is a need for "selective environment perception" methods that focus on object parameters that are important for the specific decision to be made for the task at hand and avoid wasting effort to process irrelevant data.

Multisensor data fusion techniques should be investigated for the dynamic integration of the multi-thread flow of information provided by the heterogeneous network of surveillance sensors into a coherent multimodal model of the monitored human crowd.

In the context of crowds, robust tracking of people represents an important challenge. The numerous sources of occlusions and the large diversity of interactions that might occur make difficult the long-term tracking of a particular individual over an extended period of time and using a network of sensors. Realtime image processing and computer-vision algorithms need to be studied for the identification, tracking and recognition of gait and other relevant body-language patterns of the human agents who can be deemed of interest for security reasons. Real-time signal processing algorithms have to be designed for the identification and evaluation of environmental and human-subject multimodal parameters (such as human gait, gestures, facial emotions, human voice, background sound, ambient light, etc.) that provide the contextual information for the specific surveillance activity.

A multidisciplinary, context-aware, situation-assessment system, including human behaviour, cognitive psychology, multicultural sociology, learning systems, artificial intelligence, distributed multimedia and software design elements, has to be ultimately developed for the real-time evaluation of the activity and emotional behaviour of the human subjects identified as being potentially of security interest in the monitored dynamic environment.

The development of such a complex system requires the seamless integration of new and improved surveillance techniques and methodologies supporting both functional and non functional requirements for surveillance networks. Functional requirements are signal processing functions and data fusion, archiving and tracking human behaviours, assessment and interpretation functions of the data, and supporting human decision makers, among others. Non-functional requirements include interoperability, scalability, availability, and manageability.

The partial and heterogeneous sensor-views of the environment have to fuse into a coherent Virtualized Reality Environment (VRE) model of the explored environment. Being based on information about real/physical world objects and phenomena, as captured by a variety of sensors, VREs have more "real content" than the pure Virtual Reality environments entirely based on computer simulations. The VREs model of the explored environment allows human operators to combine their intrinsic reactive-behavior with higher-order world model representations of the immersive VRE systems.

A synthetic environment will eventually be needed to provide efficient multi granularity-level function-specific feedback and human-computer interaction interfaces for the human users who are the final assessors and decision makers in the specific security monitoring situation.

An ideal system should provide efficient multi granularity-level function-specific feedback for the human users who are the final assessors and decision makers in the specific security monitoring situation.

The rate at which surveillance systems can currently disseminate data to evaluate new threats is mainly limited due to the developed and implemented nature of existing systems and their limited ability to operate with other systems. IBM's Service-Oriented Architecture (SOA) provides the much needed deployment ready solution which supports the integration of external systems developed by diverse industrial and institutional partners.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
  <title>A versatile wire robot concept as a haptic interface for sport simulation</title>
	<abstract>
This paper presents the design of a new user-cooperative rope robot. This robot serves as a large-scale haptic interface in a multi-modal Cave environment used for sport simulation. In contrast to current rope robots, the configuration of the presented robot is adaptable to different simulation tasks what makes the robot more versatile. However, this adaptability and the high dynamics in sports lead to challenging requirements and specific design criteria of the hardware components. We present the requirements on the single robot components as well as the design of the entire setup optimized in terms of user-cooperativity and versatility. The setup includes sensors to measure the relevant parameters for user-cooperative control, i.e. position with a high resolution and the rope forces. Furthermore, an algorithm is introduced, which calculates the distance between the single ropes and the user in order to avoid collisions between the ropes and the user. Single points on the user's body are, therefore, tracked with a motion tracking system; the user's single body parts are then represented by geometrical objects whose distances to the ropes are calculated. The algorithm is programmed in such way that the collision detection runs in real-time. Both, the hardware and the algorithm, were evaluated experimentally in two applications, a rowing simulator and a tennis application. The hardware concept combined with the distance calculation allows the use of new kinematic concepts and expands the spectrum of realizable movement tasks that can be implemented into the Cave environment.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
  <title>Virtual sculpting and polyhedral machining planning system with haptic interface</title>
	<abstract>
This research proposes the methodology of a novel haptic sculpting and machining planning system for virtual prototyping and manufacturing. A lab-built 6-DOF (degree of freedom) input and 5-DOF output haptic interface system is utilized in the proposed haptic sculpting and machining planning system. A dexel-based haptic virtual prototyping CAD system and a triangulated surface-based machining planning (manufacturing) system are developed. A dexel-based collision detection method and a force-torque feedback analysis are proposed for virtual prototyping module. The output of the virtual prototyping module can be either STL polyhedral surface model, or the tool motion, which is recorded as NC (numerically-controlled) commands. Haptic interface is also used in the machining planning to help determine the feasible tool orientation for 5-axis NC tool path generation. A new machining strategy of 5-axis pencil-cut machining is proposed with the haptic interface. An OBB (object bounding box)-tree and point-cloud-based Two phase collision detection and force-torque feedback algorithm are proposed for virtual manufacturing module. Dexel-based method is developed for global tool interference avoidance with other components in a complex machining environment. To bridge the virtual prototyping module and machining planning module, a conversion marching algorithm is proposed to construct STL surface models from Dexel volume models. The algorithm can be used in both virtual prototyping system and NC simulation and verification. In the virtual sculpting module, a user can virtually sculpt a stock volume material intuitively by the haptic interface system. Hardware and software implementations of the haptic sculpting and machining planning system are also presented in this paper. The proposed methodology and developed haptic sculpting and machining planning system can be used in CAD/CAM systems and virtual prototyping.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
  <title>Dexel-based force-torque rendering and volume updating for 5-DOF haptic product prototyping and virtual sculpting</title>
	<abstract>
This paper presents new techniques of Dexel-based force-torque rendering and volume-updating for haptic virtual sculpting of complex surfaces with a developed 5-DOF (degree of freedom) haptic interface. In the proposed methodology, 5-axis tool motion and analytical tool swept volume are formulated for updating the virtual stock material, which is represented with the Dexel volume model. Based on the tool motion analysis, a Dexel-based collision detection method and a force-torque feedback algorithm are proposed for virtual sculpting. A lab-built 5-DOF force-torque output haptic interface system is developed for the proposed haptic sculpting system. With the proposed methodology, a user can virtually sculpt a volume stock to get an intuitive design by using the haptic interface. From the haptic sculpting system, both the corresponding tool motion of the creative process and the sculpted model can be recorded and output. The output STL models of the haptic sculpting system can be processed for machining planning. Based on the proposed techniques, hardware and software implementation of the haptic sculpting system as well as the illustrative examples are also presented in this paper.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
  <title>A Virtual 3D Blackboard: 3D Finger Tracking Using a Single Camera</title>
	<abstract>
We present a method for tracking the 3D position of a finger, using a single camera placed several meters away from the user. After skin detection, we use motion to identify the gesticulating arm. The finger point is found by analyzing the arm's outline. To derive a 3D trajectory, we first track 2D positions of the user's elbow and shoulder. Given that a human's upper arm and lower arm have consistent length, we observe that the possible locations of a finger and elbow form two spheres with constant radii. From the previously tracked body points, we can reconstruct these spheres, computing the 3D position of the elbow and finger. These steps are fully automated and do not require human intervention. The system presented can be used as a visualization tool, or as a user input interface, in cases when the user would rather not be constrained by the camera system.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
  <item>
  <title>A fast and robust face detection based on module switching network</title>
	<abstract>
We suggest a face detection method for video surveillance system to store a user's images fast and robustly. Our system consists of face and motion detectors. The face detector adopts a gentle AdaBoost algorithm to detect a face. Employing a module switching network, we extend the detectable facial pose range without loss of time. The motion detector, using temporal edges and temporal variance, decides whether a user exists when the face detector fails to detect a face. The proposed method, implemented in the form of software and a PCI interface card, displays 92.3% detection ratio in the test for CMU test DB.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
  <item>
  <title>Multimodal Speaker Detection Using Input/Output Dynamic Bayesian Networks</title>
	<abstract>
Inferring users' actions and intentions forms an integral part of design and development of any human-computer interface. The presence of noisy and at times ambiguous sensory data makes this problem challenging. We formulate a framework for temporal fusion of multiple sensors using input-output dynamic Bayesian networks (IODBNs). We find that contextual information about the state of the computer interface, used as an input to the DBN, and sensor distributions learned from data are crucial for good detection performance. Nevertheless, classical DBN learning methods can cause such models to fail when the data exhibits complex behavior. To further improve the detection rate we formulate an error-feedback learning strategy for DBNs. We apply this framework to the problem of audio/visual speaker detection in an interactive kiosk application using "off-the-shelf" visual and audio sensors (face, skin, texture, mouth motion, and silence detectors). Detection results obtained in this setup demonstrate numerous benefits of our learning-based framework.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
  <item>
  <title>Towards natural child-computer interaction: recognizing spoken communicative styles</title>
	<abstract>
The need for automatic recognition of user's communicative styles within a spoken dialog system framework has received increased attention with demand for computer interfaces that provide natural spoken interaction. This thesis addresses the design of automatic communicative-style recognition systems for computer interfaces that target children users. The specific focus is on recognizing two communicative aspects: (1) recognizing a child's emotional state during an interaction, and (2) detection of disfluencies in spontaneous speech of children. Knowledge about a child's emotional state and meta linguistic events such as disfluency helps system to understand child's communicative intent so that the interaction will be successful and more natural. We adopt a data-driven approach. An important requirement of most data-driven processing systems is the availability of transcribed and annotated data. This thesis also addresses the problems of multimodal data collection and annotation in the context of child's verbal and non-verbal interactions with the spoken dialog agent.

The first study deals with automatically detecting frustration and politeness attitudes from the child's speech communication cues and examines their differences as a function of age and gender. Several information sources such as acoustic, lexical, and discourse features as well as their combinations are used for this purpose. Results show that discourse and acoustic information have more discriminative power than language information for detection of frustration whereas language information is more discriminative for politeness. Results also point out the effects of age and gender on recognition performance.

The second study analyzes disfluencies in children's spontaneous speech in the context of spoken dialog based computer game play and addresses the automatic detection of disfluency boundaries by means of audio-visual information. Visual cues are obtained directly from the video sequence by using an optical flow technique in which the motion properties were estimated based on motion intensity changes between frames. The proposed algorithm improves the performance and robustness of the detection system over conventional approaches by utilizing visual information along with acoustic and language information.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
  <title>Moticons: detection, distraction and task</title>
	<abstract>
In this paper, we describe an empirical investigation of the utility of several perceptual properties of motion in information-dense displays applied to notification. Notification relates to awareness and how dynamic information is communicated from the system to the user. Key to a notification technique is how easily the notification is detected and identified. Our initial studies show that icons with simple motions, termed moticons, are effective coding techniques for notification and in fact are often better detected and identified than colour and shape codes, especially in the periphery. A subsequent experiment compared the detection and distraction effects of different motion types in several task conditions. Our resutts reveal how different attributes of motion contribute to detection, identification and distraction and provide initial guidelines on how motion codes can be designed to support awareness in information-rich interfaces while minimizing unwanted side effects of distraction and irritation.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
  <title>Fast and Robust Object Detection Framework in Compressed Domain</title>
	<abstract>
In this paper we propose a novel approach for fast and robust motion vector (MV) based object detection in MPEG-1 video streams. By processing the extracted MV fields that are directly extracted from MPEG-1/2 video streams in the compressed domain, through post processing operations, in order to reduce the noise within the MV content, obtain more robust object information, and refine this information through our proposed system which composed of a Spatial filter Component, a Temporal filter Component and a Texture filter component. As a result, the object detection algorithm is more capable of accurately detecting objects with more efficient performance in terms of runtime. We compare the performance of our proposed system with other popular and commonly related work and techniques. Based on the experimental results performed over the MPEG7 testing dataset and measuring performance by using the standard recall and precision metrics, object detection using our proposed system is remarkably superior to the alternative techniques. In addition to these results, we describe a user system interface that we developed, where users can maintain the parameters interactively.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
  <title>Design for peripheral vision</title>
	<abstract>
The visual interface for computer-performed tasks has typically been designed with the implicit assumption that the given task constitutes the single focus of the user's attention. This approach becomes problematic when users have to access multiple devices simultaneously. Switching attention between devices decreases performance or even causes safety problems. This thesis advocates that this problem can be alleviated if a visual interface is designed to be peripheral-vision-friendly. Such an interface is characterized as (a) being perceivable when situated away from one's gaze position, and (b) capable of being processed without impairing the on-going primary task. It is suggested that to achieve this goal, one could convey information through the periphery by changing attributes of motion. This argument is supported by a series of empirical studies, in which subjects had to detect and discriminate peripheral signals in a dual-task setting. Stemming from the experimental results, a proposed Peripheral Signal Detection Theory was able to predict the observed performance by computing a quantity called Transit Density, which is intended to be an index of peripheral salience. The concept of a Kinetic Pixel was then developed from the theory. A Kinetic Pixel functions as a fundamental element for peripheral visual designs; it enables Transit Density to be easily manipulated in order to reach a desired perceptual performance.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
  <title>Sensory-motor enhancement in a virtual therapeutic environment</title>
	<abstract>
The sensory-motor skills of persons with neuromuscular disabilities have been shown to be enhanced by intensive and repetitive therapeutic interventions. This paper describes a form of low immersion virtual reality and a prototype, open source system that allow a user with significant physical disability to actively interact with computer-generated objects whose behaviors promote a game-like interaction. Unlike fully immersive and haptic virtual reality, this approach frees the user from head-mounted displays and gloves. It extracts the user’s real-time silhouette from the output of a remote video camera and uses that two-dimensional outline to interact with graphical objects on screen. In contrast to video games that have been modified with specialized interfaces, this virtual interaction system promotes the repetitive use of goal directed movements of the arms and body, which are essential to promote cortical reorganization, as well as discourage unwanted changes in muscle tissue that result in contracture. A prototype system demonstrates the potential of low immersion technology to motivate users and encourage participation in therapy. It also offers the potential of accommodating the sensory-motor skills of individuals with very significant impairment. The behaviors of the computer-generated graphics can be altered to allow use by those with very limited range of motion and/or motor control. These behaviors can be adjusted to provide a continuing challenge as the user’s skills improve. This prototype is described in terms of functional capabilities that include a silhouette extraction from a video image, and generation of graphical objects that interact with the silhouette. The work is extended with a discussion of a more sophisticated region of interest detection algorithm that can select specific parts of the body.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
  <item>
  <title>Learning, detection, representation, indexing and retrieval of multi-agent events in videos</title>
	<abstract>
The world that we live in is a complex network of agents and their interactions which are termed as events. An instance of an event is composed of directly measurable low-level actions (which I term sub-events) having a temporal order. Also, the agents can act independently (e.g. voting) as well as collectively (e.g. scoring a touch-down in a football game) to perform an event. With the dawn of the new millennium, the low-level vision tasks such as segmentation, object classification, and tracking have become fairly robust. But a representational gap still exists between low-level measurements and high-level understanding of video sequences. This dissertation is an effort to bridge that gap where I propose novel learning, detection, representation, indexing and retrieval approaches for multi-agent events in videos.

In order to achieve the goal of high-level understanding of videos, firstly, I apply statistical learning techniques to model the multiple agent events. For that purpose, I use the training videos to model the events by estimating the conditional dependencies between sub-events. Thus, given a video sequence, I track the people (heads and hand regions) and objects using a Meanshift tracker. An underlying rule-based system detects the sub-events using the tracked trajectories of the people and objects, based on their relative motion. Next, an event model is constructed by estimating the sub-event dependencies, that is, how frequently sub-event B occurs given that sub-event A has occurred. The advantages of such an event model are two-fold. First, I do not require prior knowledge of the number of agents involved in an event. Second, no assumptions are made about the length of an event.

Secondly, after learning the event models, I detect events in a novel video by using graph clustering techniques. To that end, I construct a graph of temporally ordered sub-events occurring in the novel video. Next, using the learnt event model, I estimate a weight matrix of conditional dependencies between sub-events in the novel video. Further application of Normalized Cut (graph clustering technique) on the estimated weight matrix facilitate in detecting events in the novel video. The principal assumption made in this work is that the events are composed of highly correlated chains of sub-events that have high conditional dependency (association) within the cluster and relatively low conditional dependency (disassociation) between clusters.

Thirdly, in order to represent the detected events, I propose an extension of CASE representation of natural languages. I extend CASE to allow the representation of temporal structure between sub-events. Also, in order to capture both multi-agent and multi-threaded events, I introduce a hierarchical CASE representation of events in terms of sub-events and case-lists. The essence of the proposition is that, based on the temporal relationships of the agent motions and a description of its state, it is possible to build a formal description of an event. Furthermore, I recognize the importance of representing the variations in the temporal order of sub-events, that may occur in an event, and encode the temporal probabilities directly into my event representation. The proposed extended representation with probabilistic temporal encoding is termed P-CASE that allows a plausible means of interface between users and the computer. Using the P-CASE representation I automatically encode the event ontology from training videos. This offers a significant advantage, since the domain experts do not have to go through the tedious task of determining the structure of events by browsing all the videos.

Finally, I utilize the event representation for indexing and retrieval of events. Given the different instances of a particular event, I index the events using the P-CASE representation. Next, given a query in the P-CASE representation, event retrieval is performed using a two-level search. At the first level, a maximum likelihood estimate of the query event with the different indexed event models is computed. This provides the maximum matching event. At the second level, a matching score is obtained for all the event instances belonging to the maximum matched event model, using a weighted Jaccard similarity measure. Extensive experimentation was conducted for the detection, representation, indexing and retrieval of multiple agent events in videos of the meeting, surveillance, and railroad monitoring domains. To that end, the Semoran system was developed that takes in user inputs in any of the three forms for event retrieval: using pre-defined queries in P-CASE representation, using custom queries in P-CASE representation, or query by example video. The system then searches the entire database and returns the matched videos to the user. I used seven standard video datasets from the computer vision community as well as my own videos for testing the robustness of the proposed methods.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
  <title>Real-time vision-based hand tracking and gesture recognition</title>
	<abstract>
Hand gestures can be used for natural and intuitive human-computer interaction. To achieve this goal, computers should be able to visually recognize hand gestures from video input. However, vision-based hand tracking and gesture recognition is an extremely challenging problem due to the complexity of hand gestures, which are rich in diversities due to high degrees of freedom involved by the human hand. On the other hand, computer vision algorithms are notoriously brittle and computation intensive, which make most current gesture recognition systems fragile and inefficient.

This thesis proposes a new architecture to solve the problem of real-time vision-based hand tracking and gesture recognition with the combination of statistical and syntactic analysis. The fundamental idea is to use a divide-and-conquer strategy based on the hierarchical composition property of hand gestures so that the problem can be decoupled into two levels. The low-level of the architecture focuses on hand posture detection and tracking with Haar-like features and the AdaBoost learning algorithm. The Haar-like features can effectively catch the appearance properties of the hand postures. The AdaBoost learning algorithm can significantly speed up the performance and construct an accurate cascade of classifiers by combining a sequence of weak classifiers. To recognize different hand postures, a parallel cascades structure is implemented. This structure achieves real-time performance and high classification accuracy. The 3D position of the hand is recovered according to the camera's perspective projection. To make the system robust against cluttered backgrounds, background subtraction and noise removal are applied.

For the high-level hand gestures recognition, a stochastic context-free grammar (SCFG) is used to analyze the syntactic structure of the hand gestures with the terminal strings converted from the postures detected by the low-level of the architecture. Based on the similarity measurement and the probabilities associated with the production rules, given an input string, the corresponding hand gesture can be identified by looking for the production rule that has the greatest probability to generate this string. For the hand motion analysis, two SCFGs are defined to analyze two structured hand gestures with different trajectory patterns: the rectangle gesture and the diamond gesture. Based on the different probabilities associated with these two grammars, the SCFGs can effectively disambiguate the distorted trajectories and classify them correctly.

An application of gesture-based interaction with a 3D gaming virtual environment is implemented. With this system, the user can navigate the 3D gaming world by driving the avatar car with a set of hand postures. When the user wants to manipulate the virtual objects, he can use a set of hand gestures to select the target traffic sign and open a window to check the information of the correspondent learning object. This application demonstrates the gesture-based interface can achieve an improved interaction, which are more intuitive and flexible for the user.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
  <item>
  <title>The Office of the Past: Document Discovery and Tracking from Video</title>
	<abstract>
This paper presents an approach for reconstructing the physical state of documents and other objects on a desk over time using an overhead video camera. The history of the desktop is subsequently analyzed to enable a range of interesting queries. A desktop object is discovered when it first moves, and is tracked through the image sequence using change detection and motion segmentation techniques. The space-time structure of the scene is represented by a sequence of graphs where an "event", i.e., a change in the state of the scene, is modeled as a transition between subsequent graphs. We present two prototype applications that are based on these desktop reconstruction techniques. The first one allows the user to browse the structure and history of the desktop, and query documents of interest. The second application is a virtual desktop interface where the user can virtually manipulate the document stacks with mouse interaction. 
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>0</relevance>
  </item>
  <item>
  <title>An Algorithm for Real-Time Stereo Vision Implementation of Head Pose and Gaze Direction Measurement</title>
	<abstract>
To build smart human interfaces, it is necessary for a system to know a user's intention and point of attention. Since the motion of a person's head pose and gaze direction are deeply related with his/her intention and attention, detection of such information can be utilized to build natural and intuitive interfaces. In this paper, we describe our real-time stereo face tracking and gaze detection system to measure head pose and gaze direction simultaneously. The key aspect of our system is the use of real-time stereo vision together with a simple algorithm which is suitable for real-time processing. Since the 3D coordinates of the features on a face can be directly measured in our system, we can significantly simplify the algorithm for 3D model fitting to obtain the full 3D pose of the head compared with conventional systems that use monocular camera. Consequently we achieved a non-contact, passive, real-time, robust, accurate and compact measurement system for head pose and gaze direction.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>
  <item>
  <title>Tracking, Analysis, and Recognition of Human Gestures in Video</title>
	<abstract>
An overview of research in automated gesture spotting, tracking and recognition by the Image and Video Computing Group at Boston University is given. Approaches for localization and tracking human hands in video, estimation of hand shape and upper body pose, tracking head and facial motion, as well as efficient spotting and recognition of specific gestures in video streams are summarized. Methods for efficient dimensionality reduction of gesture time series, boosting of classifiers for nearest neighbor search in pose space, and model-based pruning of gesture alignment hypotheses are described. Algorithms are demonstrated in three domains: American Sign Language, hand signals like those employed by flight-directors on airport runways, and gesture-based interfaces for severely disabled users. The methods described are general and can be applied in other domains that require efficient detection and analysis of patterns in time-series, images or video.
	</abstract>
	<search_task_number>1</search_task_number>
	<query>motion detection user interface</query>
	<relevance>1</relevance>
  </item>


  <item>
    <title>3D scene analysis from a single range image through occlusion graphs</title>
    <abstract>This paper presents a new strategy to extract knowledge about the objects and their relative location in a complex scene when a single range image is taken. The analysis process is based on a range data distributed segmentation technique, which separates the components of the scene, and on a silhouette segmentation method, which classified the silhouette in real (non occluded) and false (occluded) parts. Finally, an occlusion graph provides a compact representation about the layout and relationship of the objects in the scene. This information is essential before higher level tasks in complex scenes - like recognition, understanding and robot interaction - are carried out. An extensive experimentation has been accomplished under real conditions in scenes of up to 12 objects yielding a very good performance. The experiments and results carried out validate the goodness of this approach in 3D environments.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A symbolic approach to polyhedral scene analysis by parametric calotte propagation</title>
    <abstract>Polyhedral scene analysis studies whether a 2D line drawing of a 3D polyhedron is realizable in space, and if so, it gives the results of parameterizing the space of all possible realizations. For generic 2D data, symbolic computation with Grassmann–Cayley algebra is needed in the analysis. In this paper, we propose a method called parametric calotte propagation to solve the realization and parameterization problems for general polyhedral scenes at the same time. In algebraic manipulation, parametric propagation is more efficient than elimination. In applications, it can lead to linear construction sequences for nonspherical polyhedra whose resolvable sequences do not exist.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Visual attention models for far-field scene analysis</title>
    <abstract>The amount of information available to an intelligent monitoring system is simply too vast to process in its entirety. One way to address this issue is by developing attentive mechanisms that recognize parts of the input as more interesting than others. We apply this concept to the domain of far-field activity analysis by addressing the problem of determining where to look in a scene in order to capture interesting activity in progress.

We pose the problem of attention as an unsupervised learning problem, in which the task is to learn from long-term observation a model of the usual pattern of activity. Such a statistical scene model then makes it possible to detect and attend to examples of unusual activity.

We present two data-driven scene modeling approaches. In the first, we model the pattern of individual observations (instances) of moving objects at each scene location as a mixture of Gaussians. In the second approach, we model the pattern of sequences of observations—tracks—by grouping there into clusters. We employ a similarity measure that combines comparisons of multiple attributes—such as size, position, and velocity—in a principled manner so that only tracks that are spatially similar and have similar attributes at spatially corresponding points are grouped together. We group the tracks using spectral clustering and represent the scene model as a mixture of Gaussians in the spectral embedding space. New examples of activity can be efficiently classified by projection into the embedding space.

We demonstrate clustering and unusual activity detection results on a week of activity in the scene (about 40,000 moving object tracks) and show that human perceptual judgments of unusual activity are well-correlated with the statistical model. The human validation suggests that the track-based anomaly detection framework would perform well as a classifier for unusual events. To our knowledge, our work is the first to evaluate a statistical scene modeling and anomaly detection framework against human judgments. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Computer vision for scene text analysis</title>
    <abstract>The motivation of this dissertation is to develop a ‘Seeing-Eye’ video-based interface for the visually impaired to access environmental text information. We are concerned with those daily activities of the low-vision people involved with interpreting ‘environmental text’ or ‘scene text’ e.g., reading a newspaper, can labels and street signs.

First, we discuss the development of such a video-based interface. In this interface, the processed image of a scene text is read by off-the-shelf OCR and converted back to speech by Text-to-Speech (TTS) software. Our challenge is to feed a high quality image of a scene text for off-the-shelf OCR software under general pose of the surface on which text is printed. To achieve this, various problems related to feature detection, mosaicing, auto-focus, zoom, and systems integration were solved in the development of the system, and these are described.

We employ the video-based interface for the analysis of video of lectures/posters. In this application, the text is assumed to be on a plane. It is necessary for automatic analysis of video content to add modules such as enhancement, text segmentation, preprocessing video content, metric rectification, etc. We provide qualitative results to justify the algorithm and system integration.

For more general classes of surfaces that the text is printed on, such as bent or worked paper, we develop a novel method for 3D structure recovery and unwarping method. Deformed paper is isometric with a plane and the Gaussian curvature vanishes on every point on the surface. We show that these constraints lead to a closed set of equations that allow the recovery of the full geometric structure from a single image. We prove that these partial differential equations can be reduced to the Hopf equation that arises in non-linear wave propagation, and deformations of the paper can be interpreted in terms of the characteristics of this equation. A new exact integration of these equations relates the 3D structure of the surface to an image of a paper. In addition, we can generate such surfaces using the underlying equations. This method only uses information derived from the image of the boundary.

Furthermore, we employ the shape-from-texture method as an alternative to the method above to infer its 3D structure. We showed that for the consistency of normal vector field, we need to add extra conditions based on the surface model. Such conditions are isometry and zero Gaussian curvature of the surface. (Abstract shortened by UMI.)</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Stereo Scene Flow for 3D Motion Analysis</title>
    <abstract>This book presents methods for estimating optical flow and scene flow motion with high accuracy, focusing on the practical application of these methods in camera-based driver assistance systems. Clearly and logically structured, the book builds from basic themes to more advanced concepts, culminating in the development of a novel, accurate and robust optic flow method. Features: reviews the major advances in motion estimation and motion analysis, and the latest progress of dense optical flow algorithms; investigates the use of residual images for optical flow; examines methods for deriving motion from stereo image sequences; analyses the error characteristics for motion variables, and derives scene flow metrics for movement likelihood and velocity; introduces a framework for scene flow-based moving object detection and segmentation; includes Appendices on data terms and quadratic optimization, and scene flow implementation using Euler-Lagrange equations, in addition to a helpful Glossary.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Scene analysis under variable illumination using gradient domain methods</title>
    <abstract>The goal of this research is to develop algorithms for reconstruction and manipulation of gradient fields for scene analysis, from intensity images captured under variable illumination. These methods utilize gradients or differential measurements of intensity and depth for analyzing a scene, such as estimating shape and intrinsic images, and edge suppression under variable illumination. The differential measurements lead to robust reconstruction from gradient fields in the presence of outliers and avoid hard thresholds and smoothness assumptions in manipulating image gradient fields.

Reconstruction from gradient fields is important in several applications including shape extraction using Photometric Stereo and Shape from Shading, image editing and matting, retinex, mesh smoothing and phase unwrapping. In these applications, a non-integrable gradient field is available, which needs to be integrated to obtain the final image or surface. Previous approaches for enforcing integrability have focused on least square solutions which do not work well in the presence of outliers and do not locally confine errors during reconstruction. I present a generalized equation to represent a continuum of surface reconstructions of a given non-integrable gradient field. This equation is used to derive new types of feature preserving surface reconstructions in the presence of noise and outliers. The range of solutions is related to the degree of anisotropy of the weights applied to the gradients in the integration process.

Traditionally, image gradient fields have been manipulated using hard thresholds for recovering reflectance/illumination maps or to remove illumination effects such as shadows. Smoothness of reflectance/illumination maps is often assumed in such scenarios. By analyzing the direction of intensity gradient vectors in images captured under different illumination conditions, I present a framework for edge suppression which avoids hard thresholds and smoothness assumptions. This framework can be used to manipulate image gradient fields to synthesize computationally useful and visually pleasing images, and is based on two approaches: (a) gradient projection and (b) affine transformation of gradient fields using cross-projection tensors. These approaches are demonstrated in the context of several applications such as removing shadows and glass reflections, and recovering reflectance/illumination maps and foreground layers under varying illumination.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Deriving implicit indoor scene structure with path analysis</title>
    <abstract>Indoor video surveillance is now widely used in government, public, and private facilities. While the capacity to generate such video data is increasing, our ability to derive a coherent scene understanding of the structure of the scene and how it is being utilized, using only motion data, is still lagging behind. This paper proposes a framework for deriving indoor scene structure identifying abnormal motion behavior using only video tracking data, and without requiring a floor plan. The proposed framework, which is data-driven, is based on four sequential processing steps, namely detection of entrance and exit points, the analysis of the connectivity between entrance and exit points, the extraction of mean paths and motion corridors, and the statistical analysis of the length and velocity parameters of motion for the detection of abnormal motion behavior. The paper outlines the proposed framework and demonstrates its implementation using a real-world data set comprising 1138 trajectories.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Multimodal conversation scene analysis for understanding people's communicative behaviors in face-to-face meetings</title>
    <abstract>This presentation overviews our recent progress in multimodal conversation scene analysis, and discusses its future in terms of designing better human-to-human communication systems. Conversation scene analysis aims to provide the automatic description of conversation scenes from the multimodal nonverbal behaviors of participants as captured by cameras and microphones. So far, the author's group has proposed a research framework based on the probabilistic modeling of conversation phenomena for solving several basic problems including speaker diarization, i.e. "who is speaking when", addressee identification, i.e. "who is talking to whom", interaction structure, i.e. "who is responding to whom", the estimation of visual focus of attention (VFOA), i.e. "who is looking at whom", and the inference of interpersonal emotion such as "who has empathy/antipathy with whom", from observed multimodal behaviors including utterances, head pose, head gestures, eye-gaze, and facial expressions. This paper overviews our approach and discusses how conversation scene analysis can be extended to enhance the design process of computer-mediated communication systems.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Polyhedral scene analysis combining parametric propagation with calotte analysis</title>
    <abstract>Polyhedral scene analysis studies whether a 2D line drawing of a 3D polyhedron is realizable in the space, and if so, parameterizing the space of all possible realizations. For generic 2D data, symbolic computation with Grassmann-Cayley algebra is necessary. In this paper we propose a general method, called parametric calotte propagation, to solve the realization and parameterization problems in polyhedral scene analysis at the same time. Starting with the fundamental equations of Sugihara in the form of bivector equations, we can parameterize all the bivectors by introducing new parameters. The realization conditions are implied in the scalar equations satisfied by the new parameters, and can be derived by further analysis of the propagation result. The propagation procedure generally does not bifurcate, and the result often contains equations in factored form, thus makes further algebraic manipulation easier. In application, the method can be used to find linear construction sequences for non-spherical polyhedra.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Sound classification in hearing aids inspired by auditory scene analysis</title>
    <abstract>A sound classification system for the automatic recognition of the acoustic environment in a hearing aid is discussed. The system distinguishes the four sound classes "clean speech," "speech in noise," "noise," and "music." A number of features that are inspired by auditory scene analysis are extracted from the sound signal. These features describe amplitude modulations, spectral profile, harmonicity, amplitude onsets, and rhythm. They are evaluated together with different pattern classifiers. Simple classifiers, such as rule-based and minimum-distance classifiers, are compared with more complex approaches, such as Bayes classifier, neural network, and hidden Markov model. Sounds from a large database are employed for both training and testing of the system. The achieved recognition rates are very high except for the class "speech in noise." Problems arise in the classification of compressed pop music, strongly reverberated speech, and tonal or fluctuating noises.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>A computational auditory scene analysis system for speech segregation and robust speech recognition</title>
    <abstract>A conventional automatic speech recognizer does not perform well in the presence of multiple sound sources, while human listeners are able to segregate and recognize a signal of interest through auditory scene analysis. We present a computational auditory scene analysis system for separating and recognizing target speech in the presence of competing speech or noise. We estimate, in two stages, the ideal binary time-frequency (T-F) mask which retains the mixture in a local T-F unit if and only if the target is stronger than the interference within the unit. In the first stage, we use harmonicity to segregate the voiced portions of individual sources in each time frame based on multipitch tracking. Additionally, unvoiced portions are segmented based on an onset/offset analysis. In the second stage, speaker characteristics are used to group the T-F units across time frames. The resulting masks are used in an uncertainty decoding framework for automatic speech recognition. We evaluate our system on a speech separation challenge and show that our system yields substantial improvement over the baseline performance. </abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Foreground auditory scene analysis for hearing aids</title>
    <abstract>Although a wide variety of signal enhancement algorithms are available for hearing aids, selection and parameterization of the best algorithm at any given time is highly dependent upon the environment of the hearing aid user. The use of auditory scene analysis has been proposed by several groups to categorize the background noise. In this work, an algorithm is proposed to categorize a foreground speaker as opposed to the background noise and parameterize a frequency-based compression algorithm which has been previously shown to improve speech understanding for some individuals with severe sensorineural hearing loss in the 2-3kHz range. </abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Object tracking in crowded video scenes based on the undecimated wavelet features and texture analysis</title>
    <abstract>We propose a new algorithm for object tracking in crowded video scenes by exploiting the properties of undecimated wavelet packet transform (UWPT) and interframe texture analysis. The algorithm is initialized by the user through specifying a region around the object of interest at the reference frame. Then, coefficients of the UWPT of the region are used to construct a feature vector (FV) for every pixel in that region. Optimal search for the best match is then performed by using the generated FVs inside an adaptive search window. Adaptation of the search window is achieved by interframe texture analysis to find the direction and speed of the object motion. This temporal texture analysis also assists in tracking of the object under partial or short-term full occlusion. Moreover, the tracking algorithm is robust to Gaussian and quantization noise processes. Experimental results show that the proposed algorithm has good performance for object tracking in crowded scenes on stairs, in airports, or at train stations in the presence of object translation, rotation, small scaling, and occlusion.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Xetal-II: A Low-Power Massively-Parallel Processor for Video Scene Analysis</title>
    <abstract>A processor architecture combining high-performance and low-power is presented. A prototype chip, Xetal-II, has been realized in 90 nm CMOS technology based on the proposed architecture. Recent experimental results show a compute performance of up to 140 GOPS at 785 mW when operating at 110 MHz. The main architectural feature that allows high computational efficiency is the massively-parallel single-instruction multiple-data (MP-SIMD) compute paradigm. Due to the high data-level parallelism, applications like video scene analysis can efficiently exploit the proposed architecture. The chip has an internal 16-bit datapath and 10 Mbit of on-chip video memory facilitating energy efficient implementation of video processing kernels.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Real world scene analysis in perspective</title>
    <abstract>This paper examines the applicability of current scene analysis techniques to real world problems. The majority of the current techniques have been developed for simple scenes with straight lines, simple shapes, good contrast, and little texture. This paper shows several examples illustrating that many of these techniques are directly applicable to real world problems, particularly the schemes for finding primitive scene information. It is also shown, however, that the achievement of significant real world analysis requires further development of the representations of complex shapes and texture, decision processes specific to spatial problems and realistic control techniques that utilize every piece of available information. Finally, the concept of a full system is advocated as a paradigm that has more promise than the past piecemeal approach.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Unsupervised scene analysis: a hidden Markov model approach</title>
    <abstract>This paper presents a new approach to scene analysis, which aims at extracting structured information from a video sequence using directly low-level data. The method models the sequence using a forest of Hidden Markov models (HMMs), which are able to extract two kinds of data, namely, static and dynamic information. The static information results in a segmentation that explains how the chromatic aspect of the static part of the scene evolves. The dynamic information results in the detection of the areas which are more affected by foreground activity. The former is obtained by a spatial clustering of HMMs, resulting in a spatio-temporal segmentation of the video sequence, which is robust to noise and clutter and does not consider the possible moving objects in the scene. The latter is estimated using an entropy-like measure defined on the stationary probability of the Markov chain associated to the HMMs, producing a partition of the scene in activity zones in a consistent and continuous way. The proposed approach constitutes a principled unified probabilistic framework for low level scene analysis and understanding, showing several key features with respect to the state of the art methods, as it extracts information at the lowest possible level (using only pixel gray-level temporal behavior), and is unsupervised in nature. The obtained results on real sequences, both indoor and outdoor, show the efficacy of the proposed approach.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>From surfaces to objects: computer vision and three dimensional scene analysis
</title>
    <abstract>An abstract is not available. </abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Robust image thresholding techniques for automated scene analysis</title>
    <abstract>An abstract is not available. </abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Probabilistic scene analysis of two dimensional images
</title>
    <abstract>An abstract is not available. </abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A Model of Saliency-Based Visual Attention for Rapid Scene Analysis
</title>
    <abstract>A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Analysis of cluttered scenes using an elastic matching approach for stereo images
</title>
    <abstract>We present a system for the automatic interpretation of cluttered scenes containing multiple partly occluded objects in front of unknown, complex backgrounds. The system is based on an extended elastic graph matching algorithm that allows the explicit modeling of partial occlusions. Our approach extends an earlier system in two ways. First, we use elastic graph matching in stereo image pairs to increase matching robustness and disambiguate occlusion relations. Second, we use richer feature descriptions in the object models by integrating shape and texture with color features. We demonstrate that the combination of both extensions substantially increases recognition performance. The system learns about new objects in a simple one-shot learning approach. Despite the lack of statistical information in the object models and the lack of an explicit background model, our system performs surprisingly well for this very difficult task. Our results underscore the advantages of view-based feature constellation representations for difficult object recognition problems.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Using articulated scene models for dynamic 3d scene analysis in vista spaces
</title>
    <abstract>In this paper we describe an efficient but detailed new approach to analyze complex dynamic scenes directly in 3D. The arising information is important for mobile robots to solve tasks in the area of household robotics. In our work a mobile robot builds an articulated scene model by observing the environment in the visual field or rather in the so-called vista space. The articulated scene model consists of essential knowledge about the static background, about autonomously moving entities like humans or robots and finally, in contrast to existing approaches, information about articulated parts. These parts describe movable objects like chairs, doors or other tangible entities, which could be moved by an agent. The combination of the static scene, the self-moving entities and the movable objects in one articulated scene model enhances the calculation of each single part. The reconstruction process for parts of the static scene benefits from removal of the dynamic parts and in turn, the moving parts can be extracted more easily through the knowledge about the background. In our experiments we show, that the system delivers simultaneously an accurate static background model, moving persons and movable objects. This information of the articulated scene model enables a mobile robot to detect and keep track of interaction partners, to navigate safely through the environment and finally, to strengthen the interaction with the user through the knowledge about the 3D articulated objects and 3D scene analysis.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Unsupervised scene analysis: A hidden Markov model approach
</title>
    <abstract>This paper presents a new approach to scene analysis, which aims at extracting structured information from a video sequence using directly low-level data. The method models the sequence using a forest of Hidden Markov models (HMMs), which are able to extract two kinds of data, namely, static and dynamic information. The static information results in a segmentation that explains how the chromatic aspect of the static part of the scene evolves. The dynamic information results in the detection of the areas which are more affected by foreground activity. The former is obtained by a spatial clustering of HMMs, resulting in a spatio-temporal segmentation of the video sequence, which is robust to noise and clutter and does not consider the possible moving objects in the scene. The latter is estimated using an entropy-like measure defined on the stationary probability of the Markov chain associated to the HMMs, producing a partition of the scene in activity zones in a consistent and continuous way. The proposed approach constitutes a principled unified probabilistic framework for low level scene analysis and understanding, showing several key features with respect to the state of the art methods, as it extracts information at the lowest possible level (using only pixel gray-level temporal behavior), and is unsupervised in nature. The obtained results on real sequences, both indoor and outdoor, show the efficacy of the proposed approach.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Video scene analysis of interactions between humans and vehicles using event context
</title>
    <abstract>We present a methodology to estimate a detailed state of a video scene involving multiple humans and vehicles. In order to automatically annotate and retrieve videos containing activities of humans and vehicles, the system must correctly identify their trajectories and relationships even in a complex dynamic environment. Our methodology constructs various joint 3-D models describing possible configurations of humans and vehicles in each image frame and performs maximum-a-posteriori tracking to obtain a sequence of scene states that matches the video. Reliable and view-independent scene state analysis is performed by taking advantage of event context. We focus on the fact that events occurring in a video must contextually coincide with scene states of humans and vehicles. Our experimental results verify that our system using event context is able to analyze and track 3-D scene states of complex human-vehicle interactions more reliably and accurately than previous systems.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A heuristic algorithm for video scene detection using shot cluster sequence analysis
</title>
    <abstract>In this paper, we present a novel scheme for segmenting video data into scenes. Based on visual similarity, the shots are first classified into clusters using modified k-means algorithm. Number of optimal clusters is decided using cluster validity analysis based on Davies-Bouldin index. Each shot is assigned a tag denoting the cluster it belongs to. Thus, the video data is represented by a sequence of cluster tags. The sequence is then analyzed by introducing the concept of stable and quasi-stable state. The elements of the sequence are merged into states and isolated elements are linked with the states to generate the scenes. The scheme is free from the dependency on critical parameters and capable of handling different types of scenes.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Projection defocus analysis for scene capture and image display
</title>
    <abstract>In order to produce bright images, projectors have large apertures and hence narrow depths of field. In this paper, we present methods for robust scene capture and enhanced image display based on projection defocus analysis. We model a projector's defocus using a linear system. This model is used to develop a novel temporal defocus analysis method to recover depth at each camera pixel by estimating the parameters of its projection defocus kemel in frequency domain. Compared to most depth recovery methods, our approach is more accurate near depth discontinuities. Furthermore, by using a coaxial projector-camera system, we ensure that depth is computed at all camera pixels, without any missing parts. We show that the recovered scene geometry can be used for refocus synthesis and for depth-based image composition. Using the same projector defocus model and estimation technique, we also propose a defocus compensation method that filters a projection image in a spatially-varying, depth-dependent manner to minimize its defocus blur after it is projected onto the scene. This method effectively increases the depth of field of a projector without modifying its optics. Finally, we present an algorithm that exploits projector defocus to reduce the strong pixelation artifacts produced by digital projectors, while preserving the quality of the projected image. We have experimentally verified each of our methods using real scenes.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>A belief-based architecture for scene analysis: From sensorimotor features to knowledge and ontology
</title>
    <abstract>Based on the neurobiological and cognitive principles of human information processing, we develop a system for the automatic visual identification and exploration of scenes. The system architecture consists of three layers: a bottom-up feature extraction stage, a top-down object identification stage and knowledge from a domain ontology for scene analysis. The uncertainty in the latter two stages is managed by Dempster-Shafer belief measures. The system sequentially selects ''informative'' image regions, identifies the local structure in these regions, and uses this information for drawing efficient conclusions about an object in the scene. The selection process involves low-level, bottom-up processes for sensory feature extraction, and cognitive top-down processes for the generation of active motor commands that control the positioning of the sensors towards the most informative regions. Both processing levels have to deal with uncertain data, and have to take into account learned statistical knowledge. For bottom-up feature extraction this is achieved by integrating a nonlinear filtering stage modeled after the neural computations performed in the early stages of the visual system. The top-down cognitive reasoning strategy operates in an adaptive fashion on a belief distribution. The resulting object hypotheses in combination with knowledge from the domain ontology in the third layer are used for generating a scene hypothesis.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Non-goal scene analysis for soccer video
</title>
    <abstract>The broadcast soccer video is usually recorded by one main camera, which is constantly gazing somewhere of playfield where a highlight event is happening. So the camera parameters and their variety have close relationship with semantic information of soccer video, and much interest has been caught in camera calibration for soccer video. The previous calibration methods either deal with goal scene, or have strict calibration conditions and high complexity. So, it does not properly handle the non-goal scene such as midfield or center-forward scene. In this paper, based on a new soccer field model, a field symbol extraction algorithm is proposed to extract the calibration information. Then a two-stage calibration approach is developed which can calibrate camera not only for goal scene but also for non-goal scene. The preliminary experimental results demonstrate its robustness and accuracy.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Important scene analysis model using result importance and situation importance
</title>
    <abstract>Functional improvements to mobile terminals have made possible the spread of multimedia information delivery services. Especially for sports, a service that extracts important scenes and reports them to users has become popular. We take the approach that situational changes in a game's progress can be presented as state transitions, and devise an important scene analysis model based on Result Importance (RI), which represents each situation's actual importance, and Situation Importance (SI), which shows the degree of a chance at the situation. With our model, significant scenes are extracted dynamically. Experimental results indicate that the proposed method is very promising.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>The Tractability of Segmentation and Scene Analysis
</title>
    <abstract>One of the fundamental problems in computer vision is the segmentation of an image into semantically meaningful regions, based only on image characteristics. A single segmentation can be determined using a linear number of evaluations of a uniformity predicate. However, minimising the number of regions is shown to be an NP-complete problem. We also show that the variational approach to segmentation, based on minimising a criterion combining the overall variance of regions and the number of regions, also gives rise to an NP-complete problem.

When a library of object models is available, segmenting the image becomes a problem of scene analysis. A sufficient condition for the reconstruction of a 3D scene from a 2D image to be solvable in polynomial time is that the scene contains no cycles of mutually occluding objects and that no range information can be deduced from the image. It is known that relaxing the no cycles condition renders the problem NP-complete. We show that relaxing the no range information condition also produces an NP-complete problem.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Effects of Attentional Load on Auditory Scene Analysis
</title>
    <abstract>The effects of attention on the neural processes underlying auditory scene analysis were investigated through the manipulation of auditory task load. Participants were asked to focus their attention on tuned and mistuned stimuli presented to one ear and to ignore similar stimuli presented to the other ear. For both tuned and mistuned sounds, long (standard) and shorter (deviant) duration stimuli were presented in both ears. Auditory task load was manipulated by varying task instructions. In the easier condition, participants were asked to press a button for deviant sounds (target) at the attended location, irrespective of tuning. In the harder condition, participants were further asked to identify whether the targets were tuned or mistuned. Participants were faster in detecting targets defined by duration only than by both duration and tuning. At the unattended location, deviant stimuli generated a mismatch negativity wave at frontocentral sites whose amplitude decreased with increasing task demand. In comparison, standard mistuned stimuli generated an object-related negativity at central sites whose amplitude was not affected by task difficulty. These results show that the processing of sound sequences is differentially affected by attentional load than is the processing of sounds that occur simultaneously (i.e., sequential vs. simultaneous grouping processes), and that they each recruit distinct neural networks.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>From motion patterns to visual concepts for event analysis in dynamic scenes
</title>
    <abstract>The analysis of events in dynamic scenes has become an important and challenging problem increasingly in recent years. Events can be considered as obvious changes of important features with semantic meanings. From this viewpoint, the fundamental task of events analysis is to extract semantically meaningful changes and associate all of these basic motion patterns and changes with relevant visual concepts of moving objects in dynamic scenes. In this paper, we propose a method to extract lower level motion patterns and associate them with visual concepts respectively in a well-defined structure. Furthermore we also analyze latent spatial-temporal relationships among these basic visual concepts for event modeling and analysis. Finally, we present experimental results which prove the effectiveness of our approach on some real-world videos of dynamic scenes.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Sequential organization of speech in computational auditory scene analysis
</title>
    <abstract>A human listener has the ability to follow a speaker's voice over time in the presence of other talkers and non-speech interference. This paper proposes a general system for sequential organization of speech based on speaker models. By training a general background model, the proposed system is shown to function well with both interfering talkers and non-speech intrusions. To deal with situations where prior information about specific speakers is not available, a speaker quantization method is employed to extract representative models from a large speaker space and obtained generic models are used to perform sequential grouping. Our systematic evaluations show that grouping performance using generic models is only moderately lower than the performance level achieved with known speaker models.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>A computational auditory scene analysis-enhanced beamforming approach for sound source separation</title>
    <abstract>Hearing aid users have difficulty hearing target signals, such as speech, in the presence of competing signals or noise. Most solutions proposed to date enhance or extract target signals from background noise and interference based on either location attributes or source attributes. Location attributes typically involve arrival angles at a microphone array. Source attributes include characteristics that are specific to a signal, such as fundamental frequency, or statistical properties that differentiate signals. This paper describes a novel approach to sound source separation, called computational auditory scene analysis-enhanced beamforming (CASA-EB), that achieves increased separation performance by combining the complementary techniques of CASA (a source attribute technique) with beamforming (a location attribute technique), complementary in the sense that they use independent attributes for signal separation. CASA-EB performs sound source separation by temporally and spatially filtering a multichannel input signal, and then grouping the resulting signal components into separated signals, based on source and location attributes. Experimental results show increased signal-to-interference ratio with CASA-EB over beamforming or CASA alone.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Scene-level analysis for tennis sports video using weighted linear combination of visual cues
</title>
    <abstract>We present a scene-level analysis system that takes TV broadcasted tennis footage as input and produces a behavior analysis of the moving-players in the scene. To achieve this functionality, our algorithm relies on two modular blocks. The first one detects and tracks a number of key objects in the image domain, like moving-players and the playing-field. Afterwards, a camera-calibration algorithm is applied that uses the lines of the court as a reference and transforms image coordinates to physical positions to compute the camera parameters. The second block firstly models several important events of a tennis game, such as service and net-approach, based on four real-world visual features provided by the first block. This paper proposes a new improved model, since it weights the importance of each visual cue to different events, rather than providing a simple combination of these four visual cues as was done in previous work. Based on the new model, we can accurately determine what kind of event the current input frame belongs to. Furthermore, we detect the start time and end time of each event using a simple but efficient temporal filter. Our proposed system is capable of classifying each tennis play into three semantic categories, which are popular and familiar to most viewers. This paper presents details of the system, together with results on a number of tennis video sequences.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Learning dictionaries of stable autoregressive models for audio scene analysis</title>
    <abstract>In this paper, we explore an application of basis pursuit to audio scene analysis. The goal of our work is to detect when certain sounds are present in a mixed audio signal. We focus on the regime where out of a large number of possible sources, a small but unknown number combine and overlap to yield the observed signal. To infer which sounds are present, we decompose the observed signal as a linear combination of a small number of active sources. We cast the inference as a regularized form of linear regression whose sparse solutions yield decompositions with few active sources. We characterize the acoustic variability of individual sources by autoregressive models of their time domain waveforms. When we do not have prior knowledge of the individual sources, the coefficients of these autoregressive models must be learned from audio examples. We analyze the dynamical stability of these models and show how to estimate stable models by substituting a simple convex optimization for a difficult eigenvalue problem. We demonstrate our approach by learning dictionaries of musical notes and using these dictionaries to analyze polyphonic recordings of piano, cello, and violin.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>A Lagrangian Half-Quadratic approach to robust estimation and its applications to road scene analysis
</title>
    <abstract>We consider the problem of fitting linearly parameterized models, that arises in many computer vision problems such as road scene analysis. Data extracted from images usually contain non-Gaussian noise and outliers, which makes non-robust estimation methods ineffective. In this paper, we propose an overview of a Lagrangian formulation of the Half-Quadratic approach by, first, revisiting the derivation of the well-known Iterative Re-weighted Least Squares (IRLS) robust estimation algorithm. Then, it is shown that this formulation helps derive the so-called Modified Residuals Least Squares (MRLS) algorithm. In this framework, moreover, standard theoretical results from constrained optimization can be invoked to derive convergence proofs easier. The interest of using the Lagrangian framework is also illustrated by the extension to the problem of the robust estimation of sets of linearly parameterized curves, and to the problem of robust fitting of linearly parameterized regions. To demonstrate the relevance of the proposed algorithms, applications to lane markings tracking, road sign detection and recognition, road shape fitting and road surface 3D reconstruction are presented.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>An automatic speech recognition system based on the scene analysis account of auditory perception
</title>
    <abstract>Despite many years of concentrated research, the performance gap between automatic speech recognition (ASR) and human speech recognition (HSR) remains large. The difference between ASR and HSR is particularly evident when considering the response to additive noise. Whereas human performance is remarkably robust, ASR systems are brittle and only operate well within the narrow range of noise conditions for which they were designed. This paper considers how humans may achieve noise robustness. We take the view that robustness is achieved because the human perceptual system treats the problems of speech recognition and sound source separation as being tightly coupled. Taking inspiration from Bregman's Auditory Scene Analysis account of auditory organisation, we present a speech recognition system which couples these processes by using a combination of primitive and schema-driven processes: first, a set of coherent spectro-temporal fragments is generated by primitive segmentation techniques; then, a decoder based on statistical ASR techniques performs a simultaneous search for the correct background/foreground segmentation and word sequence hypothesis. Mutually supporting solutions to both the source segmentation and speech recognition problems arise as a result. The decoder is tested on a challenging corpus of connected digit strings mixed monaurally at 0dB and recognition performance is compared with that achieved by listeners using identical data. The results, although preliminary, are encouraging and suggest that techniques which interface ASA and statistical ASR have great potential. The paper concludes with a discussion of future research directions that may further develop this class of perceptually motivated ASR solutions.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Computational Auditory Scene Analysis: Principles, Algorithms, and Applications</title>
    <abstract>An abstract is not available.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Dynamic 3-D Scene Analysis Through Synthesis Feedback Control
</title>
    <abstract>The analysis of 3-D scenes consisting of nonrigid moving objects using 2-D image sequences is discussed. A parametric description of dynamic objects is extracted, and the time-variant scene parameters are estimated throughout the sequence by employing an analysis-by-synthesis approach. Images are synthesized from the parametric scene description and compared with the original images input to the camera. Frame differences between the synthesized and original images are evaluated to obtain an estimated scene parameter update. The analysis method is applied to videophone scenes in the form of a data compression algorithm, whereby the scene parameters are transmitted and the output sequence is synthesized at the receiver.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Clustering Appearance for Scene Analysis
</title>
    <abstract>We propose a new approach called "appearance clustering" for scene analysis. The key idea in this approach is that the scene points can be clustered according to their surface normals, even when the geometry, material and lighting are all unknown. We achieve this by analyzing an image sequence of a scene as it is illuminated by a smoothly moving distant source. Each pixel thus gives rise to a "continuous appearance profile" that yields information about derivatives of the BRDF w.r.t source direction. This information is directly related to the surface normal of the scene point when the source path follows an unstructured trajectory (obtained, say, by "hand-waving"). Based on this observation, we transform the appearance profiles and propose a metric that can be used with any unsupervised clustering algorithm to obtain iso-normal clusters. We successfully demonstrate appearance clustering for complex indoor and outdoor scenes. In addition, iso-normal clusters serve as excellent priors for scene geometry and can strongly impact any vision algorithm that attempts to estimate material, geometry and/or lighting properties in a scene from images. We demonstrate this impact for applications such as diffuse and specular separation, both calibrated and uncalibrated photometric stereo of non-lambertian scenes, light source estimation and texture transfer.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Sequential organization in computational auditory scene analysis
</title>
    <abstract>A human listener has the ability to follow a speaker’s voice while others are speaking simultaneously. In particular, the listener can organize the time-frequency (T-F) energy of the same speaker into a single stream. This aspect of auditory perception is termed auditory scene analysis (ASA). ASA comprises two organization processes: segmentation and grouping. Segmentation decomposes the auditory scene into T-F segments. Grouping combines the segments from the same source into a single perceptual stream. Within the grouping process, simultaneous organization integrates segments that overlap in time, and sequential organization groups segments across time.

Inspired by ASA research, computational auditory scene analysis (CASA) aims to organize sound based on ASA principles. CASA systems seek to segregate target speech from a complex auditory scene. However, almost all the existing systems focus on simultaneous organization. This dissertation presents a systematic effort on sequential organization. The goal is to organize T-F segments from the same speaker that are separated in time into a single stream. This study proposes to employ speaker characteristics for sequential organization.

This study first explores bottom-up methods for sequential grouping. Subsequently, a speaker-model-based sequential organization framework is proposed and shown to yield better grouping performance than feature-based methods. Specifically, a computational objective is derived for sequential grouping in the context of cochannel speaker recognition. Cochannel speech occurs when two utterances are transmitted in a single communication channel. This formulation leads to a grouping system that searches for the optimal grouping of separated speech segments. To reduce search space and computation time, a hypothesis pruning method is then proposed and it achieves performance close to that of exhaustive search. Systematic evaluations show that the proposed system improves not only grouping performance but also speech recognition accuracy.

The model-based grouping system is then extended to handle multi-talker as well as non-speech intrusions using generic models. This generalization is shown to function well regardless of interference types and the number of interfering sources. The grouping system is further extended to deal with noisy inputs from unknown speakers. Specifically, it employs a speaker quantization method that extracts representative speakers from a large speaker space and performs sequential grouping using obtained generic models. The resulting grouping performance is only moderately lower than that with known speaker models.

In addition to sequential grouping, this dissertation presents a systematic effort in robust speaker recognition. A novel usable speech extraction method is proposed that significantly improves recognition performance. Then, missing-data recognition is combined with the use of CASA as a front-end processor. Substantial performance improvements are achieved in speaker recognition evaluations under various noisy conditions. Finally, a general solution is proposed for robust speaker recognition in the presence of additive noise. Novel speaker features are derived from auditory filtering and cepstral analysis, and are used in conjunction with an uncertainty decoder that accounts for mismatch introduced in front-end processing. Systematic evaluations show that the proposed system achieves significant performance improvement over the use of typical speaker features and a state-of-the-art robust front-end processor for noisy speech.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>ARTSTREAM: a neural network model of auditory scene analysis and source segregation</title>
    <abstract>Multiple sound sources often contain harmonics that overlap and may be degraded by environmental noise. The auditory system is capable of teasing apart these sources into distinct mental objects, or streams. Such an 'auditory scene analysis' enables the brain to solve the cocktail party problem. A neural network model of auditory scene analysis, called the ARTSTREAM model, is presented to propose how the brain accomplishes this feat. The model clarifies how the frequency components that correspond to a given acoustic source may be coherently grouped together into a distinct stream based on pitch and spatial location cues. The model also clarifies how multiple streams may be distinguished and separated by the brain. Streams are formed as spectral-pitch resonances that emerge through feedback interactions between frequency-specific spectral representations of a sound source and its pitch. First, the model transforms a sound into a spatial pattern of frequency-specific activation across a spectral stream layer. The sound has multiple parallel representations at this layer. A sound's spectral representation activates a bottom-up filter that is sensitive to the harmonics of the sound's pitch. This filter activates a pitch category which, in turn, activates a top-down expectation that is also sensitive to the harmonics of the pitch. Resonance develops when the spectral and pitch representations mutually reinforce one another. Resonance provides the coherence that allows one voice or instrument to be tracked through a noisy multiple source environment. Spectral components are suppressed if they do not match harmonics of the top-down expectation that is read-out by the selected pitch, thereby allowing another stream to capture these components, as in the 'old-plus-new heuristic' of Bregman. Multiple simultaneously occurring spectral-pitch resonances can hereby emerge. These resonance and matching mechanisms are specialized versions of Adaptive Resonance Theory, or ART, which clarifies how pitch representations can self-organize during learning of harmonic bottom-up filters and top-down expectations. The model also clarifies how spatial location cues can help to disambiguate two sources with similar spectral cues. Data are simulated from psychophysical grouping experiments, such as how a tone sweeping upwards in frequency creates a bounce percept by grouping with a downward sweeping tone due to proximity in frequency, even if noise replaces the tones at their intersection point. Illusory auditory percepts are also simulated, such as the auditory continuity illusion of a tone continuing through a noise burst even if the tone is not present during the noise, and the scale illusion of Deutsch whereby downward and upward scales presented alternately to the two ears are regrouped based on frequency proximity, leading to a bounce percept. Since related sorts of resonances have been used to quantitatively simulate psychophysical data about speech perception, the model strengthens the hypothesis that ART-like mechanisms are used at multiple levels of the auditory system. Proposals for developing the model to explain more complex streaming data are also provided.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Image Chunking: Defining Spatial Building Blocks for Scene Analysis
</title>
    <abstract>Rapid judgments about the properties and spatial relations of objects are the crux of visually guided interaction with the world. Vision begins, however, with essentially pointwise representations of the scene, such as arrays of pixels or small edge fragments. For adequate time-performance in recognition, manipulation, navigation, and reasoning, the processes that extract meaningful entities from the pointwise representations must exploit parallelism. This report develops a framework for the fast extraction of scene entities, based on a simple, local model of parallel computation. An image chunk is a subset of an image that can act as a unit in the course of spatial analysis. A parallel preprocessing stage constructs a variety of simple chunks uniformly over the visual array. On the basis of these chunks, subsequent serial processes locate relevant scene components and assemble detailed descriptions of them rapidly. This thesis defines image chunks that facilitate the most potentially time-consuming operations of spatial analysis---boundary tracing, area coloring, and the selection of locations at which to apply detailed analysis. Fast parallel processes for computing these chunks from images, and chunk-based formulations of indexing, tracing, and coloring, are presented. These processes have been simulated and evaluated on the lisp machine and the connection machine.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>On the relation between probabilistic inference and fuzzy sets in visual scene analysis
</title>
    <abstract>Strict probabilistic inference is a difficult and costly procedure, and generally unfeasible in practice for interesting cases. It requires knowledge, storage, and computational handling of usually very complicated probability-density functions of the data. Independence assumptions commonly made to alleviate these problems are often wrong and may lead to unsatisfactory results. By contrast, working with fuzzy sets in data space is simple, while the underlying assumptions have remained largely obscure. Here I derive from probabilistic principles a fuzzy-set-type formulation of visual scene interpretation. The argument is focused on making explicit the conditions for reasoning with fuzzy sets and how their membership function should be constructed. It turns out that the conditions may be fulfilled to a good approximation in some cases of visual scene analysis.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>HMM-Based Mask Estimation for a Speech Recognition Front-End Using Computational Auditory Scene Analysis</title>
    <abstract>In this paper, we propose a new mask estimation method for the computational auditory scene analysis (CASA) of speech using two microphones. The proposed method is based on a hidden Markov model (HMM) in order to incorporate an observation that the mask information should be correlated over contiguous analysis frames. In other words, HMM is used to estimate the mask information represented as the interaural time difference (ITD) and the interaural level difference (ILD) of two channel signals, and the estimated mask information is finally employed in the separation of desired speech from noisy speech. To show the effectiveness of the proposed mask estimation, we then compare the performance of the proposed method with that of a Gaussian kernel-based estimation method in terms of the performance of speech recognition. As a result, the proposed HMM-based mask estimation method provided an average word error rate reduction of 61.4% when compared with the Gaussian kernel-based mask estimation method.</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Extraction of primitive features in scene analysis of images corrupted by dependent noise
</title>
    <abstract>An abstract is not available.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Understanding concurrent earcons: Applying auditory scene analysis principles to concurrent earcon recognition
</title>
    <abstract>Two investigations into the identification of concurrently presented, structured sounds, called earcons were carried out. One of the experiments investigated how varying the number of concurrently presented earcons affected their identification. It was found that varying the number had a significant effect on the proportion of earcons identified. Reducing the number of concurrently presented earcons lead to a general increase in the proportion of presented earcons successfully identified. The second experiment investigated how modifying the earcons and their presentation, using techniques influenced by auditory scene analysis, affected earcon identification. It was found that both modifying the earcons such that each was presented with a unique timbre, and altering their presentation such that there was a 300 ms onset-to-onset time delay between each earcon were found to significantly increase identification. Guidelines were drawn from this work to assist future interface designers when incorporating concurrently presented earcons.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Video inpainting and scene analysis
</title>
    <abstract>Videos, in the form of movies, YouTube clips, surveillance feeds, etc, have become an integral part of day to day human life. This has lead to a number of problems in addressing the automatic storage, manipulation and understanding of a large amount of video data generated every minute. This thesis is a result of research motivated mainly by 2 such interesting problems: first, inpainting i.e., automatic filling in of missing data in videos and second, automatic understanding of a video scene.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Combining attention and recognition for rapid scene analysis
</title>
    <abstract>Bottom-up visual attention allows primates to quickly select regions of an image that contain salient objects. In artificial systems, restricting the task of object recognition to these regions allows faster recognition and unsupervised learning of multiple objects in cluttered scenes. A problem is that objects superficially dissimilar to the target are given the same consideration in recognition as similar objects. Here we investigate rapid pruning of the recognition search space using the already-computed low-level features that guide attention. Itti and Koch¿s bottom-up visual attention algorithm selects salient locations based on low-level features such as contrast, orientation, color, and intensity. Lowe¿s SIFT recognition algorithm then extracts a signature of the attended object, for comparison with the object database. The database search is prioritized for objects which better match the low-level features used to guide attention to the current candidate for recognition. The SIFT signatures of prioritized database objects are then checked for match against the attended candidate. By comparing performance of Lowe¿s recognition algorithm and Itti and Koch¿s bottom-up attention model with or without search space pruning, we demonstrate that our pruning approach improves the speed of object recognition in complex natural scenes.
</abstract>
    <search_task_number>4</search_task_number>
    <query>scene analysis</query>
    <relevance>1</relevance>
  </item>


	<item>
		<title>
Dimensional compositing for visualizing high-dimensional dataset</title>
		<abstract>The scalability problem in high-dimensional dataset visualization has been a difficult problem. In this poster we propose a novel visualization technique which utilizes image compositing to solve this scalability problem, and a cube-like visual metaphor to integrate the new dimensional compositing and traditional parallel co-ordinate visualization. In order to effectively visualize the complex high-dimensional datasets, we also developed supporting interaction and navigation functionalities.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Assisted navigation for large information spaces</title>
		<abstract>This paper presents a new technique for visualizing large, complex collections of data. The size and dimensionality of these datasets make them challenging to display in an effective manner. The images must show the global structure of spatial relationships within the dataset, yet at the same time accurately represent the local detail of each data element being visualized. We propose combining ideas from information and scientific visualization together with a navigation assistant, a software system designed to help users identify and explore areas of interest within their data. The assistant locates data elements of potential importance to the user, clusters them into spatial regions, and builds underlying graph structures to connect the regions and the elements they contain. Graph traversal algorithms, constraint-based viewpoint construction, and intelligent camera planning techniques can then be used to design animated tours of these regions. In this way, the navigation assistant can help users to explore any of the areas of interest within their data. We conclude by demonstrating how our assistant is being used to visualize a multidimensional weather dataset.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Parallel visualization of large-scale aerodynamics calculations: a case study on the Cray T3E</title>
		<abstract>This paper reports the performance of a parallel volume rendering algorithm for visualizing a large-scale unstructed-grid dataset produced by a three-dimensional aerodynamics simulation. This dataset, containing over 18 million tetrahedra, allows us to extend our performance results to a problem which is more than 30 times larger than the one we examined previously. This high resolution dataset also allows us to see fine, three-dimensional features in the flow field. All our tests were performed on the SGI/Cray T3E operated by NASA's Goddard Space Flight Center. Using 511 processors, a rendering rate of almost 9 million tetrahedra/second was achieved with a parallel overhead of 26%.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Exploratory monitoring of large-scale networks using clustering algorithms</title>
		<abstract>Building and maintaining a reliable, high performance network infrastructure requires the ability of accurately visualizing, rapidly navigating and effectively resolving performance impacting issues. With the growing number of network entities and services, exploratory monitoring of a large-scale telecommunication network is becoming increasingly difficult. This paper presents a density hierarchy clustering algorithm, designed for real-time visualization of large telecommunications networks. The density histogram is calculated, which replaces the original dataset in further processing. The elements (cells) of the density histogram are compared to their neighbors in order to assign them to density hierarchies, which in turn identify the clusters. The experimental results have shown that the proposed algorithm provides high accuracy in visualizing node clusters, while significantly outperforming k-means in terms of clustering speed. This makes the algorithm a practical exploratory monitoring solution.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Visualizing tags over time</title>
		<abstract>We consider the problem of visualizing the evolution of tags within the Flickr (flickr.com) online image sharing community. Any user of the Flickr service may append a tag to any photo in the system. Over the past year, users have on average added over a million tags each week. Understanding the evolution of these tags over time is therefore a challenging task. We present a new approach based on a characterization of the most interesting tags associated with a sliding interval of time. An animation provided via Flash in a Web browser allows the user to observe and interact with the interesting tags as they evolve over time.

New algorithms and data structures are required to support the efficient generation of this visualization. We combine a novel solution to an interval covering problem with extensions to previous work on score aggregation in order to create an efficient backend system capable of producing visualizations at arbitrary scales on this large dataset in real time.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Using R-trees for interactive visualization of large multidimensional datasets</title>
		<abstract>Large, multidimensional datasets are difficult to visualize and analyze. Visualization interfaces are constrained in resolution and dimension, so cluttering and problems of projecting many dimensions into the available low dimensions are inherent. Methods of real-time interaction facilitate analysis, but often these are not available due to the computational complexity required to use them. By organizing the dataset into a level-of-detail (LOD) hierarchy, our proposed method solves problems of both inefficient interaction and visual cluttering. We do this by introducing an implementation of R-trees for large multidimensional datasets. We introduce several useful methods for interaction, by queries and refinement, to explain the relevance of interaction and show that it can be done efficiently with R-trees.We examine the applicability of hierarchical parallel coordinates to datasets organized within an R-tree, and build upon previous work in hierarchical star coordinates to introduce a novel method for visualizing bounding hyperboxes of internal R-tree nodes. Finally, we examine two datasets using our proposed method and present and discuss results.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>A Parallel Visualization Pipeline for Terascale Earthquake Simulations</title>
		<abstract>This paper presents a parallel visualization pipeline implemented at the Pittsburgh Supercomputing Center (PSC) for studying the largest earthquake simulation ever performed. The simulation employs 100 million hexahedral cells to model 3D seismic wave propagation of the 1994 Northridge earthquake. The time-varying dataset produced by the simulation requires terabytes of storage space. Our solution for visualizing such terascale simulations is based on a parallel adaptive rendering algorithm coupled with a new parallel I/O strategy which effectively reduces interframe delay by dedicating some processors to I/O and preprocessing tasks. In addition, a 2D vector field visualization method and a 3D enhancement technique are incorporated into the parallel visualization framework to help scientists better understand the wave propagation both on and under the ground surface. Our test results on the HP/Compaq AlphaServer operated at the PSC show that we can completely remove the I/O bottlenecks commonly present in time-varying data visualization. The high-performance visualization solution we provide to the scientists allows them to explore their data in the temporal, spatial, and variable domains at high resolution. The new high-resolution explorability, likely not available to most computational science groups, will help lead to many new insights.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Beyond co-occurrence: discovering and visualizing tag relationships from geo-spatial and temporal similarities</title>
		<abstract>Studying relationships between keyword tags on social sharing websites has become a popular topic of research, both to improve tag suggestion systems and to discover connections between the concepts that the tags represent. Existing approaches have largely relied on tag co-occurrences. In this paper, we show how to find connections between tags by comparing their distributions over time and space, discovering tags with similar geographic and temporal patterns of use. Geo-spatial, temporal and geo-temporal distributions of tags are extracted and represented as vectors which can then be compared and clustered. Using a dataset of tens of millions of geo-tagged Flickr photos, we show that we can cluster Flickr photo tags based on their geographic and temporal patterns, and we evaluate the results both qualitatively and quantitatively using a panel of human judges. We also develop visualizations of temporal and geographic tag distributions, and show that they help humans recognize semantic relationships between tags. This approach to finding and visualizing similar tags is potentially useful for exploring any data having geographic and temporal annotations.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Proceedings of the international workshop on Very-large-scale multimedia corpus, mining and retrieval</title>
		<abstract>Welcome to the International Workshop on Very-Large-Scale Multimedia Corpus, Mining and Retrieval (VLS-MCMR'10). The purpose of this workshop is to bring together researchers interested in the construction and analysis of Very Large Scale Multimedia Corpus, as well as the methodologies to Mine and Retrieve information from them. The Workshop will provide a forum to consolidate key issues related to research on very large scale multimedia dataset such as the construction of dataset, creation of ground truth, sharing and extension of such resources in terms of ground truth, features, algorithms and tools etc. The Workshop will discuss and formulate action plan towards these goals.

This workshop welcomes contributions on the following topics:

Construction, Unification and Evolution of Corpus
Framework for sharing of dataset, ground truth, features, algorithms and tools
Indexing and retrieval for large multimedia collections (including images, video, audio and other multi-modal systems)
Large-scale video event and temporal analysis over diverse sources
Automatic machine tagging, semantic annotation and object recognition on massive multimedia collections
Interfaces for exploring, browsing and visualizing large multimedia collections
Scalable and distributed machine learning and data mining methods for multimedia data
Performance evaluation methodologies and standards
Large-scale copy detection and near-duplicate detection
Web-scale combined analysis of social and content networks
Scalable and distributed systems for multimedia content analysis
Large-Scale multimedia applications are among the potential topics for the ACM multimedia 2010 hosts the "Multimedia Grand Challenge." The availability of Large-Scale Corpus would effectively boost research in this direction and foster many new applications for the years to come.

The call for papers attracted 26 submissions from Asia, North-America, Europe and Africa. The program committee accepted 10 high quality papers. In addition, the program includes a panel on the topics addressed by the workshop and a keynote speech. We hope that the proceedings will serve as a valuable reference for multimedia researchers and developers as well as encourage new research direction and results.

Looking over the papers accepted for the workshop, we observe three major trends. First there are approaches that attempt to benefit from the user-contributed data in order to facilitate modeling, mining and retrieval. Second, there are studies that focus on the algorithmic issues related to the use of massively parallel computing facilities. Finally, there is work that addresses the scalability issues when going very-large-scale.

Tong tackles large scale image annotation using user contributed annotation (tags etc.) provided from social media network; where scalability is achieved through the use of the GRID'5000 computation mresources. Zhou et al. identify relevant text terms from text blocks that surround the web images in order to improve the accuracy of web image annotation on a 5 million image dataset. Wang et al. propose a deep model-based and data-driven hybrid architecture for annotating images. It is shown that DMD can scale-up well, thanks to its sparse regularization and scalable supervised learning steps. Creating a corpus is both expensive and time consuming. Liu and Huet propose a technique to automatically augment the training set for concept detector refinement. Two kinds of information is used to select the training data, one is visual feature, where video shots with high confidence scores are selected, the other is tags, in which tags are used to filter out video shots not tagged with the concept.

Wu et al. present an unsupervised fully automatic algorithm for detecting commercials in broadcast TV. Their solution is scalable and efficient for fast, large scale, unsupervised commercial detection. Gudmund et al. revisit a cluster pruning algorithm, considering factors such as CPU/IO cost and memory constraint for large-scale copy detection. The method shows interesting clustering and retrieval computational cost when scaling up. In Kosh, processing and optimizing strategies are presented along with a cost model for integrating a similarity-based image join in a multimedia database.

Nagy et al. addresses the scalability issues for visual vocabulary based image annotation algorithms as new object categories are added. To this end, a hierarchical approach is proposed based on classspecific vocabulary and a scoring function. Fan et al. reported an extensive analysis of user behavior in online video streaming based on a large-scale trace database of online video access sessions. The study of the statistical characteristics of user behavior patterns shows that user behavior in a video access session is not only related to the content of the video, but also has strong correlation with the behaviors of previous access sessions. Wang and Merialdo propose an approach to boost the performance of video concept detection based on the Bag-of-Words through the assignment different weights to the visual words according to their informativeness for the detection of different concepts.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Efficient ray tracing of volume data</title>
		<abstract>Volume rendering is a technique for visualizing sampled scalar or vector fields of three spatial dimensions without fitting geometric primitives to the data. A subset of these techniques generates images by computing 2-D projections of a colored semitransparent volume, where the color and opacity at each point are derived from the data using local operators. Since all voxels participate in the generation of each image, rendering time grows linearly with the size of the dataset. This paper presents a front-to-back image-order volume-rendering algorithm and discusses two techniques for improving its performance. The first technique employs a pyramid of binary volumes to encode spatial coherence present in the data, and the second technique uses an opacity threshold to adaptively terminate ray tracing. Although the actual time saved depends on the data, speedups of an order of magnitude have been observed for datasets of useful size and complexity. Examples from two applications are given: medical imaging and molecular graphics.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>T2: a customizable parallel database for multi-dimensional data</title>
		<abstract>As computational power and storage capacity increase, processing and analyzing large volumes of data play an increasingly important part in many domains of scientific research. Typical examples of large scientific datasets include long running simulations of time-dependent phenomena that periodically generate snapshots of their state (e.g. hydrodynamics and chemical transport simulation for estimating pollution impact on water bodies [4, 6, 20], magnetohydrodynamics simulation of planetary magnetospheres [32], simulation of a flame sweeping through a volume [28], airplane wake simulations [21]), archives of raw and processed remote sensing data (e.g. AVHRR [25], Thematic Mapper [17], MODIS [22]), and archives of medical images (e.g. confocal light microscopy, CT imaging, MRI, sonography).

These datasets are usually multi-dimensional. The data dimensions can be spatial coordinates, time, or experimental conditions such as temperature, velocity or magnetic field. The importance of such datasets has been recognized by several database research groups and vendors, and several systems have been developed for managing and/or visualizing them [2, 7, 14, 19, 26, 27, 29, 31].

These systems, however, focus on lineage management, retrieval and visualization of multi-dimensional datasets. They provide little or no support for analyzing or processing these datasets -- the assumption is that this is too application-specific to warrant common support. As a result, applications that process these datasets are usually decoupled from data storage and management, resulting in inefficiency due to copying and loss of locality. Furthermore, every application developer has to implement complex support for managing and scheduling the processing.

Over the past three years, we have been working with several scientific research groups to understand the processing requirements for such applications [1, 5, 6, 10, 18, 23, 24, 28]. Our study of a large set of applications indicates that the processing for such datasets is often highly stylized and shares several important characteristics. Usually, both the input dataset as well as the result being computed have underlying multi-dimensional grids, and queries into the dataset are in the form of ranges within each dimension of the grid. The basic processing step usually consists of transforming individual input items, mapping the transformed items to the output grid and computing output items by aggregating, in some way, all the transformed input items mapped to the corresponding grid point. For example, remote-sensing earth images are often generated by performing atmospheric correction on several days worth of raw telemetry data, mapping all the data to a latitude-longitude grid and selecting those measurements that provide the clearest view.

In this paper, we present T2, a customizable parallel database that integrates storage, retrieval and processing of multi-dimensional datasets. T2 provides support for many operations including index generation, data retrieval, memory management, scheduling of processing across a parallel machine and user interaction. It achieves its primary advantage from the ability to seamlessly integrate data retrieval and processing for a wide variety of applications and from the ability to maintain and process multiple datasets with different underlying grids. Most other systems for multi-dimensional data have focused on uniformly distributed datasets, such as images, maps, and dense multi-dimensional arrays. Many real datasets, however, are non-uniform or unstructured. For example, satellite data is a two dimensional strip that is embedded in a three dimensional space; water contamination studies use unstructured meshes to selectively simulate regions and so on. T2 can handle both uniform and non-uniform datasets.

T2 has been developed as a set of modular services. Since its structure mirrors that of a wide variety of applications, T2 is easy to customize for different types of processing. To build a version of T2 customized for a particular application, a user has to provide functions to pre-process the input data, map input data to elements in the output data, and aggregate multiple input data items that map to the same output element.

T2 presents a uniform interface to the end users (the clients of the database system). Users specify the dataset(s) of interest, a region of interest within the dataset(s), and the desired format and resolution of the output. In addition, they select the mapping and aggregation functions to be used. T2 analyzes the user request, builds a suitable plan to retrieve and process the datasets, executes the plan and presents the results in the desired format.

In Section 2 we first present several motivating applications and illustrate their common structure. Section 3 then presents an overview of T2, including its distinguishing features and a running example. Section 4 describes each database service in some detail. An example of how to customize several of the database services for a particular application is given in Section 5. T2 is a system in evolution. We conclude in Section 6 with a description of the current status of both the T2 design and the implementation of various applications with T2.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>DataMeadow: A Visual Canvas for Analysis of Large-Scale Multivariate Data</title>
		<abstract>Supporting visual analytics of multiple large-scale multidimensional datasets requires a high degree of interactivity and user control beyond the conventional challenges of visualizing such datasets. We present the DataMeadow, a visual canvas providing rich interaction for constructing visual queries using graphical set representations called DataRoses. A DataRose is essentially a starplot of selected columns in a dataset displayed as multivariate visualizations with dynamic query sliders integrated into each axis. The purpose of the DataMeadow is to allow users to create advanced visual queries by iteratively selecting and filtering into the multidimensional data. Furthermore, the canvas provides a clear history of the analysis that can be annotated to facilitate dissemination of analytical results to outsiders. Towards this end, the DataMeadow has a direct manipulation interface for selection, filtering, and creation of sets, subsets, and data dependencies using both simple and complex mouse gestures. We have evaluated our system using a qualitative expert review involving two researchers working in the area. Results from this review are favorable for our new method.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Infrastructures for Research on Multimodal Moving Objects</title>
		<abstract>Moving objects with multiple transportation modes (e.g., $Car$, $Bus$, $Walk$) can be represented by the reference method where the location is mapped to infrastructure objects, e.g., roads, buses, pavements. A scalable dataset is required to evaluate the performance of a database system managing these moving objects. To produce multimodal moving objects, infrastructure objects are essentially needed. Due to the major challenge of gaining real datasets, in this demonstration we propose a method create these infrastructure objects for different environments including pavement areas, bus network and indoor just from a given road network and some floor plans. An indoor viewer is also implemented in a database system for visualizing 3D indoor moving objects and trajectories. The work is to be used for evaluating the performance of a database system managing generic moving objects.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>DEVise (demo abstract): integrated querying and visual exploration of large datasets</title>
		<abstract>DEVise is a data exploration system that allows users to easily develop, browse, and share visual presentations of large tabular datasets (possibly containing or referencing multimedia objects) from several sources. The DEVise framework, implemented in a tool that has been already successfully applied to a variety of real applications by a number of user groups, makes several contributions. In particular, it combines support for extended relational queries with powerful data visualization features. Datasets much larger than available main memory can be handled—DEVise is currently being used to visualize datasets well in excess of 100MB—and data can be interactively examined at several levels of detail: all the way from meta-data summarizing the entire dataset, to large subsets of the actual data, to individual data records. Combining querying (in general, data processing) with visualizations gives us a very versatile tool, and presents several novel challenges. Our emphasis is on developing an intuitive yet powerful set of querying and visualization primitives that can be easily combined to develop a rich set of visual presentations that integrate data from a wide range of application domains. In this demo, we will present a number of examples of the use of the DEVise tool for visualizing and interactively exploring very large datasets, and report on our experience in applying it to several real applications.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Mobile video browsing and retrieval with the OVIDIUS platform</title>
		<abstract>This paper describes a mobile video browsing and retrievalapproach, based on the so-called OVIDIUS (On-line VIDeo Indexing Universal System) platform. In contrast with traditional and commercial video retrieval platforms, where video content is treated in a more or less monolithic manner (i.e. with global descriptions associated with the whole document), the proposed approach makes it possible to browse and access video content in a finer, per-segment basis. The hierarchical metadata structure exploits the MPEG-7 approach for structural description of video content. The MPEG-7 description schemes have been here enriched with both semantic and content-based metadata. The developed approach shows all its pertinence within a multiterminal context and in particular for video access from mobile devices. The platform has been recently (February, 2010) validated within the framework of the Médi@TIC French national project.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Visualizing geospatial data</title>
		<abstract>This course reviews concepts and highlights new directions in GeoVisualization. We review four levels of integrating geospatial data and geographic information systems (GIS) with scientific and information visualization (VIS) methods. These include:• Rudimentary: minimal data sharing between the GIS and Vis systems• Operational: consistency of geospatial data• Functional: transparent communication between the GIS and Vis systems• Merged: one comprehensive toolkit environmentWe review how to apply both information and scientific visualization fundamentals to the visual display of geospatial and geoinformatics data. Distributed GeoVisualization systems that allow for collaborative synchronous and asynchronous visual exploration and analysis of geospatial data via the Web, Internet, and large-screen group-enabled displays are discussed. This includes the application of intelligent agent and spatial data mining technologies. Case study examples are shown in real time during the course.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>iVIBRATE: Interactive visualization-based framework for clustering large datasets</title>
		<abstract>With continued advances in communication network technology and sensing technology, there is astounding growth in the amount of data produced and made available through cyberspace. Efficient and high-quality clustering of large datasets continues to be one of the most important problems in large-scale data analysis. A commonly used methodology for cluster analysis on large datasets is the three-phase framework of sampling/summarization, iterative cluster analysis, and disk-labeling. There are three known problems with this framework which demand effective solutions. The first problem is how to effectively define and validate irregularly shaped clusters, especially in large datasets. Automated algorithms and statistical methods are typically not effective in handling these particular clusters. The second problem is how to effectively label the entire data on disk (disk-labeling) without introducing additional errors, including the solutions for dealing with outliers, irregular clusters, and cluster boundary extension. The third obstacle is the lack of research about issues related to effectively integrating the three phases. In this article, we describe iVIBRATE---an interactive visualization-based three-phase framework for clustering large datasets. The two main components of iVIBRATE are its VISTA visual cluster-rendering subsystem which invites human interplay into the large-scale iterative clustering process through interactive visualization, and its adaptive ClusterMap labeling subsystem which offers visualization-guided disk-labeling solutions that are effective in dealing with outliers, irregular clusters, and cluster boundary extension. Another important contribution of iVIBRATE development is the identification of the special issues presented in integrating the two components and the sampling approach into a coherent framework, as well as the solutions for improving the reliability of the framework and for minimizing the amount of errors generated within the cluster analysis process. We study the effectiveness of the iVIBRATE framework through a walkthrough example dataset of a million records and we experimentally evaluate the iVIBRATE approach using both real-life and synthetic datasets. Our results show that iVIBRATE can efficiently involve the user in the clustering process and generate high-quality clustering results for large datasets.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>The Omni-family of all-purpose access methods: a simple and effective way to make similarity search more efficient</title>
		<abstract>Similarity search operations require executing expensive algorithms, and although broadly useful in many new applications, they rely on specific structures not yet supported by commercial DBMS. In this paper we discuss the new Omni-technique, which allows to build a variety of dynamic Metric Access Methods based on a number of selected objects from the dataset, used as global reference objects. We call them as the Omni-family of metric access methods. This technique enables building similarity search operations on top of existing structures, significantly improving their performance, regarding the number of disk access and distance calculations. Additionally, our methods scale up well, exhibiting sub-linear behavior with growing database size.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Ownership protection of shape datasets with geodesic distance preservation</title>
		<abstract>Protection of one's intellectual property is a topic with important technological and legal facets. The significance of this issue is amplified nowadays due to the ease of data dissemination through the internet. Here, we provide technological mechanisms for establishing the ownership of a dataset consisting of multiple objects. The objects that we consider in this work are shapes (i.e., two dimensional contours), which abound in disciplines such as medicine, biology, anthropology and natural sciences. The protection of the dataset is achieved through means of embedding of an imperceptible ownership 'seal', that imparts only minute visual distortions. This seal needs to be embedded in the proper data space so that its removal or destruction is particularly difficult. Our technique is robust to many common transformations, such as data rotation, translation, scaling, noise addition and resampling. In addition to that, the proposed scheme also guarantees that important distances between the dataset shapes/objects are not distorted. We achieve this by preserving the geodesic distances between the dataset objects. Geodesic distances capture a significant part of the dataset structure, and their usefulness is recognized in many machine learning, visualization and clustering algorithms. Therefore, if a practitioner uses the protected dataset as input to a variety of mining, machine learning, or database operations, the output will be the same as on the original dataset. We illustrate and validate the applicability of our methods on image shapes extracted from anthropological and natural science data.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Dual Assessment of Data Quality in Customer Databases</title>
		<abstract>Quantitative assessment of data quality is critical for identifying the presence of data defects and the extent of the damage due to these defects. Quantitative assessment can help define realistic quality improvement targets, track progress, evaluate the impacts of different solutions, and prioritize improvement efforts accordingly. This study describes a methodology for quantitatively assessing both impartial and contextual data quality in large datasets. Impartial assessment measures the extent to which a dataset is defective, independent of the context in which that dataset is used. Contextual assessment, as defined in this study, measures the extent to which the presence of defects reduces a dataset’s utility, the benefits gained by using that dataset in a specific context. The dual assessment methodology is demonstrated in the context of Customer Relationship Management (CRM), using large data samples from real-world datasets. The results from comparing the two assessments offer important insights for directing quality maintenance efforts and prioritizing quality improvement solutions for this dataset. The study describes the steps and the computation involved in the dual-assessment methodology and discusses the implications for applying the methodology in other business contexts and data environments.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>ClusterMap: labeling clusters in large datasets via visualization</title>
		<abstract>With the rapid increase of data in many areas, clustering on large datasets has become an important problem in data analysis. Since cluster analysis is a highly iterative process, cluster analysis on large datasets prefers short iteration on a relatively small representative set. Thus, a two-phase framework "sampling/summarization - iterative cluster analysis" is often applied in practice. Since the clustering result only labels the small representative set, there are problems with extending the result to the entire large dataset, which are almost ignored by the traditional clustering research. This extending is often named as labeling process. Labeling irregular shaped clusters, distinguishing outliers and extending cluster boundary are the main problems in this stage. We address these problems and propose a visualization-based approach to dealing with them precisely. This approach partially involves human into the process of defining and refining the structure "ClusterMap". Based on this structure, the ClusterMap algorithm scans the large dataset to adapt the boundary extension and generate the cluster labels for the entire dataset. Experimental result shows that ClusterMap can preserve cluster quality considerably with low computational cost, compared to the distance-comparison-based labeling algorithms.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Finding haystacks with needles: ranked search for data using geospatial and temporal characteristics</title>
		<abstract>The past decade has seen an explosion in the number and types of environmental sensors deployed, many of which provide a continuous stream of observations. Each individual observation consists of one or more sensor measurements, a geographic location, and a time. With billions of historical observations stored in diverse databases and in thousands of datasets, scientists have difficulty finding relevant observations. We present an approach that creates consistent geospatial-temporal metadata from large repositories of diverse data by blending curated and automated extracts. We describe a novel query method over this metadata that returns ranked search results to a query with geospatial and temporal search criteria. Lastly, we present a prototype that demonstrates the utility of these ideas in the context of an ocean and coastalmargin observatory.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Using a kernel: based approach to visualize integrated chronic fatigue syndrome datasets</title>
		<abstract>We describe the use of a kernel-based approach using the Laplacian matrix to visualize an integrated Chronic Fatigue Syndrome dataset comprising symptom and fatigue questionnaire and patient classification data, complete blood evaluation data and patient gene expression profiles. We present visualizations of the individual and integrated datasets with the linear and Gaussian kernel functions. An efficient approach inspired by computational linguistics for constructing a linear kernel matrix for the gene expression data is described. Visualizations of the questionnaire data show a cluster of non-fatigued individuals distinct from those suffering from Chronic Fatigue Syndrome that supports the fact that diagnosis is generally made using this kind of data. Clusters unrelated to patient classes were found in the gene expression data. Structure from the gene expression dataset dominated visualizations of integrated datasets that included gene expression data.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Attribute preserving dataset simplification</title>
		<abstract>This paper describes a novel application of feature preserving mesh simplification to the problem of managing large, multidimensional datasets during scientific visualization. To allow this, we view a scientific dataset as a triangulated mesh of data elements, where the attributes embedded in each element form a set of properties arrayed across the surface of the mesh. Existing simplification techniques were not designed to address the high dimensionality that exists in these types of datasets. As well, vertex operations that relocate, insert, or remove data elements may need to be modified or restricted. Principal component analysis provides an algorithm-independent method for compressing a dataset's dimensionality during simplification. Vertex locking forces certain data elements maintain their spatial locations; this technique is also used to guarantee a minimum density in the simplified dataset. The result is a visualization that significantly reduces the number of data elements to display, while at the same time ensuring that high-variance regions of potential interest remain intact. We apply our techniques to a number of well-known feature preserving algorithms, and demonstrate their applicability in a real-world context by simplifying a multidimensional weather dataset. Our results show a significant improvement in execution time with only a small reduction in accuracy; even when the dataset was simplified to 10% of its original size, average per attribute error was less than 1%.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Level set and PDE methods for computer graphics</title>
		<abstract>Level set methods, an important class of partial differential equation (PDE) methods, define dynamic surfaces implicitly as the level set (iso-surface) of a sampled, evolving nD function. The course begins with preparatory material that introduces the concept of using partial differential equations to solve problems in computer graphics, geometric modeling and computer vision. This will include the structure and behavior of several different types of differential equations, e.g. the level set equation and the heat equation, as well as a general approach to developing PDE-based applications. The second stage of the course will describe the numerical methods and algorithms needed to actually implement the mathematics and methods presented in the first stage. The course closes with detailed presentations on several level set/PDE applications, including image/video inpainting, pattern formation, image/volume processing, 3D shape reconstruction, image/volume segmentation, image/shape morphing, geometric modeling, anisotropic diffusion, and natural phenomena simulation.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Event correlation for process discovery from web service interaction logs</title>
		<abstract>Understanding, analyzing, and ultimately improving business processes is a goal of enterprises today. These tasks are challenging as business processes in modern enterprises are implemented over several applications and Web services, and the information about process execution is scattered across several data sources. Understanding modern business processes entails identifying the correlation between events in data sources in the context of business processes (event correlation is the process of finding relationships between events that belong to the same process execution instance). In this paper, we investigate the problem of event correlation for business processes that are realized through the interactions of a set of Web services. We identify various ways in which process-related events could be correlated as well as investigate the problem of discovering event correlation (semi-) automatically from service interaction logs. We introduce the concept of process view to represent the process resulting from a certain way of event correlation and that of process space referring to the set of possible process views over process events. Event correlation is a challenging problem as there are various ways in which process events could be correlated, and in many cases, it is subjective. Exploring all the possibilities of correlations is computationally expensive, and only some of the correlated event sets result in process views that are interesting. We propose efficient algorithms and heuristics to identify correlated event sets that lead potentially to interesting process views. To account for its subjectivity, we have designed the event correlation discovery process to be interactive and enable users to guide it toward process views of their interest and organize the discovered process views into a process map that allows users to effectively navigate through the process space and identify the ones of interest. We report on experiments performed on both synthetic and real-world datasets that show the viability and efficiency of the approach.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Perceptually-motivated graphics, visualization and 3D displays</title>
		<abstract>This course presents timely, relevant examples on how researchers have leveraged perceptual information for optimization of rendering algorithms, to better guide design and presentation in (3D stereoscopic) display media, and for improved visualization of complex or large data sets. Each presentation will provide references and short overviews of cutting-edge current research pertaining to that area. We will ensure that the most up-to-date research examples are presented by sourcing information from recent perception and graphics conferences and journals such as ACM Transactions on Perception, paying particular attention work presented at the 2010 Symposium on Applied Perception in Graphics and Visualization.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Untangling the usability of fisheye menus</title>
		<abstract>Fisheye menus have become a prominent example of fisheye interfaces, yet contain several nonfisheye elements and have not been systematically evaluated. This study investigates whether fisheye menus are useful, and tries to untangle the impact on usability of the following properties of fisheye menus: use of distortion, index of letters for coarse navigation, and the focus-lock mode for accurate movement. Twelve participants took part in an experiment comparing fisheye menus with three alternative menu designs across known-item and browsing tasks, as well as across alphabetical and categorical menu structures. The results show that for finding known items, conventional hierarchical menus are the most accurate and by far the fastest. In addition, participants rate the hierarchical menu as more satisfying than fisheye and multifocus menus, but do not consistently prefer any one menu. For browsing tasks, the menus neither differ with respect to accuracy nor selection time. Eye-movement data show that participants make little use of nonfocus regions of the fisheye menu, though these are a defining feature of fisheye interfaces. Nonfocus regions are used more with the multifocus menu, which enlarges important menu items in these regions. With the hierarchical menu, participants make shorter fixations and have shorter scanpaths, suggesting lower requirements for mental activity and visual search. We conclude by discussing why fisheye menus are inferior to the hierarchical menu and how both may be improved.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>A real-world noisy unstructured handwritten notebook corpus for document image analysis research</title>
		<abstract>Traditionally, document image analysis (DIA) is conducted on datasets that are prepared for research purposes. Many existing handwriting datasets, however, do not necessarily represent the range of problems we wish to solve in real life. In this work, we introduce a noisy and unstructured handwriting dataset that aims for promoting and evaluating robust document analysis algorithms for real-world challenges, as a result of emphasizing the process of building and curating a dataset. First, we explain the data acquisition process and characterize its critical features as noisy and unstructured. Then, we discuss a set of real-world scenarios that might benefit from using our notebook dataset. As an on-going activity, so far we have collected 18 handwritten note-books from nine college students, resulting in a total of 499 pages. We expect to collect over 100 notebooks, or equivalently about 3,000 pages, from at least 50 students. This dataset is available to the research community via the Lehigh document analysis and exploitation (DAE) platform.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>RIC: Parameter-free noise-robust clustering</title>
		<abstract>How do we find a natural clustering of a real-world point set which contains an unknown number of clusters with different shapes, and which may be contaminated by noise? As most clustering algorithms were designed with certain assumptions (Gaussianity), they often require the user to give input parameters, and are sensitive to noise. In this article, we propose a robust framework for determining a natural clustering of a given dataset, based on the minimum description length (MDL) principle. The proposed framework, robust information-theoretic clustering (RIC), is orthogonal to any known clustering algorithm: Given a preliminary clustering, RIC purifies these clusters from noise, and adjusts the clusterings such that it simultaneously determines the most natural amount and shape (subspace) of the clusters. Our RIC method can be combined with any clustering technique ranging from K-means and K-medoids to advanced methods such as spectral clustering. In fact, RIC is even able to purify and improve an initial coarse clustering, even if we start with very simple methods. In an extension, we propose a fully automatic stand-alone clustering method and efficiency improvements. RIC scales well with the dataset size. Extensive experiments on synthetic and real-world datasets validate the proposed RIC framework.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>What happened in my network: mining network events from router syslogs</title>
		<abstract>Router syslogs are messages that a router logs to describe a wide range of events observed by it. They are considered one of the most valuable data sources for monitoring network health and for trou- bleshooting network faults and performance anomalies. However, router syslog messages are essentially free-form text with only a minimal structure, and their formats vary among different vendors and router OSes. Furthermore, since router syslogs are aimed for tracking and debugging router software/hardware problems, they are often too low-level from network service management perspectives. Due to their sheer volume (e.g., millions per day in a large ISP network), detailed router syslog messages are typically examined only when required by an on-going troubleshooting investigation or when given a narrow time range and a specific router under suspicion. Automated systems based on router syslogs on the other hand tend to focus on a subset of the mission critical messages (e.g., relating to network fault) to avoid dealing with the full diversity and complexity of syslog messages. In this project, we design a Sys-logDigest system that can automatically transform and compress such low-level minimally-structured syslog messages into meaningful and prioritized high-level network events, using powerful data mining techniques tailored to our problem domain. These events are three orders of magnitude fewer in number and have much better usability than raw syslog messages. We demonstrate that they provide critical input to network troubleshooting, and net- work health monitoring and visualization.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Feature estimation for efficient streaming</title>
		<abstract>In this paper we propose a method to estimate the number of primitives that are extracted from an iso-surface extraction procedure. A decimated version of the data is used to estimate the number of triangles at the finest resolution through a combinatorial audit of the features (iso-contours) and their changes across resolutions. The estimates allow for the efficient implementation of visualization pipelines across a spectrum of hardware platforms. We demonstrate the efficacy of our methods through suitable experiments with The Visible Woman dataset.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Discovering Knowledge-Sharing Communities in Question-Answering Forums</title>
		<abstract>In this article, we define a knowledge-sharing community in a question-answering forum as a set of askers and authoritative users such that, within each community, askers exhibit more homogeneous behavior in terms of their interactions with authoritative users than elsewhere. A procedure for discovering members of such a community is devised. As a case study, we focus on Yahoo! Answers, a large and diverse online question-answering service. Our contribution is twofold. First, we propose a method for automatic identification of authoritative actors in Yahoo! Answers. To this end, we estimate and then model the authority scores of participants as a mixture of gamma distributions. The number of components in the mixture is determined using the Bayesian Information Criterion (BIC), while the parameters of each component are estimated using the Expectation-Maximization (EM) algorithm. This method allows us to automatically discriminate between authoritative and nonauthoritative users. Second, we represent the forum environment as a type of transactional data such that each transaction summarizes the interaction of an asker with a specific set of authoritative users. Then, to group askers on the basis of their interactions with authoritative users, we propose a parameter-free transaction data clustering algorithm which is based on a novel criterion function. The identified clusters correspond to the communities that we aim to discover. To evaluate the suitability of our clustering algorithm, we conduct a series of experiments on both synthetic data and public real-life data. Finally, we put our approach to work using data from Yahoo! Answers which represent users’ activities over one full year.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Clustering very large multi-dimensional datasets with MapReduce</title>
		<abstract>Given a very large moderate-to-high dimensionality dataset, how could one cluster its points? For datasets that don't fit even on a single disk, parallelism is a first class option. In this paper we explore MapReduce for clustering this kind of data. The main questions are (a) how to minimize the I/O cost, taking into account the already existing data partition (e.g., on disks), and (b) how to minimize the network cost among processing nodes. Either of them may be a bottleneck. Thus, we propose the Best of both Worlds -- BoW method, that automatically spots the bottleneck and chooses a good strategy. Our main contributions are: (1) We propose BoW and carefully derive its cost functions, which dynamically choose the best strategy; (2) We show that BoW has numerous desirable features: it can work with most serial clustering methods as a plugged-in clustering subroutine, it balances the cost for disk accesses and network accesses, achieving a very good tradeoff between the two, it uses no user-defined parameters (thanks to our reasonable defaults), it matches the clustering quality of the serial algorithm, and it has near-linear scale-up; and finally, (3) We report experiments on real and synthetic data with billions of points, using up to 1,024 cores in parallel. To the best of our knowledge, our Yahoo! web is the largest real dataset ever reported in the database subspace clustering literature. Spanning 0.2 TB of multi-dimensional data, it took only 8 minutes to be clustered, using 128 cores.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>REDUS: finding reducible subspaces in high dimensional data</title>
		<abstract>Finding latent patterns in high dimensional data is an important research problem with numerous applications. The most well known approaches for high dimensional data analysis are feature selection and dimensionality reduction. Being widely used in many applications, these methods aim to capture global patterns and are typically performed in the full feature space. In many emerging applications, however, scientists are interested in the local latent patterns held by feature subspaces, which may be invisible via any global transformation.

In this paper, we investigate the problem of finding strong linear and nonlinear correlations hidden in feature subspaces of high dimensional data. We formalize this problem as identifying reducible subspaces in the full dimensional space. Intuitively, a reducible subspace is a feature subspace whose intrinsic dimensionality is smaller than the number of features. We present an effective algorithm, REDUS, for finding the reducible subspaces. Two key components of our algorithm are finding the overall reducible subspace, and uncovering the individual reducible subspaces from the overall reducible subspace. A broad experimental evaluation demonstrates the effectiveness of our algorithm.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Why we search: visualizing and predicting user behavior</title>
		<abstract>The aggregation and comparison of behavioral patterns on the WWW represent a tremendous opportunity for understanding past behaviors and predicting future behaviors. In this paper, we take a first step at achieving this goal. We present a large scale study correlating the behaviors of Internet users on multiple systems ranging in size from 27 million queries to 14 million blog posts to 20,000 news articles. We formalize a model for events in these time-varying datasets and study their correlation. We have created an interface for analyzing the datasets, which includes a novel visual artifact, the DTWRadar, for summarizing differences between time series. Using our tool we identify a number of behavioral properties that allow us to understand the predictive power of patterns of use.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Website fingerprinting in onion routing based anonymization networks</title>
		<abstract>Low-latency anonymization networks such as Tor and JAP claim to hide the recipient and the content of communications from a local observer, i.e., an entity that can eavesdrop the traffic between the user and the first anonymization node. Especially users in totalitarian regimes strongly depend on such networks to freely communicate. For these people, anonymity is particularly important and an analysis of the anonymization methods against various attacks is necessary to ensure adequate protection. In this paper we show that anonymity in Tor and JAP is not as strong as expected so far and cannot resist website fingerprinting attacks under certain circumstances. We first define features for website fingerprinting solely based on volume, time, and direction of the traffic. As a result, the subsequent classification becomes much easier. We apply support vector machines with the introduced features. We are able to improve recognition results of existing works on a given state-of-the-art dataset in Tor from 3% to 55% and in JAP from 20% to 80%. The datasets assume a closed-world with 775 websites only. In a next step, we transfer our findings to a more complex and realistic open-world scenario, i.e., recognition of several websites in a set of thousands of random unknown websites. To the best of our knowledge, this work is the first successful attack in the open-world scenario. We achieve a surprisingly high true positive rate of up to 73% for a false positive rate of 0.05%. Finally, we show preliminary results of a proof-of-concept implementation that applies camouflage as a countermeasure to hamper the fingerprinting attack. For JAP, the detection rate decreases from 80% to 4% and for Tor it drops from 55% to about 3%.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>TopicNets: Visual Analysis of Large Text Corpora with Topic Modeling</title>
		<abstract>We present TopicNets, a Web-based system for visual and interactive analysis of large sets of documents using statistical topic models. A range of visualization types and control mechanisms to support knowledge discovery are presented. These include corpus- and document-specific views, iterative topic modeling, search, and visual filtering. Drill-down functionality is provided to allow analysts to visualize individual document sections and their relations within the global topic space. Analysts can search across a dataset through a set of expansion techniques on selected document and topic nodes. Furthermore, analysts can select relevant subsets of documents and perform real-time topic modeling on these subsets to interactively visualize topics at various levels of granularity, allowing for a better understanding of the documents. A discussion of the design and implementation choices for each visual analysis technique is presented. This is followed by a discussion of three diverse use cases in which TopicNets enables fast discovery of information that is otherwise hard to find. These include a corpus of 50,000 successful NSF grant proposals, 10,000 publications from a large research center, and single documents including a grant proposal and a PhD thesis.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>CloudVista: visual cluster exploration for extreme scale data in the cloud</title>
		<abstract>The problem of efficient and high-quality clustering of extreme scale datasets with complex clustering structures continues to be one of the most challenging data analysis problems. An innovate use of data cloud would provide unique opportunity to address this challenge. In this paper, we propose the Cloud-Vista framework to address (1) the problems caused by using sampling in the existing approaches and (2) the problems with the latency caused by cloud-side processing on interactive cluster visualization. The CloudVista framework aims to explore the entire large data stored in the cloud with the help of the data structure visual frame and the previously developed VISTA visualization model. The latency of processing large data is addressed by the RandGen algorithm that generates a series of related visual frames in the cloud without user's intervention, and a hierarchical exploration model supported by cloud-side subset processing. Experimental study shows this framework is effective and efficient for visually exploring clustering structures for extreme scale datasets stored in the cloud.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Probabilistic polyadic factorization and its application to personalized recommendation</title>
		<abstract>Multiple-dimensional, i.e., polyadic, data exist in many applications, such as personalized recommendation and multiple-dimensional data summarization. Analyzing all the dimensions of polyadic data in a principled way is a challenging research problem. Most existing methods separately analyze the marginal relationships among pairwise dimensions and then combine the results afterwards. Motivated by the fact that various dimensions of polyadic data jointly affect each other, we propose a probabilistic polyadic factorization approach to directly model all the dimensions simultaneously in a unified framework. We then show the connection between the probabilistic polyadic factorization and a non-negative version of the Tucker tensor factorization. We provide detailed theoretical analysis of the new modeling framework, discuss implementation techniques for our models, and propose several extensions to the basic framework. We then apply the proposed models to the application of personalized recommendation. Extensive experiments on a social bookmarking dataset, Delicious, and a paper citation dataset, CiteSeer, demonstrate the effectiveness of the proposed models.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Extraction, characterization and utility of prototypical communication groups in the blogosphere</title>
		<abstract>This article analyzes communication within a set of individuals to extract the representative prototypical groups and provides a novel framework to establish the utility of such groups. Corporations may want to identify representative groups (which are indicative of the overall communication set) because it is easier to track the prototypical groups rather than the entire set. This can be useful for advertising, identifying “hot” spots of resource consumption as well as in mining representative moods or temperature of a community. Our framework has three parts: extraction, characterization, and utility of prototypical groups. First, we extract groups by developing features representing communication dynamics of the individuals. Second, to characterize the overall communication set, we identify a subset of groups within the community as the prototypical groups. Third, we justify the utility of these prototypical groups by using them as predictors of related external phenomena; specifically, stock market movement of technology companies and political polls of Presidential candidates in the 2008 U.S. elections.

We have conducted extensive experiments on two popular blogs, Engadget and Huffington Post. We observe that the prototypical groups can predict stock market movement/political polls satisfactorily with mean error rate of 20.32%. Further, our method outperforms baseline methods based on alternative group extraction and prototypical group identification methods. We evaluate the quality of the extracted groups based on their conductance and coverage measures and develop metrics: predictivity and resilience to evaluate their ability to predict a related external time-series variable (stock market movement/political polls). This implies that communication dynamics of individuals are essential in extracting groups in a community, and the prototypical groups extracted by our method are meaningful in characterizing the overall communication sets.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Isolation-Based Anomaly Detection</title>
		<abstract>Anomalies are data points that are few and different. As a result of these properties, we show that, anomalies are susceptible to a mechanism called isolation. This article proposes a method called Isolation Forest (iForest), which detects anomalies purely based on the concept of isolation without employing any distance or density measure---fundamentally different from all existing methods.

As a result, iForest is able to exploit subsampling (i) to achieve a low linear time-complexity and a small memory-requirement and (ii) to deal with the effects of swamping and masking effectively. Our empirical evaluation shows that iForest outperforms ORCA, one-class SVM, LOF and Random Forests in terms of AUC, processing time, and it is robust against masking and swamping effects. iForest also works well in high dimensional problems containing a large number of irrelevant attributes, and when anomalies are not available in training sample.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Fast detection of meaningful isosurfaces for volume data visualization</title>
		<abstract>Automatic detection of meaningful isosurfaces is important for producing informative visualizations of volume data, especially when no information about the data origin and imaging protocol is available. We propose a computationally efficient method for the automated detection of intensity transitions in volume data. In this approach, the dominant transitions correspond to clear maxima in cumulative Laplacian-weighted gray value histograms. Only one pass through the data volume is required to compute the histogram. Several other features which may be useful for exploration of data of unknown origin can be efficiently computed in a similar manner, e.g. enclosed volume, isosurface area, mean gradient.The detected intensity transitions can be used for setting of visualization parameters for surface rendering, as well as for direct volume rendering of 3-D datasets. When using surface rendering, the detected dominant intensity transition values correspond to the optimal surface isovalues for extraction of boundaries of the objects of interest. In direct volume rendering, such transitions are important for generation of the transfer functions, which are used to assign visualization properties to data voxels and determine the appearance of the rendered image. The proposed method is illustrated by examples with synthetic data as well as real biomedical datasets.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Pruthak: mining and analyzing graph substructures</title>
		<abstract>In many scientific and commercial domains, graph as a data structure has become increasingly important for modeling of sophisticated structures. In the past few years, there has been sharp increase in research on mining graph data. We had proposed a unified framework for graph mining and analysis of extracted substructures, which was then an unattended task. Pruthak, a graph mining tool is developed based on this proposed framework. The tool provides preprocessing, frequent substructure discovery, dense substructure extraction and visualization techniques for graph representation of data. In this paper we discuss the approach taken in design and implementation of Pruthak. We then talk about our study on the Digital Bibliography &amp; Library Project (DBLP) dataset for mining and analyzing substructures using this tool. The study results have demonstrated the intended correctness and usability of the tool.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>A hierarchical information theoretic technique for the discovery of non linear alternative clusterings</title>
		<abstract>Discovery of alternative clusterings is an important method for exploring complex datasets. It provides the capability for the user to view clustering behaviour from different perspectives and thus explore new hypotheses. However, current algorithms for alternative clustering have focused mainly on linear scenarios and may not perform as desired for datasets containing clusters with non linear shapes. Our goal in this paper is to address this challenge of non linearity. In particular, we propose a novel algorithm to uncover an alternative clustering that is distinctively different from an existing, reference clustering. Our technique is information theory based and aims to ensure alternative clustering quality by maximizing the mutual information between clustering labels and data observations, whilst at the same time ensuring alternative clustering distinctiveness by minimizing the information sharing between the two clusterings. We perform experiments to assess our method against a large range of alternative clustering algorithms in the literature. We show our technique's performance is generally better for non-linear scenarios and furthermore, is highly competitive even for simpler, linear scenarios.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Making data analysis expertise broadly accessible through workflows</title>
		<abstract>The demand for advanced skills in data analysis spans many areas of science, computing, and business analytics. This paper discusses how non-expert users reuse workflows created by experts and representing complex data mining processes for text analytics. They include workflows for document classification, document clustering, and topic detection, all assembled from components available in well-known text analytics software libraries. The workflows expose to non-experts expert-level knowledge on how these individual components need to be combined with data preparation and feature selection steps to make the underlying statistical learning algorithms most effective. The framework allows non-experts to easily experiment with different combinations of data analysis processes, represented as workflows of computations that they can easily reconfigure. We report on our experiences to date on having users with limited data analytic knowledge and even basic programming skills to apply workflows to their data.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>CURLER: finding and visualizing nonlinear correlation clusters</title>
		<abstract>While much work has been done in finding linear correlation among subsets of features in high-dimensional data, work on detecting nonlinear correlation has been left largely untouched. In this paper, we present an algorithm for finding and visualizing nonlinear correlation clusters in the subspace of high-dimensional databases.Unlike the detection of linear correlation in which clusters are of unique orientations, finding nonlinear correlation clusters of varying orientations requires merging clusters of possibly very different orientations. Combined with the fact that spatial proximity must be judged based on a subset of features that are not originally known, deciding which clusters to be merged during the clustering process becomes a challenge. To avoid this problem, we propose a novel concept called co-sharing level which captures both spatial proximity and cluster orientation when judging similarity between clusters. Based on this concept, we develop an algorithm which not only detects nonlinear correlation clusters but also provides a way to visualize them. Experiments on both synthetic and real-life datasets are done to show the effectiveness of our method.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>1</relevance>
	</item>
	<item>
		<title>Parallel netCDF: A High-Performance Scientific I/O Interface</title>
		<abstract>Dataset storage, exchange, and access play a critical role in scientific applications. For such purposes netCDF serves as a portable, efficient file format and programming interface, which is popular in numerous scientific application domains. However, the original interface does not provide an efficient mechanism for parallel data storage and access. In this work, we present a new parallel interface for writing and reading netCDF datasets. This interface is derived with minimal changes from the serial netCDF interface but defines semantics for parallel access and is tailored for high performance. The underlying parallel I/O is achieved through MPI-IO, allowing for substantial performance gains through the use of collective I/O optimizations. We compare the implementation strategies and performance with HDF5. Our tests indicate programming convenience and significant I/O performance improvement with this parallel netCDF (PnetCDF) interface.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Mining interlacing manifolds in high dimensional spaces</title>
		<abstract>Real world data are often composed of conceptually meaningful subspaces, e.g., for portraits in a facial image database, the illumination factor corresponds to a nonlinear subspace and the rotation factor corresponds to another. The interlacement of these subspaces may greatly increase the complexity of the data and impede our understanding and further processing. To identify interlacing subspaces and extract the essential structural knowledge from a given dataset, we present a novel approach termed Multi-Manifold Partition (MMP). Global manifolds that corresponds to conceptually meaningful subspaces are discovered in three steps: First, a neighborhood graph is built to capture the intrinsic topological structure of the input data; then, the uniformity of neighboring nodes is analyzed and segments of manifolds are created by connecting adjacent samples that are conform in dimension; finally, segments possibly from the same manifold are combined to obtain a global representation of underlying subspaces. Experimental results on two synthetic datasets and a practical Optical Character Recognition (OCR) problem show that MMP is effective in extracting interlacing structures and thus offers us better interpretation of the nature of the data.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>
	<item>
		<title>Interactive view-dependent rendering of large isosurfaces</title>
		<abstract>We present an algorithm for interactively extracting and rendering isosurfaces of large volume datasets in a view-dependent fashion. A recursive tetrahedral mesh refinement scheme, based on longest edge bisection, is used to hierarchically decompose the data into a multiresolution structure. This data structure allows fast extraction of arbitrary isosurfaces to within user specified view-dependent error bounds. A data layout scheme based on hierarchical space filling curves provides access to the data in a cache coherent manner that follows the data access pattern indicated by the mesh refinement.</abstract>
		<search_task_number>13</search_task_number>
		<query>visualizing dataset</query>
		<relevance>0</relevance>
	</item>





<item>
<title>Evaluation of spam detection and prevention frameworks for email and image spam: a state of art</title>
<abstract>In recent years, online spam has become a major problem for the sustainability of the Internet. Excessive amounts of spam are not only reducing the quality of information available on the Internet but also creating concern amongst search engines and web users. This paper aims to analyse existing works in two different categories of spam domains - email spam and mage spam to gain a deeper understanding of this problem. Future reserch directions are also presented in these spam domains.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Identifying Web Spam with the Wisdom of the Crowds</title>
<abstract>Combating Web spam has become one of the top challenges for Web search engines. State-of-the-art spam-detection techniques are usually designed for specific, known types of Web spam and are incapable of dealing with newly appearing spam types efficiently. With user-behavior analyses from Web access logs, a spam page-detection algorithm is proposed based on a learning scheme. The main contributions are the following. (1) User-visiting patterns of spam pages are studied, and a number of user-behavior features are proposed for separating Web spam pages from ordinary pages. (2) A novel spam-detection framework is proposed that can detect various kinds of Web spam, including newly appearing ones, with the help of the user-behavior analysis. Experiments on large-scale practical Web access log data show the effectiveness of the proposed features and the detection framework.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Learning spam: simple techniques for freely-available software</title>
<abstract>The problem of automatically filtering out spam email using a classifier based on machine learning methods is of great recent interest. This paper gives an introduction to machine learning methods for spam filtering, reviewing some of the relevant ideas and work in the open source community. An overview of several feature detection and machine learning techniques for spam filtering is given. The authors' freely-available implementations of these techniques are discussed. The techniques' performance on several different corpora are evaluated. Finally, some conclusions are drawn about the state of the art and about fruitful directions for spam filtering for freely-available UNIX software practitioners.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Link spam target detection using page farms</title>
<abstract>Currently, most popular Web search engines adopt some link-based ranking methods such as PageRank. Driven by the huge potential benefit of improving rankings of Web pages, many tricks have been attempted to boost page rankings. The most common way, which is known as link spam, is to make up some artificially designed link structures. Detecting link spam effectively is a big challenge. In this article, we develop novel and effective detection methods for link spam target pages using page farms. The essential idea is intuitive: whether a page is the beneficiary of link spam is reflected by how it collects its PageRank score. Technically, how a target page collects its PageRank score is modeled by a page farm, which consists of pages contributing a major portion of the PageRank score of the target page. We propose two spamicity measures based on page farms. They can be used as an effective measure to check whether the pages are link spam target pages. An empirical study using a newly available real dataset strongly suggests that our method is effective. It outperforms the state-of-the-art methods like SpamRank and SpamMass in both precision and recall.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Link analysis for Web spam detection</title>
<abstract>"We propose link-based techniques for automatic detection of Web spam, a term referring to pages which use deceptive techniques to obtain undeservedly high scores in search engines. The use of Web spam is widespread and difficult to solve, mostly due to the large size of the Web which means that, in practice, many algorithms are infeasible.

We perform a statistical analysis of a large collection of Web pages. In particular, we compute statistics of the links in the vicinity of every Web page applying rank propagation and probabilistic counting over the entire Web graph in a scalable way. These statistical features are used to build Web spam classifiers which only consider the link structure of the Web, regardless of page contents. We then present a study of the performance of each of the classifiers alone, as well as their combined performance, by testing them over a large collection of Web link spam. After tenfold cross-validation, our best classifiers have a performance comparable to that of state-of-the-art spam classifiers that use content attributes, but are orthogonal to content-based methods."</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>SpamWatcher: a streaming social network analytic on the IBM wire-speed processor</title>
<abstract>The proliferation of mobile devices, coupled with continuous connectivity, has resulted in a world where massive amounts of data is being produced, on a daily basis, as a result of online interactions between people. These interactions are often captured as relationships in a social network graph, by service providers such as mobile carriers or social web applications. Social network analysis is becoming a common technique for extracting business intelligence from social network graphs in order to improve customer experience and provide better service. Some applications in this domain require processing massive data flows with high throughput and low-latency, in order to deliver timely results. SpamWatcher is a streaming social network analysis application that fits this description. It is used for real-time filtering of short messages in mobile communications, with the goal of preventing spam. The ever increasing volume of mobile users and rates of messages make realtime detection of spam a challenging problem with respect to performance and scalability. In this paper, we present a solution for the SpamWatcher application using the IBM wire-speed processor - a system-on-a-chip with specialized co-processors and integrated network I/O. This solution goes beyond the state-of-the-art by (i) using a novel implementation technique that takes advantage of the pattern matching accelerator to minimize the latency of spam detection, and (ii) employing hardware primitives to reduce the overhead caused by thread synchronization in order to achieve good scalability with respect to number of cores used. Furthermore, the solution is implemented on System S - a commercial-grade stream processing middleware. We evaluate our approach using real-world data sets and experimentally demonstrate the substantial performance improvements it achieves compared to previously published results.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Anomaly Detection in Dynamic Systems Using Weak Estimators</title>
<abstract>Anomaly detection involves identifying observations that deviate from the normal behavior of a system. One of the ways to achieve this is by identifying the phenomena that characterize “normal” observations. Subsequently, based on the characteristics of data learned from the “normal” observations, new observations are classified as being either “normal” or not. Most state-of-the-art approaches, especially those which belong to the family of parameterized statistical schemes, work under the assumption that the underlying distributions of the observations are stationary. That is, they assume that the distributions that are learned during the training (or learning) phase, though unknown, are not time-varying. They further assume that the same distributions are relevant even as new observations are encountered. Although such a “stationarity” assumption is relevant for many applications, there are some anomaly detection problems where stationarity cannot be assumed. For example, in network monitoring, the patterns which are learned to represent normal behavior may change over time due to several factors such as network infrastructure expansion, new services, growth of user population, and so on. Similarly, in meteorology, identifying anomalous temperature patterns involves taking into account seasonal changes of normal observations. Detecting anomalies or outliers under these circumstances introduces several challenges. Indeed, the ability to adapt to changes in nonstationary environments is necessary so that anomalous observations can be identified even with changes in what would otherwise be classified as “normal” behavior. In this article we propose to apply a family of weak estimators for anomaly detection in dynamic environments. In particular, we apply this theory to spam email detection. Our experimental results demonstrate that our proposal is both feasible and effective for the detection of such anomalous emails.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Identifying web spam with user behavior analysis</title>
<abstract>Combating Web spam has become one of the top challenges for Web search engines. State-of-the-art spam detection techniques are usually designed for specific known types of Web spam and are incapable and inefficient for newly-appeared spam. With user behavior analyses into Web access logs, we propose a spam page detection algorithm based on Bayesian Learning. The main contributions of our work are: (1) User visiting patterns of spam pages are studied and three user behavior features are proposed to separate Web spam from ordinary ones. (2) A novel spam detection framework is proposed that can detect unknown spam types and newly-appeared spam with the help of user behavior analysis. Preliminary experiments on large scale Web access log data (containing over 2.74 billion user clicks) show the effectiveness of the proposed features and detection framework.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Relaxed online SVMs for spam filtering</title>
<abstract>Spam is a key problem in electronic communication, including large-scale email systems and the growing number of blogs. Content-based filtering is one reliable method of combating this threat in its various forms, but some academic researchers and industrial practitioners disagree on how best to filter spam. The former have advocated the use of Support Vector Machines (SVMs) for content-based filtering, as this machine learning methodology gives state-of-the-art performance for text classification. However, similar performance gains have yet to be demonstrated for online spam filtering. Additionally, practitioners cite the high cost of SVMs as reason to prefer faster (if less statistically robust) Bayesian methods. In this paper, we offer a resolution to this controversy. First, we show that online SVMs indeed give state-of-the-art classification performance on online spam filtering on large benchmark data sets. Second, we show that nearly equivalent performance may be achieved by a Relaxed Online SVM (ROSVM) at greatly reduced computational cost. Our results are experimentally verified on email spam, blog spam, and splog detection tasks.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Wikipedia vandalism detection: combining natural language, metadata, and reputation features</title>
<abstract>"Wikipedia is an online encyclopedia which anyone can edit. While most edits are constructive, about 7% are acts of vandalism. Such behavior is characterized by modifications made in bad faith; introducing spam and other inappropriate content.

In this work, we present the results of an effort to integrate three of the leading approaches to Wikipedia vandalism detection: a spatio-temporal analysis of metadata (STiki), a reputation-based system (WikiTrust), and natural language processing features. The performance of the resulting joint system improves the state-of-the-art from all previous methods and establishes a new baseline for Wikipedia vandalism detection. We examine in detail the contribution of the three approaches, both for the task of discovering fresh vandalism, and for the task of locating vandalism in the complete set of Wikipedia revisions."</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>User behavior oriented web spam detection</title>
<abstract>Combating Web spam has become one of the top challenges for Web search engines. State-of-the-art spam detection techniques are usually designed for specific known types of Web spam and are incapable and inefficient for recently-appeared spam. With user behavior analyses into Web access logs, we propose a spam page detection algorithm based on Bayes learning. Preliminary experiments on Web access data collected by a commercial Web site (containing over 2.74 billion user clicks in 2 months) show the effectiveness of the proposed detection framework and algorithm.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Fourth international workshop on adversarial information retrieval on the web (AIRWeb 2008)</title>
<abstract>Adversarial IR in general, and search engine spam, in particular, are engaging research topics with a real-world impact for Web users, advertisers and publishers. The AIRWeb workshop will bring researchers and practitioners in these areas together, to present and discuss state-of-the-art techniques as well as real-world experiences. Given the continued growth in search engine spam creation and detection efforts, we expect interest in this AIRWeb to surpass that of the previous three editions of the workshop (held jointly with WWW 2005, SIGIR 2006, and WWW 2007 respectively).</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>The changing face of web search</title>
<abstract>Dr. Prabhakar Raghavan is an invited keynote speaker for PAKDD 2006. Web search has come to dominate our consciousness as a convenience we take for granted, as a medium for connecting advertisers and buyers, and as a fast-growing revenue source for the companies that provide this service. Following a brief overview of the state of the art and how we got there, this talk covers a spectrum of technical challenges arising in web search – ranging from spam detection to auction mechanisms.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Web search: bridging information retrieval and microeconomic modeling</title>
<abstract>Web search has come to dominate our consciousness as a convenience we take for granted, as a medium for connecting advertisers and buyers, and as a fast-growing revenue source for the companies that provide this service. Following a brief overview of the state of the art and how we got there, this talk covers a spectrum of technical challenges arising in web search- ranging from spam detection to auction mechanisms.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>An introspective behavior based methodology to mitigate e-mail based threats</title>
<abstract>"The first part of this dissertation attempts to tackle the problem of detecting phishing e-mails before they reach users’ inboxes. To begin with, shortcomings of existing spam filters toward classifying phishing e-mails are highlighted. To overcome them, a customizable and usable spam filter (CUSP) that detects phishing e-mails from the absence of personalized user information contained in them is proposed. However, as solely relying on the presence of personalized information as the criteria to detect phishing e-mails is not entirely foolproof, a novel machine learning based classifier that separates phishing e-mails based on their underlying semantic behavior is proposed. Experimentation on real word phishing and financial e-mail datasets demonstrates that the proposed methodology can detect phishing e-mails with over 90% accuracy while keeping false positive rate minimum. Also, feasibility of generating context-sensitive warnings that better educate the users about the ill-effects of phishing attacks is explored.
Classification techniques that operate on features confined to the phishing e-mails’ body can be thwarted by using simple obfuscation techniques, which substitute spurious content appearing in them with seemingly innocuous characters or images. To address such scenarios, the second part of this dissertation takes the classification process a step further to analyze the behavior and structural characteristics of Websites referred by URLs contained in e-mails. Specifically, a challenge-response based technique called PHONEY is proposed to detect phishing Websites based on their inability to distinguish fake and genuine inputs apart. Experimental results based on evaluation on both “live” and “synthesized” phishing Websites reveal that PHONEY can detect almost of all the e-mails that link to live phishing Websites with zero false positives and minimal computation overhead. In a similar vein, this dissertation proposes a novel technique to identify spam e-mails by analyzing the content of the linked-to Websites. A combination of textual and structural features extracted from the linked-to Websites is supplied as input to five machine learning algorithms employed for the purpose of classification. Testing on live spam feeds reveal that the proposed technique can detect spam e-mails with over 95% detection rate, thereby exhibiting better performance than two popular open source anti-spam filters.

Information leaks pose significant risk to users’ privacy. An information leak could reveal users’ browsing characteristics or sensitive material contained in their e-mail inboxes to attackers allowing them to launch more targeted social engineering attacks (e.g., spear phishing attacks). The third part of this dissertation focuses on addressing these two facets of information leaks, i.e., information leak triggered by spyware and user by detailing out the limitations with the state-of-the-art detection techniques. In order to bring out the deficiencies in existing anti-spyware techniques, first, a new class of intelligent spyware that efficiently blends in with user activities to evade detection is proposed. As a defensive countermeasure, this dissertation proposes a novel randomized honeytoken based methodology that can separate normal and spyware activities with near perfect accuracy. Similarly, to detect inadvertent informational leaks caused by users sending misdirected e-mails to unintended recipient(s), this dissertation advances the existing bag-of-words based outlier detection techniques by using a set of stylometric and linguistic features that better encapsulate the previously exchanged e-mails between the sender and the recipient. Experimentation on real world e-mail corpus shows that the proposed technique detects over 78% of synthesized information leak outperforming other existing techniques.

Another important point to be considered while devising specialized filters to address each of the e-mail based threat is the need to make them interoperable. For example, an e-mail supposedly sent from a financial domain, but having an URL referring to a domain blacklisted for spam is very likely a phishing e-mail. Identifying sources of attacks helps in developing attack agnostic solutions that block all sensitive communication from and to misbehaving nodes. From this perspective, this dissertation explores the feasibility of building a holistic framework that not only operates in conjunction with intrusion detection systems (IDS) to block incoming and outgoing traffic from and to misbehaving nodes, but also safeguard the underlying e-mail infrastructure from zero-day attacks. (Abstract shortened by UMI.)"</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Detecting spam blogs: an adaptive online approach</title>
<abstract>"Weblogs, or blogs, are an important new way to publish information, engage in discussions, and form communities on the Internet. Blogs are a global phenomenon, and with numbers well over 100 million they form the core of the emerging paradigm of Social Media. While the utility of blogs is unquestionable, a serious problem now afflicts them, that of spam. Spam blogs, or splogs are blogs with auto-generated or plagiarized content with the sole purpose of hosting profitable contextual ads and/or inflating importance of linked-to sites. Though estimates vary, splogs account for more than 50% of blog content, and present a serious threat to their continued utility.
Splogs impact search engines that index the entire Web or just the blogosphere by increasing computational overhead and reducing user satisfaction. Hence, search engines try to minimize the influence of spam, both prior to indexing and after indexing, by eliminating splogs, comment spam, social media spam, or generic web spam. In this work we further the state of the art of splog detection prior to indexing.

First, we have identified and developed techniques that are effective for splog detection in a supervised machine learning setting. While some of these are novel, a few others confirm the utility of techniques that have worked well for e-mail and Web spam detection in a new domain i.e. the blogosphere. Specifically, our techniques identify spam blogs using URL, home-page, and syndication feeds. To enable the utility of our techniques prior to indexing, the emphasis of our effort is fast online detection.

Second, to effectively utilize identified techniques in a real-world context, we have developed a novel system that filters out spam in a stream of update pings from blogs. Our approach is based on using filters serially in increasing cost of detection that better supports balancing cost and effectiveness. We have used such a system to support multiple blog related projects, both internally and externally.

Next, motivated by these experiences, and input from real-world deployments of our techniques for over a year, we have developed an approach for updating classifiers in an adversarial setting. We show how an ensemble of classifiers can co-evolve and adapt when used on a stream of unlabeled instances susceptible to concept drift. We discuss how our system is amenable to such evolution by discussing approaches that can feed into it.

Finally, over the course of this work we have characterized the specific nature of spam blogs along various dimensions, formalized the problem and created general awareness of the issue. We are the first to formalize and address the problem of spam in blogs and identify the general problem of spam in Social Media. We discuss how lessons learned can guide follow-up work on spam in social media, an important new problem on the Web."</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Behavior-based email analysis with application to spam detection</title>
<abstract>"Email is the ""killer network application"". Email is ubiquitous and pervasive. In a relatively short timeframe, the Internet has become irrevocably and deeply entrenched in our modern society primarily due to the power of its communication substrate linking people and organizations around the globe. Much work on email technology has focused on making email easy to use, permitting a wide variety of information and information types to be conveniently, reliably, and efficiently sent throughout the Internet. However, the analysis of the vast storehouse of email content accumulated or produced by individual users has received relatively little attention other than for specific tasks such as spam and virus filtering. As one paper in the literature puts it, ""the state of the art is still a messy desktop"" (Denning, 1982).
The Problem: Email clients provide only partial information - users have to manage much on their own, making it hard to search or prioritize large amounts of email. Our thesis is that advanced data mining can provide new opportunities for applications to increase email productivity and extract new information from email archives.

This thesis presents an implemented framework for data mining behavior models from email data. The Email Mining Toolkit (EMT) is a data mining toolkit designed to analyze offline email corpora, including the entire set of email sent and received by an individual user, revealing much information about individual users as well as the behavior of groups of users in an organization. A number of machine learning and anomaly detection algorithms are embedded in the system to model the user's email behavior in order to classify email for a variety of tasks. The work has been successfully applied to the tasks of clustering and classification of similar emails, spam detection, and forensic analysis to reveal information about user's behavior.

We organize the core functionality of EMT into a lightweight package called the Profiling Email Toolkit (PET). A novel contribution in PET is the focus on analyzing real time email flow information from both an individual and an organization in a standard framework. PET includes new algorithms that combine multiple models using a variety of features extracted from email to achieve higher accuracy and lower false positive than any one individual model for a variety of analytical tasks."</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Using Projection Gradient Method to Train Linear Support Vector Machines</title>
<abstract>Linear Support Vector Machines(SVMs) have broad application in supervised classification problem with high dimensional feature space, such as text classification, word sense disambiguation, email spam detection and etc.. Considering the large volume of available training data, efficient training algorithm for linear SVMs draws many attention from the research community in recent years. Cutting-plane based method is one of the state-of-the-art training algorithms for linear SVMs. Within this cutting-plane framework, the quadratic programming(QP) subproblem, which consists of boundary constraints and a single inequality constraint, need to be solved at each iteration. This step is one of the most time consuming tasks in the whole method. In the current software, the QP subproblems are usually solved by the interior point method. In order to improve the efficiency of the cutting-plane based training algorithm, we transform the inequality constraint to an equation by introducing the slack variable and propose using projection gradient algorithm to solve the transformed QP subproblem. Compared with the existing method, the new algorithm has the following advantages. Firstly, because the special structure information in the subproblem is used carefully, the efficiency of solving the subproblem can be improved significantly. Secondly, through projecting the variables to the bound constraints explicitly, the variables that are not related to support vectors can be identified directly. Therefore, the rounding techniques, which is a necessary step in the widely used interior point method based solvers, is not required anymore. Experimental results on several public data sets also show the effectiveness and efficiency of our new algorithm.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Multi-softcore architectures and algorithms for a class of sparse computations</title>
<abstract>"Field-programmable gate array (FPGA) is a representative reconfigurable computing platform. It has been used in many applications to execute computationally intensive workloads. In this work, we study architectures and algorithms on FPGA for sparse computations. These computations have unique features: (1) the ratio of input and output operations to computation is high and (2) most memory accesses are random with little or no data locality, which leads to low memory bandwidth utilization.
We propose Multiple Application Specific Softcore architecture to overcome the performance hurdles that are inherent to sparse computations. We identify the critical issues, demonstrate our solutions, and validate the proposed architecture using two case studies: large dictionary string matching and breadth-first search on a graph. Our architecture utilizes multiple application-specific processing units (softcores) to exploit the potential thread-level parallelism in these computations. To alleviate the impact of long latency from accessing external memory on system performance, a specialized memory architecture and a scheduling mechanism are devised to reduce the number of accesses to external memory and to hide the effects of the remaining accesses. By utilizing customized interconnects which are adaptive to communication demand, flexible and efficient inter-softcore data exchange and synchronization mechanism are well supported.

The two kernels in our study are among the most common sparse computation algorithms and are of practical significance on their own. String matching searches for all occurrences of a set of patterns (the dictionary) in a string of input data. It is the core function of search engines, intrusion detection systems (IDS), virus scanners, and spam and content filters. In our study on large dictionary string matching, our design achieved a throughput comparable to implementations on state-of-the-art multi-core computing systems. Breadth-first search is a fundamental building block for many graph algorithms, with applications in network analysis, image processing, and database query. Breadth-first search is a difficult kernel to parallelize on cache-based multi-core systems due to its fine-grained random data access and synchronization between threads. We demonstrate that, by using a message passing multi-core architecture with a distributed barrier design, high throughput performance can be obtained using a modest amount of logic resources on FPGA."</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Input-independent, scalable and fast string matching on the Cray XMT</title>
<abstract>String searching is at the core of many security and network applications like search engines, intrusion detection systems, virus scanners and spam filters. The growing size of on-line content and the increasing wire speeds push the need for fast, and often real-time, string searching solutions. For these conditions, many software implementations (if not all) targeting conventional cache-based microprocessors do not perform well. They either exhibit overall low performance or exhibit highly variable performance depending on the types of inputs. For this reason, real-time state of the art solutions rely on the use of either custom hardware or Field-Programmable Gate Arrays (FPGAs) at the expense of overall system flexibility and programmability. This paper presents a software based implementation of the Aho-Corasick string searching algorithm on the Cray XMT multithreaded shared memory machine. Our solution relies on the particular features of the XMT architecture and on several algorithmic strategies: it is fast, scalable and its performance is virtually content-independent. On a 128-processor Cray XMT, it reaches a scanning speed of ≈ 28 Gbps with a performance variability below 10%. In the 10 Gbps performance range, variability is below 2.5%. By comparison, an Intel dual-socket, 8-core system running at 2.66 GHz achieves a peak performance which varies from 500 Mbps to 10 Gbps depending on the type of input and dictionary size.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>System Level Hardening by Computing with Matrices</title>
<abstract>Continuous advances in transistor manufacturing have enabled technology scaling along the years, sustaining Moore's law. As transistors sizes rapidly shrink, and voltage scales, the amount of charge in a node also rapidly decreases. A particle hitting the core will probably cause a transient fault to spam over several clock cycles. In this scenario, embedded systems using state-of-the-art technologies will face the challenge of operating in an environment susceptible to multiple errors, but with restricted resources available to deploy fault-tolerance, as these techniques severely increase power consumption. One possible solution to this problem is the adoption of software based fault-tolerance at the system level, aiming at reduced energy levels to ensure reliability and low energy dissipation. In this paper, we claim the detection and correction of errors on generic data structures at system level by using matrices to encode any program and algorithm. With such encoding, it is possible to employ established techniques of detection and correction of errors occurring in matrices, running with inexpressive overhead of power and energy. We evaluated this proposal using two case studies significant for the embedded system domain. Using the proposed approach, we observed in some cases an overhead of only 5% in performance and 8% in program size.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>0</relevance>
</item>
<item>
<title>Learning user interaction models for predicting web search result preferences</title>
<abstract>Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance. We present a real-world study of modeling the behavior of web search users to predict web search result preferences. Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks. Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected "noisy" user behavior. We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods. We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone. We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>0</relevance>
</item>
<item>
<title>Deep Belief Networks for Spam Filtering</title>
<abstract>This paper proposes a novel approach for spam filter- ing based on the use of Deep Belief Networks (DBNs). In contrast to conventional feedfoward neural networks hav- ing one or two hidden layers, DBNs are feedforward neural networks with many hidden layers. Until recently it was not clear how to initialize the weights of deep neural networks, which resulted in poor solutions with low gene- ralization capabilities. A greedy layer-wise unsupervised algorithm was recently proposed to tackle this problem with successful results. In this work we present a metho- dology for spam detection based on DBNs and evaluate its performance on three widely used datasets. We also compare our method to Support Vector Machines (SVMs) which is the state-of-the-art method for spam filtering in terms of classification performance. Our experiments indicate that using DBNs to filter spam e-mails is a viable methodology, since they achieve similar or even better performance than SVMs on all three datasets.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>The Changing Face of Web Search</title>
<abstract>Web search has come to dominate our consciousness as a convenience we take for granted, as a medium for connecting advertisers and buyers, and as a fast-growing revenue source for the companies that provide this service. Following a brief overview of the state of the art and how we got there, this talk covers a spectrum of technical challenges arising in web search - ranging from spam detection to auction mechanisms.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Wikipedia vandalism detection</title>
<abstract>"Wikipedia is an online encyclopedia that anyone can access and edit. It has become one of the most important sources of knowledge online and many third party projects rely on it for a wide-range of purposes. The open model of Wikipedia allows pranksters, lobbyists and spammers to attack the integrity of the encyclopedia and this endangers it as a public resource. This is known in the community as vandalism.

A plethora of methods have been developed within the Wikipedia and the scientific community to tackle this problem. We have participated in this effort and developed one of the leading approaches. Our research aims to create a fully-working antivandalism system and get it working in the real world."</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>A survey of emerging approaches to spam filtering</title>
<abstract>From just an annoying characteristic of the electronic mail epoch, spam has evolved into an expensive resource and time-consuming problem. In this survey, we focus on emerging approaches to spam filtering built on recent developments in computing technologies. These include peer-to-peer computing, grid computing, semantic Web, and social networks. We also address a number of perspectives related to personalization and privacy in spam filtering. We conclude that, while important advancements have been made in spam filtering in recent years, high performance approaches remain to be explored due to the large scale of the problem.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Information fusion for computer security: State of the art and open issues</title>
<abstract>In this paper, we critically review the issue of information fusion for computer security, both in terms of problem formulation and in terms of state-of-the-art solutions. We also analyze main strengths and weaknesses of currently used approaches and propose some research issues that should be investigated in the future.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>0</relevance>
</item>
<item>
<title>Multimedia data mining: state of the art and challenges</title>
<abstract>Advances in multimedia data acquisition and storage technology have led to the growth of very large multimedia databases. Analyzing this huge amount of multimedia data to discover useful knowledge is a challenging problem. This challenge has opened the opportunity for research in Multimedia Data Mining (MDM). Multimedia data mining can be defined as the process of finding interesting patterns from media data such as audio, video, image and text that are not ordinarily accessible by basic queries and associated results. The motivation for doing MDM is to use the discovered patterns to improve decision making. MDM has therefore attracted significant research efforts in developing methods and tools to organize, manage, search and perform domain specific tasks for data from domains such as surveillance, meetings, broadcast news, sports, archives, movies, medical data, as well as personal and online media collections. This paper presents a survey on the problems and solutions in Multimedia Data Mining, approached from the following angles: feature extraction, transformation and representation techniques, data mining techniques, and current multimedia data mining systems in various application domains. We discuss main aspects of feature extraction, transformation and representation techniques. These aspects are: level of feature extraction, feature fusion, features synchronization, feature correlation discovery and accurate representation of multimedia data. Comparison of MDM techniques with state of the art video processing, audio processing and image processing techniques is also provided. Similarly, we compare MDM techniques with the state of the art data mining techniques involving clustering, classification, sequence pattern mining, association rule mining and visualization. We review current multimedia data mining systems in detail, grouping them according to problem formulations and approaches. The review includes supervised and unsupervised discovery of events and actions from one or more continuous sequences. We also do a detailed analysis to understand what has been achieved and what are the remaining gaps where future research efforts could be focussed. We then conclude this survey with a look at open research directions.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>0</relevance>
</item>
<item>
<title>Detecting splogs via temporal dynamics using self-similarity analysis</title>
<abstract>"This article addresses the problem of spam blog (splog) detection using temporal and structural regularity of content, post time and links. Splogs are undesirable blogs meant to attract search engine traffic, used solely for promoting affiliate sites. Blogs represent popular online media, and splogs not only degrade the quality of search engine results, but also waste network resources. The splog detection problem is made difficult due to the lack of stable content descriptors.

We have developed a new technique for detecting splogs, based on the observation that a blog is a dynamic, growing sequence of entries (or posts) rather than a collection of individual pages. In our approach, splogs are recognized by their temporal characteristics and content. There are three key ideas in our splog detection framework. (a) We represent the blog temporal dynamics using self-similarity matrices defined on the histogram intersection similarity measure of the time, content, and link attributes of posts, to investigate the temporal changes of the post sequence. (b) We study the blog temporal characteristics using a visual representation derived from the self-similarity measures. The visual signature reveals correlation between attributes and posts, depending on the type of blogs (normal blogs and splogs). (c) We propose two types of novel temporal features to capture the splog temporal characteristics. In our splog detector, these novel features are combined with content based features. We extract a content based feature vector from blog home pages as well as from different parts of the blog. The dimensionality of the feature vector is reduced by Fisher linear discriminant analysis. We have tested an SVM-based splog detector using proposed features on real world datasets, with appreciable results (90% accuracy)."</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Fast and Compact Web Graph Representations</title>
<abstract>Compressed graph representations, in particular for Web graphs, have become an attractive research topic because of their applications in the manipulation of huge graphs in main memory. The state of the art is well represented by the WebGraph project, where advantage is taken of several particular properties of Web graphs to offer a trade-off between space and access time. In this paper we show that the same properties can be exploited with a different and elegant technique that builds on grammar-based compression. In particular, we focus on Re-Pair and on Ziv-Lempel compression, which, although cannot reach the best compression ratios of WebGraph, achieve much faster navigation of the graph when both are tuned to use the same space. Moreover, the technique adapts well to run on secondary memory and in distributed scenarios. As a byproduct, we introduce an approximate Re-Pair version that works efficiently with severely limited main memory.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>0</relevance>
</item>
<item>
<title>Laplacian Support Vector Machines Trained in the Primal</title>
<abstract>In the last few years, due to the growing ubiquity of unlabeled data, much effort has been spent by the machine learning community to develop better understanding and improve the quality of classifiers exploiting unlabeled data. Following the manifold regularization approach, Laplacian Support Vector Machines (LapSVMs) have shown the state of the art performance in semi-supervised classification. In this paper we present two strategies to solve the primal LapSVM problem, in order to overcome some issues of the original dualformulation. In particular, training a LapSVM in the primal can be efficiently performed with preconditioned conjugate gradient. We speed up training by using an early stopping strategy based on the prediction on unlabeled data or, if available, on labeled validation examples. This allows the algorithm to quickly compute approximate solutions with roughly the same classification accuracy as the optimal ones, considerably reducing the training time. The computational complexity of the training algorithm is reduced from O(n3) to O(kn2), where nis the combined number of labeled and unlabeled examples and k is empirically evaluated to be significantly smaller than n. Due to its simplicity, training LapSVM in the primal can be the starting point for additional enhancements of the original LapSVM formulation, such as those for dealing with large data sets. We present an extensive experimental evaluation on real world data showing the benefits of the proposed approach.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>0</relevance>
</item>
<item>
<title>The Best Damn Cybercrime and Digital Forensics Book Period</title>
<abstract>"Electronic discovery refers to a process in which electronic data is sought, located, secured, and searched with the intent of using it as evidence in a legal case. Computer forensics is the application of computer investigation and analysis techniques to perform an investigation to find out exactly what happened on a computer and who was responsible. IDC estimates that the U.S. market for computer forensics will be grow from $252 million in 2004 to $630 million by 2009. Business is strong outside the United States, as well. By 2011, the estimated international market will be $1.8 billion dollars. The Techno Forensics Conference has increased in size by almost 50% in its second year; another example of the rapid growth in the market. 

This book is the first to combine cybercrime and digital forensic topics to provides law enforcement and IT security professionals with the information needed to manage a digital investigation. Everything needed for analyzing forensic data and recovering digital evidence can be found in one place, including instructions for building a digital forensics lab.

* Digital investigation and forensics is a growing industry
* Corporate I.T. departments needing to investigate incidents related to corporate espionage or other criminal activities are learning as they go and need a comprehensive step-by-step guide to e-discovery
* Appeals to law enforcement agencies with limited budgets"</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>0</relevance>
</item>
<item>
<title>A survey of learning-based techniques of email spam filtering</title>
<abstract>Email spam is one of the major problems of the today's Internet, bringing financial damage to companies and annoying individual users. Among the approaches developed to stop spam, filtering is an important and popular one. In this paper we give an overview of the state of the art of machine learning applications for spam filtering, and of the ways of evaluation and comparison of different filtering methods. We also provide a brief description of other branches of anti-spam protection and discuss the use of various approaches in commercial and non-commercial anti-spam software solutions.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Adversarial Web Search</title>
<abstract>"Web search engines have become indispensable tools for finding content. As the popularity of the Web has increased, the efforts to exploit the Web for commercial, social, or political advantage have grown, making it harder for search engines to discriminate between truthful signals of content quality and deceptive attempts to game search engines' rankings. This problem is further complicated by the open nature of the Web, which allows anyone to write and publish anything, and by the fact that search engines must analyze ever-growing numbers of Web pages. Moreover, increasing expectations of users, who over time rely on Web search for information needs related to more aspects of their lives, further deepen the need for search engines to develop effective counter-measures against deception.

In this monograph, we consider the effects of the adversarial relationship between search systems and those who wish to manipulate them, a field known as ""Adversarial Information Retrieval"". We show that search engine spammers create false content and misleading links to lure unsuspecting visitors to pages filled with advertisements or malware. We also examine work over the past decade or so that aims to discover such spamming activities to get spam pages removed or their effect on the quality of the results reduced.

Research in Adversarial Information Retrieval has been evolving over time, and currently continues both in traditional areas (e.g., link spam) and newer areas, such as click fraud and spam in social media, demonstrating that this conflict is far from over."</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>A Decision Support System for Placement of Intrusion Detection and Prevention Devices in Large-Scale Networks</title>
<abstract>This article describes an innovative Decision Support System (DSS) for Placement of Intrusion Detection and Prevention Systems (PIDPS) in large-scale communication networks. PIDPS is intended to support network security personnel in optimizing the placement and configuration of malware filtering and monitoring devices within Network Service Providers’ (NSP) infrastructure, and enterprise communication networks. PIDPS meshes innovative and state-of-the-art mechanisms borrowed from the domains of graph theory, epidemic modeling, and network simulation. Scalable network exploitation models enable to define the communication patterns induced by network users (thereby establishing a virtual overlay network), and parallel attack models enable a PIDPS user to define various interdependent network attacks such as: Internet worms, Trojans horses, Denial of Service (DoS) attacks, and others. PIDPS incorporates a set of deployment strategies (employing graph-theoretic centrality measures) in order to facilitate intelligent placement of filtering and monitoring devices; as well as a dedicated network simulator in order to evaluate the various deployments. Experiments with PIDPS indicate that incorporating knowledge on the overlay network (network exploitation patterns) into the placement and configuration of malware filtering and monitoring devices substantially improves the effectiveness of intrusion detection and prevention systems in NSP and enterprise networks.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>0</relevance>
</item>
<item>
<title>Efficient Search Engine Measurements</title>
<abstract>"We address the problem of externally measuring aggregate functions over documents indexed by search engines, like corpus size, index freshness, and density of duplicates in the corpus. State of the art estimators for such quantities [Bar-Yossef and Gurevich 2008b; Broder et al. 2006] are biased due to inaccurate approximation of the so called “document degrees”. In addition, the estimators in Bar-Yossef and Gurevich [2008b] are quite costly, due to their reliance on rejection sampling.

We present new estimators that are able to overcome the bias introduced by approximate degrees. Our estimators are based on a careful implementation of an approximate importance sampling procedure. Comprehensive theoretical and empirical analysis of the estimators demonstrates that they have essentially no bias even in situations where document degrees are poorly approximated.

By avoiding the costly rejection sampling approach, our new importance sampling estimators are significantly more efficient than the estimators proposed in Bar-Yossef and Gurevich [2008b]. Furthermore, building on an idea from Broder et al. [2006], we discuss Rao-Blackwellization as a generic method for reducing variance in search engine estimators. We show that Rao-Blackwellizing our estimators results in performance improvements, without compromising accuracy."</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>0</relevance>
</item>
<item>
<title>Workload models of spam and legitimate e-mails</title>
<abstract>This article presents an extensive characterization of a spam-infected e-mail workload. The study aims at identifying and quantifying the characteristics that significantly distinguish spam from non-spam (i.e., legitimate) traffic, assessing the impact of spam on the aggregate traffic, providing data for creating synthetic workload models, and drawing insights into more effective spam detection techniques. Our analysis reveals significant differences in the spam and non-spam workloads. We conjecture that these differences are consequence of the inherently different mode of operation of the e-mail senders. Whereas legitimate e-mail transmissions are driven by social bilateral relationships, spam transmissions are a unilateral spammer-driven action.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>A review of evolutionary and immune-inspired information filtering</title>
<abstract>In recent years evolutionary and immune-inspired approaches have been applied to content-based and collaborative filtering. These biologically inspired approaches are well suited to problems like profile adaptation in content-based filtering and rating sparsity in collaborative filtering, due to their distributed and dynamic characteristics. In this paper we introduce the relevant concepts and algorithms and review the state of the art in evolutionary and immune-inspired information filtering. Our intention is to promote the interplay between information filtering and biologically inspired computing and boost developments in this emerging interdisciplinary field.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Web page classification: Features and algorithms</title>
<abstract>"Classification of Web page content is essential to many tasks in Web information retrieval such as maintaining Web directories and focused crawling. The uncontrolled nature of Web content presents additional challenges to Web page classification as compared to traditional text classification, but the interconnected nature of hypertext also provides features that can assist the process.

As we review work in Web page classification, we note the importance of these Web-specific features and algorithms, describe state-of-the-art practices, and track the underlying assumptions behind the use of information from neighboring pages."</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Web Crawling</title>
<abstract>This is a survey of the science and practice of web crawling. While at first glance web crawling may appear to be merely an application of breadth-first-search, the truth is that there are many challenges ranging from systems concerns such as managing very large data structures to theoretical questions such as how often to revisit evolving content sources. This survey outlines the fundamental challenges and describes the state-of-the-art models and solutions. It also highlights avenues for future work.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>0</relevance>
</item>
<item>
<title>Bayesian instance selection for the nearest neighbor rule</title>
<abstract>The nearest neighbors rules are commonly used in pattern recognition and statistics. The performance of these methods relies on three crucial choices: a distance metric, a set of prototypes and a classification scheme. In this paper, we focus on the second, challenging issue: instance selection. We apply a maximum a posteriori criterion to the evaluation of sets of instances and we propose a new optimization algorithm. This gives birth to Eva, a new instance selection method. We benchmark this method on real datasets and perform a multi-criteria analysis: we evaluate the compression rate, the predictive accuracy, the reliability and the computational time. We also carry out experiments on synthetic datasets in order to discriminate the respective contributions of the criterion and the algorithm, and to illustrate the advantages of Eva over the state-of-the-art algorithms. The study shows that Eva outputs smaller and more reliable sets of instances, in a competitive time, while preserving the predictive accuracy of the related classifier.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Social Network Analysis and Mining for Business Applications</title>
<abstract>"Social network analysis has gained significant attention in recent years, largely due to the success of online social networking and media-sharing sites, and the consequent availability of a wealth of social network data. In spite of the growing interest, however, there is little understanding of the potential business applications of mining social networks. While there is a large body of research on different problems and methods for social network mining, there is a gap between the techniques developed by the research community and their deployment in real-world applications. Therefore the potential business impact of these techniques is still largely unexplored.

In this article we use a business process classification framework to put the research topics in a business context and provide an overview of what we consider key problems and techniques in social network analysis and mining from the perspective of business applications. In particular, we discuss data acquisition and preparation, trust, expertise, community structure, network dynamics, and information propagation. In each case we present a brief overview of the problem, describe state-of-the art approaches, discuss business application examples, and map each of the topics to a business process classification framework. In addition, we provide insights on prospective business applications, challenges, and future research directions. The main contribution of this article is to provide a state-of-the-art overview of current techniques while providing a critical perspective on business applications of social network analysis and mining."</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>0</relevance>
</item>
<item>
<title>Fast and Scalable Local Kernel Machines</title>
<abstract>A computationally efficient approach to local learning with kernel methods is presented. The Fast Local Kernel Support Vector Machine (FaLK-SVM) trains a set of local SVMs on redundant neighbourhoods in the training set and an appropriate model for each query point is selected at testing time according to a proximity strategy. Supported by a recent result by Zakai and Ritov (2009) relating consistency and localizability, our approach achieves high classification accuracies by dividing the separation function in local optimisation problems that can be handled very efficiently from the computational viewpoint. The introduction of a fast local model selection further speeds-up the learning process. Learning and complexity bounds are derived for FaLK-SVM, and the empirical evaluation of the approach (with data sets up to 3 million points) showed that it is much faster and more accurate and scalable than state-of-the-art accurate and approximated SVM solvers at least for non high-dimensional data sets. More generally, we show that locality can be an important factor to sensibly speed-up learning approaches and kernel methods, differently from other recent techniques that tend to dismiss local information in order to improve scalability.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Online Learning for Matrix Factorization and Sparse Coding</title>
<abstract>Sparse coding--that is, modelling data vectors as sparse linear combinations of basis elements--is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to specific data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>0</relevance>
</item>
<item>
<title>LETOR: A benchmark collection for research on learning to rank for information retrieval</title>
<abstract>LETOR is a benchmark collection for the research on learning to rank for information retrieval, released by Microsoft Research Asia. In this paper, we describe the details of the LETOR collection and show how it can be used in different kinds of researches. Specifically, we describe how the document corpora and query sets in LETOR are selected, how the documents are sampled, how the learning features and meta information are extracted, and how the datasets are partitioned for comprehensive evaluation. We then compare several state-of-the-art learning to rank algorithms on LETOR, report their ranking performances, and make discussions on the results. After that, we discuss possible new research topics that can be supported by LETOR, in addition to algorithm comparison. We hope that this paper can help people to gain deeper understanding of LETOR, and enable more interesting research projects on learning to rank and related topics.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>0</relevance>
</item>
<item>
<title>The application of hidden Markov models in speech recognition</title>
<abstract>"Hidden Markov Models (HMMs) provide a simple and effective framework for modelling time-varying spectral vector sequences. As a consequence, almost all present day large vocabulary continuous speech recognition (LVCSR) systems are based on HMMs.

Whereas the basic principles underlying HMM-based LVCSR are rather straightforward, the approximations and simplifying assumptions involved in a direct implementation of these principles would result in a system which has poor accuracy and unacceptable sensitivity to changes in operating environment. Thus, the practical application of HMMs in modern systems involves considerable sophistication.

The aim of this review is first to present the core architecture of a HMM-based LVCSR system and then describe the various refinements which are needed to achieve state-of-the-art performance. These refinements include feature projection, improved covariance modelling, discriminative parameter estimation, adaptation and normalisation, noise compensation and multi-pass system combination. The review concludes with a case study of LVCSR for Broadcast News and Conversation transcription in order to illustrate the techniques described."</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>0</relevance>
</item>
<item>
<title>Quality evaluation of product reviews using an information quality framework</title>
<abstract>The ubiquity of Web2.0 makes the Web an invaluable source of business information. For instance, product reviews composed collaboratively by many independent Internet reviewers can help consumers make purchase decisions and enable enterprises to improve their business strategies. As the number of reviews is increasing exponentially, opinion mining and retrieval techniques are needed to identify important reviews and opinions to answer users' queries. Most opinion mining and retrieval approaches try to extract sentimental or bipolar expressions from a large volume of reviews. However, the process often ignores the quality of each review and may retrieve useless or even noisy documents. In this paper, we propose a method for evaluating the quality of information in product reviews. We treat the evaluation of review quality as a classification problem and employ an effective information quality framework to extract representative review features. Experiments based on an expert-composed data corpus demonstrate that the proposed method outperforms state-of-the-art approaches significantly.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>BOTMAGNIFIER: locating spambots on the internet</title>
<abstract>"Unsolicited bulk email (spam) is used by cybercriminals to lure users into scams and to spread malware infections. Most of these unwanted messages are sent by spam botnets, which are networks of compromised machines under the control of a single (malicious) entity. Often, these botnets are rented out to particular groups to carry out spam campaigns, in which similar mail messages are sent to a large group of Internet users in a short amount of time. Tracking the bot-infected hosts that participate in spam campaigns, and attributing these hosts to spam botnets that are active on the Internet, are challenging but important tasks. In particular, this information can improve blacklist-based spam defenses and guide botnet mitigation efforts.

In this paper, we present a novel technique to support the identification and tracking of bots that send spam. Our technique takes as input an initial set of IP addresses that are known to be associated with spam bots, and learns their spamming behavior. This initial set is then ""magnified"" by analyzing large-scale mail delivery logs to identify other hosts on the Internet whose behavior is similar to the behavior previously modeled. We implemented our technique in a tool, called BOTMAGNIFIER, and applied it to several data streams related to the delivery of email traffic. Our results show that it is possible to identify and track a substantial number of spam bots by using our magnification technique. We also perform attribution of the identified spam hosts and track the evolution and activity of well-known spamming botnets over time. Moreover, we show that our results can help to improve state-of-the-art spam blacklists."</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>1</relevance>
</item>
<item>
<title>Intelligent agent based artificial immune system for computer security--a review</title>
<abstract>Since its introduction in the 1990s the internet has proliferated in the life of human kind in many numbers of ways. The two by-products of the internet are intelligent agents and intrusions which are far away from each other in the intention of their creation while similar in their characteristics. With automated code roaming the network intruding the users on one side as worms, viruses, and Trojans and autonomous agents tending to help the users on the other side, the internet has given great research challenges to the computer scientists. The greatest challenge of the internet is intrusion, which has increased and never decreased. There are various security systems for the internet. As the Human Immune System protects human body from external attacks, these security systems tend to protect the internet from intruders. Thus the internet security systems are comparable with human immune systems in which autonomous cells move throughout the body to protect it while learning to tackle new threats and keeping them in their memory for the future. These properties are comparable with that of autonomous agents in the internet. Thus intelligent agent technology combined with ideas from human immune system is a great area of research which is still in its developing phase. In this paper, state of the art of security systems which use both these technologies of intelligent agents and artificial immune system i.e., Agent Based Artificial Immune System (ABAIS) for security are reviewed, paying special attention to features of human immune system used in the system, the role of the agents in the ABAIS and the security mechanisms provided against intrusions.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>0</relevance>
</item>
<item>
<title>Fundamental Nonmodularity in Electronic Mail</title>
<abstract>Electronic mail (email) systems have grown in complexity to the point where their reliability and usability are coming into question. The authors of individual email components are widely distributed in both time and space, bound together only by message structure and transfer protocol specifications; consequently filters, autoresponders, and various security features may interact in unanticipated and, often, incomprehensible ways. This paper describes a formalism for modeling and composing specifications of email features, and a feature interaction detection methodology based in part on human intuition and in part on simulation and formal test coverage. The appendix lists 27 interactions found from applying the methodology to ten common email features, a result of independent interest considering the age and relative maturity of the email domain. The paper then proceeds to categorize the interactions according to their impact on the design of the system. This includes a design study of the most natural ways to fix the undesirable behaviors. From this we can infer non-modular dependencies among the features, leading to the result that 9 of the 10 features must be revised (and a custom user interface must be built for them) after they are composed and after feature interactions are detected. This pervasive nonmodularity shows that feature interaction analysis is necessary to optimizing the correctness of an email system design.</abstract>
<search_task_number>12</search_task_number>
<query>spam detection state of the art</query>
<relevance>0</relevance>
</item>


  <item>
    <title>Social Behavior Change in an Evolutionary Online-Game</title>
	<abstract>The purpose of this study is to identify the types of social behavior which change in children during game play in cooperative and competitive scenarios. Researchers developed an evolutionary game named Strike up game, in which students have to calculate numbers to move their flags to the end, to investigate behaviors children display in competitive situations that require cooperation among participants. As the need of validity of study, the teacher and three other graduates watched the videotapes to observe and interpret the interactions taking place among children during the each Co-competition scenario. Consensus is reached after connecting, recording dialogues, and analyzing scenarios of the interaction modes.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>The Social Behaviors of Experts in Massive Multiplayer Online Role-Playing Games</title>
	<abstract>We examine the social behaviors of game experts in Everquest II, a popular massive multiplayer online role-playing game (MMO). We rely on Exponential Random Graph Models (ERGM) to examine the anonymous privacy-protected social networks of 1,457 players over a five-day period. We find that those who achieve the most in the game send and receive more communication, while those who perform the most efficiently at the game show no difference in communication behavior from other players. Both achievement and performance experts tend to communicate with those at similar expertise levels, and higher-level experts are more likely to receive communication from other players.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Expressing My Inner Gnome: Appearance and Behavior in Virtual Worlds</title>
	<abstract>Role-playing gamers take on the behavior they think appropriate for the "body" they inhabit in a virtual environment.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Factors Affecting Online Game Players' Loyalty</title>
	<abstract>In the past decade, online games have become an important electronic commerce application. A good understanding of customer online game behaviors is critical for both researchers and practitioners, such as game vendors and game developers. Many researchers focus their studies on the consumers' intention to play online games. However, the industry becomes more and more interested in the key factors to retain customers. To tackle the retention problem, this paper proposes a research framework of online game play loyalty. Based on this framework, thirteen hypotheses were developed and tested through a survey in U.S. universities. Overall, the results indicate the following: 1) Online game technology factors, such as the game story, game graphics, game length, game control and online game services, have significant impact on players' game enjoyment; 2) Game enjoyment and social norms have positive effects on intention to play; 3) Social norms, quality of online game community and intention to play are important predictors of online game loyalty.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Falling in love with online games: The uses and gratifications perspective</title>
	<abstract>Playing online games is experience-oriented but few studies have explored the user's initial (trial) reaction to game playing and how this further influences a player's behavior. Drawing upon the Uses and Gratifications theory, we investigated players' multiple gratifications for playing (i.e. achievement, enjoyment and social interaction) and their experience with the service mechanisms offered after they had played an online game. This study explores the important antecedents of players' proactive ''stickiness'' to a specific online game and examines the relationships among these antecedents. The results show that both the gratifications and service mechanisms significantly affect a player's continued motivation to play, which is crucial to a player's proactive stickiness to an online game.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Social organization in virtual settings depends on proximity to human visual aspect</title>
	<abstract>Virtual environments are inherently social spaces, in which humans interact through avatars. However, the parameters which favor inter-individual social structuring in those settings are still far to be understood. Particularly, the putative influence of anthropomorphic similarity of visual aspect on social organization of avatars is a key issue to understand the cognitive processes used to form social interactions in virtual worlds. Using the highly popular massively multiplayer online role-playing game World of Warcraft as a model of socially-active virtual setting, we analyzed the social behavior of 11,649 avatars as a function of their visual aspect. Our results show that social structuring in virtual settings depends on proximity to human visual aspect. Social groups formed by human-like avatars display more homogeneity than what the optimal use of the interface would predict, while this effect is not observed for social groups formed by non-human avatars. Thus, immersion in virtual environments depends more on visually-triggered social dynamics (role-play) than on optimal use of the interface (game-play). Furthermore, social aspect may override the immediate reward of interface optimization, thus representing a major factor of immersion in virtual environments.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Why would online gamers share their innovation-conducive knowledge in the online game user community? Integrating individual motivations and social capital perspectives</title>
	<abstract>The user community has been an important external source of a firm's product or service innovation. Users' innovation-conducive knowledge sharing enables the community to work as a vital source of innovation. But, traditional economic theories of innovation seem to provide few explanations about why such knowledge sharing takes place for free in the user community. Therefore, this study investigates what drives community users to freely share their innovation-conducive knowledge, using the theory of planned behavior. Based on an empirical analysis of the data from 1244 members of a South Korean online game user community, it reveals that intrinsic motivation, shared goals, and social trust are salient factors in promoting users' innovation-conducive knowledge sharing. Extrinsic motivation and social tie, however, were found to affect such sharing adversely, contingent upon whether a user is an innovator or a non-innovator. The study illustrates how social capital, in addition to individual motivations, forms and influences users' innovation-conducive knowledge sharing in the online gaming context.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Mediating roles of self-presentation desire in online game community commitment and trust behavior of Massive Multiplayer Online Role-Playing Games</title>
	<abstract>Massive Multiplayer Online Role-Playing Games (MMORPGs), which allow simultaneous participation of several gamers, have attracted a great deal of attention recently. Since MMORPGs can be categorized as a type of online community, the behavior of MMORPGs users needs to be considered as the general behavior in online communities. However, previous studies of online communities did not pay enough attention to MMORPGs, in which users can express themselves by interacting actively through games and game avatars. Understanding the characteristics of MMORPGs as online game communities where users communicate and interact will allow games to be vitalized and users to be immersed in games in a more positive way. Hence, using self-presentation theory and social identity theory, this study examined the factors influencing self-presentation desire and the mediating role of self-presentation desire examined in terms of trust of and commitments to online game communities. The results showed that the interactivity in the spaces of MMORPGs had the biggest impacts on self-presentation desire; personal innovativeness and game design quality also was influential. The results also indicated that self-presentation desire caused trust of online games and eventually led to even stronger commitments to gamers.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>An analysis of social gaming networks in online and face to face bridge communities</title>
	<abstract>Online social games are Internet-based games that use the social networks formed by players to extend in-game functionality. For example, gamers participating in the BBO Fans community combine online bridge play with social networking. Despite an increase in the popularity of online social gaming---currently, there exist over one million online bridge players---, and of decades of research on social networks, the activity characteristics and the community structure of online social gaming remain relatively unknown. In this work we investigate and contrast these aspects for two bridge communities, BBO Fans (online) and Locomotiva (face to face). We propose the use of playing relationships instead of traditional social relationships such as friends and friends-of-friends. Using long-term, large-scale data we have collected from both the online and face to face bridge communities, we analyze user behavior, social network structure, and playing style in bridge communities. We find many similar characteristics in the two studied communities, but we also find more variation in the activity levels and fewer stable partnerships for the face to face bridge community.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Network game design: hints and implications of player interaction</title>
	<abstract>While psychologists analyze network game-playing behavior in terms of players' social interaction and experience, understanding user behavior is equally important to network researchers, because how users act determines how well network systems, such as online games, perform. To gain a better understanding of patterns of player interaction and their implications for game design, we analyze a 1, 356-million-packet trace of ShenZhou Online, a mid-sized commercial MMORPG. This work is dedicated to draw out hints and implications of player interaction patterns, which is inferred from network-level traces, for online games.

We find that the dispersion of players in a virtual world is heavy-tailed, which implies that static and fixed-size partitioning of game worlds is inadequate. Neighbors and teammates tend to be closer to each other in network topology. This property is an advantage, because message delivery between the hosts of interacting players can be faster than between those of unrelated players. In addition, the property can make game playing fairer, since interacting players tend to have similar latencies to their servers. We also find that participants who have a higher degree of social interaction tend to play much longer, and players who are closer in network topology tend to team up for longer periods. This suggests that game designers could increase the "stickiness" of games by encouraging, or even forcing, team playing.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Social regulation in an online game: uncovering the problematics of code</title>
	<abstract>More and more interaction is becoming code-based. Indeed, in online worlds, it is all there is. If software is providing a new basis for social interaction, then changing the infrastructure of interaction may necessarily change social interaction in important ways. As such, it is critical to understand the implications of code - we want to know what the use of code means for socio-technical design.

In this paper, based on an ethnographic study of an online game, we examine social regulation in an online game world as a case study of socio-technical design using code. We wanted to know how changing interaction based in code conditioned use in our site. We found that code changed social regulation in three specific ways. First, code made some user actions that were socially unwanted to be immediately visible. Second, code could prevent some actions from occurring or punish users immediately. Finally, software was not able to see all action. Some user actions were too nuanced or subtle for code to catch; others were too ambiguous to place into code. Following Agre, we argue i that a "grammar of action" resulting from the use of code limits the kinds of behaviors that can be seen and dealt with.

These findings suggest that there is more than just a gap between the social world and technical capabilities. There are new possibilities, tradeoffs, and limitations that must be considered in socio-technical design, and all come simultaneously.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>    
  <item>
    <title>Segmenting online gamers by motivation</title>
	<abstract>Online games are becoming one of the most profitable online entertainment businesses. Nevertheless, online game players may have diverse motivations for playing online games, which in turn influence their behaviors and preferences in playing online games. Through factor analysis for dimensionality reduction, this study identifies two underlying motivational factors: (1) the need for exploration, and (2) the need for aggression. From these two factors, this study clusters online gamers into three segments: (1) the aggressive gamers, (2) the social gamers, and (3) the inactive gamers. The aggressive gamers score highest on both factors. The social gamers score high on the first factor, but score lowest on the second factor. The inactive gamers score lowest on the first factor, and score in the middle on the second factor. Significant differences exist in consumer behaviors across different segments of online gamers. Theoretical supports for the two motivational factors are discussed, and managerial implications for online game service firms are provided.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>    
  <item>
    <title>When social networking meets online games: the activity system of grouping in world of warcraft</title>
	<abstract>Using activity theory and genre theory as bases for analysis, this article examines the activity of grouping in the massively multiplayer online role-playing game World of Warcraft. The article first examines what grouping does in the overall context of the game, including its socialization and gameplay functions. Grouping involves a series of interactions and conventions that structure gameplay objectives and model expected behavior. Groups are formed through specific interfaces in the game that enact social networking processes and can be examined comparatively alongside web-based social networking technologies. By looking at interface design, the article identifies how grouping as an activity is mediated and what social expectations are put in place when players participate in groups.

Next, the article considers the role of other texts in the activity system of grouping, focusing on two examples: the game FAQ and message board. The FAQ is especially noteworthy as an instance of player-produced technical writing. Players write FAQs to document basic game information, present game strategies and walkthroughs, and help other players solve problems. As a genre, this type of FAQ is characterized by a number of tensions that can productively challenge how we think about the conventions of electronic and printed text. FAQ writers make different kinds of rhetorical moves that situate the text within the larger activity of gaming and outline specific purposes and audiences. The FAQ is also examined here in the context of message board interaction, which is used in part for more localized discourse that the FAQ does not address specifically. Working at various levels of abstraction and consolidation, this system of online texts mediates group activity and provides a space for extra-game interactions to directly influence the in-game interactions and behaviors of players (and vice versa).</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>    
  <item>
    <title>The influence of social experience in online games</title>
	<abstract>The objective of this study is to explore the influence of social behaviors in the online games on the behaviors in the real world. Four kinds of social behaviors in the online games such as chatting, making friends, role playing, team work are proposed, which these behaviors tie virtual world and real society. Based on the data from 101 effective respondents, this study analyzes the relationship between average playing time, social behavior in the real world and social behaviors in the game world. The results find that social behaviors in the game environment change significantly players' social behavior in the real world, and also contribute largely to more behavior repetition.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>    
  <item>
    <title>Time will tell: in-game social interactions that facilitate second language acquisition</title>
	<abstract>Prior research indicates that gameplay experiences attributed to Massively Multiplayer Online Role Playing Games (MMORPGs) foster foreign language students' ability to communicate with players in the target language. We take a closer look at the role of language socialization in MMORPGs, specifically dialogue between native and non-native speakers. We modify the Dialogue Acts Markup in Several Layers code scheme to represent in-game social interactions and apply this code scheme to players' game dialogue. We then develop ClockWerk©, an evaluation tool that visually depicts communication patterns of game dialogue attributed to MMORPGs. ClockWerk© graphically detects dominant behaviors of linguistically diverse groups of players over time. ClockWerk© enables users to temporally correlate the type of social interactions with gameplay activities, gauging their impact on second language acquisition.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Principles of emergent design in online games: Mermaids phase 1 prototype</title>
	<abstract>This paper outlines the first phase prototype of Mermaids, a massively multiplayer online game (MMOG) being developed by Georgia Tech's Emergent Game Group {EGG}. We describe Mermaids in the context of the group's research mission, to develop specific games, techniques and design features that promote large-scale emergent social behavior in multiplayer games. We also discuss some of the innovative design features of the Mermaids game, and describe the rapid prototyping and iterative development process that enabled us to create a working prototype in a relatively short period of time on a zero budget project using a student-based development team. We also discuss the special challenges encountered when trying to develop a nontraditional game, one of whose stated research goals is to interrogate MMOG conventions, using a relatively conventional game engine.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>A characterization of online browsing behavior</title>
	<abstract>In this paper, we undertake a large-scale study of online user behavior based on search and toolbar logs. We propose a new CCS taxonomy of pageviews consisting of Content (news, portals, games, verticals, multimedia), Communication (email, social networking, forums, blogs, chat), and Search (Web search, item search, multimedia search). We show that roughly half of all pageviews online are content, one-third are communications, and the remaining one-sixth are search. We then give further breakdowns to characterize the pageviews within each high-level category.

We then study the extent to which pages of certain types are revisited by the same user over time, and the mechanisms by which users move from page to page, within and across hosts, and within and across page types. We consider robust schemes for assigning responsibility for a pageview to ancestors along the chain of referrals. We show that mail, news, and social networking pageviews are insular in nature, appearing primarily in homogeneous sessions of one type. Search pageviews, on the other hand, appear on the path to a disproportionate number of pageviews, but cannot be viewed as the principal mechanism by which those pageviews were reached.

Finally, we study the burstiness of pageviews associated with a URL, and show that by and large, online browsing behavior is not significantly affected by "breaking" material with non-uniform visit frequency.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Strategic Behavior in Interaction Selection and Contact Selection</title>
	<abstract>Social search platforms like Aardvark or Yahoo Answers have attracted a lot of attention lately. In principle, participants have two strategic dimensions in social search systems: (1) Interaction selection, i.e., forwarding/processing incoming requests (or not), and (2) contact selection, i.e., adding or dropping contacts. In systems with these strategic dimensions, it is unclear whether nodes cooperate, and if they form efficient network structures. To shed light on this fundamental question, we have conducted a study to investigate human behavior in interaction selection and to investigate the ability of humans to form efficient networks. In order to limit the degree of problem understanding necessary by the study participants, we have introduced the problem as an online game. 193 subjects joined the study that was online for 67 days. One result is that subjects choose contacts strategically and that they use strategies that lead to cooperative and almost efficient systems. Surprisingly, subjects tend to overestimate the value of cooperative contacts and keep cooperative but costly contacts. This observation is important: Assisting agents that help subjects to avoid this behavior might yield more efficiency.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Decreasing online 'bad' behavior</title>
	<abstract>'Bad' behavior is a serious problem in many online social situations, such as chat rooms. One potential reason is that social norms for 'proper' interpersonal behavior are not invoked in these situations as they are in face-to-face interactions. We describe a game we developed to explore good and bad behavior in computer-mediated situations. We found that increasing the 'social' nature of the interaction through voice communication between game partners decreased aversive behavior, but having profile information about the other person had little impact.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Design implications of social interaction in online games</title>
	<abstract>While psychologists analyze network game-playing behavior in terms of players' social interaction and experience, understanding user behavior is equally important to network researchers, because how players act determines how well systems, such as MMORPGs, perform. To gain a better understanding of patterns of player interaction and their implications for game design, we analyze a 1,356-million-packet trace of ShenZhou Online, a mid-sized commercial MMORPG. To the best of our knowledge, this work is the first to put forward architectural design recommendations for online games based on analysis of player interaction.

We find that the dispersion of players in a virtual world is heavy-tailed, which implies that static and fixed-size partitioning of game worlds is inadequate. Neighbors and teammates tend to be closer to each other in network topology. This property is an advantage, because message delivery between the hosts of interacting players can be faster than between those of unrelated players. In addition, the property can make game playing fairer, since interacting players tend to have similar latencies to their servers. We also find that participants who have a higher degree of social interaction tend to play much longer, and players who are closer in network topology tend to team up for longer periods. This suggests that game designers could increase the “stickiness” of games by supporting, or even forcing, team playing.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>My guild, my people: role of guilds in massively multiplayer online games</title>
	<abstract>Massively Multiplayer Online Games continue to grow and attract more users. The social aspect of MMOs differentiates them from single person games, increase user loyalty and often result in users spending increasing amounts of time in these virtual environments. We examine World of Warcraft guilds and identify three components of group identity: affective, behavioral and cognitive components. We present the results of our online survey indicating that the affective component, users liking each other and enjoying their interaction with each other is the strongest component of group identity. The result is significant in understanding user behavior and loyalty in MMOs.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Behavioral traits of the online parent-child game players: a case study and its inspirations</title>
	<abstract>By examining the online parent-child game entitled Island Survival, this paper probes into the behavioral traits of parent-child players by means of case study. The research mainly focuses on the following three points: (1). What kind of behaviors do the players conduct in the Online Parent-Child Game? (2). How many types of family relationship can be concluded on the basis of analyzing the players' behavioral traits? (3). With different types of family relationship sampled, check whether those types of family relationship are in accordance with the degrees of family intimacy based on the adaptive scales(FACESII-CV) or not. In the experiment, we choose students of two classes in a junior high school, as our samples, totaling 60 pairs of parent-child who are invited to experience a kind of parent-child game for 8 weeks, accumulating the storable and renewable database by means of snap-shooting and the temporarily deposited database in the server in the operation of games by a passive information-driven and an active rule-driven accumulation, and analyzing the dynamic data from the players' behaviors; We also choose four pairs of typical parent-child players. By further analyzing the social relationships of the families taking part in the online game, the time spanning of the parent-child players, and the branch task success ratio of parent role in parental education, the parent- kid family are classified into three different types: harmonious type, constructive type and crisis type, according to the players' behavioral traits; more importantly, the way of classifying offers critical enlightenment to the design of online parent-child games.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
      <item>
    <title>Proceedings of the third international workshop on Large-scale system and application performance</title>
	<abstract>It is our great pleasure to welcome you to the third Workshop on Large-scale System and Application Performance -- LSAP2011. We initiated this workshop two years ago as we think it addresses a very important and timely subject. Over the last decade, computer systems and applications in everyday use have grown to unprecedented scales. Large clusters serving millions of search requests per day, grids executing large workflows and parameter sweeps consisting of thousands of jobs, and supercomputers running complex e-science applications, have now hundreds of thousands of processing cores. In addition, clouds are quickly emerging as a large-scale computing infrastructure. Peer-to-peer systems and centralized video distribution systems that dominate the internet and complicated internet applications such as massive multiplayer online games are used by millions of people every day.

In view of this tremendous growth, understanding the performance of large-scale computer systems and applications has become vital to institutional, commercial, and private interests. This workshop intends to be a venue for papers on performance evaluation methods, tools, and studies focusing on the challenges of large scale, such as decentralization, predictable performance, reliability, and scalability. It aims to bring together system designers and researchers involved with the modeling and performance evaluation of largescale systems and applications.

The call for papers of the workshop attracted 8 submissions, out of which the program committee accepted 5 papers that cover a variety of topics, ranging from lock thrashing in multicore systems and file systems for exascale computing, to the visual analysis of I/O system behavior, the multi-scale analysis of large distributed systems, and the analysis of social gaming networks. In addition, the workshop program features a keynote presentation by Marc Snir of the University of Illinois at Urbana-Champaign, USA, on performance engineering for petascale systems and beyond.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>0</relevance>
  </item>
      <item>
    <title>Exploring Game Leadership and Online Game Community</title>
	<abstract>Online games have become a major leisure activity in recent years. Players go through a variety of social experiences, building up game communities such as guilds, blood pledges, and other small teams in virtual worlds. Game communities offer similar organizational experiences to those in real life. The game environment works as the third place where we can learn and develop our leadership skills. However, so far, not much research has been done in this area. This study examines the relationship between game players' usage behavior focusing on game community participation and game leadership. A total of 808 South Korean online game players participated in an online survey during one week. The results show that game community participation, team play, and the active attitude in accomplishing the mission of a game were positively related with game leadership. The outcome of this study provides a new perspective on the online game community to expanding our social experiences and improving game leadership in the virtual world.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
      <item>
    <title>Project massive: the social and psychological impact of online gaming</title>
	<abstract>Millions of people play online games around the world, some for forty hours per week or more. Speculation abounds about both the positive and negative effects such a popular and time intensive activity might have on those who take part in it. In order to investigate how participation in these virtual worlds and the communities surrounding them might affect the player, three general research questions are posed. What factors contribute to players reporting that gaming has gone beyond being an engaging pass-time and begun to cause problems in their real life? Does play lead to social isolation or, instead, to an expansion in the social connectedness a player feels? Does involvement in online gaming lead to depression or can participation reduce depressive affect? 

Following a pilot survey involving 1836 respondents, a revised online survey was used to gather information from avid gainers about their gaming habits, attitudes, and feelings. Employing a longitudinal design, three waves of data were collected over a 14 month period from a sample including 2883 online gainers. Prospective analysis was used to establish causal and temporal linkages among the repeatedly measured factors. While the data provide some indication that a player's reasons for playing do influence the development of problematic usage, these effects are overshadowed by the central importance of self-regulation in managing both the timing and amount of play. An individual's level of self-regulatory activity is shown to be very important in allowing them to avoid negative outcomes like problematic use and, more broadly, depression. Further, the results indicate that participation in online gaming can lead to decreased isolation and enhanced social integration for those players who use online gaming as a medium in which to spend time and interact with real life friends and relatives. No causal link between online gaming and depression is observed, even in those individuals who report viewing their use as problematic. With responsible use, online gaming appears to be a healthy recreational activity that provides millions of people with hours of social entertainment and adaptive diversion. However, failure to manage play behavior can lead to feelings of dependency. Strategies and tools for addressing these self-regulatory deficits and supporting self-regulatory activity with respect to gaming behavior are discussed.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
      <item>
    <title>Lifestyles of Virtual World Residents, Living in the on-line game, "Lineage"</title>
	<abstract>This study attempted to explore different lifestyles ofgame players who have adopted the virtual world as partof their life in the online game world. An online surveywas conducted to the players of one of famous Internetbased games called 'Lineage'. 'Lineage' has become thebiggest online game in the world accommodating over 6million users worldwide, where people create his or hernew identities and play various roles. 4,786 game playershave participated in the survey, and their lifestyles wereidentified with their values and attitudes in the virtualworld. Upon classification of their lifestyles, thebehavioral tendencies and characteristic desires werecompared by the lifestyles in virtual world. The studyshowed that game players have developed their owndistinctive lifestyles, and the lifestyles were strongcriterion on explaining the behavior patterns and theirdifferent desire to achieve in the world among them. Thelifestyles were classified into three general categories;Single-Oriented Player, Community-Oriented Player, Off-realWorld Gamer. Each group displayed distinctdifferences in their values and game activities, as well asanti-social behavior tendencies. The differences werereflecting not only their personality but also their socio-economicstatus within the virtual world that isconstructed through the game activities. This study hopesto serve as a model to understand how players fromdifferent real life backgrounds will behave to the variousgame features and how they adopt the virtual world fortheir new social identities.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
      <item>
    <title>Guest Editors' Introduction: Social Computing</title>
	<abstract>Broadly stated, social computing takes a computational approach to the study and modeling of social interactions and communications. It also encompasses the development of technologies supporting these interactions. In recent years, we've seen social computing impact numerous information and communications technology (ICT) fields. It's attracted significant interest from not only researchers in the computing and social sciences but also software and online game vendors, Web entrepreneurs, political analysts, and digital-government practitioners, among others. This special issue samples the state of the art social-computing research from several perspectives: the overall paradigm of social-computing research; technological support for social-computing applications; cognitive modeling and architecture of agents and agent societies; and social-computing applications in areas such as terrorist network analysis, competitive business strategies, and agent behavior in financial markets. This article is part of a special issue on social computing.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>0</relevance>
  </item>
      <item>
    <title>Personality Matters: Incorporating Detailed User Attributes and Preferences into the Matchmaking Process</title>
	<abstract>Finding ways of reducing undesired behavior in online interactions is at the forefront of the social computing research agenda. One promising way to reduce perceived "bad behavior" is by matching members of online social environments with corresponding behavioral preferences. We present an empirical study (N = 267) of an experimental system for matching users. As a test bed we chose the online game MechAssault, which largely supports one-off encounters within a socially homogenous population. Even within this population we found great variability in the way users selected their gaming partners. One type of player chose partners mainly on their skill, another mainly on a friendly gaming personality, and a third preferred aggressive players. Detailed analyses revealed the underlying attributes of user profiles that generated these user types. The findings suggest that a matchmaking system can better promote desired online interactions than the enforcement of uniform behavioral standards.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
      <item>
    <title>Beyond Barbie and Mortal Kombat: New Perspectives on Gender and Gaming</title>
	<abstract>Ten years after the groundbreaking From Barbie to Mortal Kombat highlighted the ways gender stereotyping and related social and economic issues permeate digital game play, the number of women and girl gamers has risen considerably. Despite this, gender disparities remain in gaming. Women may be warriors in World of Warcraft, but they are also scantily clad "booth babes" whose sex appeal is used to promote games at trade shows. Player-generated content has revolutionized gaming, but few games marketed to girls allow "modding" (game modifications made by players). Gender equity, the contributors to Beyond Barbie and Mortal Kombat argue, requires more than increasing the overall numbers of female players. Beyond Barbie and Mortal Kombat brings together new media theorists, game designers, educators, psychologists, and industry professionals, including some of the contributors to the earlier volume, to look at how gender intersects with the broader contexts of digital games today: gaming, game industry and design, and serious games. The contributors discuss the rise of massively multiplayer online games (MMOs) and the experience of girl and women players in gaming communities; the still male-dominated gaming industry and the need for different perspectives in game design; and gender concerns related to emerging serious games (games meant not only to entertain but also to educate, persuade, or change behavior). In today's game-packed digital landscape, there is an even greater need for games that offer motivating, challenging, and enriching contexts for play to a more diverse population of players. Contributors: Cornelia Brunner, Shannon Campe, Justine Cassell, Mia Consalvo, Jill Denner, Mary Flanagan, Janine Fron, Tracy Fullerton, Elisabeth Hayes, Carrie Heeter, Kristin Hughes, Mizuko Ito, Henry Jenkins, Yasmin B. Kafai, Caitlin Kelleher, Brenda Laurel, Nicole Lazzaro, Holin Lin, Jacki Morie, Helen Nissenbaum, Celia Pearce, Caroline Pelletier, Jennifer Y. Sun, T. L. Taylor, Brian Winn, Nick Yee. Interviews with: Nichol Bradford, Brenda Braithwaite, Megan Gaiser, Sheri Graner Ray, Morgan Romine.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
      <item>
    <title>Users' Influence on the Success of Online Communities</title>
	<abstract>This paper examines how different facets of favorable user behavior drive the success of an online community by using a unique data set with surveys from 1,389 participants from the German branch of an international operating massively multiplayer online game (MMOG) community and enhancing it with the game's log files. We argue that success of the community depends on how these users contribute to the underlying success dimensions content, revenue, loyalty and growth. At this, we identify the importance and impact of overlapping online and offline social networks. Moreover, we find that satisfaction with the platform and commitment to the community have a major influence on our success variables. Interestingly, the influence of commitment on favorable user behavior is not moderated by membership tenure.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
      <item>
    <title>Competitive Advantages of the Adoption and Use of Technological Systems in Hospitality Marketing: The Case Study of Croatia</title>
	<abstract>Variable economic conditions, new forms of customer buying behavior and in particular new technology is likely to cause the emergence of new or growing existing hospitality markets. In developed European economies, all the more attention is paid to studying the role of new technologies in the field of hospitality marketing. Research focus will be directed towards the usage of technological systems in marketing of hospitality. Internet, as one of the most significant technological phenomena of our time, provides hotel managers completely new competitive opportunities, of which the most significant opportunity to provide immediate and always open access to information throughout the world. According to latest researches, Croatian hotel managers usually use these technological systems: Own web site, System to access the Internet, Points of sale (POS), System of calculating the telephone calls, Local area network (LAN), Uniform system of accounts for lodging Industry (USALI), Intranet system, Central reservation system (CRS), Marketing information system (MIS) and. Object management system (PMS). Modern hotel managers want to achieve automation of marketing strategies and interactive communication with potential guests. However, certain hotels are still not developed a marketing strategy through the Internet pages. Adjustment trends must be timely and possible, so it is necessary to intensify investment in that direction. As a result, it is assumed that the possibility of information and purchase via the Internet to stimulate the future purchase of tourists and change their previous spending habits. To make hotel companies improve their e-marketing strategy, the research shows insufficient usage of the following Web 2.0 tools in Croatian hospitality market: Instant Messaging, Internet Relay Chat, Internet Forums, Social Network Services, Social Guides, Social Bookmarking, Social Reputation Network, Web logs, Social Citations, Peer-to-peer Social Networks, Virtual Presence, Virtual Worlds &amp; Massively Multiplayer Online Games, VOIP -- Internet Telephony and Mobile Internet. Establishing relationships and technological innovations significantly affect the shortening of business cycles and thus the importance of creating competitive advantages in the hospitality industry of Croatia.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>0</relevance>
  </item>
      <item>
    <title>The Study of Human Behavior Dynamics Based on Blogosphere</title>
	<abstract>Blog and microblog have become one of the most popular applications which individuals could be the message source. Therefore, interactivities between individuals have been largely enhanced in today’s world. In terms of message delivery and human behavior, understanding the mechanism behind is of significance. Though human dynamics has become a popular subject, most of relevant studies so far used the Poisson process to approximately describe the human behavior. However, extensive evidences like email exchange, online game, and mobile communication showed a non-Poisson statistics with heavy tail in those applications. Meanwhile the study in human dynamics is still rare to some extent. Thus in this paper, the authors provided empirical evidences to support such a non-Poisson statistics being existed in the web2.0 applications blog and microblog, also showing the difference between these two applications. The fact that both human behavior in blog and microblog follow a heavy tailed distribution with α≈1.3 and α≈2 will be revealed. At last, the study of human behavior related to social network can also be enriched.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>0</relevance>
  </item>
      <item>
    <title>Impacts of net-generation attributes, seductive properties of the internet, and gratifications-obtained on internet use</title>
	<abstract>The Net-generation is a new generation who was born between 1977 and 1997. But unlike their parents, they are not defined by demographics alone, but rather by a combination of their demographic cohort, values, life experiences, and behavior. Based on the assumption that the Net-geners will exhibit differences from the boomers, the purpose of this study is to (1) identify attributes that can distinctly characterize the Net-generation and (2) examine how these attributes, together with perceived seductive properties of the Internet (e.g., pleasure of control and fluidity of identity) and gratifications-obtained from Internet use can predict Net-geners' popular Internet activities. Using a probability sample, a telephone survey was conducted with 976 respondents in the age of 16-24. Exploratory factor analysis found that Net-geners are (1) strongly principled and believe in fundamental rights to information, (2) emotionally open on the Net, (3) innovative and investigative, and (4) independent, confident, and preoccupied with maturity. Results of hierarchical regression analysis show that Net-geners are emotionally open individuals who use the Internet primarily as a social technology. Heavy users of the Internet were motivated by the ability through the Internet to show affections, to establish social bonds, and to escape. Most important, heavy users of the Internet often enjoyed the illusory power of being able to control the world inside the computer when playing online games and attracted by the ability of the Internet to offer companionship in the virtual world. Furthermore, Net-geners were fascinated by the capacity of the Internet to allow them to present a different persona when interacting online such as chatting on ICQ. This "fluidity of identity" gives Net-geners a feeling of status and modernity, which may bolster their self-esteem.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>0</relevance>
  </item>
        <item>
    <title>Hanging out in the Virtual Pub: Masculinities and Relationships Online</title>
	<abstract>From the Publisher:
"Hanging Out at the Virtual Pub provides a richly detailed and theoretically mediated understanding of gender's significance in online-game playing. This examination of the behavior of educated techno-elites helps us understand communities and larger social trends. Kendall's vision is neither utopian nor dystopian. People will buy this timely book."-Anita Allen, professor of Law and Philosophy, University of Pennsylvania
"This wonderful book is readable, enjoyable, and lively, providing a fascinating look into a world not known by many. It will appeal to those who have interests in computers and the friendship communities that evolve in cyberspace, as well as to those working on gender and race issues."-Peter Nardi, author of Gay Men's Friendships:Invincible Communities

Author Biography:Lori Kendall is Assistant Professor of Sociology at Purchase College—State University of New York.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
        <item>
    <title>Raising a Digital Child: A Digital Citizenship Handbook for Parents</title>
	<abstract>As a parent, do you ever wonder how you can possibly keep up with all the new technologies your children take for granted? Cell phones, online games, instant messaging, social networking, and other technologies have all become so important in the daily of lives of young people. The kids view this new digital culture as a normal way of life, even though as parents you may feel overwhelmed by all the unfamiliar challenges. Cyber bullies, stalkers, identity theft, intellectual property theftits hard to know just what you can do to confront the risks. You want your children to enjoy all the benefits a technological society has to offer, but at the same time, you want them to stay safe and act as responsible members of society. Raising a Digital Child is your guide to many of the newest and most popular technologies, in parent-friendly language, along with discussions of the risks each might harbor, and the types of behaviors that every child should learn in order to become good a citizen in this new digital world.Also available: Web 2.0: New Tools, New Schools - ISBN 1564842347 Safe Practices for Life Online: A Guide for Middle and High School - ISBN 1564842487The International Society for Technology in Education (ISTE) is the trusted source for professional development, knowledge generation, advocacy and leadership for innovation. ISTE is the premier membership association for educators and education leaders engaged in improving teaching and learning by advancing the effective use of technology in PK-12 and teacher education. Home of the National Educational Technology Standards (NETS), the Center for Applied Research in Educational Technology (CARET), and ISTE's annual conference (formerly known as the National Educational Computing Conference, or NECC), ISTE represents more than 100,000 professionals worldwide. We support our members with information, networking opportunities, and guidance as they face the challenge of transforming education. Some of the areas in which we publish are: -Web. 2.0 in the classroom-RSS, podcasts, and more -National Educational Technology Standards (NETS) -Professional development for educators and administrators -Integrating technology into the classroom and curriculum -Safe practices for the Internet and technology -Educational technology for parents</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>0</relevance>
  </item>
        <item>
    <title>Jump Point: How Network Culture is Revolutionizing Business</title>
	<abstract>Plug into the nonstop global economy of billion-selling products and trillion-dollar markets

The Web 3.0 world of “pandemic economics” is a new economy that will function outside the traditional laws of commerce, free from today's impediments to business growth, and in a world where every person is connected to each other. Jump Point is the powerful guide that will help you to challenge old assumptions, rethink your business models, and take advantage of this fast-moving, unfettered, and fiercely competitive environment.

Silicon Valley guru Tom Hayes explores how the new economy will arrive at a single jump point by 2011, bringing with it virulent market trends. Only those prepared for the new marketplace dynamics will be left standing amidst unfamiliar players, shape-shifting consumers, and wealth-evaporating forces. This forward-thinking book examines
The implications of collaborative behavior on the global market 
The human drive behind the “agency” impulse, which spawns social media communities, multiplayer online games, and crowdsourcing sites 
How to act on and react to real-time external events 
The pitfalls of “response latency,” and why too much information can kill a company 
How to create a “virion,” or marketmaking product, by tapping the power of person-to-person viral dynamics 

Don't get left holding yesterday's toolkit. Rethink your business in terms of the global network, and take it from the jump point into exponential growth.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>0</relevance>
  </item>
        <item>
    <title>Make new friends or keep the old: Gender and personality differences in social networking use</title>
	<abstract>The present study examined the influence of gender and personality on individuals' use of online social networking websites such as Facebook and MySpace. Participants were 238 undergraduate students who reported being members of Facebook, MySpace, or both. Based on prior research examining online behavior, we expected that gender and scores on the Big Five personality scale would moderate online social networking behavior. The results supported our predictions. Specifically, men reported using social networking sites for forming new relationships while women reported using them more for relationship maintenance. Furthermore, women low in agreeableness reported using instant messaging features of social networking sites more often than women high in agreeableness, whereas men low in openness reported playing more games on social networking sites compared to men high in openness. Overall, these results indicate the importance of examining individual differences in online behavior.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>0</relevance>
  </item>
        <item>
    <title>VivoSpace: towards health behavior change using social gaming</title>
	<abstract>Social gaming is now surpassing traditional gaming in terms of the total number of players. We describe our research in creating VivoSpace, an online social network application that applies the rapid uptake of social gaming to the domain of serious games for personal health. Specifically, VivoSpace aims to leverage social gaming to motivate positive health behavior change. A user centered design process has begun for designing VivoSpace based on an initial user inquiry questionnaire that revealed key motivations for using online social networks and users' thoughts on health. Interview feedback of the paper prototypes highlighted reluctance to share particular types of health information and apprehension in logging daily information. However, people were fond of the social aspect of sharing personal health information in the context of group challenges and participating in group health activities.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
        <item>
    <title>Towards methods for the collective gathering and quality control of relevance assessments</title>
	<abstract>Growing interest in online collections of digital books and video content motivates the development and optimization of adequate retrieval systems. However, traditional methods for collecting relevance assessments to tune system performance are challenged by the nature of digital items in such collections, where assessors are faced with a considerable effort to review and assess content by extensive reading, browsing, and within-document searching. The extra strain is caused by the length and cohesion of the digital item and the dispersion of topics within it. We propose a method for the collective gathering of relevance assessments using a social game model to instigate participants' engagement. The game provides incentives for assessors to follow a predefined review procedure and makes provisions for the quality control of the collected relevance judgments. We discuss the approach in detail, and present the results of a pilot study conducted on a book corpus to validate the approach. Our analysis reveals intricate relationships between the affordances of the system, the incentives of the social game, and the behavior of the assessors. We show that the proposed game design achieves two designated goals: the incentive structure motivates endurance in assessors and the review process encourages truthful assessment.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>0</relevance>
  </item>
        <item>
    <title>Utilization of learning management systems &amp; social networking systems not only in the process of education</title>
	<abstract>Social software applications such as Facebook, Youtube, sharing photos on the web, on-line computer games, blogs, learning management systems have received a widespread attention. Social software has quickly become an inseparable part of a daily life not only of young people. It has been implemented in higher and further education to support teaching and learning processes. Strength of these applications lies in their possibilities in the area of social and cognitive stimulation of students. We investigated utilization of social applications and user's satisfaction with them. The two main areas surveyed in this paper were awareness of social networking systems among university students and the other rate of satisfaction with them. The aspect of education was intentionally incorporated. Social applications used in the higher and further education were surveyed. The contribution is one of the outcomes of Specific research no. 2145/2010 - "Construction of Identity and Contemporary Adolescent's Identity Experiments within the Framework of Social Networks". This work was also supported by the project GACR 406/09/0669 "Evaluation of the modern technologies contributing towards forming and development university students' competences. These researches have been run for two years by a team of researchers from two faculties - Faculty of Informatics and Management and Faculty of Education, University of Hradec Kralove. Data presented in this contribution were gained from 272 respondents. We believe the outcomes will provide readers with a general view of utilization of social networking systems including learning management systems in both the process of education and just in daily life as a common current activity. The findings relating to the evaluation - rate of satisfaction with these applications will be a significant contribution to research in the area, and provide us with a better understanding of online consumer behavior which will be beneficial to a suitable approach in further implementation of these applications into the process of education.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>0</relevance>
  </item>
        <item>
    <title>A tale of two online communities: fostering collaboration and creativity in scientists and children</title>
	<abstract>There has been much recent interest in the development of tools to foster remote collaboration and shared creative work. An open question is: what are the guidelines for this process? What are the key socio-technical preconditions required for a geographically distributed group to collaborate effectively on creative work, and are they different from the conditions of a decade or two ago? In an attempt to answer these questions, we conducted empirical studies of two seemingly very different online communities, both requiring effective collaboration and creative work: an international collaboration of astrophysicists studying supernovae to learn more about the expansion rate of the universe, and a group of children, ages 8-15, from different parts of the world, creating and sharing animated stories and video games on the Scratch online community developed at MIT. Both groups produced creative technical work jointly and were considered successful in their communities. Data included the analysis of thousands of lines from chat and comment logs over a period of several months, and interviews with community members. We discovered some surprising commonalities and some intriguing possibilities, and suggest guidelines for successful creative collaborations. Specifically, systems that support social creativity must facilitate sharing and play, and their design must consider the effects of repurposing, augmentation and behavior adaptation.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>0</relevance>
  </item>
        <item>
    <title>"Your team cohesion is low": a systematic study of the effects of social network feedback on mediated activity</title>
	<abstract>Collaborative mediated environments compete to provide visitors with social feedback, whose actual effects on visitors' behavior is poorly known. This study considers feedback based on Social Network Analysis (SNA) and assesses whether this feedback is able to affect user activity in an online collaborative game. The results show that SNA feedback is able to modify group activity beyond a mere novelty effect, especially on the dimensions of the behavior covered by the feedback itself. The results also point to the possible role of task type in accounting for the feedback effect on behavior.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
        <item>
    <title>User behavior, social networking, and playing style in online and face to face bridge communities</title>
	<abstract>Traditional games have recently started to become online social games. Once accessible only through face to face encounters or slow mail exchanges, games such as bridge, chess, and go are now played online by millions of gamers. Other online social games, such as FarmVille and Cafe World, already exploit the characteristics of the social network formed by gamers to improve and grow the online communities. For example, FarmVille routinely gives high-level (expert) players new items and broadcasts gameplay achievements through the social links. User behavior, social network, and play style analysis are not new research topics [1]--[4], but the study of online social gaming communities provides a new domain of application with the potential to influence millions of lives. In this work we analyze and compare two communities of bridge players, Locomotiva and BBO Fans.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
        <item>
    <title>Understanding and enabling online social networks to support healthy behaviors</title>
	<abstract>Recent advances in digital technologies invite consideration of social influence and social support as processes that are accomplished by global, flexible, adaptive, and ad hoc networks that can be created, maintained, dissolved, and reconstituted with remarkable alacrity. This presentation describes and empirically tests a multi-theoretical multilevel (MTML) model of the socio-technical motivations for creating, maintaining, dissolving, and reconstituting knowledge and social networks.

The presentation argues that MTML insights based on understanding the social motivations to create links in online social networks should be used to design more effective social networks to support healthy behaviors. Specifically, these insights should be used to implement algorithms that make network recommendations that can serve as interventions to enhance and target social support and social influence within online networks such as smoking cessation communities.

The insights we propose are derived from our study of large scale social networks within massively multi-player online role-playing games (MMORPGs). Enabled by advanced graphic and networking technologies, MMORPGs provide three-dimensional playgrounds for people to interact with one another. In this study, we analyzed activities in an MMORPG, Sony's EverQuest II. We analyzed the entire network of 3140 players who were on one server (Antonia Bayle) from Aug 25 to Aug 31 2006. Of these, 2998 were from the US, 142 were from Canada. 2447 were males. We examined whether their geographic distance offline and their demographic similarity (or homophily) influence the likelihood of four online interactions: partnering, instant messaging, trading, and mailing. The results show that geographical proximity of distance and temporal proximity of time zones have a strong impact in players' online behavior in creating relations. Individuals were 22.6 times more likely to link with others within 50 kms than from someone who was within 50 to 800 kms. In addition, homophily in age and game experience also had a strong impact on creating relations. However, there was no evidence of gender homophily in the virtual world.

These results indicate that online social networks to support healthy behaviors should implement network recommendation algorithms that underscore the importance of connecting individuals who are geographically proximate as well as exhibit homophily on a variety of personal traits.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Real-Time case-based reasoning for interactive digital entertainment</title>
	<abstract>User-generated content is everywhere: photos, videos, news, blogs, art, music, and every other type of digital media on the Social Web. Games are no exception. From strategy games to immersive virtual worlds, game players are increasingly engaged in creating and sharing nearly all aspects of the gaming experience: maps, quests, artifacts, avatars, clothing, even games themselves. Yet, there is one aspect of computer games that is not created and shared by game players: the AI. Building sophisticated personalities, behaviors, and strategies requires expertise in both AI and programming, and remains outside the purview of the end user.

To understand why authoring Game AI is hard, we need to understand how it works. AI can take digital entertainment beyond scripted interactions into the arena of truly interactive systems that are responsive, adaptive, and intelligent. I will discuss examples of AI techniques for character-level AI (in embedded NPCs, for example) and game-level AI (in the drama manager, for example). These types of AI enhance the player experience in different ways. The techniques are complicated and are usually implemented by expert game designers.

I propose an alternative approach to designing Game AI: Real-Time CBR. This approach extends CBR to real-time systems that operate asynchronously during game play, planning, adapting, and learning in an online manner. Originally developed for robotic control, Real-Time CBR can be used for interactive games ranging from multiplayer strategy games to interactive believable avatars in virtual worlds.

As with any CBR technique, Real-Time CBR integrates problem solving with learning. This property can be used to address the authoring problem. I will show the first Web 2.0 application that allows average users to create AIs and challenge their friends to play them—without programming. I conclude with some thoughts about the role of CBR in AI-based Interactive Digital Entertainment.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Automatic learning and generation of social behavior from collective human gameplay</title>
	<abstract>Current approaches to authoring behavior and dialogue for agents that interact with humans in virtual environments are labor intensive, yet often yield less robust results than desired in the face of the incredible variance possible in human input. The growing number of people playing multiplayer games online provides a potentially better alternative to hand-authored content -- capturing behavior and dialogue from human-human interactions, and automating agents with this data. This paper documents promising results from the first iteration of a Collective Artificial Intelligence system that generates behavior and dialogue in real-time from data captured from over 11,000 players of The Restaurant Game. We first describe the game, the collective memory system, and the proposal-critique driven agent architecture, and then demonstrate quantitatively that our system preserves the texture, or meaningful local coherence, of human social interaction.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Automated negotiation in open environments: (by ACM/SIGART autonomous agents research award winner)</title>
	<abstract>Agents in open environments lack a central mechanism for controlling their behavior, and they may encounter human decision-makers whose behavior is affected by social and psychological factors. Examples include online markets, patient care-delivery systems; virtual reality and simulation systems used for training and IT systems administration. As open environments increase in number and complexity, they pose significant challenges for agent designers. In this talk, I will discuss several research thrusts that address some of the challenges of automated agent design and evaluation in open environments. These agents are self-interested in the sense that they aim to fulfill their own goals as efficiently as possible. However, they may still cooperate if such behavior can serve either their short-or long-term interests or goals. I will present algorithms for representing and learning agents' goals and capabilities that have been evaluated under diverse settings varying in the extent to which agents are cooperative or competitive. Lastly, I will describe Color Trails, a game infrastructure for investigating negotiation strategies in open environments. Color Trails provides a realistic but modeling-tractable setting that facilitates the design and evaluation of automated decision making by agents.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Computer Ethics</title>
	<abstract>With the publication of the third edition of Computer Ethics, I am reminded of the day in 1984 when I received the page-proofs of the first edition. I had just returned home from the hospital after having given birth to my daughter. I had composed the book on an Osborne computer using a word processor—I think it was called WordStar—that has been obsolete for more than 10 years now. Today my daughter, now a teenager, is more comfortable with computers than I am. She spends a good deal of her day sitting in front of a computer screen chatting with friends, doing schoolwork, and exploring the Web. I composed this edition of the book on a laptop computer using a version of MS Word that automatically corrected my misspellings and grammar. And, of course, in writing this edition of the book, I frequently went to the Web to look for resources and check references. While I continue to be cautious in making grand pronouncements about the significance of these technological changes for the quality and character of human lives, the changes that have taken place in these 16 years are awe-inspiring. 

 As I began writing this edition, it was strikingly clear that my primary task was to address the technological changes that have occurred since the second edition, especially the growth and penetration of the Internet into so many domains of life. What are we to make of Web sites, cookies, data mining tools, customized online services, and e-commerce? I have addressed many of these new issues while at the same time holding on to what I continue to believe are the core issues in computer ethics: professional ethics, privacy,property,accountability, and social implications and values. Indeed, you will see that in Chapter 1, 1 continue to struggle with the question at the heart of the field, what is computer ethics? Are the ethical issues surrounding computers unique? What is the connection between ethics and technology? 

 Contemplating the connection between technology and ethics raises an interesting and important question: Does the field of computer ethics simply follow the development of computer technology? Should computer ethicists simply react to technological developments? Wouldn't it be better if the sequence were reversed so that technological development followed ethics? Historically, the field of computer ethics has been reactive to the technology. As I explain in Chapter 1, new technological developments create new possibilities and the new possibilities need to be evaluated. As in the last edition, I build on the idea in Jim Moor's seminal piece "What Is Computer Ethics?" (1985) that new technologies create policy vacuums. The task of computer ethics, he argues, is to fill these policy vacuums. In a sense, the ethical issues are the policy vacuums, and policy vacuums are created when there is a new development or use of computer technology. 

 On the other hand, I want to suggest that it would be better if at least some of the movement were in the other direction—technology following ethics. Suppose, that is, we lived in a world where ethicists (or anyone, for that matter) identified potentially unethical situations or arrangements or ethically better possibilities, and engineers and computer scientists went to work designing technologies to change or remedy or improve the situation. I can think of a few examples when this has occurred, but only a few. Arguably, privacy-enhancing technologies and anonymous re-mailers are cases in point. Perhaps freeware and shareware are also examples. For the most part, however, the ethical issues have followed, rather than led, the technology. Here in very broad brushstrokes is my understanding of the evolution of the field of computer ethics, especially in the United States. 
HISTORICAL OVERVIEW

 In the decades immediately following World War II, ethical concerns were raised about computers, though these concerns were only vaguely expressed and articulated. One of the most salient concerns was that computers threatened our notion of what it means to be human because computers could do the very thing that was considered unique to humans, rational thinking. There was much discussion of artificial intelligence. There was some fear (and fascination with the idea) that computers might take over decision making from humans. I am thinking here of the movie 2001 but the theme also ran through science fiction literature, for example, in Issac Asimov's short stories. Somewhat later, Jim Moor picked up on this theme and wrote an analytical article, "Are There Decisions That Computers Should Never Make?" (1979). 

 It could be argued that those very early concerns about computers were not exactly ethical in character. For example, no one explicitly argued that it was immoral to go forward with the development of computers because of the threat to our concept of human beings. And the science fiction literature did not suggest that it was immoral to turn over decision-making power to computers. Rather, the implicit argument seemed to be that there would be terrible consequences—possible catastrophes and degradation of human life—were decision making to be turned over to computers. 

 These concerns did not come from an effect arising from the use of computers; they arose from the mere idea of computers. The very idea of a technology that could think or do something very close to it was threatening to our understanding of what it means to be human. 

 Ironically, it could be argued that this idea, the idea that computers do what humans do, has turned out to be rich in its influence on human thinking about thinking, rather than a threat. The model of human thought that computers provide has spawned the thriving new field of cognitive science and changed a number of related disciplines. (See for example, Bynum and Moor, 1999.) 

 In the late 1970s, the ethical issues began to be more clearly articulated in the works of Joseph Weizenbaum (19'79) and Abbe Mowshowitz (1976), and it was in this period that the Privacy Protection Commission did a major study of privacy. The issues that took shape in this period had to do with the threat of big government and large-scale organizations, the related threat to privacy, and concern about the dominance of instrumental rationality. In hindsight, the concern about big government and privacy followed the technology in that in those early days, computers were being used extensively to create and maintain huge databases, databases of a variety of kinds, but especially databases of personal information. Computers were also being used for large numerical calculations. The large-scale calculations were primarily (though not exclusively) for government activities such as weapons development, space travel, and the U.S. census. 

 The next major technological shift was the development of small computers (microcomputers and personal computers). Attention turned, for a time at least, to the democratizing aspects of computers. Quietly, at the same time, remote access had come on the scene, first as remote access to large mainframes, later as a web of telecommunications connections between small computers. 

 Attention turned to software and the ethical issues surrounding it. The development and spread of microcomputers brought computer technology visibly and powerfully into the consumer marketplace. Software was recognized as something with enormous market value, and hence, all the ethical issues having to do with property arose. Should software be owned? If so, how? Would current intellectual property law provide adequate protection? Along with property rights issues came issues of liability and responsibility. In the marketplace, if consumers buy and use computers and software, they want to be able to rely on these tools and when something goes wrong, they want to know who to blame or they want to be compensated for their losses. 

 During this period, the market in computer games took off and it was also during this period that more attention began to focus on hackers. On the one hand, hackers were responding to the commercialization of computing. They did not like the idea of property rights in software. At the same time, those who were acquiring property rights and/or making a business of computing saw the threat posed by hackers, a threat to property rights and to system security. 

 In the 1990s, attention turned to the Internet. The coming together of computers, telecommunications, and media was the next major development in the technology. The development and expanded use of the Internet brought a seemingly endless set of ethical issues as the Internet came to be used in so many different ways in so many different domains of life. In effect, we are now in a process of transferring and re-creating much of the world into this new medium. At the same time, the Internet also raised all the concerns of the past. Privacy issues are exacerbated on the Internet; the democracy issue came back into play with new claims about the Internet's democratic character; property rights expanded to Web sites and global property rights became ever more important; and so on. 

 One other technological development that grew slowly during the 1980s and 1990s was the use of computer technology for a wide variety of visualization activities—not just computer graphics and gaming, but simulation activities including medical imagining and scientific models. This development expanded into the idea of virtual reality, an idea that has captivated many. Very quietly and slowly, ethical concerns have been raised about this thrust of computer technology. Unfortunately, I have been able to give only cursory attention to virtual reality issues. 

 In summary, during the 1960s and 1970s the dominant uses of the technology were for database creation and large-scale calculations. These uses of the technology brought correlated expressions of concern about centralization of power and big government, and threats to personal privacy. During this time, the very idea of computers seemed to threaten the idea of what it means to be human. During the 1980s, microcomputers were developed and made readily available. Remote access to large mainframe computers also became possible. Quietly, the system of telecommunication lines linking computers, that later became the Internet, was expanding and being made available beyond the "inner circle" of developers. Also, the computer/video game industry began to take off. With these developments came correlative concerns about property rights, liability issues, and the threat posed by hackers. In the 1990s, the coming together of telecommunications and computers reached a pinnacle of development and the Internet and the World Wide Web (Web) became widely available. These technological developments are still being assimilated, but they gave rise to a seemingly endless array of ethical issues as well as exacerbating those that were already there. 

 This is a story of computer ethical issues following technological developments. The question remains whether this pattern is as it should be. As I suggested before, reversing the order would seem to have some advantages, though scholars in the field of computer ethics do not seem to recognize the possibility of leading rather than following the technology. A central focus on the topic of design of computer technology would go a long way toward reversing this pattern. If the designers of technology were to think about the ethical and social implications of their designs before they became a reality, wouldn't the world be a better place! 
CHANGES IN THE THIRD EDITION

 Readers who are familiar with earlier editions of Computer Ethics will note that in this edition I have added two chapters specifically focused on the Internet, Chapter 4 "Ethics Online" and Chapter 8 "Social Implications and Social Values." The addition of this new material led to other changes in the organization of the book. First, instead of having a separate chapter on crime, abuse, and hacker ethics, I have situated the discussion of hackers and hacker ethics in the first chapter on the Internet, Chapter 4. This placement recognizes that hacking is a phenomenon made possible by the combination of computers and telecommunications lines that we now call the Internet. In 1994 when the second edition was published, the Internet had already been created, but it was far from clear that it would become what it has. Second, instead of having one chapter on the social implications of computer and information technology and another on the social implications of the Internet, I have combined material on the social implications of computer technology from the second edition with new material on the Internet. While I discuss both, the primary focus of Chapter 8 is on the social implications of the Internet and especially its social implications for democracy. I found this approach useful for focusing discussion of the relationships between technology and social change and between values and technology. 

 As with previous editions, there are many possible paths a reader might take through the book. The topics from chapter to chapter are interconnected, but each chapter has been written to stand essentially alone. When used as a textbook, the path students take through the book should be determined by the type of students being taught, the length of the course, and other books and materials being used in the course. For example, when teaching a class of computer science majors, it is important that the chapter on professional ethics be read early on. This sets students up to think of the issues as part of their professional responsibility. When teaching nonmajors, this chapter can comfortably be read at the end of a course, and can be presented as a way of thinking about how some of the issues discussed in the book might be addressed—control who becomes a computer professional and give computer professionals more responsibility for the effects of their work. 

 As in the previous editions, I have started each chapter with a set of short scenarios. The scenarios are intended to entice the reader into the topic, to implicitly make the case for the importance of the topic, and to make the topic concrete for those who are impatient with theory. The cases also provide the content for teaching skills in ethical analysis. As before, I have provided study questions and suggested further reading at the end of each chapter. 
OVERVIEW

Chapter 1: Introduction: Why Computer Ethics?
 In Chapter 1, I make the case for the importance of computer ethics and I explore why computer and information technology raises ethical questions when many other technologies do not. Building on Moor's idea that the task of computer ethics is to fill policy vacuums, I describe generally how computer and information technology gives rise to ethical issues. I push further addressing how these issues can be resolved and explore the traditionalist account which specifies that we can extend ordinary moral principles to situations created by computer technology. This discussion prepares the way for asking in what ways computer ethical issues are unique and in what ways not. As in the last edition, I argue that it is useful to think of the ethical issues surrounding computer and information technology as new species of generic moral issues. I support this idea by arguing that while ethics is always about human action, technology instruments human action and technology makes it possible for individuals and institutions to behave in ways they couldn't behave without technology. Traditional ethics and ethical theories have largely ignored the instrumentation of human action. Computer ethics brings this unexplored area of ethics into focus. I conclude this chapter with a brief discussion of the virtues and dangers of using analogies in analyzing computer ethical issues. 

Chapter 2: Philosophical Ethics
 This chapter is largely as it was in the second edition though I have added brief descriptions of virtue ethics and John Rawls' theory of justice. As before, the aim of this chapter is to show that ethics is not just a subjective and relativistic enterprise. The aim is to show that ethics and ethical analysis involves giving reasons and making arguments for one's claims and subjecting those reasons and arguments to critical evaluation. By reviewing traditional ethical theories, the chapter provides readers with a useful vocabulary and conceptual tools for thinking through ethical issues. The chapter is not intended to provide the theoretical framework from which answers to all ethical questions can be deduced. Rather, the aim of this chapter is to suggest that ethical analysis is a dialectic process. 

Chapter 3: Professional Ethics
 The organization of this chapter and the ideas explored are fundamentally the same as in the last edition of Computer Ethics though I have tried to clarify the ideas further, and I have updated the chapter by addressing the issue of licensing of software engineers. I have also recognized recent changes to the ACM code of ethics. The chapter begins with a discussion of how becoming a member of a profession can lead to somewhat special moral rights and responsibilities. That analysis sets the scene for defining profession and professional, and for asking whether computing is a profession. This is followed by a brief discussion of software engineering licensing. The focus of the chapter then turns to the responsibilities of computer professionals, to employers, to clients, to the public, and to co-professionals, and how they come into conflict. The chapter ends with a brief discussion of professional codes of ethics. 

Chapter 4: Ethics and the Internet: Ethics Online
 I begin this chapter by identifying what is morally significant and distinct about the Internet. Focusing on the Internet as a medium of communication, what seems morally significant is the many-to-many global scope of the Internet, the availability of a certain kind of anonymity, and the reproducibility of the medium. After drawing out the implications of these features, emphasizing the difficulties of accountability and trust, I move on to discuss hacking and hacker ethics. Here I have included some of the material from the previous edition. I conclude the chapter with a discussion of the problems the Internet seems to pose for controlling socially undesirable behavior and for encouraging civil behavior. 

Chapter 5: Privacy
 Chapter 5 is a combination of old and new material. As in the previous edition, I begin by asking how computer and information technology has changed the collection and distribution of personal information. I describe the traditional way in which the privacy issue has been framed—as necessarily involving a trade-off between individual interests in controlling information and the efficiency and improved decision making of those who can make use of the information. I argue for reframing the issue in a way that recognizes personal privacy not just as an individual good but as a social good, and I try to make clear the importance of privacy for democracy. I conclude the chapter by discussing a variety of possible approaches to improving the protection of personal privacy. 

Chapter 6: Property
 In the years since I wrote the first edition of Computer Ethics, the property rights issues have gotten more and more complicated. While there is dissatisfaction with current law, in fact, the law has not changed fundamentally. I have come to the conclusion that the most useful approach for an introductory text of this kind is to stay with the fundamentals. Thus, this chapter is very similar to the property rights chapter in the second edition. I begin by describing the problem that ownership of software poses. I describe copyright, trade secrecy, and patent law and the inadequacies each has for protecting computer software. Digging deeper into the problem, I explore the philosophical basis for property rights looking first at the natural rights arguments and then at the utilitarian arguments for and against ownership. I conclude with an argument similar to the one I made in the second edition: Making an illegal copy of proprietary software is immoral because it is illegal. While it is immoral because it is illegal, there are other kinds of immorality that would be immoral even if they were legal. I conclude with a brief discussion of how the Internet is likely to exacerbate property rights issues. 

Chapter 7: Accountability
 Chapter 7 begins with scenarios that pose a wide range of accountability issues: responsibility for rape in a virtual reality game, accountability when software recommends a decision, liability of Internet Service Providers, and responsibility for the Y2K problem. A discussion of the different meaning and uses of terms such as responsibility, accountability, liability, and blame lays the groundwork for the chapter. The focus then turns to the legal environment for the buying and selling of computer and information technology where the distinction between selling a product and providing a service is pivotal. The remainder of the chapter is devoted to issues that are unique to computer and information technology, especially the diffusion of accountability, the Y2K problem, and Internet issues. 

Chapter 8: Ethics and the Internet II: Social Implications and Social Values
 Chapter 8 is the second chapter devoted to the Internet. Drawing on material from the second edition, I begin this chapter with a general discussion of technology and social change and identify the pitfalls in asking questions such as, Is computer and information technology causing a social revolution? Is it changing things or reinforcing the status quo? Is technology good or bad? Attention is then focused on values embedded in computer and information technology. I emphasize how value-laden technology is. The chapter turns, then, to the Internet and democracy. I examine the arguments that are made to show that the Internet is a democratic technology and I critique these arguments. 

 Computer and information technology and especially the Internet have been implicated in the widening gap between haves and have-nots within countries and among countries of the world. After examining this issue which has come to be known as "the digital divide," I briefly discuss the gender gap in computing, and then I turn briefly to the value of freedom of expression. I conclude this chapter by pointing to three issues that will be particularly important to watch in the future: jurisdiction, systems of trust, and insularity. 

Practical Ethics
 Finally it may be helpful to explain what I have and have not tried to do with regard to ethical theory. I have not aimed to provide "the" ethical theory from which all ethical beliefs should be derived. Nor have I tried to provide the only possible or adequate ethical analysis of any particular issue. There are both pedagogical and theoretical reasons for taking this stance. 

 Pedagogically, I believe it is essential for students and other readers to struggle with the cases, the issues, and the relevant moral concepts and theories. This is crucial to developing skill at ethical analysis and critical thinking, and to developing moral personhood. Hence, it would be somewhat counterproductive to present (what I claim to be) "the" final answers to all the ethical questions raised in this book. Instead I have left a good deal of room for further struggle with the issues. I have tried to present ethics as an ongoing, dialectic enterprise. My analyses are intended to move the discussion forward but not to end it. 

 Moreover, this book is an undertaking in practical ethics, and practical ethics is the middle ground where abstract ethical theories and concepts meet real-world problems and decisions. It takes an enormous amount of work to understand what theories mean for real-world situations, issues, and decisions, and in some sense, we don't understand theories until we understand what they imply about real-world situations. Practical ethics is best understood as the domain in which there is negotiation between theory and real-world situations. We draw on moral concepts and theories but we must interpret them and draw out their implications for the issues at hand. In practical ethics, we work both ways, from theory to context and from context to theory. Often a theory or several theories provide illumination on a practical matter; other times, struggle with the practical problem leads to new insight into a theory. 
TERMINOLOGY

 As in earlier editions, I use ethics and morality interchangeably. Many philosophers draw a distinction between the two, but I find such distinctions too far from conventional usage to be of help. 

 Since the publication of the previous edition, linguistic conventions around computer technology have changed. Computers have come to be understood as more than simply computational—computing—machines. Moreover, there area wide variety of computer-related tools for which it may or may not be appropriate to use the term computer. It was difficult to decide what terminology to adopt to refer to the technology on which this book focuses—information technology (IT), communication technology, information and communication technology (ICT), and so on. Indeed, it was tempting to change the name of the book to something like Information Ethics or IT Ethics, but, in the end, I have concluded that it is best to keep the name of the book the same, if for no other reason than that it is a known product. Wherever possible I use the (cumbersome) phrase computer and information technology rather than just computer technology. This more closely reflects current linguistic practice and understanding of the technology. In fact, the focus of the book may more accurately be described as a focus on a family of technologies that deal with a very wide variety of types of information (signals, data, images, words, etc.). 
ACKNOWLEDGMENTS

 A number of people helped me with this edition by reading chapters and providing comments and suggestions. I particularly want to thank Keith Miller, Fran Grodzinsky, Roberta Berry, Don Gotterbarn, Andy Ward, and Amy Bruckman for their feedback on various chapters. Marc Pang Quek was a wonderful graduate research assistant and wrote several scenarios based on real cases. 

 I owe much to many others who have helped me in a wide variety of ways. Scholarship is a collective enterprise. I have learned from reading the ideas of others, from listening and talking to students and colleagues. I am always encouraged by those who are willing to tell me what they like and don't like about my writing. 

 I will always be grateful to my colleagues in the Department of Science and Technology Studies at Rensselaer Polytechnic Institute. My years there shaped my thinking and my career in profound and permanent ways. In 1998, I moved to the School of Public Policy at Georgia Institute of Technology and I have been delighted to find another robust and lively interdisciplinary environment. My new colleagues and students in public policy and in other units of the Ivan Allen College at Georgia Tech have already enriched my thinking. 

 In writing this edition, I drew on several of my previously published articles. "Ethics Online" published in Communications of the ACM, a chapter entitled "Democratic Values" in Duncan Langford's book Ethics and the Internet, and "Is the GII a Democratic Technology?" presented at several conferences and then published in Computers and Society. I developed Chapter 1 from an unpublished paper that I wrote for the Tangled Web conference. 

 As I was completing a draft of this edition, Keith Miller, Laurie King, Tracy Camp, and I received a grant from the National Science Foundation's Program on Course, Curriculum and Laboratory Improvement, to develop materials and hold workshops focused on using the Web to teach computer ethics. The first workshop took place in June of 2000. During the workshop I received extremely valuable feedback on a draft of the book. Materials developed as part of the grant are available at: www.uis.edu/~miller/dolce/ and may be useful to teachers who use this book in their courses.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Reality Is Broken: Why Games Make Us Better and How They Can Change the World</title>
	<abstract>Practical Advice for Gamers by Jane McGonigal Reality is Broken explains the science behind why games are good for us--why they make us happier, more creative, more resilient, and better able to lead others in world-changing efforts. But some games are better for us than others, and there is too much of a good thing. Here are a few secrets that arent in the book to help you (or the gamer in your life) get the most positive impact from playing games. This practical advice--5 key quidelines, plus 2 quick rules--is scientifically backed, and it can be summed up in a single sentence: Play games you enjoy no more than 21 hours a week; face-to-face with friends and family as often as you can; and in co-operative or creator modes whenever possible. 1. Dont play more than 21 hours a week. Studies show that games benefit us mentally and emotionally when we play up to 3 hours a day, or 21 hours a week. (In extremely stressful circumstances--such as serving in the military during war-time--research shows that gamers can benefit from as many as 28 hours a week.) But for virtually everyone else, whenever you play more than 21 hours a week, the benefits of gaming start to decline sharply. By the time youre spending 40 hours or more a week playing games, the psychological benefits of playing games have disappeared entirely--and are replaced with negative impacts on your physical health, relationships, and real-life goals. So always strive to keep your gaming in the sweet spot: 721 hours a week. 2. Playing with real-life friends and family is better than playing alone all the time, or with strangers. Gaming strengthens your social bonds and builds trust, two key factors in any positive relationship. And the more positive relationships you have in real life, the happier, healthier and more successful you are. You can get mental and emotional benefits from single-player games, or by playing with strangers online--but to really unlock the power of games, its important to play them with people you really know and like as often as possible. A handy rule-of-thumb: try to make half of your gaming social. If you play 10 hours a week, try to play face-to-face with real-life friends or family for at least 5 of those hours. (And if youre not a gamer yourself--but you have a family member who plays games all the time, it would do you both good to play together--even if you think you dont like games!) 3. Playing face-to-face with friends and family beats playing with them online. If youre in the same physical space, youll supercharge both the positive emotional impacts and the social bonding. Many of the benefits of games are derived from the way they make us feel--and all positive emotions are heightened by face-to-face interaction. Plus, research shows that social ties are strengthened much more when we play games in the same room than when we play games together online. Multi-player games are great for this. But single-player works too! You can get all the same benefits by taking turns at a single-player game, helping and cheering each other on. 4. Cooperative gameplay, overall, has more benefits than competitive gameplay. Studies show that cooperative gameplay lifts our mood longer, and strengthens our friendships more, than competing against each other. Cooperative gameplay also makes us more likely to help someone in real life, and better collaborators at work--boosting our real-world likeability and chances for success. Competition has its place, too, of course--we learn to trust others more when we compete against them. But if we spend all our time competing with others, we miss out on the special benefits of co-op play. So when youre gaming with others, be sure to check to see if there are co-op missions or a co-op mode available. An hour of co-op a week goes a long way. (Find great co-op games for every platform, and a family-friendly list too, at Co-Optimus, the best online resource for co-op gaming.) 5. Creative games have special positive impacts. Many games encourage or even require players to design and create as part of the gameplay process--for example: Spore, Little Big Planet, and Minecraft; the Halo level designer and the Guitar Hero song creator. These games have been shown to build up players sense of creative agency--and they make us more likely to create something outside of the game. If you want to really build up your own creative powers, creative games are a great place to start. Of course, you can always take the next creative step--and start making your own games. If youve never made a game, its easier than you think--and there are some great books to help you get started. 2 Other Important Rules: * You can get all of the benefits of a good game without realistic violence--you (or your kids) dont have to play games with guns or gore. If you feel strongly about violence, look to games in other genres--theres no shortage of amazing sports, music, racing, puzzle, role-playing, casual, strategy and adventure games. *Any game that makes you feel bad is no longer a good game for you to play. This should be obvious, but sometimes we get so caught up in our games that we forget theyre supposed to be fun. If you find yourself feeling really upset when you lose a game, or if youre fighting with friends or strangers when you play--youre too invested. Switch to a different game for a while, a game that has lower stakes for you personally. Or, especially if you play with strangers online, you might find yourself surrounded by other players who say things that make you uncomfortable--or who just generally act like jerks. Their behavior will actually make it harder for you to get the positive benefits of games--so dont waste your time playing with a community that gets you down. Meanwhile, if you start to wonder if youre spending too much time on a particular game maybe youre starting to feel just a tiny bit addicted--keep track of your gaming hours for one week. Make sure they add up to less than 21 hours! And you may want to limit yourself to even fewer for a little while if youre feeling too much gamer regret.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Development of social adaptive agents in simulation game of cross-cultural experience</title>
	<abstract>Recently, researches of Human-Agent interaction (HAI) are popular toward autonomous agents to act and cope in various human societies. If agents adapt to humans' society, they have social skills. In this paper, we introduce how developing a social adaptive agent. There are many rules in societies. Therefore, social agents should have a social skill which is obtaining social rules. We develop agents adapting group as social adaptive agents. We improve some functions from a simulation game: Online BARNGA from BARNGA to observe humans' behavior. We observe humans' behavior in Online BARNGA. As a result, we find that humans have transitions of three inner states with "notice" and "behavior" to get implicit rules, and rules fall into two categories: a byelaw and an ethic. In Online BARNGA, a byelaw is a rule of card game, an ethic is a strategy. Based on the above results, we develop agents with transitions of three inner state with "notice" and "behavior" and two type learning modules. Furthermore, we analyze agents' behavior in Online BARNGA, and compare humans' that. As a result, we confirm that agent adapt socially as if it were human.</abstract>
	<search_task_number>18</search_task_number>
	<query>social online game behavior</query>
	<relevance>1</relevance>
  </item>


  <item>
    <title>OMAR a haptic display for speech perception by deaf and deaf-blind individuals</title>
	<abstract>A system for haptic (i.e. kinesthetic and cutaneous) stimulation of the hand is described. While the immediate application involves display of speech information, a number of other man-machine interface applications may be feasible, including force-feedback devices for computer interaction and human pattern extraction from multiple datastreams. In an attempt to model more closely the information streams available via the Tadoma method, OMAR was developed to stimulate kinesthetic as well as tactile receptors, by moving and vibrating fingers in one or two dimensions using hard-disk head-positioning actuators. OMAR is being used in experiments involving basic haptic perception and supplementation of speechreading with haptic codings of speech correlates obtained via X-ray microbeam measurements.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Handheld haptic display with braille I/O</title>
	<abstract>This paper describes the implementation of a handheld haptic display using verbal and nonverbal communication technologies for visually impaired pedestrians. Wearable and mobile human-computer-interface technologies provide the means to use the display in daily life. Six ring-mounted vibrators for the finger-braille method, one of the commonly used communication methods among deaf-blind people in Japan, and a textual input interface designed on the basis of the braille input method, are adopted as the verbal I/O interface. As the non-verbal I/O interface, a perceptual force attraction method, which can convey "pull" or "push" sensations on handheld devices, is adopted for intuitive way-finding. The handheld haptic display with these technologies integrated has the potential to support wayfinding not only for blind people but also for sighted people.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Can we work together?</title>
	<abstract>People have a versatility to adapt to various situations in order to communicate with each other regardless of a person's disability. We research separate computer interfaces to support remote synchronous collaboration in two situations. First, a deaf person collaborating with a hearing person uses a shared workspace with video conferencing, such as the Facetop system. Second, a blind person collaborating with a sighted person uses our loosely coupled custom shared workspace called Deep View. The design features of the respective interfaces accommodate the disability of a deaf person or a blind person and enable communication with a person without a disability. The interfaces expand the ways in which people with disabilities participate in a collaborative task to a level of detail not possible without our interfaces. The design features of our user interfaces provide alternative channels for the collaborators with disabilities to communicate ideas or coordinate actions that collaborators without disabilities would otherwise do verbally or visually. We evaluate the interfaces through three user studies where collaborators complete full fledged tasks that require managing all aspects of communication to complete the task. Throughout the research we collaborated with members of the Deaf community and members of the blind community. We incorporated the feedback from members of these communities into the implementation of our interfaces. The members participated in our user studies to evaluate the interfaces.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Use of personal computing technology by deaf-blind individuals</title>
	<abstract>This paper describes a system that enables deaf-blind people to work with micro-computers. This system utilizes the International Morse Code as a general communication medium. The deaf-blind person "hears" Morse Code via a vibrotactile device to "see" the computer's screen. This technique enables deaf-blind individuals to receive immediate feedback of their typing and scan the screen, thus making it possible for them to use the keyboard and screen in much the normal manner of seeing persons. A side benefit is that it provides a means for deaf-blind people to communicate with the sighted through a common medium: the sighted person can see the screen while the deaf-blind feels it. Hardware cost to equip a standard personal computer with this interface is negligible. Vibrotactile Morse Code is particularly viable because it can be adapted fro the individual's particular tactile sensitivities. Morse encoded tactile communication fits well in a social facilitation context for learning. It is technologically simple and standard. This work can significantly improve the quality of life for dear-blind individuals because it provides new opportunities for communication and vocation.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Virtual Leading Blocks for the Deaf-Blind: A Real-Time Way-Finder by Verbal-Nonverbal Hybrid Interface and High-Density RFID Tag Space</title>
	<abstract>In this paper, we discuss application possibilities ofaugmented reality technologies in the field of mobilitysupport for the Deaf-Blind. We propose the navigationsystem called Virtual Leading Blocks for the Deaf-Blind,which consists of a wearable interface for Finger-Braille,one of the commonly used communication methods amongDeaf-Blind people in Japan, and a ubiquitous environmentfor barrier-free application, which consists of floor-embedded active radio-frequency identification (RFID) tags. The wearable Finger-Braille interface using twoLinux-based wristwatch computers has been developed asa hybrid interface of verbal and nonverbal communicationin order to inform users of their direction and positionthrough the tactile sensation. We propose the metaphor of"watermelon splitting" for navigation by this system andverify the feasibility of the proposed system through experiments.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A Sign Language Screen Reader for Deaf</title>
	<abstract>Screen reader technology has appeared first to allow blind and people with reading difficulties to use computer and to access to the digital information. Until now, this technology is exploited mainly to help blind community. During our work with deaf people, we noticed that a screen reader can facilitate the manipulation of computers and the reading of textual information. In this paper, we propose a novel screen reader dedicated to deaf. The output of the reader is a visual translation of the text to sign language. The screen reader is composed by two essential modules: the first one is designed to capture the activities of users (mouse and keyboard events). For this purpose, we adopted Microsoft MSAA application programming interfaces. The second module, which is in classical screen readers a text to speech engine (TTS), is replaced by a novel text to sign (TTSign) engine. This module converts text into sign language animation based on avatar technology.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>HCI Beyond the GUI: Design for Haptic, Speech, Olfactory, and Other Nontraditional Interfaces</title>
	<abstract>As technology expands and evolves, one-dimensional, graphical user interface (GUI) design becomes increasingly limiting and simplistic. Designers must meet the challenge of developing new and creative interfaces that adapt to meet human needs and technological trends. HCI Beyond the GUI provides designers with this know how by exploring new ways to reach users that involve all of the human senses. Dr. Kortum gathers contributions from leading human factors designers to present a single reference for professionals, researchers, and students.
	          Explores the human factors involved in the design and implementation of the nontraditional interfaces, detailing design strategies, testing methodologies, and implementation techniques
              Provides an invaluable resource for practitioners who design interfaces for children, gamers and users with accessibility needs
              Offers extensive case studies, examples and design guidelines</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>User-Centered Design Stories: Real-World UCD Case Studies</title>
	<abstract>Intended for both the student and the practitioner, this is the first user-centered design casebook. It follows the Harvard Case study method, where the reader is placed in the role of the decision-maker in a real-life professional situation. In this book, the reader is asked to perform analysis of dozens of UCD work situations and propose solutions for the problem set.

              The problems posed in the cases cover a wide variety of key tasks and issues facing practitioners today, including those that are related to organizational/managerial topics, UCD methods and processes, and technical/ project issues. The benefit of the casebook and its organization is that it offers the new practitioner (as well as experienced practitioners working in new settings) the valuable practice in decision-making that one cannot get by reading a book or attending a seminar.

              *The first User-Centered Design Casebook, with cases covering the key tasks and issues facing UCD practitioners today.
              *Each chapter based on real world cases with complex problems, giving readers as close to a real-world experience as possible.
              *Offers "the things you don't learn in school," such as innovative and hybrid solutions that were actually used on the problems discussed.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Text Entry Systems: Mobility, Accessibility, Universality</title>
	<abstract>Text entry has never been so important as it is today. This is in large part due to the phenomenal, relatively recent success of mobile computing, text messaging on mobile phones, and the proliferation of small devices like the Blackberry and Palm Pilot. Compared with the recent past, when text entry was primarily through the standard "qwerty" keyboard, people today use a diverse array of devices with the number and variety of such devices ever increasing.

The variety is not just in the devices, but also in the technologies used: Entry modalities have become more varied and include speech recognition and synthesis, handwriting recognition, and even eye-tracking using image processing on web-cams. Statistical language modeling has advanced greatly in the past ten years and so therein is potential to facilitate and improve text entry--increasingly, the way people communicate.

This book consists of four parts, and covers these areas: Guidelines for Designing Better Entry Systems (including research methodologies, measurement, and language modelling); Devices and Modalities; Languages of the world and entry systems in those languages; and variety in users and their difficulties with text entry--and the possible design and guideline solutions for those individual user groups.

This book covers different aspects of text entry systems and offers prospective researchers and developers

* global guidelines for conducting research on text entry, in terms of design strategy, evaluation methodology, and requirements;

* history and current state of the art of entry systems, including coverage of recent research topics;

* specific guidelines for designing entry systems for a specific target, depending on devices, modalities, language, and different physical conditions of users</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Proceedings of the 2011 annual conference on Human factors in computing systems</title>
	<abstract>Over the last year or so, we have been blessed with the challenge, the opportunity, and the distinct pleasure of organizing the CHI 2011 Conference on Human Factors in Computing Systems, the premier international conference for the field of human-computer interaction.

CHI 2011 takes place in gorgeous, energetic, sophisticated Vancouver BC, a city renowned for its innovation in entertainment, sustainability, accessibility, and inclusivity. The New York Times calls it, "a liquid city, a tomorrow city, equal parts India, China, England, France and the Pacific Northwest." Vancouver lays a beautiful backdrop for our conference, which boasts nearly 30 years of wonderful work.

Behind the success of the conference is our diverse community of faculty and students, of researchers and practitioners, of young and, well, also of experienced. It is a community of designers, technologists, psychologists, social scientists, biologists, artists, engineers, anthropologists, musicians; the list goes on. Wherever we are, we are always a community of near and far. Most impressively, ours is a community that cares deeply about innovating, learning, sharing, and interacting; all with the common goal of using technology to shape the way people around the world live and play.

Returning attendees will recognize the general conference format - 2 days of small intimate workshops, followed by 4 days of technical content, all surrounded by social and intellectual exchanges. In addition to the familiar venues that form the core of the conference, we have also arranged various special events, such an keynotes by Howard Rheingold and Ethan Zuckerman; invited talks by ACM SIGCHI award winners Terry Winograd, Larry Tesler, Alan Newell, and Clayton Lewis; an HCI museum exhibit hosted by Bill Buxton; and a panel celebrating Stu Card's achievements and contributions to the field of HCI.

With the record number of submissions and accepted content this year, we hope that you will utilize the print and electronic programs, but also the daily CHI Madness presentations that provide a glimpse of the day ahead. In the interest of continuing to evolve the conference to best serve our needs, we will experiment with shorter talks this year (20 minute slots for long pieces of content and 10 for shorter ones) to infuse even more energy into the program. We will also have a pretty full slate of social media applications to help you connect with other attendees and to provide you with the fullest experience possible.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Proceedings of the 2011 annual conference extended abstracts on Human factors in computing systems</title>
	<abstract>Over the last year or so, we have been blessed with the challenge, the opportunity, and the distinct pleasure of organizing the CHI 2011 Conference on Human Factors in Computing Systems, the premier international conference for the field of human-computer interaction.

CHI 2011 takes place in gorgeous, energetic, sophisticated Vancouver BC, a city renowned for its innovation in entertainment, sustainability, accessibility, and inclusivity. The New York Times calls it, "a liquid city, a tomorrow city, equal parts India, China, England, France and the Pacific Northwest." Vancouver lays a beautiful backdrop for our conference, which boasts nearly 30 years of wonderful work.

Behind the success of the conference is our diverse community of faculty and students, of researchers and practitioners, of young and, well, also of experienced. It is a community of designers, technologists, psychologists, social scientists, biologists, artists, engineers, anthropologists, musicians; the list goes on. Wherever we are, we are always a community of near and far. Most impressively, ours is a community that cares deeply about innovating, learning, sharing, and interacting; all with the common goal of using technology to shape the way people around the world live and play.

Returning attendees will recognize the general conference format - 2 days of small intimate workshops, followed by 4 days of technical content, all surrounded by social and intellectual exchanges. In addition to the familiar venues that form the core of the conference, we have also arranged various special events, such an keynotes by Howard Rheingold and Ethan Zuckerman; invited talks by ACM SIGCHI award winners Terry Winograd, Larry Tesler, Alan Newell, and Clayton Lewis; an HCI museum exhibit hosted by Bill Buxton; and a panel celebrating Stu Card's achievements and contributions to the field of HCI.

With the record number of submissions and accepted content this year, we hope that you will utilize the print and electronic programs, but also the daily CHI Madness presentations that provide a glimpse of the day ahead. In the interest of continuing to evolve the conference to best serve our needs, we will experiment with shorter talks this year (20 minute slots for long pieces of content and 10 for shorter ones) to infuse even more energy into the program. We will also have a pretty full slate of social media applications to help you connect with other attendees and to provide you with the fullest experience possible.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Enhancing independence and safety for blind and deaf-blind public transit riders</title>
	<abstract>Blind and deaf-blind people often rely on public transit for everyday mobility, but using transit can be challenging for them. We conducted semi-structured interviews with 13 blind and deaf-blind people to understand how they use public transit and what human values were important to them in this domain. Two key values were identified: independence and safety. We developed GoBraille, two related Braille-based applications that provide information about buses and bus stops while supporting the key values. GoBraille is built on MoBraille, a novel framework that enables a Braille display to benefit from many features in a smartphone without knowledge of proprietary, device-specific protocols. Finally, we conducted user studies with blind people to demonstrate that GoBraille enables people to travel more independently and safely. We also conducted co-design with a deaf-blind person, finding that a minimalist interface, with short input and output messages, was most effective for this population.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Evaluating existing audio CAPTCHAs and an interface optimized for non-visual use</title>
	<abstract>Audio CAPTCHAs were introduced as an accessible alternative for those unable to use the more common visual CAPTCHAs, but anecdotal accounts have suggested that they may be more difficult to solve. This paper demonstrates in a large study of more than 150 participants that existing audio CAPTCHAs are clearly more difficult and time-consuming to complete as compared to visual CAPTCHAs for both blind and sighted users. In order to address this concern, we developed and evaluated a new interface for solving CAPTCHAs optimized for non-visual use that can be added in-place to existing audio CAPTCHAs. In a subsequent study, the optimized interface increased the success rate of blind participants by 59% on audio CAPTCHAs, illustrating a broadly applicable principle of accessible design: the most usable audio interfaces are often not direct translations of existing visual interfaces.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Interaction Design and Children</title>
	<abstract>Children are increasingly using computer technologies as reflected inï¾ reports of computer use in schools in the United States. Given theï¾ greater exposure of children to these technologies, it is imperative that they be designed taking into account children's abilities, interests, and developmental needs. This survey aims to contribute toward this goal through a review of research on children's cognitive and motor development, safety issues related to technologies and design methodologies and principles. It also provides and overview of current research trends in the field of interaction design and children and identifies challenges for future research.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>CHI '05 extended abstracts on Human factors in computing systems</title>
	<abstract>On behalf of the Technical Program Committee, welcome to the CHI 2005 conference DVD! The Extended Abstracts portion of this disc includes submissions from all conference venues except Papers (which you can find in the Conference Proceedings section). Here you will find a formal record of the discussions, demonstrations, and debates that will occur during the conference. Highlights include:

    short papers from the Late-Breaking Results venue,
    videos of demonstrations from our new Interactivity Chamber,
    case studies in innovation and excellence from the Design Expo, a forum for best practices in HCI,
    ideas that are controversial, innovative, or "hard-to-publish" from alt.chi,
    position papers from the Development Consortium, which looks at SIGCHI's role in serving an increasingly multidisciplinary membership, and
    panels and SIGs that reflect on the foundations of our discipline, and look forward at issues affecting the community today, with topics ranging from outsourcing to the CHI Papers process.

You will also find brief summaries from Workshop organizers, Doctoral Consortium participants, Student Competition teams, and Late Breaking Results posters. If you are viewing this disc at the conference, we invite you to interact with these authors during the Highlights on Posters sessions on Wednesday and Thursday in the Commons; check your conference program for more information.On a personal note: bringing together such a large and diverse program is no small task, and I am grateful to the dedication of our venue chairs and Web master - from developing the Calls for Participation and handling the submission/review process, to dealing with my endless requests for information and scheduling conference sessions. Many, many thanks for volunteering your efforts for over a year to make this conference happen!</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Proceedings of the 27th international conference extended abstracts on Human factors in computing systems</title>
	<abstract>Welcome to CHI 2009! CHI comprises many events, ranging from archival material stored in the ACM digital library, to transient interactions such as the presentations, panels, and poster discussions, to the many social interactions and activities that make CHI a collegial and intimate experience. All parts are important, but it is the archival material -- especially the papers and notes -- that establishes CHI as the leading academic conference in Human Computer Interaction. Yet there are significant challenges in managing paper and notes within CHI. The HCI field has been very successful at creating new generations of research and practitioners over the years. The many people who are part of this community see CHI as the place to share their knowledge and experiences with others, primarily by publishing and presenting papers and notes. This has stressed the system in many ways. As submissions increase, so do the difficulties in managing the review process, finding good reviewers and other volunteers, matching papers to those competent in the subject matter, deciding which papers to accept or reject, maintaining consistent standards across both paper and notes, and not falling into the trap of overly narrowing our view of what is an 'acceptable' CHI paper.

This year, we introduced several large changes to the CHI papers/notes process to mitigate these challenges, most which will be transparent to attendees. First, we reorganized the CHI program committee into nine topical subcommittees - each a mini program committee - comprising sub-committee chairs (SCs) and various associate chairs (ACs) knowledgeable on the topic. Authors could select the subcommittee that he or she felt could best handle their submission. We did this to improve the match of a submission to AC and ultimately to reviewers, to have more focused and relevant discussions in the program committee meeting, and to minimize the load on individual volunteers. Second, we combined papers and notes, where all were handled in exactly the same way. We did this to ensure a consistent decision standard across both submission types. Third, we introduced contribution types, where each type described a different way that a CHI submission could contribute to the field as well as typical questions such a contribution should address. Authors identified their submission by contribution type, and (hopefully) used the information to help structure their paper. The idea is that we wanted to encourage a broad variety of submissions from authors (rather than 'formula' papers), while also providing guidance to referees by supplying criteria appropriate to the type of contribution the submission was making.

It will likely take several years before the full impact of these changes are known. We know that subcommittees did help us manage the large number of submissions. We also believe there was an overall better match between referees and submissions, and that papers and notes were handled consistently. We don't yet know about the effect of contribution types: this is a cultural change where we are hoping that authors will be more willing to write papers that don't match a particular formula, and that reviewers will be more accepting of those submissions.

Now for the numbers. This year, there were 1130 submissions, comprising 711 full papers and 419 notes. This is the highest number of submissions ever to CHI. Of these, we accepted 24.5%. The papers/notes committee involved 107 volunteers: the 2 co-chairs, 10 sub-committee chairs, and 95 associate chairs (ACs). Each AC managed 10-14 submissions, and personally recruited at least three -- sometimes more -- referees knowledgeable in the paper's topic. Refereeing was through blind review. Each referee returned a recommendation along with a detailed review, and authors had opportunity to rebut these reviews. Additional reviews were sometimes solicited. Almost all program committee members then attended a two day meeting in Boston in December. Rigorous discussions took place at the PC meeting, and the majority of papers were read by a second AC as well. The decision process was highly visible so that the committee could calibrate itself.

Finally, the various committees nominated 5% of the submissions as potential best papers. A separate committee deliberated over these papers, where only 1% of papers and notes received a best paper award. In total, as you will see in the program, 32 papers and four notes were designated as honorable mentions, while seven papers and four notes honored as best papers. Congratulations to all authors who achieved this significant status!</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Computer Usage by Children with Down Syndrome: Challenges and Future Research</title>
	<abstract>Children with Down syndrome, like neurotypical children, are growing up with extensive exposure to computer technology. Computers and computer-related devices have the potential to help these children in education, career development, and independent living. Our understanding of computer usage by this population is quite limited. Most of the software, games, and Web sites that children with Down syndrome interact with are designed without consideration of their special needs, making the applications less effective or completely inaccessible. We conducted a large-scale survey that collected computer usage information from the parents of approximately six hundred children with Down syndrome. This article reports the text responses collected in the survey and is intended as a step towards understanding the difficulties children with Down syndrome experience while using computers. The relationship between the age and the specific type of difficulties, as well as related design challenges are also reported. A number of potential research directions and hypotheses are identified for future studies. Due to limitations in survey methodology, the findings need to be further validated through hypothesis-driven, empirical studies.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>NTID International Symposium on Technology and Deaf Education: a review</title>
	<abstract>The International Symposium on Technology and Deaf Education at the National Technical Institute for the Deaf (NTID) at Rochester Institute of Technology is a bi-yearly (occurring once every two years) conference that attracts professionals, educators, technologists, and researchers who are interested in, developing for, and affected by technology for the education of deaf and hard of hearing students. This mixture of professionals leads to many interesting discussions, presentations, and exhibits with the purpose of dissemination of full research projects and best practices. This year the symposium, June 21--23, 2010, accepted 64 papers each with presentations 45 minutes long, 15 minutes of which were devoted to question and answer, and discussion. Presenters were chosen to reflect the current issues and solutions in both educational and access technologies. The symposium is also a great venue to share works-in-progress, current trends, and state of the art technology shown by 28 posters and 11 commercial exhibits. Over 300 people attended from 17 different countries including presentations by participants from Japan, Thailand, Canada, China, Czech Republic, Russia, Philippines, Poland, and Rwanda.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>SEA: Segment-enrich-annotate paradigm for adapting dialog-based content for improved accessibility</title>
	<abstract>While navigation within complex information spaces is a problem for all users, the problem is most evident with individuals who are blind who cannot simply locate, point, and click on a link in hypertext documents with a mouse. Users who are blind have to listen searching for the link in the document using only the keyboard and a screen reader program, which may be particularly inefficient in large documents with many links or deep hierarchies that are hard to navigate. Consequently, they are especially penalized when the information being searched is hidden under multiple layers of indirections. In this article, we introduce a segment-enrich-annotate (SEA) paradigm for adapting digital content with deep structures for improved accessibility. In particular, we instantiate and evaluate this paradigm through the iCare-Assistant, an assistive system for helping students who are blind in accessing Web and electronic course materials. Our evaluations, involving the participation of students who are blind, showed that the iCare-Assistant system, built based on the SEA paradigm, reduces the navigational overhead significantly and enables user who are blind access complex online course servers effectively.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Unconventional human computer interfaces</title>
	<abstract>This course focuses on how we can use the potential of the human body in experimental or unconventional interface techniques. It explores the biological or physiological characteristics of the separate parts of the body, from head to toe, and from skin to heart, showing how their sensor (input) and control (output) capabilities can be applied to human computer interfaces. We demonstrate a wide variety of applications that make use proven interfaces as well as extremely experimental systems. Example systems vary from desktop--based to mixed and virtual reality, with applications from areas like art, entertainment, and science.Participants will learn to look beyond the restrictions bound to traditional multimedia systems by discovering and understanding how the human body can deliver great potential for new kinds of applications and systems. Theory on the human body and practical knowledge on (hardware) interfaces is balanced with a good portion of examples to provide a foundation for assessing and using experimental and unconventional interaction.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Usable interface design for everyone</title>
	<abstract>When designing "interfaces for everyone" for interactive systems, it is important to consider factors such as cost, the intended market, the state of the environment, etc. User interfaces are fundamental for the developmental process in any application, and its design must be contemplated from the start. Of the distinct parts of a system (hardware and software), it is the interface that permits the user access to computer resources. The seven principles of "Universal Design" or "Design for Everyone" focus on a universal usable design, but at the same time acknowledge the influences of internal and external factors. Structural changes in social and health services could provide an increase in the well-being of a country's citizens through the use of self-care programming and proactive management/prevention of disease. Automated home platforms can act as an accessibility instrument which permits users to avoid, compensate, mitigate, or neutralize the deficiencies and dependencies caused by living alone.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Dawn explorer: a framework for multimodal accessibility to computer systems</title>
	<abstract>Technology is advancing at a rapid pace, automating many everyday chores in the process, changing the way we perform work and providing various forms of entertainment. Makers of technology, however, often do not consider the needs of the disabled in their design of products by, for example, providing some alternative means of input. The use of computers presents a challenge to many disabled users who are not able to see graphical user interfaces, use a mouse or keyboard or otherwise interact with standard computers. This paper presents a multimodal user interface, emulating and extending the functionality of the Windows Explorer application, with alternative input and output methods. The project utilizes auditory and visual interaction technologies, comprises a modular and extendible architecture and utilises off-the-shelf hardware to reduce implementation cost and maximize accessibility.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Improving public transit usability for blind and deaf-blind people by connecting a braille display to a smartphone</title>
	<abstract>We conducted interviews with blind and deaf-blind people to understand how they use the public transit system. In this paper, we discuss key challenges our participants faced and present a tool we developed to alleviate these challenges. We built this tool on MoBraille, a novel framework that enables a Braille display to benefit from many features in an Android phone without knowledge of proprietary, device-specific protocols. We conducted participatory design with a deaf-blind person and describe the lessons learned about designing an interface for a deaf-blind person.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>XPL the eXtensible presentation language</title>
	<abstract>The last decade has witnessed a growing interest in the development of web interfaces enabling both multiple ways to access contents and, at the same time, fruition by multiple modalities of interaction (point-and-click, contents reading, voice commands, gestures, etc.). In this paper we describe a framework aimed at streamlining the design process of multi-channel, multimodal interfaces enabling full reuse of software components. This framework is called the eXtensible Presentation architecture and Language (XPL), a presentation language based on design pattern paradigm that keeps separated the presentation layer from the underlying programming logic. The language supplies a methodology to expedite multimodal interface development and to reduce the effort to implement interfaces for multiple access devices, by means of using the same code. This paper describes a methodology approach based on Visual Design Pattern (ViDP) and Verbal Design Pattern (VeDP), offering examples of multimodal and multichannel interfaces created with the XPL Editor.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Intent driven interfaces to ubiquitous computers</title>
	<abstract>An intent driven interface allows a person to control a computer by stating an intended outcome rather than entering the sequence of tasks required to achieve the same outcome. Techniques that were originally developed as part of a universal access accelerator for individuals with disabilities are now being applied as convenience and productivity tools for accessing any computer based device, appliance or system. An intelligent universal serial bus (USB) Hub, called the iTASK Module, determines user intent independently of the input source or the system that is accessed. iTASK Modules can be interconnected to support multi-user collaboration and sharing of system resources without requiring any hardware or software changes to the accessed system.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Proceedings of the 2nd International Convention on Rehabilitation Engineering &amp; Assistive Technology</title>
	<abstract>Assistive and Rehabilitative Technologies (ART) are the systematic applications of scientific and engineering principles to improve the quality of life for people with disabilities.

Though the providing of technology needed to accomplish tasks that were formerly impossible or difficult to achieved, ART promote independence.

An ART device can be loosely defined as "any item, piece of equipment, or product system, whether acquired commercially off-the-shelf, modified or customized that is used to increase, maintain, or improve functional capabilities of individuals with disabilities." It has been long observed that people with disabilities has increased vocational and independent living opportunities with adoption of ART.

The importance of Assistive and Rehabilitative Technology is well recognized and undeniable, and the international Convention for Rehabilitation Engineering &amp; Assistive Technology (i-CREATe) 2008 is organized to provide information exchange, knowledge-sharing, networking, publications of researches, and professional interactions among the end-users, practitioners, policy-makers, researchers, and manufacturers.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Comparative analysis of tactile sensitivity between blind, deaf and unimpaired people</title>
	<abstract>This paper presents the methodological procedures and the results of a tactile sensitivity analysis to vibrotactile stimuli. The results were obtained through the analyses between the rightness mean while identifying the target frequency, among the following characteristics: visually and hearing impaired people as well as unimpaired ones; visually and hearing impaired men and women and unimpaired ones; visually and hearing impaired children, teenagers and adults as well as unimpaired ones; visually and hearing impaired people with congenital and acquired deficiency. This is an expanded version of a paper presented at the 3rd IEEE International Workshop on Medical Measurements and Applications, 9 10 May 2008, Ottawa, ON, Canada.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Proceedings of the 27th international conference on Human factors in computing systems</title>
	<abstract>Welcome to CHI 2009! CHI comprises many events, ranging from archival material stored in the ACM digital library, to transient interactions such as the presentations, panels, and poster discussions, to the many social interactions and activities that make CHI a collegial and intimate experience. All parts are important, but it is the archival material -- especially the papers and notes -- that establishes CHI as the leading academic conference in Human Computer Interaction. Yet there are significant challenges in managing paper and notes within CHI. The HCI field has been very successful at creating new generations of research and practitioners over the years. The many people who are part of this community see CHI as the place to share their knowledge and experiences with others, primarily by publishing and presenting papers and notes. This has stressed the system in many ways. As submissions increase, so do the difficulties in managing the review process, finding good reviewers and other volunteers, matching papers to those competent in the subject matter, deciding which papers to accept or reject, maintaining consistent standards across both paper and notes, and not falling into the trap of overly narrowing our view of what is an 'acceptable' CHI paper.

This year, we introduced several large changes to the CHI papers/notes process to mitigate these challenges, most which will be transparent to attendees. First, we reorganized the CHI program committee into nine topical subcommittees - each a mini program committee - comprising sub-committee chairs (SCs) and various associate chairs (ACs) knowledgeable on the topic. Authors could select the subcommittee that he or she felt could best handle their submission. We did this to improve the match of a submission to AC and ultimately to reviewers, to have more focused and relevant discussions in the program committee meeting, and to minimize the load on individual volunteers. Second, we combined papers and notes, where all were handled in exactly the same way. We did this to ensure a consistent decision standard across both submission types. Third, we introduced contribution types, where each type described a different way that a CHI submission could contribute to the field as well as typical questions such a contribution should address. Authors identified their submission by contribution type, and (hopefully) used the information to help structure their paper. The idea is that we wanted to encourage a broad variety of submissions from authors (rather than 'formula' papers), while also providing guidance to referees by supplying criteria appropriate to the type of contribution the submission was making.

It will likely take several years before the full impact of these changes are known. We know that subcommittees did help us manage the large number of submissions. We also believe there was an overall better match between referees and submissions, and that papers and notes were handled consistently. We don't yet know about the effect of contribution types: this is a cultural change where we are hoping that authors will be more willing to write papers that don't match a particular formula, and that reviewers will be more accepting of those submissions.

Now for the numbers. This year, there were 1130 submissions, comprising 711 full papers and 419 notes. This is the highest number of submissions ever to CHI. Of these, we accepted 24.5%. The papers/notes committee involved 107 volunteers: the 2 co-chairs, 10 sub-committee chairs, and 95 associate chairs (ACs). Each AC managed 10-14 submissions, and personally recruited at least three -- sometimes more -- referees knowledgeable in the paper's topic. Refereeing was through blind review. Each referee returned a recommendation along with a detailed review, and authors had opportunity to rebut these reviews. Additional reviews were sometimes solicited. Almost all program committee members then attended a two day meeting in Boston in December. Rigorous discussions took place at the PC meeting, and the majority of papers were read by a second AC as well. The decision process was highly visible so that the committee could calibrate itself.

Finally, the various committees nominated 5% of the submissions as potential best papers. A separate committee deliberated over these papers, where only 1% of papers and notes received a best paper award. In total, as you will see in the program, 32 papers and four notes were designated as honorable mentions, while seven papers and four notes honored as best papers. Congratulations to all authors who achieved this significant status!</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Review: An overview of the Internet of Things for people with disabilities</title>
	<abstract>Currently, over a billion people including children (or about 15% of the world's population) are estimated to be living with disability. The lack of support services can make handicapped people overly dependent on their families, which prevents them from being economically active and socially included. The Internet of Things can offer people with disabilities the assistance and support they need to achieve a good quality of life and allows them to participate in the social and economic life. In this paper, an overview of the Internet of Things for people with disabilities is provided. For this purpose, the proposed architecture of the Internet of Things is introduced. Different application scenarios are considered in order to illustrate the interaction of the components of the Internet of Things. Critical challenges have been identified and addressed. </abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A model to design interactive learning environments for children with visual disabilities</title>
	<abstract>Current interactive learning environments cannot be accessed by learners with disabilities, in particular for students with vision disabilities. Modeling techniques are necessary to map real-world experiences to virtual worlds by using 3D auditory representations of objects for blind people. In this paper, a model to design multimedia software for blind learners is presented. The model was validated with existing educational software for these users. We describe the modeling of the real world including cognitive usability testing tasks by considering not only the representation of the real world but also modeling the learner's knowledge of the virtual world. The software architecture is also described by using this model, displaying the components needed to impact learning. Finally, we analyze critical issues in designing software for learners with visual disabilities and propose some recommendations and guidelines.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Developing and evaluating a non-visual memory game</title>
	<abstract>This paper describes the development of a non-visual memory game based on the classic game 'Simon™', where users are presented with a sequence of stimuli, which they need to replicate in the same order to progress to the next level. Information is presented using a combination of speech, non-speech audio and/or haptic cues, designed to aid blind users who are often excluded from mainstream gaming applications. Findings from an empirical study have revealed that when haptic feedback was presented in combination with other modalities, users successfully replicated more sequences, compared with presenting haptic feedback alone. We suggest that when developing a non-visual game using an unfamiliar input device, speech-based feedback is presented in conjunction with haptic cues.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Proceedings of the 3rd International Convention on Rehabilitation Engineering &amp; Assistive Technology</title>
	<abstract>We are glad and honoured to host the third episode of the international Convention on Rehabilitation Engineering &amp; Assistive Technology (i-CREATe) in Singapore. This year, i-CREATe is pleased to partner the International Conference on Accessible Tourism (ICAT) to bring you two conferences under one common banner --- Assistive Technology &amp; Accessible Tourism (AT&amp;AT). All registered delegates will have free access to all the programme of the two conferences.

Follow our successful showings of the first two i-CREATe (Singapore 2007, Bangkok 2008), the programme for this year's convention promises to be even more exciting. We have assembled 5 plenary speeches by decorated award-winning domain expert, prominent industrial leader, experienced expert user of assistive technology and renowned academics and physicians. There are more than 100 presentations, workshops and discussion forums covering a wide spectrum of topics, a comprehensive exhibition of the latest products and services by companies, research institutions and NGOs from more than 10 countries, and a showcase of more than 25 projects by the finalists of the IBM Assistive &amp; Rehabilitative Technology Student Design Challenge.

Unlike most academic conferences in the Asia Pacific region, i-CREATe is not an usual annual meeting of academics and professionals to share their latest studies and research, it provides a much needed but glaringly missing platform for all relevant stakeholders to share information and experience. These include the medical practitioners, NGO executives, end users &amp; caregivers, policy makers &amp; government officials, industrial players, academicians, researchers, students and anyone interested in this field. In the past two events, we have been trying hard to strike a balance among the interests of these stakeholders. We want scientific rigor, but do not wish to shut out the people that do not speak equations, algorithms or Latin medical lingos; we want social relevance, but do not wish to make the technologists and medical professionals feel out of place; we want industrial participation, but do not wish to turn it into a hard selling session; we want to provide content for every relevant domain, but we wish even harder for free interaction and cross fertilization.

We wish that you will find i-CREATe 2009 enjoyable as it is useful.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Usability of optically simulated haptic feedback</title>
	<abstract>In this contribution a method will be described to optically simulate haptic feedback without resorting to mechanical force feedback devices. This method exploits the domination of the visual over the haptic modality. The perception of haptic feedback, usually generated by force feedback devices, was simulated by tiny displacements on the cursor position relative to the intended force. The usability of optically simulated haptic feedback (OSHF) was tested experimentally by measuring effectiveness, efficiency and satisfaction of its users in a Fitts' type target-acquisition task and comparing the results with the usability of mechanically simulated force feedback and normal feedback. Results show that OSHF outperforms mechanically simulated haptic feedback and normal feedback, especially in the case of small targets. </abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>V-braille: haptic braille perception using a touch-screen and vibration on mobile phones</title>
	<abstract>V-Braille is a novel way to haptically represent Braille characters on a standard mobile phone using the touch-screen and vibration. V-Braille may be suitable for deaf-blind people who rely primarily on their tactile sense. A preliminary study with deaf-blind Braille users found that, with minimal training, V-Braille can be used to read individual characters and sentences.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Requirements for Maintaining Web Access for Hearing-Impaired Individuals</title>
	<abstract>The current textual and graphical interfaces to computing, including the Web, is a dream come true for the hearing impaired. However, improved technology for voice and audio interface threaten to end this dream. Requirements are identified for continued access to computing for the hearing impaired. Consideration is given also to improving access to the sight impaired.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An investigation of current virtual reality interfaces</title>
	<abstract>Virtual Reality hype is becoming a large part of everyday life. This paper explores the components of actual virtual reality systems, critiquing each in terms of human factors. The hardware and software of visual, aural, and haptic input and feedback are considered. Technical and human factor difficulties are discussed and some potential solutions are offered.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Setting the table for the blind</title>
	<abstract>We report on research concerning the rendering of tables for blind individuals with an emphasis on exploring the potential of a new planar haptic device in combination with sound.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Tactons: structured tactile messages for non-visual information display</title>
	<abstract>Tactile displays are now becoming available in a form that can be easily used in a user interface. This paper describes a new form of tactile output. Tactons, or tactile icons, are structured, abstract messages that can be used to communicate messages non-visually. A range of different parameters can be used for Tacton construction including: frequency, amplitude and duration of a tactile pulse, plus other parameters such as rhythm and location. Tactons have the potential to improve interaction in a range of different areas, particularly where the visual display is overloaded, limited in size or not available, such as interfaces for blind people or in mobile and wearable devices. This paper describes Tactons, the parameters used to construct them and some possible ways to design them. Examples of where Tactons might prove useful in user interfaces are given.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Designing for all in the house</title>
	<abstract>The elderly population in the world is increasing rapidly and consequently so is demand for new technologies that allow them to live independently. Facilitating the control of household appliances and the home environment through various devices that encompass multimodal and ambient interfaces seems a way to achieve this. We tested three modalities of output - audio, visual and multimodal - using two different devices - palmtop and laptop - as realistic prototypes of household appliance controllers. Through experimental design the applicability of using icons and musical earcons as a medium to transmit information to the user and its suitability to the home was investigated. Results showed participants performed markedly better using the multimodal and visual interfaces than with the audio interface. In addition both groups performed better using the palmtop as compared to the laptop.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Mapping transparency through metaphor: towards more expressive musical instruments</title>
	<abstract>We define a two-axis transparency framework that can be used as a predictor of the expressivity of a musical device. One axis is the player's transparency scale, while the other is the audience's transparency scale. Through consideration of both traditional instruments and new technology-driven interfaces, we explore the role that metaphor plays in developing expressive devices. Metaphor depends on a literature, which forms the basis for making transparent device mappings. We examine four examples of systems that use metaphor: Iamascope, Sound Sculpting, MetaMuse and Glove-TalkII; and discuss implications on transparency and expressivity. We believe this theory provides a framework for design and evaluation of new human–machine and human–human interactions, including musical instruments.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Personalized emergency medical assistance for disabled people</title>
	<abstract>Being able to promptly and accurately choose a proper course of action in the field is a crucial aspect of emergency response. For this reason, emergency medical services (EMS) rely on well established procedures that apply to the most frequent cases first responders encounter in their practice, but do not include special cases concerning (sensory, motor or cognitive) disabled persons. In these cases, first responders may end up applying suboptimal or possibly wrong procedures or lose precious time trying to adapt on-the-fly to the special case. This paper proposes both (i) a detailed patient model for EMS that can account for peculiar aspects of the many existing disabilities and (ii) an adaptive information system called PRESYDIUM (Personalized Emergency System for Disabled Humans) that provides tailored instructions in the field for helping medical first responders in dealing with disabled persons. More precisely, we will illustrate and discuss: (i) the design and development process of PRESYDIUM, (ii) the patient model, which is partly based on the ICF (International Classification of Functioning, Disability and Health) standard proposed by the World Health Organization, (iii) the knowledge base used by the system to provide tailored instructions to medical first responders, (iv) the Web-based architecture of the system, (v) the different interfaces--including one for mobile devices--the system provides to enable all the identified stakeholders (disabled persons, their families, clinicians, EMS call center operators, medical first responders) to easily access and possibly provide data to the system, (vi) the evaluation of the validity of the patient model and of the system usability which has been conducted with end users.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Annotation-based video enrichment for blind people: a pilot study on the use of earcons and speech synthesis</title>
	<abstract>Our approach to address the question of online video accessibility for people with sensory disabilities is based on video annotations that are rendered as video enrichments during the playing of the video. We present an exploratory work that focuses on video accessibility for blind people with audio enrichments composed of speech synthesis and earcons (i.e. nonverbal audio messages). Our main results are that earcons can be used together with speech synthesis to enhance understanding of videos; that earcons should be accompanied with explanations; and that a potential side effect of earcons is related to video rhythm perception.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Aesthetics and experience-centered design</title>
	<abstract>The aesthetics of human-computer interaction and interaction design are conceptualized in terms of a pragmatic account of human experience. We elaborate this account through a framework for aesthetic experience built around three themes: (1) a holistic approach wherein the person with feelings, emotions, and thoughts is the focus of design; (2) a constructivist stance in which self is seen as continuously engaged and constituted in making sense of experience; and (3) a dialogical ontology in which self, others, and technology are constructed as multiple centers of value. We use this framework to critically reflect on research into the aesthetics of interaction and to suggest sensibilities for designing aesthetic interaction. Finally, a digital jewelery case study is described to demonstrate a design approach that is open to the perspectives presented in the framework and to consider how the framework and sensibilities are reflected in engagement with participants and approach to design.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>What Next?: A Dozen Information-Technology Research Goals</title>
	<abstract>Charles Babbage's vision of computing has largely been realized. We are on the verge of realizing Vannevar Bush's Memex. But, we are some distance from passing the Turing Test. These three visions and their associated problems have provided long-range research goals for many of us. For example, the scalability problem has motivated me for several decades. This talk defines a set of fundamental research problems that broaden the Babbage, Bush, and Turing visions. They extend Babbage's computational goal to include highly-secure, highlyavailable, self-programming, self-managing, and self-replicating systems. They extend Bush's Memex vision to include a system that automatically organizes, indexes, digests, evaluates, and summarizes information (as well as a human might). Another group of problems extends Turing's vision of intelligent machines to include prosthetic vision, speech, hearing, and other senses. Each problem is simply stated and each is orthogonal from the others, though they share some common core technologies.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>The design of natural interaction</title>
	<abstract>This paper addresses the problem of the relationship between humans and technology-enhanced spaces and physical objects (later defined as artifacts). The class of cases here analyzed includes interactive digital signage, information kiosks, home media centers and interactive spaces whose purpose is the communication of a meaning. In this domain, complex interfaces are not needed, as common people interaction with information, content and media is in most cases extremely simple. The topic of specialized interfaces for expert users is not addressed here; the focus is on interfaces for the general public, whose main purpose is the basic fruition of digital information, although such information can be large and complex in its organization. This paper is centered on the need of conceiving computer sensing and information presentation as different aspects of the same interaction design problem, instead of separate research entities.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Human computer intelligent interaction using augmented cognition and emotional intelligence</title>
	<abstract>Human-Computer Interaction (HCI) has mostly developed along two competing methodologies: direct manipulation and intelligent agents. Other possible but complementary methodologies are those of augmented cognition and affective computing and their adaptive combination. Augmented cognition harnesses computation to exploit explicit or implicit knowledge about context, mental state, and motivation for the user, while affective computing provides the means to recognize emotional intelligence and affects human-computer interfaces and interactions people are engaged with. Most HCI studies elicit emotions in relatively simple settings, whereas augmented cognition and affective computing include bodily (physical) embedded within mental (cognitive) and emotional events. Recognition of affective states currently focuses on their physical form (e.g., blinking or face distortions underlying human emotions) rather than implicit behavior and function (their impact on how the user employs the interface or communicates with others). Augmented cognition and affective computing are examined throughout this paper regarding design, implementation, and benefits. Towards that end we have designed an HCII interface that diagnoses and predicts whether the user was fatigued, confused, frustrated, momentarily distracted, or even alive through non-verbal information, namely paralanguage, in a virtual reality (VR) learning environment.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Is Wikipedia usable for the blind?</title>
	<abstract>Today wikis are becoming increasingly widespread, and offer great benefits in a variety of collaborative environments. Therefore, to be universally valuable, wiki systems should be easy to use for anyone, regardless of ability. This paper describes obstacles that a blind user may encounter when interacting via screen reader with Wikipedia, and offers some suggestions for improving usability.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Computers and People with Disabilities</title>
	<abstract>Editors' comments: “Computers and People with Disabilities” is a reprint of an article originally published in Communications of the ACM in 1992. In this article, Glinert and York issued a “call-to-arms” for research and development on technologies for people with disabilities. Specifically, they highlighted that human-computer interfaces at the time generally failed to take into account the needs of disabled users. Their challenge was to change computing culture to address this need. Their article remains timely today in its consideration of government, industry, and private foundations working with researchers to achieve accessible technology. With the recent launch of Transactions on Accessible Computing, this seems an appropriate time to consider progress in the field since, as well as current research trends.

The reprinting of this article is followed by four commentaries by leaders in accessibility research. Each was cited in the 1992 article and each now gives their view on how the field has progressed since that time. In their commentaries, some themes emerge and new technologies are discussed. In short, their commentaries point to both a great deal of progress and a lack of progress. All four of the commentators note areas where computing continues to present barriers rather than assist users with disabilities.

Alistair Edwards sets the stage with a look back at interfaces and input technologies popular in 1992, with attention paid to access problems related to graphical user interfaces (GUIs) that have consumed much research energy since 1992. Alan Newell highlights disability concerns that were not given large research consideration in 1992, but which have now become significant due, in large part, to changes in global demographics. Specifically, he brings visibility to research on older adults and cognitively disabled users.

A number of advances in technology and methodology since 1992 are discussed by the commentators. The ubiquity of computing and its critical adoption in the world today are highlighted. The commentators reflect, for example, on technologies produced by research for disabled users that have now impacted mainstream offerings on standards for accessibility that have emerged worldwide and their impact and on assistive technologies that have been developed. Critically, the proliferation of the World Wide Web was not foreseen in 1992 and its use by people with disabilities is discussed. Gregg Vanderheiden considers the opportunity afforded by the Web to provide widespread availability of accessible software.

Glinert and York discussed the need for design for disability. While research relevant to users with disabilities is gaining momentum, the commentators indicate that users with disabilities still struggle with much of today's IT. The commentators note current trends toward designs that take into account disabled users. Notably, Richard Ladner ends his commentary by mentioning the issue of empowerment. Users with disabilities have moved beyond simply needing the protections of regulation that were emerging in 1992, to being active participants in designing solutions to allow full participation in the current social, political, and economic environments.

Together, these articles provide a great deal of food for thought on technology advances and new considerations of accessible technology. Has the change in computing culture envisioned by Glinert and York taken hold?

Vicki L. Hanson and Andrew Sears

Co-Editors in Chief</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Tactile sensory substitution: Models for enaction in HCI</title>
	<abstract>To apply enactive principles within human-computer interaction poses interesting challenges to the way that we design and evaluate interfaces, particularly those that possess a strong sensorimotor character. This article surveys the field of tactile sensory substitution, an area of science and engineering that lies at the intersection of such research domains as neuroscience, haptics, and sensory prosthetics. It is argued that this area of research is of high relevance to the design and understanding of enactive interfaces that make use of touch, and is also a fertile arena for revealing fundamental issues at stake in the design and implementation of enactive interfaces, ranging from engineering, to human sensory physiology, and the function and plasticity of perception. A survey of these questions is provided, alongside a range of current and historical examples. </abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Using screen readers to reinforce web accessibility education</title>
	<abstract>This paper discusses an approach for Web accessibility education tacking advantage of short introductory courses. In this approach, screen readers were used toallimmerse students in the problems experienced by blind users while using the Web. Experimental results from an evaluation exercise carried out by students showedallthat the use ofallscreen readers was helpful to better understand and to reinforce the concepts of Web accessibility.</abstract>
	<search_task_number>17</search_task_number>
	<query>computer interface blind deaf</query>
	<relevance>1</relevance>
  </item>



  <item>
    <title>Motion Control Based on Dimensional Reduction and Human Computer Interaction</title>
	<abstract>owing to the high dimension characteristic of motion in catching original data, the high dimensional original data will be projected into low dimensional sub space. This paper applies key frame and dimension reduction method based on several machine learning methods to handle motion capture data. Based on subspace, the human-computer interaction techniques are used to shrink the gap between user’s high-level perception and the low-level features of three-dimensional human motion in the system. After a series of experimental results, our method improves performance of motion retrieval and control.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Social interactive human video synthesis</title>
	<abstract>In this paper, we propose a computational model for social interaction between three people in a conversation, and demonstrate results using human video motion synthesis. We utilised semi-supervised computer vision techniques to label social signals between the people, like laughing, head nod and gaze direction. Data mining is used to deduce frequently occurring patterns of social signals between a speaker and a listener in both interested and not interested social scenarios, and the mined confidence values are used as conditional probabilities to animate social responses. The human video motion synthesis is done using an appearance model to learn a multivariate probability distribution, combined with a transition matrix to derive the likelihood of motion given a pose configuration. Our system uses social labels to more accurately define motion transitions and build a texture motion graph. Traditional motion synthesis algorithms are best suited to large human movements like walking and running, where motion variations are large and prominent. Our method focuses on generating more subtle human movement like head nods. The user can then control who speaks and the interest level of the individual listeners resulting in social interactive conversational agents.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Manual and cognitive benefits of two-handed input: an experimental study
</title>
	<abstract>One of the recent trends in computer input is to utilize users' natural bimanual motor skills. This article further explores the potential benefits of such two-handed input. We have observed that bimanual manipulation may bring two types of advantages to human-computer interaction: manual and cognitive. Manual benefits come from increased time-motion efficiency, due to the twice as many degrees of freedom simultaneously available to the user. Cognitive benefits arise as a result of reducing the load of mentally composing and visualizing the task at an unnaturally low level which is imposed by traditional unimanual techniques. Area sweeping was selected as our experimental task. It is representative of what one encounters, for example, when sweeping out the bounding box surrounding a set of objects in a graphics program. Such tasks cannot be modeled by Fitts' Law alone and have not been previously studied in the literature. In our experiments, two bimanual techniques were compared with the conventional one-handed GUI approach. Both bimanual techniques employed the two-handed “stretchy” technique first demonstrated by Krueger in 1983. We also incorporated the “Toolglass” technique introduced by Bier et al. in 1993. Overall, the bimanual techniques resulted in significantly faster performance than the status quo one-handed technique, and these benefits increased with the difficulty of mentally visualizing the task, supporting our bimanual cognitive advantage hypothesis. There was no significant difference between the two bimanual techniques. This study makes two types of contributions to the literature. First, practically we studied yet another class of transaction where significant benefits can be realized by applying bimanual techniques. Furthermore, we have done so using easily available commercial hardware in the context to our understanding of why bimanual interaction techniques have an advantage over unimanual techniques. A literature review on two-handed computer input and some of the relevant bimanual human mototr control studies is also included.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Interaction and reflection via 3D path shape qualities in a mediated constructive learning environment
</title>
	<abstract>The mixed-reality environment, or hybrid physical-digital space, is an emerging human-computer interaction paradigm with great potential to support constructive learning in everyday settings as a complement to traditional classroom methods. Key advantages over screen-based media and immersive (virtual-reality) environments include a) dynamic, multimodal feedback, which engages diverse learning styles through multiple modes of representation; b) affordance of unencumbered, full-body movement, which enables interactions to be physically embodied; and c) physical continuity with the classroom, which fosters informal collaborative and social interactions. For this potential to be realized, however, we must address significant challenges in interaction design. We must develop modes of interaction which are implicitly learnable, which afford full-body movement, and which are cognitively well adapted to large physical spaces. Furthermore, we must create a mechanism by which students can reflect on what they have "constructed" through interacting with the space, so that implicit learning can be leveraged in the interest of explicit understanding. To these ends, we have developed a novel interaction paradigm based around 3D path shape qualities: straight, curved, random, and stop, which describe motion of an illuminated object (glowball) guided by the participant through the space. We infer these qualities in real-time (online) and also offline using a robust Bayesian framework operating on a low-cost, non-intrusive video sensing apparatus. Online inference drives the interaction and offline segmentation the post-interaction display, our mechanism for reflection where segmentation results are mapped onto a physical trace of the participant.s motion. An informal study a) validates the implicit learnability of straight, curved, and stop mappings based on shape quality controls, and b) highlights the comparative advantage of the postinteraction display for all mappings, when subjects are asked to identify the actions responsible for specific target outcomes.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>An integrated framework for universal motion control</title>
	<abstract>The advance of the motion tracking and 3D display technologies impacts the input and output devices of the general human-computer interaction framework nowadays. The motion sensing devices can provide great tracking accuracy at relatively low prices, which make motion-based interactions affordable and popular for general use. Taking into account all the general interactions required on graphic user interfaces, we propose an integrated framework for motion control, which seamlessly supports 2D, 3D and motion gesture interactions. We categorize the general tasks and define four corresponding operating modes: 2D cursor, 3D manipulation, 3D navigation, and motion gesture. Trade-offs are made between generality, performance, and usability. With a careful design of mapping, we believe the generality of the motion control can outweigh the compromise in performance.

In the implementation, a hybrid framework of optical and inertial sensing is used to achieve precise 6 DOF motion tracking. We develop two interesting applications to demonstrate the usability of the integrated motion control framework between the first three operating modes. The motion gesture mode is proposed but not covered in this work.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>6DMG: a new 6D motion gesture database
</title>
	<abstract>Motion-based control is gaining popularity, and motion gestures form a complementary modality in human-computer interactions. To achieve more robust user-independent motion gesture recognition in a manner analogous to automatic speech recognition, we need a deeper understanding of the motions in gesture, which arouses the need for a 6D motion gesture database. In this work, we present a database that contains comprehensive motion data, including the position, orientation, acceleration, and angular speed, for a set of common motion gestures performed by different users. We hope this motion gesture database can be a useful platform for researchers and developers to build their recognition algorithms as well as a common test bench for performance comparisons.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>HandPuppet3D: Motion capture and analysis for character animation
</title>
	<abstract>Motion capture is a technique of digitally recording the movements of real entities, usually humans. It was originally developed as an analysis tool in biomechanics research, but has grown increasingly important as a source of motion data for computer animation. In this context it has been widely used for both cinema and video games. Hand motion capture and tracking in particular has received a lot of attention because of its critical role in the design of new Human Computer Interaction methods and gesture analysis. One of the main difficulties is the capture of human hand motion. This paper gives an overview of ongoing research "HandPuppet3D" being carried out in collaboration with an animation studio to employ computer vision techniques to develop a prototype desktop system and associated animation process that will allow an animator to control 3D character animation through the use of hand gestures. The eventual goal of the project is to support existing practice by providing a softer, more intuitive, user interface for the animator that improves the productivity of the animation workflow and the quality of the resulting animations. To help achieve this goal the focus has been placed on developing a prototype camera based desktop gesture capture system to capture hand gestures and interpret them in order to generate and control the animation of 3D character models. This will allow an animator to control 3D character animation through the capture and interpretation of hand gestures. Methods will be discussed for motion tracking and capture in 3D animation and in particular that of hand motion tracking and capture. HandPuppet3D aims to enable gesture capture with interpretation of the captured gestures and control of the target 3D animation software. This involves development and testing of a motion analysis system built from algorithms recently developed. We review current software and research methods available in this area and describe our current work.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Toward natural interaction through visual recognition of body gestures in real-time
</title>
	<abstract>In most of the existing human-computer interfaces, enactive knowledge as new natural interaction paradigm has not been fully exploited yet. Recent technological advances have created the possibility to enhance naturally and significantly the interface perception by means of visual inputs, the so-called Vision-Based Interfaces (VBI). In the present paper, we explore the recovery of the user's body posture by means of combining robust computer vision techniques and a well known inverse kinematics algorithm in real-time. Specifically, we focus on recognizing the user's motions with a particular mean, that is, a body gesture. Defining an appropriate representation of the user's body posture based on a temporal parameterization, we apply non-parametric techniques to learn and recognize the user's body gestures. This scheme of recognition has been applied to control a computer videogame in real-time to show the viability of the presented approach.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>A study on usability of human-robot interaction using a mobile computer and a human interface device
</title>
	<abstract>A variety of devices are used for robot control such as personal computers or other human interface devices, haptic devices, and so on. However, sometimes it is not easy to select a device which fits the specific character of varied kinds of robots while at the same time increasing the user's convenience. Under these circumstances, in this study, we have tried to measure user convenience. We tried to understand the characteristics of several devices used to achieve human robot interaction by using each of these devices that could be used with a personal computer: We used a button type device, a joystick, a driving device which consisted of a handle and pedals, and a motion-based human interface device including an acceleration sensor.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Near-infrared composite pattern projection for continuous motion hand-computer interaction
</title>
	<abstract>Traditional structured light techniques require multiple patterns to be projected onto a scene and captured for three-dimensional (3D) reconstruction without ambiguity and resistance to variation of albedo. However, multi-pattern techniques can be corrupted by the object movement during the sequential projection. We have introduced a single projected pattern which efficiently combines multiple patterns into a single composite pattern projection-allowing for video rate three-dimensional data acquisition. Attaining low cost 3D video acquisition would have a profound impact on applications such as human-computer interaction and assistive technology. So far the composite pattern technique has only been performed with visible light which would cause user annoyances for these applications. To solve the problem, we use near-infrared to project the pattern. But this raises another more general issue of spatial skin response. In this study, near-infrared illumination is used for imperceptible measurement of hand pose. In particular, we are studying continuous motion depth acquisition for tracking hand motion and rotation as an interface to a virtual reality. Applications include advanced cockpit controls and computer interfacing for the disabled.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Computer Graphics in Italy: Haptic technologies for the conceptual and validation phases of product design
</title>
	<abstract>The paper presents two applications of haptic technologies to demonstrate how they can increase human computer interaction during different steps of design process. The first application aims at developing a system to generate digital shapes by manipulating haptic tools that resemble the physical ones that the modelers use in everyday work. The second is focused on the use of haptic interfaces to evaluate ergonomics of virtual products control boards. We designed and developed the mentioned haptic devices; the first uses two FCS HapticMaster equipped with a innovative strong and stiff 6 DOF device carrying simulated clay modeling tools. The second is an ''ad hoc'' mechatronic device able to simulate some controls with rotary motions (knobs). The described haptic devices are integrated in more complex virtual reality applications; the paper describes their architecture and the methodologies proposed to simulate material shaping and ergonomic validation. The main aspects of haptic modeling and rendering are also discussed.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>AcouMotion – an interactive sonification system for acoustic motion control
</title>
	<abstract>This paper introduces AcouMotion as a new hard-/software system for combining human body motion, tangible interfaces and sonification to a closed-loop human computer interface that allows non-visual motor control by using sonification (non-speech auditory displays) as major feedback channel. AcouMotion's main components are (i) a sensor device for measuring motion parameters (ii) a computer simulation to represent the dynamical evolution of a model world, and (iii) a sonification engine which generates an auditory representation of objects and any interactions in the model world. The intended applications of AcouMotion range from new kinds of sport games that can be played without visual displays and therefore may be particularly interesting for people with visual impairment to further applications in data mining, physiotherapy and cognitive research. The first application of AcouMotion presented in this paper is Blindminton, a sport game similar to Badminton which is particularly adapted to the abilities of people with visual impairment. We describe our current system and its state of development, and we present first sound examples for interactive sonification using an early prototype. Finally, we discuss some interesting research directions based on the fact that AcouMotion binds auditory stimuli and body motion, and thus can represent a counterpart to the Eye-tracker device that exploits the binding of visual stimuli and eye-movement in cognitive research.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Human-computer interaction system based on nose tracking
</title>
	<abstract>This paper presents a novel Human-Computer Interaction (HCI) system with calibrated mono-camera which integrates active computer vision technology and embedded speech command recognition technology. Mainly by tracking the nose tip motion robustly as the mouse trace, this system completes mouse mission with recognition rate more than 85% at the speed 15 frame per second. To achieve the goal, we adopt a novel approach based on the symmetry of the nose plane feature to localize and track invariantly to the varied environment. Comparing to other kinds of pointing device, this hand-free HCI system is hands-free, cheap, real-time, convenient and unpolluted, which can be used in the field of disabled aid, entertainment and remote control.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Head pose tracking and gesture detection using block motion vectors on mobile devices
</title>
	<abstract>We present a novel and practical algorithm for head pose tracking and head gesture detection applicable to avatar applications and Human Computer Interaction (HCI) on mobile devices. The algorithm takes advantage of block motion vectors estimated for real-time video encoding on the device. After spatial and temporal smoothing, block motion vectors are mapped into a video pointer signal, which is further mapped into head pose signals for avatar animation control. In contrast to conventional tracking algorithms, our algorithm processes block motion vectors rather than pixel data, leading to drastically reduced computational requirement. This makes it a practical solution for head tracking in real time on high end mobile devices. A simple and reliable way of detecting head nod and shake gestures is also presented, using a deterministic finite state machine.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>A novel human computer interface based on hand gesture recognition using computer vision techniques
</title>
	<abstract>In daily life, human beings communicate with each other and use broad range of gestures in the process of interaction. Apart of the interpersonal communication, many hours are spent in the interaction with the electronic devices. In the last decade, new classes of devices for accessing information have emerged along with increased connectivity. In parallel to the proliferation of these devices, new interaction styles have been explored. The objective of this paper is to provide a gesture based interface for controlling applications like media player using computer vision techniques. The human computer interface application consists of a central computational module which uses the Principal Component Analysis for gesture images and finds the feature vectors of the gesture and save it into a XML file. The Recognition of the gesture is done by K Nearest Neighbour algorithm. The Training Images are made by cropping the hand gesture from static background by detecting the hand motion using Lucas Kanade Pyramidical Optical Flow algorithm. This hand gesture recognition technique will not only replace the use of mouse to control the media player but also provide different gesture commands which will be useful in controlling the application.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Hand Tracking Using Spatial Gesture Modeling and Visual Feedback for a Virtual DJ System
</title>
	<abstract>The ability to accurately track hand movement provides new opportunities for human computer interaction (HCI). Many of today's commercial hand tracking devices based on gloves can be cumbersome and expensive. An approach that avoids these problems is to use computer vision to capture hand motion.In this paper, we present a complete real-time hand tracking and 3-D modeling system based on a single camera. In our system, we extract feature points from a video stream of a hand to control a virtual hand model with 2-D global motion and 3-D local motion. The on screen model gives the user instant feedback on the estimated position of the hand. This visual feedback allows a user to compensate for the errors in tracking.The system is used for three example applications. The first application uses hand tracking and gestures to take on the role of the mouse. The second interacts with a 3D virtual environment using the 3D hand model. The last application is a virtual DJ system that is controlled by hand motion tracking and gestures.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Wii can do it: using co-design for creating an instructional game
</title>
	<abstract>There are many children for whom learning is difficult if they need to remain still. The Nintendo Wii, with its motion-controlled sensors, can support learning experiences that enable children to be physically active learners. This paper presents the methodologies and results from a multi-day, co-design session at the University of Maryland's Human-Computer Interaction Lab. The goal of the sessions was to design an instructional game that leveraged the Nintendo Wii's motion controls to teach about U.S. National Parks.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Tabletop interaction: research alert
</title>
	<abstract>At the t2i Lab we focus on tangible user interfaces (TUIs) to advance and improve the user experience in computer supported learning and problem solving. By directly interacting with physical controls, complex concepts such as the chemistry of a molecule, dynamics of physics, or the rules of a market may be more easily understood. Through their capacity to closely track user actions TUIs may offer more direct interaction. Several disciplines are involved in tabletop interaction, such as gesture-based interaction, social protocols, haptic rendering, tracking-, and display-hardware. This demo paper presents the two projects Augmented Chemistry (AC) and eMotion. AC is a combination of TUIs and computer graphics for organic chemistry education with a focus on concepts like the octet rule and tetrahedrons. eMotion focuses on bridging the gap between computers and human emotions by enabling computers to estimate their users' emotions by evaluating their mouse motions.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Movement and music: designing gestural interfaces for computer-based musical instruments
</title>
	<abstract>The concept of body-mediated or embodied interaction, of the coupling of interface and actor, has become increasingly relevant within the domain of HCI. With the reduced size and cost of a wide variety of sensor technologies and the ease with which they can be wirelessly deployed, on the body, in devices we carry with us and in the environment, comes the opportunity to use a wide range of human motion as an integral part of our interaction with many applications. While movement is potentially a rich, multidimensional source of information upon which interface designers can draw, its very richness poses many challenges in developing robust motion capture and gesture recognition systems. In this talk, I will suggest that lessons learned by designers of computer-based musical instruments whose task is to translate expressive movement into nuanced control of sound may now help to inform the design of movement-based interfaces for a much wider range of applications.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Image-based gesture recognition with support vector machines
</title>
	<abstract>Recent advances in various display and virtual technologies, coupled with an explosion in available computing power, have given rise to a number of novel human-computer interaction (HCI) modalities, among which gesture recognition is undoubtedly the most grammatically structured and complex. However, despite the abundance of novel interaction devices, the naturalness and efficiency of HCI has remained low. This is due in particular to the lack of robust sensory data interpretation techniques. To address the task of gesture recognition, this dissertation establishes novel probabilistic approaches based on support vector machines (SVM). Of special concern in this dissertation are the shapes of contact images on a multi-touch input device for both 2D and 3D. Five main topics are covered in this work.

The first topic deals with the hand pose recognition problem. To perform classification of different gestures, a recognition system must attempt to leverage between class variations (semantically varying gestures), while accommodating potentially large within-class variations (different hand poses to perform certain gestures). For recognition of gestures, a sequence of hand shapes should be recognized. We present a novel shape recognition approach using Active Shape Model (ASM) based matching and SVM based classification. Firstly, a set of correspondences between the reference shape and query image are identified through ASM. Next, a dissimilarity measure is created to measure how well any correspondence in the set aligns the reference shape and candidate shape in the query image. Finally, SVM classification is employed to search through the set to find the best match from the kernel defined by the dissimilarity measure above. Results presented show better recognition results than conventional segmentation and template matching methods.

In the second topic, dynamic time alignment (DTA) based SVM gesture recognition is addressed. In particular, the proposed method combines DTA and SVM by establishing a new kernel. The gesture data is first projected into a common eigenspace formed by principal component analysis (PCA) and a distance measure is derived from the DTA. By incorporating DTA in the kernel function, general classification problems with variable-sized sequential data can be handled.

In the third topic, a C++ based gesture recognition application for the multi-touchpad is implemented. It uses the proposed gesture classification method along with a recursive neural networks approach to recognize definable gestures in real time, then runs an associated command. This application can further enable users with different disabilities or preferences to custom define gestures and enhance the functionality of the multi-touchpad.

Fourthly, an SVM-based classification method that uses the DTW to measure the similarity score is presented. The key contribution of this approach is the extension of trajectory based approaches to handle shape information, thereby enabling the expansion of the system's gesture vocabulary. It consists of two steps: converting a given set of frames into fixed-length vectors and training an SVM from the vectorized manifolds. Using shape information not only yields discrimination among various gestures, but also enables gestures that cannot be characterized solely based on their motion information to be classified, thus boosting overall recognition scores.

Finally, a computer vision based gesture command and communication system is developed. This system performs two major tasks: the first is to utilize the 3D traces of laser pointing devices as input to perform common keyboard and mouse control; the second is supplement free continuous gesture recognition, i.e., data gloves or other assistive devices are not necessary for 3D gestures recognition. As a result, the gesture can be used as a text entry system in wearable computers or mobile communication devices, though the recognition rate is lower thanthe approaches with the assistive tools. The purpose of this system is to develop new perceptual interfaces for human computer interaction based on visual input captured by computer vision systems, and to investigate how such interfaces can complement or replace traditional interfaces.

Original contributions of this work span the areas of SVMs and interpretation of computer sensory inputs, such as gestures for advanced HCI. In particular, we have addressed the following important issues: (1) ASM base kernels for shape recognition. (2) DTA based sequence kernels for gesture classification. (3) Recurrent neural networks (RNN). (4) Exploration of a customizable HCI. (5) Computer vision based 3D gesture recognition algorithms and system.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Phrases of the kinetic: dynamic physicality as a dimension of the design process
</title>
	<abstract>At its core, the concept of Tangible Interfaces leverages the idea of using the movement of the body as an inherent part of the human side of a human-computer interaction, assuming that bodily engagement and tactile manipulation can facilitate deeper understanding and more intuitive experiences. However, as an interaction principle in our era of digital design, motion construction and control has been underutilized and little examined as a design tool, leaving open the possibilities of motion's natural ability to draw our attention, provide physical feedback, and convey information through physical change. This dissertation postulates that the ability to experiment, prototype, and model with programmable kinetic forms is becoming increasingly important as digital technology becomes more readily embedded in our objects and environments and need for tools and systems with which to create, manipulate and finesse motion in response to computational and material input remains an under-developed design area. This thesis aims to establish principles of kinetic design through the exploration of two approaches to motion construction and manipulation: motion prototyping as a methodology for design thinking, learning and communication and physically dynamic state memory as a methodology for organic form finding and transformation in the design process. To demonstrate these aims, I present three interface systems: Topobo, a system for motion construction and dynamics physics education with children; Kinetic Sketchup, a system for motion construction and prototyping in architecture and product design; and Bosu, an augmented textile interface offering an experimental approach to digitally augmented organic form finding in fashion and product design. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Recognition and Synthesis of Human Movements by Parametric HMMs
</title>
	<abstract>The representation of human movements for recognition and synthesis is important in many application fields such as: surveillance, human-computer interaction, motion capture, and humanoid robots. Hidden Markov models (HMMs) are a common statistical framework in this context, since they are generative and are able to deal with the intrinsic dynamic variation of movements performed by humans. In this work we argue that many human movements are parametric, i.e., a parametric variation of the movements in dependence of, e.g., a position a person is pointing at. The parameter is part of the semantic of a movement. And while classic HMMs treat them as noise, we will use parametric HMMs (PHMMs) [6,19] to model the parametric variability of human movements explicitly. In this work, we discuss both types of PHMMs, as introduced in [6] and [19], and we will focus our considerations on the recognition and synthesis of human arm movements. Furthermore, we will show in various experiments the use of PHMMs for the control of a humanoid robot by synthesizing movements for relocating objects at arbitrary positions. In vision-based interaction experiments, PHMM are used for the recognition of pointing movements, where the recognized parameterization conveys to a robot the important information which object to relocate and where to put it. Finally, we evaluate the accuracy of recognition and synthesis for pointing and grasping arm movements and discuss that the precision of the synthesis is within the natural uncertainty of human movements.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Embedded Control System of Digital Operating Table
</title>
	<abstract>Aiming at the existing problems in the control of traditional operating table such as the lack of accuracy in the motion control of table, the poor display of objects' status in the operation space, the unstructured and incompatible interfaces for multiple digital devices and apparatus, the control system based on the embedded technology on implemented on the prototype of a novel digital operating table. This system can precisely control the positions and orientations of the digitization surgery platform which has 4 DOF as follows: the horizontal translation, the vertical translation, the pitching and the rolling. In the meantime, there are also interactive functions such as button control, speech control, PC control and LCD display that provide a good human-computer interaction for surgeon's operation during the procedure.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Active capture: integrating human-computer interaction and computer vision/audition to automate media capture
</title>
	<abstract>While the devices for media capture have advanced from mechanical to computational since the invention of photography and motion pictures in the 19th century, their underlying user interaction paradigms have remained largely unchanged. Current interaction techniques for media capture do not leverage computation to solve key problems: the skill required to capture high quality media assets; the effort required to select useable assets from captured assets; and the lack of metadata describing the content and structure of media assets that could enable them to be retrieved and (re)used. We describe a new interaction and processing paradigm for media capture that redefines capture as a control process with feedback. By integrating human-computer interaction and computer vision and audition into an "active capture" process, we overcome the limitations of current media capture devices, algorithms, and interaction techniques. Active capture leverages media production knowledge to automate direction and cinematography and thus enables the automated production of annotated, high quality, reusable media assets.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Video Gesture Recognition System
</title>
	<abstract>The primary goal of a Gesture Recognition system proposed in this paper is to identify specific human gestures and use them to communicate information or for device control. It involves recognizing hand gestures through a video feed and is used to control appliances like LIGHT, FAN, etc. Motion detection in the continuous video stream is done using a Motion Detection Algorithm. This simple video gesture recognizer takes us one step closer to the emerging ubiquitous environment, allowing device control by visual input thereby eliminating the need for standard input devices. Keywords: Gesture, Video, Capture, Rrecognition, Human Computer Interaction (HCI)
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Representation of Human Postures for Vision-Based Gesture Recognition in Real-Time
</title>
	<abstract>Using computer vision to sense and perceive the user and his/her actions in a Human-Computer Interaction context is often referred to as Vision-Based Interfaces. In this paper, we present a Vision-Based Interface guided by the user gestures. Previously to recognition, the user's movements are obtained through a real-time vision-based motion capture system. This motion capture system is capable to estimate the user 3D body joints position in real-time. By means of an appropriate representation of limbs orientations based on temporal histograms, we present a scheme of gesture recognition that also works in real-time. This scheme of recognition has been tested through control of a classical computer videogame showing an excellent performance in on-line classification and it allows the possibility to achieve a learning phase in real-time due to its computational simplicity.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Behind Fitts' law: kinematic patterns in goal-directed movements
</title>
	<abstract>Half a century ago, Paul Fitts first discovered that the time necessary to complete a pointing movement (MT) linearly increases with the amount of information (ID) necessary to specify the target width (W) relative to the distance (D). The so-called Fitts' law states that MT = a + b ID, with ID being a logarithmic function of the D/W ratio. With the rising importance of pointing in human-computer interaction, Fitts' law is nowadays an important tool for the quantitative evaluation of user interface design. We show that changes in ID give rise to systematic changes in the kinematics patterns that determine MT, and provide evidence that the observed patterns result from the interplay between basic oscillatory motion and visual control processes. We also emphasize the generality and abstract nature of Fitts' robust model of human psychomotor behavior, and suggest that some adaptations in the design of the (computer-mediated) coupling of perception and production of movement might improve the efficiency of the interaction.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>A Novel Human Computer Interaction Paradigm for Volume Visualization in Projection-Based Virtual Environments
</title>
	<abstract>We propose a novel Human Computer Interaction (HCI) paradigm for volume visualization in projection-based immersive virtual environments (VEs). This paradigm is intuitive, highly efficient and allows accurate control over the virtual objects. A <em>fine control mode</em>for direct manipulation is proposed to address the low accuracy problem of virtual object manipulation in VEs. An <em>agent object</em>interaction method is proposed to provide more flexibility in manipulating the volume objects. A <em>two-handed scaling</em>method is proposed to conveniently scale the volume object along one, two, or three axes. Finally, a <em>ghost object paradigm</em>is proposed to address the motion constraint problem for virtual objects. An implementation using a 3-state tracked glove setup as the input interface is discussed. How basic functionality for volume visualization can be transferred from the 2D WIMP (Window, Icon, Menu, and Pointer) interface to a 3D VR interface is also systematically discussed.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Building Virtual Entertainment Environment with Tiled Display Wall and Motion Tracking
</title>
	<abstract>Presented in this paper is an immersive and interactive entertainment environment which integrates multiprojector tiled display wall and motion tracking. Calibration methods are proposed for the geometry alignment and color consistency of the tiled display wall, achieving immersive visual experience. To track a player's motion, two markers are clung to the left and right leg, and a robust motion tracking algorithm is designed to obtain the player's moving direction and speed from video streams captured by a web camera. The resulted parameters are finally used to control the avatar in virtual scene on the display wall. The system prototype is featured with low-cost devices, easy and scalable installation, fast calibration and real-time tracking, which provides a natural human-computer interaction environment.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Interactive Intelligent Media Player Based on Head Motion Recognition
</title>
	<abstract>Over the last decade, human computer interaction has become an active research area, which releases people from inactive, inflexible communication with machine. In this paper, a human-machine interaction framework for intelligent media player controller is presented. The overall goal is to develop a video player system which can take action automatically based upon head interactive motion recognition. Combined with image processing technique and pattern recognition theories, the hair cue is used as prior information, and the symmetry along with distribution can be extracted to recognize the head moving direction, which controls the media player action individually, such as going to next or back. The experiment show that the method is easily to implemented and has pretty performance. The system could be transplanted into intelligent home appliance conveniently as well.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Wireless Vision Based Mobile Robot Control Using Hand Gesture Recognition Through Perceptual Color Space
</title>
	<abstract>In this paper we have implemented a wireless vision based mobile robot control through hand gesture recognition based on perceptual color space such as HIS, HSV/HSB, HSL. Vision-based hand gesture recognition is an important problem in the field of human-computer interaction, since hand motions and gestures could potentially be used to interact with computers in more natural ways. The robot control was done purely based on the orientation histograms a simple and fast algorithm on the system which would recognize static hand gestures with HSV color spaces as major parameters. The wireless based mobile robot system using hand gestures is a new innovative user interface that resolves the complications of using numerous remote controls for various applications. Based on one unified set of hand gestures, this system interprets the user hand gestures into pre-defined commands to control the remote robot. The experimental results are very encouraging as the system produces real-time responses and highly accurate recognition towards various gestures under different lighting conditions.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Towards an adaptive man-machine interface for virtual environments
</title>
	<abstract>We describe an approach to developing an adaptive virtual environment user interface to enable the user to perform a wide variety of tasks while immersed in the virtual environment. Currently, virtual environment operation places an unmanageable cognitive burden upon the user. While some advances in user interface design can alleviate some of this problem, the basic problem of information overload can not be adequately addressed solely through development of a better interface or provision of ad hoc decision support tools. We contend that a comprehensive design approach to the interface can improve user access to the virtual environment display parameters, analysis reports, conferencing and collaboration capabilities, intelligent agents for user assistance, motion and orientation controls, recording devices, and situation awareness aids. Our intelligent interface research effort, called Symbiotic Information Reasoning and Decision Support (SIRDS), addresses the issues related to the design and development of an adaptive, intelligent, learning man machine interface. Construction of the interface requires a mix of traditional human computer interaction, data visualization, and intelligent agents within a software engineering framework. The framework supports the symbiosis of human cognition and computational power that is required to deal with complex virtual environments.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Intelligent Design and Kinematics Analysis of Picking Robot Manipulator
</title>
	<abstract>Aiming at the feature of picking for the biological fruits, an intelligent design of the 5 degree of freedom of lichi picking manipulator was introduced. Based on the first version of the manipulator, its mechanism and manufacture processes were improved, and the key technology in the mechanism design of the manipulator was studied, including some novel design thought and methods of the arm joint and end-effector. Meanwhile, according to the design scheme, the Denavit-Hatenberg method, which is a common method for solving robot kinematics, and a geometry method were adopted to analyze and calculate the manipulator’s kinematics solution and its inverse solution. It provides the foundation of continuous optimization of the manipulator’s mechanism and control design. At last, the Intelligent design system of picking manipulator was developed in the platform of Microsoft VisualC++, the design scheme and structure design of the manipulator can be optimized in the design system, moreover, through the human-computer interaction, the realtime simulation of the mechanism motion and coordinate inverse calculation can be realized.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Mechanical constraints as computational constraints in tabletop tangible interfaces
</title>
	<abstract>This paper presents a new type of human-computer interface called Pico (Physical Intervention in Computational Optimization) based on mechanical constraints that combines some of the tactile feedback and affordances of mechanical systems with the abstract computational power of modern computers. The interface is based on a tabletop interaction surface that can sense and move small objects on top of it. The positions of these physical objects represent and control parameters inside a software application, such as a system for optimizing the configuration of radio towers in a cellular telephone network. The computer autonomously attempts to optimize the network, moving the objects on the table as it changes their corresponding parameters in software. As these objects move, the user can constrain their motion with his or her hands, or many other kinds of physical objects. The interface provides ample opportunities for improvisation by allowing the user to employ a rich variety of everyday physical objects as mechanical constraints. This approach leverages the user's mechanical intuition for how objects respond to physical forces. As well, it allows the user to balance the numerical optimization performed by the computer with other goals that are difficult to quantify. Subjects in an evaluation were more effective at solving a complex spatial layout problem using this system than with either of two alternative interfaces that did not feature actuation.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Head or gaze?: controlling remote camera for hands-busy tasks in teleoperation: a comparison
</title>
	<abstract>Head motion and eye gaze are general models of natural human interaction. Recent computer vision based head tracking and eye tracking technologies have expanded the possibilities of designing and developing more natural and intuitive user interfaces for a wide range of applications. In this work, we focus on common hands-busy situations in teleoperation activities, where operators often have to control multiple devices simultaneously by hand in order to accomplish operational tasks. This overloads an operator's hand control ability and also reduces productivity. We present an empirical user study comparing head motion and eye gaze as different input modalities for remote camera control when a user is carrying out a hands-busy task. Both objective measures and subjective measures were used for the study. According to the results, we demonstrate the advantages of using gaze for remote camera control in such hands-busy settings.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Face as mouse through visual face tracking
</title>
	<abstract>This paper introduces a novel camera mouse driven by visual face tracking based on a 3D model. As the camera becomes standard configuration for personal computers (PCs) and computation speed increases, achieving human-machine interaction through visual face tracking becomes a feasible solution to hands-free control. Human facial movements can be broken down into rigid motions, such as rotation and translation, and non-rigid motions such as opening, closing, and stretching of the mouth. First, we describe our face tracking system which can robustly and accurately retrieve these motion parameters from videos in real time [H. Tao, T. Huang, Explanation-based facial motion tracking using a piecewise Bezier volume deformation model, in: Proceedings of IEEE Computer Vision and Pattern Recogintion, vol. 1, 1999, pp. 611-617]. The retrieved (rigid) motion parameters can be employed to navigate the mouse cursor; the detection of mouth (non-rigid) motions triggers mouse events in the operating system. Three mouse control modes are investigated and their usability is compared. Experiments in the Windows XP environment verify the convenience of our camera mouse in hands-free control. This technology can be an alternative input option for people with hand and speech disability, as well as for futuristic vision-based games and interfaces.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Implementation of interactive home control system
</title>
	<abstract>Trends in terms of future home environments continue to show a tendency toward mechanical interfaces such as keyboards, computer mice, and remote control devices disappearing in the emergence of a ubiquitous network infrastructure, incorporating various sensors as well as the development of the sensing technology. Moreover, home environments continue to move in the direction of better practicality and comfortable domesticity by giving prominence to versatile and realization factors for users.

In this paper, the natural interaction technology that supports the natural interoperability between user and home information appliances is analyzed by taking cognizance of the instinctive motion to abide by user intentions in ubiquitous home environments. In addition, the design of human-centralized natural user interaction is considered based on the analyzed data.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Ubi-touch: designing an interactive home control system
</title>
	<abstract>Trends in terms of future home environments continue to show a tendency toward mechanical interfaces such as keyboards, computer mice, and remote control devices disappearing in the emergence of a ubiquitous network infrastructure, incorporating various sensors as well as the development of the sensing technology. Moreover, home environments continue to move in the direction of better practicality and comfortable domesticity by giving prominence to versatile and realization factors for users.

In this paper, the natural interaction technology that supports the natural interoperability between user and home information appliances is analyzed by taking cognizance of the instinctive motion to abide by user intentions in ubiquitous home environments. In addition, the design of human-centralized natural user interaction is considered based on the analyzed data.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Interactive tools to support animation tasks
</title>
	<abstract>Interactive tools have long been important fundamental components of computer animation systems. Interactive tools in the form of electronic paint systems, cel painting systems, and in-betweening systems are major components of two-dimensional character animation systems. Major interactive components of three-dimensional animation systems include motion design and motion previewing systems as well as geometric modeling systems. Two-dimensional animation systems have as design criteria the application of computer systems and technology to increase productivity and cost effectiveness while at the same time utilizing the skill and talent of non-computer oriented animation production staffs. One of the most effective ways to meet both criteria is through the use of highly interactive system components. One of the original motivations for the development of electronic paint systems was to support the task of creating background paintings for computer animation. Additional applications of paint systems to animation include color selection and matching, storyboard development, and character design. One of the major tasks in two-dimensional animation is the creation of very large numbers of line drawings. These drawings are of the characters, portions of characters, and other objects contributing to the animation action. In conventional systems, these are created by a hierarchy of animation artists using pencil and paper techniques. A relatively few “key” frame drawings are drawn by lead animators with the remainder of the drawings done by junior animators and inbetweeners. In computer based systems considerable economy can be realized by using algorithms to generate the in-between drawings based on key frame drawings created interactively by the lead animators. Once these many drawings are created, they must be colored. In conventional systems this is done by transferring the drawings to acetate sheets which are colored on the reverse surface with specially formulated paints. In computer based systems, interactive workstations based on specially modified paint systems are used. These workstations usually include the capability to automatically fill in similar areas of drawing sequences. These interactive cel painting stations can greatly increase productivity over conventional techniques. These interactive workstations usually use tablets and menus to support artist interaction. The paint and cel painting systems use raster displays driven from frame buffers. The in-betweening systems make use of high performance two-dimensional calligraphic vector displays. In current three-dimensional animation systems, interaction is used in two major task areas; motion design and geometric modeling. Because the rendering processes commonly used in such systems are so computationally intensive there is great motivation for being able to carefully design and preview the various motions before actually rendering the finished frames. As a result, most serious three-dimensional animation systems include interactive workstations specifically designed to allow animators to set-up and preview the animation in real-time. Such systems are built around very high performance display systems which support three-dimensional coordinate system transformation and high speed vector drawing. This high performance is needed to allow the three-dimensional object positions to be updated in real-time at video or film frame rates. The graphic display systems that have been used include the Evans &amp; Sutherland MPS and PS-300 as well as the Silicon Graphics Iris. Examples of such motion design systems include the BBOP [1,2,3] system originally developed by Garland Stern at NYIT and the TWIXT [4] system developed by Julian Gomez at Ohio State. The BBOP system is currently in its third generation. It has been supported on a succession of hardware systems including the E&amp;S PS1, PS2, and MPS systems. Since the E&amp;S MPS systems are no longer being manufactured, the BBOP system has been ported to the PS-300 family and a port to the Trillium 1100 system is expected. These motion control systems are basically “key pose” oriented. The animator can interactively position or pose the animated objects at specified key frames. Software then automatically creates the object positions for all the intermediate frames based on the key poses specified by the animator. The objects or characters being animated typically consist of a hierarchically structured tree of sub-objects. Each node of these structures corresponds to a coordinate system transformation. By manipulating the transformations at each node, the animator can pose the objects or characters as desired. The intermediate frames are created by interpolating the key frame node transformations. In addition to node transformations, the animator can control the camera parameters such as viewing position path and field of view. Interaction with this system is through tablet, joystick, and “joyswitch.” The joyswitch allows the animator to select the current transformation node by moving up, down, right, or left in the tree structure. The three-axis joystick allows interactive control of node rotation, scaling, and translation. The tablet provides an additional interactive input mechanism. In addition, the system allows all actions to be specified from the keyboard if desired. The other major use of interaction in three-dimensional animation is to support the creation of object models. To create the synthetic images and to drive the motion control systems, geometric models of the various objects and sub-objects must be specified. Without interactive modeling systems this can be a very labor intensive and time consuming task. The goal of interactive modeling systems is to support the rapid development of complex object descriptions. The current modeling system in use at NYIT is supported on both E&amp;S MPS and PS-300 systems. The original work was done by Pat Hanrahan [5]. This has since been expanded into a framework for interactive modeling packages. The framework provides an interaction core which provides windowing, menuing, and interactive device handling support. A number of specific modeling functions have been developed which are supported by this framework. This framework makes developing a new modeling function relatively easy. To date, interactive three-dimensional motion specification and geometric modeling systems have utilized high performance vector display systems. This is simply because these have been the only reasonable systems available. High performance, full color, shaded systems have been in existence for some time in the form of real-time visual simulators. But until recently these have all been much too expensive for regular use in animation systems. Recently, however, relatively inexpensive but still high performance full color shaded systems such as the Trillium 1100 have become available. The Trillium system provides real-time performance as good or better than previous vector systems but with the advantage of full color shaded images and a movable light source. Major software development efforts are underway to port motion control and modelling software to this system. In addition to rapid vector generation and/or polygon shading the major hardware feature utilized by current interactive workstations is rapid coordinate transformation hardware. Interactive modeling systems could greatly benefit from rapid hardware to support a much wider range of geometric operations such as the computation of planar equations, surface intersections, spatial set operations, etc. The current hardware systems are well suited for articulated structures of rigid bodies. However, they currently provide little support for the modeling or motion specification of objects with deformable surfaces. The incorporation of real-time interpolation hardware would be a great help in dealing with “organic” structures such as the human form.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Technological Developments in Networking, Education and Automation
</title>
	<abstract>Technological Developments in Networking, Education and Automation includes a set of rigorously reviewed world-class manuscripts addressing and detailing state-of-the-art research projects in the following areas: Computer Networks: Access Technologies, Medium Access Control, Network architectures and Equipment, Optical Networks and Switching, Telecommunication Technology, and Ultra Wideband Communications. Engineering Education and Online Learning: including development of courses and systems for engineering, technical and liberal studies programs; online laboratories; intelligent testing using fuzzy logic; taxonomy of e-courses; and evaluation of online courses. Pedagogy: including benchmarking; group-learning; active learning; teaching of multiple subjects together; ontology; and knowledge management. Instruction Technology: including internet textbooks; virtual reality labs, instructional design, virtual models, pedagogy-oriented markup languages; graphic design possibilities; open source classroom management software; automatic email response systems; tablet-pcs; personalization using web mining technology; intelligent digital chalkboards; virtual room concepts for cooperative scientific work; and network technologies, management, and architecture. Coding and Modulation: Modeling and Simulation, OFDM technology , Space-time Coding, Spread Spectrum and CDMA Systems. Wireless technologies: Bluetooth , Cellular Wireless Networks, Cordless Systems and Wireless Local Loop, HIPERLAN, IEEE 802.11, Mobile Network Layer, Mobile Transport Layer, and Spread Spectrum. Network Security and applications: Authentication Applications, Block Ciphers Design Principles, Block Ciphers Modes of Operation, Electronic Mail Security, Encryption &amp; Message Confidentiality, Firewalls, IP Security, Key Cryptography &amp; Message Authentication, and Web Security. Robotics, Control Systems and Automation: Distributed Control Systems, Automation, Expert Systems, Robotics, Factory Automation, Intelligent Control Systems, Man Machine Interaction, Manufacturing Information System, Motion Control, and Process Automation. Vision Systems: for human action sensing, face recognition, and image processing algorithms for smoothing of high speed motion. Electronics and Power Systems: Actuators, Electro-Mechanical Systems, High Frequency Converters, Industrial Electronics, Motors and Drives, Power Converters, Power Devices and Components, and Power Electronics.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Real-time distributed video tracking of multiple objects from single and multiple cameras
</title>
	<abstract>Multiple object tracking in video has received tremendous attention because of its wide practical applications such as video surveillance, human activity analysis, target identification, and human computer interfaces.

Due to the complexity associated with high dimensionality, occlusions, motion changes and background clutters, robust and efficient video tracking of multiple objects remains a challenging task. Multiple independent single object trackers fail when objects are in close proximity or present occlusions. In such circumstances, modeling the interaction among objects, establishing a correspondence between objects and observations, and decreasing the computational complexity to achieve real-time implementation are critical problems.

In this dissertation, we investigate issues towards solving these problems. Specifically, the thesis comprises five fundamental contributions: The first is a Detection-Based Particle Filter, which extends the particle filter theory and achieves robust performance of single object tracking. Secondly, a distributed Bayesian formulation is proposed for real-time multiple object tracking using a single camera. It avoids the common practice of using a complex joint state space representation and performing tedious joint data association. It extends the conventional Bayesian tracking framework by modeling multiple object interaction in terms of potential functions. The third contribution is a distributed framework using multiple collaborative cameras for multiple object tracking with significant and persistent occlusion. Specifically, we propose to model the camera collaboration likelihood density by using epipolar geometry with sequential Monte Carlo implementation. Fourthly, we have proposed two novel approaches for articulated object tracking. Instead of using a high dimensional joint state representation, we introduce a decentralized scheme and model the inter-part interaction within an innovative framework. Finally, we present a novel video tracking framework using control-based observer design. It unifies several kernel-based approaches into a consistent theoretical framework by modeling tracking as an inverse problem. It relies on observability theory from control systems to handle the "singularity" problem and provides explicit criteria for kernel design and dynamics evaluation.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Wireless Hand Gesture Capture through Wearable Passive Tag Sensing
</title>
	<abstract>For wearable computing to become more widely accepted, the associated Human-Computer Interface must move past today's keyboard, keypad, touch screen, or other bulky hand-held interfaces to allow a user to specify input through their fingers without taking their eyes and attention off their immediate focus. Accordingly, we have developed a wearable system to track hand gestures with passive RFID sensor tags. This system is composed of an ultra-high frequency (UHF) reader and small, passive, finger-worn tags powered by transmit RF energy, each equipped with a variety of sensors that could be used to detect gestures. The primary physical goals of the system were to be comfortable and wearable without interfering with other everyday activities while tracking particular hand movements that could be used to control a wearable computer or aid in interaction with ubiquitous or other wearable devices. This paper first introduces our hardware, then gives some example user interface implementations, such as a mouse scrolled by hand position and a click specified by finger proximity, entering input by touching fingers, setting options when moving the hand to a particular spot of the user's apparel labeled with a passive RFID tag, and otherwise mapping control onto motion of the hand, arm, and fingers. The overall system was fully functional, but as this is an early implementation, it was still very much limited by transmit power and antenna efficiency, due to the constraints on the size of the passive tags. Means of scaling to lower power and smaller size are suggested.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Precise selection techniques for multi-touch screens
</title>
	<abstract>The size of human fingers and the lack of sensing precision can make precise touch screen interactions difficult. We present a set of five techniques, called Dual Finger Selections, which leverage the recent development of multi-touch sensitive displays to help users select very small targets. These techniques facilitate pixel-accurate targeting by adjusting the control-display ratio with a secondary finger while the primary finger controls the movement of the cursor. We also contribute a "clicking" technique, called SimPress, which reduces motion errors during clicking and allows us to simulate a hover state on devices unable to sense proximity. We implemented our techniques on a multi-touch tabletop prototype that offers computer vision-based tracking. In our formal user study, we tested the performance of our three most promising techniques (Stretch, X-Menu, and Slider) against our baseline (Offset), on four target sizes and three input noise levels. All three chosen techniques outperformed the control technique in terms of error rate reduction and were preferred by our participants, with Stretch being the overall performance and preference winner.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Physical world as an internet of things
</title>
	<abstract>A concept of the physical Universe that does not address the issue of the difference in the behavior of dead and living matter is not just incomplete, it simply cannot be correct. We have developed a cellular automaton model of the Universe where the appearing material configurations share the information control under global content-addressable holographic memory. As a result, biological information processing is organized as Cloud Computing [1]. With the rise of the Internet there is no doubts that such an organization is much more efficient; other control arrangements for material things may be simply not workable.

The Internet construction of the physical world is a sort of realization of quantum computing. The viability of this construction is most dramatically revealed by the phenomenon of quantum nonlocality -- instantaneous non-signaling correlation of distant events. Nonlocality is intrinsic to the sliced processing of holography, which brings in instantaneous interactions through common memory rather than performs gradual signaling through message passing. Traditional thinking cannot accommodate nonlocality into the paradigm of the physical world. At the moment, the given construction presents the one and only available operational explanation of this inconceivable phenomenon.

The holographic Internet milieu sets up different control patterns for molecular structures depending on their size. Small particles get immediate holographic feedbacks by returning beam establishing an interactive holography environment for quantum mechanics behavior [2]. The feedbacks for macromolecules ("aperiodic crystals" [3]) are richer as their highly developed conformational oscillations furnish access keys to the holographic storage; so, in contrast to small particles, the behavior of macromolecules is governed additionally by signals from the bulk of the holographic memory. Drastic distinctions in the behavior of dead and living objects are due to different feedbacks for small and large molecules produced by the Internet infrastructure of the material world.

Functioning of complex systems ordinarily requires inflows of two types of entities: information signals and actuation impetuses. The latter aspect in relation to the motility of macromolecules has been considered in [4]. According to [3], the purpose of feeding is not the acquisition of energy but intake of "negative entropy". The essential point in metabolism is freeing from all the entropy that an organism cannot help producing while alive. The primary hypothesis about the acquisition of energy by living organisms is that the inside burning of the sugar in one way or another provides the motive power to the muscle. Yet the amount of energy obtained with the food does not seem enough for the work the organisms perform; for example, some beetles would need daily intake of food twice their own mass. Furthermore, it is not known how exactly the energy-providing reactions are coupled to the mechanical precision and how the control signals arriving at macromolecules are transformed into purposeful actions [5].

The total amount of power required by all the living organisms on Earth can be commensurable (within some orders of magnitude) to the total amount of power used by modern human civilization. In corresponding terms, it can be said that living organisms consume the ultimate source of energy - solar radiation in the form of "biomass". This common view is confronted considering a new source of energy for biochemical motions by relating it to the external clock of the physical Internet. This kind of energy can be extracted from the pushing pulses of this clocking mechanism, the so-called "hot-clocking" effect [6], and concentrated by the mode of the parametric resonance [4]. This kind of surmised powering for the biochemical activities effectively intermingles information and energy processes. Figuratively speaking, the proposed machinery can be seen as USB port functionality incorporated in the quantum computer of the Universe.

For the Internet of the physical world the considered clocking mechanism introduces an unexpected triggering condition at its working frequency of 1011 Hz. Actually, it has been noticed that electromagnetic waves in the corresponding millimeter range produce various harmless, but otherwise unexplainable, biological effects that cannot be understood either in terms of heating or through direct action of electric fields; "it follows that the electromagnetic wave acts as a trigger to events for which the biological system is already prepared"[7]. Since biological objects operate under 1011 Hz clock cycle they might be affected by a novel environmental factor -- gigahertz radiation from the vast spread of cellular phones. Conventional physics does not foresee how this radiation can influence biological objects, while the massive epidemiological studies would take decades [8]. In the meantime, it is important to keep in mind that HF electromagnetic radiation could interfere with biological processes as long as they are driven by 1011 Hz clock of Cloud Computing.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Interaction capture and synthesis of human hands
</title>
	<abstract>This thesis addresses several issues in modelling interaction with human hands in computer graphics and animation. Modifying motion capture to satisfy the constraints of new animation is difficult when contact is involved because physical interaction involves energy or power transfer between the system of interest and the environment, and is a critical problem for computer animation of hands. Although contact force measurements provide a means of monitoring this transfer, motion capture as currently used for creating animation has largely ignored contact forces. We present a system of capturing synchronized motion and contact forces, called interaction capture. We transform interactions such as grasping into joint compliances and a nominal reference trajectory in an approach inspired by the equilibrium point hypothesis of human motor control. New interactions are synthesized through simulation of a quasi-static compliant articulated model in a dynamic environment that includes friction. This uses a novel position-based linear complementarity problem formulation that includes friction, breaking contact, and coupled compliance between contacts at different fingers. We present methods for reliable interaction capture, addressing calibration, force estimation, and synchronization. Additionally, although joint compliances are traditionally estimated with perturbation-based methods, we introduce a technique that instead produces estimates without perturbation. We validate our results with data from previous work and our own perturbation-based estimates. A complementary goal of this work is hand-based interaction in virtual environments. We present techniques for whole-hand interaction using the Tango, a novel sensor that performs interaction capture by measuring pressure images and accelerations. We approximate grasp hand-shapes from previously observed data through rotationally invariant comparison of pressure measurements. We also introduce methods involving heuristics and thresholds that make reliable drift-free navigation possible with the Tango. Lastly, rendering the skin deformations of articulated characters is a fundamental problem for computer animation of hands. We present a deformation model, called EigenSkin, which provides a means of rendering physically- or example-based deformation models at interactive rates on graphics hardware.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Virtual cooperating manipulators as a virtual reality haptic interface
</title>
	<abstract>One central element in the focus on research into human-machine interfaces is the capability to interact physically with the computer model. The sense of touch and feel is vital for realistic manipulation and control of virtual objects. The research described here is the development and implementation of a new dynamic control strategy using a standard six-degree-of-freedom robot manipulator as a force interface to virtual reality systems. The haptic element of VR interfacing is currently the subject of abundant research, some addressing the stability and control of interactive systems but with much of the focus on the development of new hardware systems to support stable interaction between humans and VR graphics displays. However, general six-degree-of-freedom manipulators are well understood today and are known to have the capability for generating the general force and motion constraints necessary for the design interaction described in the story. This approach to haptic feedback capability is based on the concept of describing a virtual manipulator model that mimics the motion constraints imposed by a virtual surface. This virtual manipulator is conceptually linked to an actual manipulator to form a closed kinematic chain system. The closed chain system equations are used to define a set of constraints that control the actual robot manipulator so as to allow motion only in the free directions of the virtual manipulator. These free directions are also the free motion directions allowed by the virtual surfaces one tremendous advantage of the approach is that the control algorithm is formulated using local error feedback schemes at the robot level providing effective, stable, and simple control of the robotic hardware. Using the proposed control scheme will allow any six-degree-of-freedom manipulator to be used as a haptic interface device.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Realistic Character Motion Response in Computer Fighting Game
</title>
	<abstract>We present a hybrid motion response model for presenting dynamic character behavior during character interactions in computer games. The method seamlessly integrates motion captured data with physics-based character simulation. The result preserves the desired character motion styles and allows physically realistic responsive behaviors for lifelike character presentation. Based on studies in human balance control and responsive behavior, we designed a mechanism to generate character balancing behaviors for a common computer fighting game scenario. The results demonstrate the capabilities of such mechanism as a set of building blocks for presenting more complex responsive behaviors in computer games. Keywords: computer game, animation, motion synthesis, motion response, physics-based simulation, motion balance, motion dynamics.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Dynamics and control of collision of multi-link humanoid robots with a rigid or elastic object
</title>
	<abstract>The main objective of this dissertation is to understand how the humanoid organisms or machines use appropriate control strategies and reference motions to achieve the desirable collision responses such as the contact time, the contact forces, the departure velocities of two colliding subjects and the physical deformation. Thereby, the collision of a subject with a rigid object such as a flat ground or a soft object such as a soccer ball is modeled and studied. Collision is a challenging task and involves coupling, motion planning, dynamics and controls.

In particular, we are interested in the collision of a humanoid organism with a ground in the step stance. This collision involves the interaction between the two rigid objects. The equations of motion of a five-link three-dimensional subject embedded in a space larger than the joint space of the system are developed by sequential elimination, orthogonality of the spaces and virtual work mechanism, and then are projected onto the sagittal plane. The model is further projected onto the joint space where we obtain a tenth-order under-actuated system with the maximal number of three outputs that can be regulated. To avoid tackling such a system with complicated internal dynamics, finally, we project the joint space model onto the subspace whose dimension is equal to the number of degrees of freedom of the system.

Collision time between two objects can be as short as several milliseconds. Fast tracking convergence and small steady state tracking error are required in numerical simulation. For the step stance leap, an integral sliding mode control strategy is designed to track the reference motion, obtained by experimental recording of humans executing the step stance leap. It eliminates the reaching phase for the sliding mode. The stability, finite-time convergence and robustness of the systems are studied, proved and verified by computer simulation. The sliding mode control algorithm is developed to track the preplanned trajectory against modeling uncertainties and impact disturbances for the ball-foot interaction.

The experiments are emphasized in this research. For the step stance leap, the predicted GRF profiles are in agreement with experimental recording of the GRFs. In particular, the predictions capture the short duration and large amplitudes of the GRFs upon impact as well as the burst of high energy required during the take-off phase. For the ball-foot collision, the collision duration, the ball's departure velocity, the average ball-foot collision force and the ball's peak deformation obtained by simulation match the reported results. The observed three phases of the ball-foot collision are confirmed by our simulation. The "follow-through" phenomenon in sports is also demonstrated.</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>

  <item>
    <title>Face as Mouse Through Visual Face Tracking
</title>
	<abstract>This paper introduces a novel camera mouse driven by 3D model based visual face tracking technique. While camera becomes standard configuration for personal computer(PC) and computer speed becomes faster and faster, achieving human machine interaction through visual face tracking becomes a feasible solution to hand-free control. The human facial movement can be decomposed into rigid movement, e.g. rotation and translation, and non-rigid movement, such as the open/close of mouth, eyes, and facial expressions, etc. We introduce our visual face tracking system that can robustly and accurately retrieve these motion parameters from video at real-time. After calibration, the retrieved head orientation and translation can be employed to navigate the mouse cursor, and the detection of mouth movement can be utilized to trigger mouse events. 3 mouse control modes are investigated and compared. Experiments in Windows XP environment verifies the convenience of navigation and operations using our face mouse. This technique can be an alternative input device for people with hand and speech disability and for futuristic vision-based game and interface.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Role determination in human-human interaction
</title>
	<abstract>Physical human-robot interaction can be significantly improved when being aware about the role each partner takes in a joint manipulation task. This holds especially in computer assisted teleoperation, where depending on the identified role of the human, different roles can be assigned to the assistance function. This work proposes a method for the determination of two non exclusive roles, an executor and conductor. Hereby the executor refers to the person who mainly contributes to the execution of the task and the conductor to the person who takes the decision and controls the motion. The method is based on the analysis of haptic data, whereby the signs of force and motion signals are evaluated. Two different cases are considered in the analysis: direct interaction of the two partners as well as interaction via an object. The effort each operator makes to either carry out the task or initiate a change in the task is also estimated. The proposed method can be used to actively influence the role assignment, to extract features or for task segmentation.
</abstract>
	<search_task_number>1</search_task_number>
	<query>motion control human computer interaction</query>
	<relevance>0</relevance>
  </item>




<item>
<title>Adaptive Approach for Reducing Energy Consumption, Considering Nodes' Mission in Wireless Sensor Networks</title>
<abstract>The computational cost in a wireless sensor network is usually considered neglected in comparison to the communication cost. While this assumption is considered true, it may fail if data volumes are significantly large. In such situations, it is more rational to consider the computational cost as well as the communication cost for energy related decisions in the network. In this paper, we want to reduce the energy consumed within a wireless sensor network, considering each node in charge of a mission. Every node, obtaining a trade-off between computation and communication energy, does its best to reduce the overall energy. In other words, each node endeavors to reduce network energy consumption adaptively, regarding application type, by means of making a trade-off between data compression at an appropriate level, considering spatial correlation, and sending row data. Simulation results, compared to similar work, show that our approach reduces energy consumption of nodes in the network.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Reducing Energy Consumption through the Union of Disjoint Set Forests Algorithm in Sensor Networks</title>
<abstract>Recently, wireless sensor networks have improved for many applications aimed at collecting information. However wireless sensor networks have many challenges to be solved. One of the most critical problems is the energy restriction. Therefore in order to extend the lifetime of sensor nodes, we need to minimize the amount of energy consumption. In many cases, sensor networks use routing schemes based on the tree routing structure. But when we collect information from a restricted area within the sensor field using the tree routing structure, the information is often assembled by sensor nodes located on different tree branches. In this case unnecessary energy consumption happens in ancestor nodes located out of the target area. In this paper, we propose the Sensor Network Subtree Merge algorithm, called SNSM, which uses the union of disjoint set forest algorithm for preventing unnecessary energy consumption in ancestor nodes for routing. SNSM algorithm has 3-phases: first finding the disjoint set of the subtree in the sensor field; second connecting each disjoint subtree with the closest node; and third virtually disconnect the subtree connected to new tree branch from previous tree structure. In the simulation, we apply SNSM algorithm to a minimum spanning tree structure. Simulation results show that SNSM algorithm reduces the energy consumption. Especially, SNSM is more efficient as number of sensor nodes in a sensor field increases.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Balancing energy efficiency and quality of aggregate data in sensor networks</title>
<abstract>In-network aggregation has been proposed as one method for reducing energy consumption in sensor networks. In this paper, we explore two ideas related to further reducing energy consumption in the context of in-network aggregation. The first is by <i>influencing the construction of the routing trees for sensor networks</i> with the goal of reducing the size of transmitted data. To this end, we propose a <i>group-aware network configuration</i> method that “clusters” along the same path sensor nodes that belong to the same group. The second idea involves imposing a <i>hierarchy of output filters</i> on the sensor network with the goal of both reducing the size of transmitted data and minimizing the number of transmitted messages. More specifically, we propose a framework to use <i>temporal coherency tolerances</i> in conjunction with in-network aggregation to save energy at the sensor nodes while maintaining specified quality of data. These tolerances are based on user preferences or can be dictated by the network in cases where the network cannot support the current tolerance level. Our framework, called TiNA, works on top of existing in-network aggregation schemes. We evaluate experimentally our proposed schemes in the context of existing in-network aggregation schemes. We present experimental results measuring energy consumption, response time, and quality of data for Group-By queries. Overall, our schemes provide significant energy savings with respect to communication and a negligible drop in quality of data.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Cell based energy density aware routing: a new protocol for improving the lifetime of wireless sensor networks</title>
<abstract>Wireless sensor networks have unique features not shared by mobile ad-hoc networks. Taking these features into consideration, we propose a new routing protocol specifically designed for wireless sensor networks. This protocol, referred to as cell based energy density aware routing (CEDA), divides a sensor field into uniform cells, thereby reducing energy consumption caused by sensor data flooding. Energy density, a novel routing metric, is used to avoid forwarding packets to subareas whose nodes have lower residual energies. By ensuring fair energy consumption of sensor nodes, CEDA makes it possible for monitoring stations to monitor all subareas for longer periods of time. Simulations were carried out to compare the performance of CEDA with those of several existing protocols for wireless networks or sensor networks. The time required for a subarea to run out of energy, called the lifetime of that area, is measured in the simulations. The simulation results show that CEDA gives a longer lifetime than the existing routing protocols. In addition, it is proved that CEDA guarantees the maximum hop count regardless of the node density, and hence does not suffer from unpredictable delays.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>A scalable and energy-efficient hybrid-based MAC protocol for wireless sensor networks</title>
<abstract>The problem of MAC protocol design for wireless sensor networks poses many challenges such as energy constraint resources, strict wireless bandwidth, channel utilization &amp; etc. These challenges make handling changes in network topology and network scalability a major issue. In this paper we present a new scalable and energy efficient hybrid-based MAC protocol for wireless sensor networks, abbreviated as SEHM. The protocol saves energy by reducing energy consumption due to idle listening and signal interference. Idle listening is reduced by making idle nodes and all nodes that have no data to send to switch early to sleep state. Signal interference is limited through using time division based medium access. Scalability of our approach is achieved through dividing the sensor network into clusters. Clusters are dynamically formed as all nodes in the sensor network are allowed to content for the position of a cluster head, to finally elect suitable cluster heads. The performance of our protocol is studied and analyzed by means of computer simulations, and we show that our approach outperforms S-MAC protocol in terms of energy consumption and packet delivery rate.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>A two-hop clustered image transmission scheme for maximizing network lifetime in wireless multimedia sensor networks</title>
<abstract>In traditional wireless sensor networks, normal sensor nodes which measure scalar physical phenomena like temperature, pressure and humidity usually compress the data before sending them out to minimize the communication energy consumption. However, this strategy may not be suitable for image transmission in wireless multimedia sensor networks. In the traditional clustering structure, when the camera-equipped node or the cluster head compresses the images, an energy hole will appear. This is a key factor that affects the lifetime of the network. To avoid the energy hole problem, a two-hop clustered image transmission scheme is proposed in this paper. In the proposed scheme, many redirectors are used to compress and forward the images for the purpose of reducing energy consumption of the camera-equipped node and the cluster head. With adaptive adjustment of the transmission radius in the camera cluster and tasks allocation based on the residual energy of the normal sensor nodes by the camera-equipped node, the energy consumption of the nodes in the network is balanced. The experimental results show that the proposed scheme can prolong the network lifetime dramatically in the case of the sensor nodes deployed densely.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>An air monitoring system to measure environmental factors in a wireless sensor network</title>
<abstract>In this paper a form of sensor network is operated to monitor air-contamination in subway stations. The system covers both field-based monitoring and infrastructure monitoring. For some sort of sensor componets consumes energy excessively, the recent sleep scheduling approach is employed to save energy of the nodes. The sensor network server congregates data gathered on schedule, and provide environmental information. Reducing energy consumption rate is the practical concern in developing the monitoring system.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Load sensitive topology control: Towards minimum energy consumption in dense ad hoc sensor networks</title>
<abstract>Sensor networks are usually composed of tiny and resource constraint devices, which make energy conservation a vital concern of their design and deployment. Reducing energy consumption has been addressed through different aspects till now. Topology Control (TC) is a well-known approach which tries to determine transmission ranges of nodes to optimize their energy utilization while keeping some network properties like connectivity. However, in current TC schemes, the transmission range of each node is mostly accounted as the exclusive estimator for its energy consumption while ignoring the amount of data it sends or relays. In this paper, we deliberately reformulate the problem of topology control, regarding both network load and transmission range parameters. Our approach is particularly formulated for dense sensor networks with one or more base stations. The problem is considered in three different environmental conditions and then, proper mathematical relations are presented to find the optimum solutions. Finally, we show the advantages of our proposal through experiments.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Reliable data delivery in wireless sensor networks: an energy-efficient, cluster-based approach</title>
<abstract>Reliable data delivery in wireless sensor networks is a challenging issue due to the error prone nature of wireless communication and the frangibility of little sensors. However, readings of geographically proximate sensors are strongly correlated, so it's not only unnecessary but also wastes energy to deliver data in all sensors reliably. Sensor nodes could be clustered and only the aggregated data need to be reliably delivered. In this paper, we propose a new reliable, energy-efficient clustering protocol CREED. CREED takes advantage of the multi-rate capabilities of 802.11 a, b, g technologies by reducing energy consumption per bit for shorter transmission range.A dynamic backup scheme EDDS based on nodes' residual energy is presented to enhance the fault tolerance of CREED. EDDS supports quick cluster head selection and failure recovery while only need exchange a few messages. CREED also includes an energy aware multipath inter-cluster routing to guarantee reliable data delivery. Simulation results verify the performance of CREED.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>A MAC/Routing cross-layer approach to geographic forwarding in wireless sensor networks</title>
<abstract>Geographic forwarding is an emerging paradigm for communications between nodes in sensor networks. No exchange of location information is required, and nodes only have to know their own coordinates and those of the destination. Due to the device's limited processing and storage capabilities, a simplified protocol architecture should be designed so as to make communications in these networks efficient and simple at the same time. Moreover, sensor nodes are battery supplied and, thus, protocol design should be aimed at reducing energy consumption in order to increase network lifetime. In this perspective, one sensor feature recently regarded as of key importance, is the ability to tune the transmission power. This allows the communication range to be varied according to node density and connectivity constraints. In this paper we propose an integrated cross-layer protocol, called MACRO, which integrates MAC and routing layer functionalities in order to support geographic forwarding in wireless sensor networks. In MACRO, a competition is triggered to select the best next relay node while forwarding information to the destination. The competition is based on the evaluation of a weighted progress factor representing the progress towards the destination per unit of transmission power. An analytical paradigm facilitating the most appropriate choice of the next relay is proposed. The proposed solution is assessed through both analysis and ns-2 simulations. Performance results show the advantages of the proposed solution when compared to other geographic forwarding protocols which do not exploit cross-layer features.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Wireless sensor networks to enable the passive house - deployment experiences</title>
<abstract>Finding solutions for the current period of climate change or "global warming" is possibly the most serious and pressing challenge faced by scientists and the wider community today. Although governments are beginning to act, a community wide approach is needed with a large proportion of individuals engaging to reduce energy consumption that depends on fossil fuels. The Passive House (or Passivhaus) standard is an ultra-low energy standard for building construction and design that aims at dramatically reducing energy consumption in the home. While appropriate for new builds, this standard may be difficult to achieve with existing buildings. In this work, Wireless Sensor Network (WSN) technology is examined as an enabling tool to support rapid progression to improved energy efficiency and increased comfort for existing buildings. As with participatory urban sensing, the home occupant could, in the future, take on the role of scientist; developing an awareness of trouble spots in the house would allow them to target problems thus reducing the need for heating and improving comfort. The paper reports on experiences and findings from several residential and commercial environmental monitoring WSN deployments using a WSN developed from off the shelf components. The sensors deployed measure temperature, relative humidity, CO2 concentration and light. Depending on the size and layout of the space to be monitored, added to the scope of deployment, between 12 and 20 nodes were deployed and the monitoring period was 7-14 days per location. The paper illustrates the value of using WSN technologies as enablers for the amateur eco-home scientist on the path towards reduced energy consumption and increased comfort. It evaluates the suitability of the system for both commercial and residential deployments and shows how large quantities of data can be reduced to meaningful high level information delivered to the user.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>1</relevance>
</item>

<item>
<title>Topology control in wireless ad hoc and sensor networks</title>
<abstract>Topology Control (TC) is one of the most important techniques used in wireless ad hoc and sensor networks to reduce energy consumption (which is essential to extend the network operational time) and radio interference (with a positive effect on the network traffic carrying capacity). The goal of this technique is to control the topology of the graph representing the communication links between network nodes with the purpose of maintaining some global graph property (e.g., connectivity), while reducing energy consumption and/or interference that are strictly related to the nodes' transmitting range. In this article, we state several problems related to topology control in wireless ad hoc and sensor networks, and we survey state-of-the-art solutions which have been proposed to tackle them. We also outline several directions for further research which we hope will motivate researchers to undertake additional studies in this field.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Variable-base tacit communication: a new energy efficient communication scheme for sensor networks</title>
<abstract>Energy conservation is a major concern in wireless sensor networking. Conventionally in wireless communications, each bit transmitted by a node consumes one unit of energy. Some recent advances, however, explore silent time intervals between signal transmissions to convey information (Zhu and Sivakumar [1]). Such a scheme of Communication through Silence (CtS), while reducing energy consumption for sensor nodes, introduces long delay. In this paper, we propose Variable-Base Tacit Communication (VarBaTaC) to mitigate the delay introduced by CtS. We also develop three MAC protocols based on VarBaTaC for different environments. We then outline experiment designs for further investigations and point out some interesting future research directions.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Research on clustering strategy for wireless sensor network based on fuzzy theory</title>
<abstract>In wireless sensor network applications, effective clustering algorithm can reduce energy consumption, which can increase network scalability and lifetime. In most of traditional clustering methods, clusters do not form until their cluster heads are selected and these algorithms are usually running under the single data transmission mode. A clustering strategy for WSNs based on Fuzzy Cluster Mean (FCM) is proposed in this paper. It is a new clustering mechanism of generating the clusters first and then selecting the cluster head(CH). This strategy based on FCM has good characteristics of clustering quick, reducing energy consumption and being applied in different data transmission modes. The correctness and feasibility is validated in simulations. It is shown that energy consumption is better than the similar clustering algorithms. When different selection modes of CH are selected in this strategy, the clustering strategy based on FCM shows the best efficiency in two data transmission modes.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>A key management method based on an AVL tree and ECC cryptography for wireless sensor networks</title>
<abstract>In this paper; we propose a security protocol for sensor networks that provides good protection while taking into account the limited resources of the sensors. The protocol is based on an effective key management method with a minimum storage of keys. Our approach is based on the combination and improvement of two methods already proposed by the research community: cryptography based on elliptic curves and key management based on an AVL tree. Compared with RECC "A Routing-Driven Elliptic Curve Cryptography Based Key Management Scheme for Heterogeneous Sensor Networks" and CECKM "High-Effect Key Management Associated With Secure Data Transmission Approaches in Sensor Networks Using a Hierarchical-based Cluster Elliptic Curve Key Agreement" two methods based on Diffie-Hellman elliptic curve cryptography method, our approach provides a positive impact on reducing energy consumption and memory storage. It saves significant time and memory and it reduces the exchanged packets during keys installation with fewer processing operations.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>The design of a bloom filter hardware accelerator for ultra low power systems</title>
<abstract>Battery-powered embedded systems require low energy usage to extend system lifetime. These systems must power many components for long periods of time and are particularly sensitive to energy use. Recent techniques for reducing energy consumption in wireless sensor networks, such as aggregation, require additional computation to reduce energy intensive radio transmissions. Larger demands on the processor will require more computational energy, but traditional energy reduction approaches, such as multi-core scaling with reduced frequency and voltage may prove heavy handed and ineffective for motes (sensor network nodes). Instead, application-specific hardware design (ASHD) architectures can reduce computational energy consumption by processing operations common to specific applications more efficiently than a general purpose processor. By the nature of their deeply embedded operation, motes support a limited set of applications, and thus the conventional general purpose computing paradigm may not be well-suited to mote operation. This paper examines the design considerations of a hardware accelerator for compressed Bloom filters, a data structure for efficiently storing set membership. We evaluate our ASHD design for three representative wireless sensor network applications and demonstrate that ASHD design reduces network latency by 59% and computational energy by 98%, showing the need for architecting processors for ASHD accelerators.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Compressed sensing for efficient random routing in multi-hop wireless sensor networks</title>
<abstract>Compressed sensing (CS), as a novel theory based on the fact that certain signals can be recovered from a relatively small number of non-adaptive linear projections, is attracting ever-increasing interests in the areas of wireless sensor networks. However, the applications of traditional CS in such settings are limited by the huge transport cost caused by dense measurement. To solve this problem, we propose several ameliorated random routing methods executed with sparse measurement based CS for efficient data gathering corresponding to different networking topologies in typical wireless sensor networking environment, and analyse the relevant performances comparing with those of the existing data gathering schemes, obtaining the conclusion that the proposed schemes are effective in signal reconstruction and efficient in reducing energy consumption cost by routing. Our proposed schemes are also available in heterogeneous networks, for the data to be dealt with in CS are not necessarily homogeneous.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Poster: INPRESS: indoor climate prediction and evaluation system for energy efficiency using sensor networks</title>
<abstract>Modern buildings include an indoor climate control system, installed and operated to maintain a comfortable environment for the building occupants. However, these climate control systems consume a significant amount of energy due to an inefficient control algorithm. Improving energy efficiency is critical to reducing energy consumption, costs, and the emissions of greenhouse gases. In this paper, we present INPRESS: Indoor Climate Prediction and Evaluation System for Energy Efficiency using Sensor Networks. INPRESS uses meteorological weather data and the indoor climate conditions collected by sensor nodes to evaluate and improve the energy efficient climate control of an indoor space.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>1</relevance>
</item>

<item>
<title>Link layer behavior of body area networks at 2.4 GHz</title>
<abstract>Body Area Networks (BANs) can perform the task of continuous, remote monitoring of a patient's physiological signals in diverse environments. Apart from providing healthcare professionals with extensive logs of a patient's physiological history, BANs can be used to identify and react to emergency situations. We identify three important factors that afflict wireless communication in BANs: impermeability of the human body to radio waves at frequencies commonly used in BANs, efficient operation in mobile and time-varying environments, and mission-critical requirements for quick response to emergencies. An understanding of the link layer behavior of wireless sensor nodes placed on the body is crucial to address these and other challenges such as reducing energy consumption and increasing network lifetime.
In this paper, we investigate link layer behavior by placing nodes on the body and directly measuring metrics of interest to engineers such as packet delivery ratio (PDR) and RSSI. Emulating a possible real-life BAN operating at the 2.4 GHz band with 12 sensor nodes, we collect over 80 hours of data from 14 volunteers in $3$ different environments that BANs are expected to operate in. We analyze the data to reveal several link layer characteristics to provide insight and guidelines for the designing of BANs. We also evaluate the performance of common routing metrics on our data.
Our analysis helps us make the following conclusions. Link PDR is highly affected by the environment and not significantly by the volunteer for the experiment. Routing between nodes on the same side of the body is preferred to routing between nodes on the opposite sides. For links with the same source, failure of packet transmission to a certain node, in some cases, implies the increased probability of reception for other nodes. Most errors occur in bursts of length 1, but a small fraction occur in longer periods ($40$ packets or more).</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Energy comparison and optimization of wireless body-area network technologies</title>
<abstract>Wireless body-area networks (WBANs) have revolutionized the way mobile and wearable computers communicate with their users and I/O devices. We investigate an energy-efficient wireless device driver for low-duty peripherals, sensors and other I/O devices employed in a WBAN to communicate with a more powerful central device. We present an extensive comparative study of two popular WBAN technologies, 802.15.1 (Bluetooth) and 802.15.4 (ZigBee), in terms of design cost, performance, and energy efficiency. We discuss the impact of tunable parameters of the wireless device driver on connection latency and energy consumption for both Bluetooth and ZigBee. We address dynamic resource management in higher-level protocols by investigating the trade-off between connection latency and energy consumption. We propose an energy-efficient power-down policy that utilizes the interval between consecutive connection requests for energy reduction; we study an adaptive connection latency management technique that adjusts various tunable parameters dynamically to achieve minimum connection latency without changing the energy consumption level. Our measurements and experimental results show that these techniques are very effective in reducing energy consumption while meeting connection latency requirements.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Medium access control protocols and routing algorithms for wireless sensor networks</title>
<abstract>In recent years, the development of a large variety of mobile computing devices has led to wide scale deployment and use of wireless ad hoc and sensor networks. Wireless Sensor Networks consist of battery powered, tiny and cheap “motes”, having sensing and wireless communication capabilities. Although wireless motes have limited battery power, communication and computation capabilities, the range of their application is vast.
In the first part of the dissertation, we have addressed the specific application of Biomedical Sensor Networks. To solve the problem of data routing in these networks, we have proposed the Adaptive Least Temperature Routing (ALTR) algorithm that reduces the average temperature rise of the nodes in the in-vivo network while routing data efficiently. For delay sensitive biomedical applications, we proposed the Hotspot Preventing Routing (HPR) algorithm which avoids the formation of hotspots (regions having very high temperature) in the network. HPR forwards the packets using the shortest path, bypassing the regions of high temperature and thus significantly reduces the average packet delivery delay, making it suitable for real-time applications of in-vivo networks. We also proposed another routing algorithm suitable for being used in a network of id-less biomedical sensor nodes, namely Routing Algorithm for networks of homogeneous and Id-less biomedical sensor Nodes (RAIN ). Finally we developed Biocomm, a cross-layer MAC and Routing protocol co-design for Biomedical Sensor Networks, which optimizes the overall performance of an in-vivo network through cross-layer interactions. We performed extensive simulations to show that the proposed Biocomm protocol performs much better than the other existing MAC and Routing protocols in terms of preventing the formation of hotspots, reducing energy consumption of nodes and preventing network congestion when used in an in-vivo network.
In the second part of the dissertation, we have addressed the problems of habitat-monitoring sensor networks, broadcast algorithms for sensor networks and the congestion problem in sensor networks as well as one non-sensor network application, namely, on-chip communication networks. Specifically, we have proposed a variation of HPR algorithm, called Hotspot Preventing Adaptive Routing (HPAR) algorithm, for efficient data routing in Networks On-Chip catering to their specific hotspot prevention issues. A protocol similar to ALTR has been shown to perform well in a sensor network deployed for habitat monitoring. We developed a reliable, low overhead broadcast algorithm for sensor networks namely Topology Adaptive Gossip (TAG) algorithm. To reduce the congestion problem in Wireless Sensor Networks, we proposed a tunable cross-layer Congestion Reducing Medium Access Control (CRMAC) protocol that utilizes buffer status information from the Network layer to give prioritized medium access to congested nodes in the MAC layer and thus preventing congestion and packet drops. CRMAC can also be easily tuned to satisfy different application-specific performance requirements. With the help of extensive simulation results we have shown how CRMAC can be adapted to perform well in different applications of Sensor Network like Emergency Situation that requires a high network throughput and low packet delivery latency or Long-term Monitoring application requiring energy conservation.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Energy-Efficient Sensor Data Acquisition Based on Periodic Patterns</title>
<abstract>Wireless sensor networks have received considerable attention in recent years and played an important role in data collection applications. Sensor nodes usually have limited supply of energy. Therefore, a major consideration for developing sensor network applications is to conserve the energy for sensor nodes. In this paper, we propose a novel energy-efficient data acquisition algorithm based on the periodic patterns derived from past sensor readings. Our key observation is that sensor readings often exhibit periodic patterns, e.g., the daily cycle of temperature readings, and the patterns provide opportunities for reducing energy consumption for sensor data acquisition. We exploit the patterns and use the patterns to build a statistic model for predicting sensor readings. In our approach, sensor data acquisition is needed only when acquired readings are unpredictable. Therefore the energy for sensor data acquisition and the associated radio communications can be conserved. The experiments performed with real data validate the effectiveness and efficiency of our approach.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Sensor networks with secure public-key over GF (2m)</title>
<abstract>In Observing and interacting with the real physical world sensor networks now take new opportunities for play the roles in almost everywhere. They are composed of a large number of sensor nodes and each sensor node has capabilities of sensing, processing, and communication, which underpinned by limited energy. Reducing energy consumption was, is and will be a major concern in sensor networks. For the given extremely limited hardware resources on sensor nodes and the inclement deploying environment, the adversary attack becomes a serious security threat toward wireless sensor networks. In this paper a secure public-key based effective and efficient sensor network is to be introduced. A saving time, reducing computing energy, algorithm is carefully investigated. The powerful elliptic curve cryptography (ECC) over GF (2m) is also investigated with hidden generator point.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Extended Concentric-Clustering Routing Scheme Adapted to Various Environments of Sensor Networks</title>
<abstract>Cluster-based routing schemes are able to prolong the network lifetime and save the energy consumption in various environments of sensor networks. On the other hand, the chain-based routing protocols construct the chain such as PEGASIS and Concentric-Clustering Routing Scheme instead of a cluster formation. The concentric-clustering scheme is hybrid mechanism that combines chain-based scheme with clustering scheme. It divides sensor networks into some levels (like cluster),and one chain is assigned per level. However, main problem of this scheme is that the number of sensor nodes in the each level is unbalanced. This means that the unbalanced energy consumption among sensor nodes is occurred. Also, as this scheme selects the next cluster head to be closely located at current cluster head, energy of cluster head is dissipated quickly on the multipath among cluster heads. To overcome these problems, we propose the Extended Concentric-Clustering Routing Scheme Adapted to Various Environments of Sensor Networks. Our scheme fixes the number of sensor node on level for the same number of sensor nodes in each level. In addition, we consider the residual energy as the criterion of cluster head election for balanced energy consumption as well as the distance from lower cluster head or base station to upper cluster head. According to the simulation results, our scheme shows a better performance of energy consumption than other multiple-chaining schemes about 23%. Thus, our proposed scheme takes advantages of reducing energy consumption and prolonging the network lifetime.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Localized Routing Protocols Based on Minimum Balanced Tree in Wireless Sensor Networks</title>
<abstract>Reducing energy consumption and prolonging lifetime of network to reduce the amount of packet loss are important issues in wireless sensor networks. Many researches derive the minimum hop path for each sensor to transmit its corresponding data to the sink. The sensors in the path forward the data. However, some common sensors in many forwarding paths will consume much more energy, and then they will die soon. Besides, the establishment and maintenance of the above routing need the whole information of the network, and this will consume more energy in gathering and synchronizing the locations of all sensors. In this paper, each sensor using the information of neighboring sensors derives the minimum hop path to the sink, and with the knowledge of the loading of its up-streaming sensors, it selects the minimum loaded sensor for its first sensor to transmit to. In this way, the loading of each sensor will be balanced. The above routing derives minimum balanced tree (MBT). This data structure will be adjusted locally while some sensors change their statuses in the network such that the control overhead needed to adjust is much less than to reconstruct all over again. Some results of simulated experimentations are shown in this paper.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Implementation of Energy-Efficient Node Management in Wireless Sensor Networks</title>
<abstract>In sensor network-related researches, communication has been the most important target for reducing energy consumption. The frequencies in communication and sensing in a sensor node affect its energy efficiency. Therefore, the sensing cycle and the transmission cycle in a sensor node need to be managed to increase energy efficiency in communication. In this paper we present mechanisms to manage sensor nodes within a sensor network energy efficiently and describe their implementation.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Honeycomb-Based Data Aggregation for Range Query in WSNs</title>
<abstract>Sensor networks are composed of a great number of sensor nodes. Sensor nodes are typically energy restricted and hard to recharge, so reducing energy consumption has recently been a hot focus on wireless sensor network researches. One common approach is to make use of a logical structure, such as a grid, to construct an efficient routing topology for shorten the routing paths. This paper proposed a new approach to improve previous works. By using the honeycomb cell structure, a novel data aggregation method is designed for range query in wireless sensor networks. Due to the intrinsic characteristics, honeycomb structure enables shorter the data transmission path length and reduces the in-transit packet collision than that of the traditional rectangular grid. Also, by suggesting an aggregation tree, we show that the spreading events can be effectively overseen in the data aggregation scenario.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>A New Optimization Approach for Energy Consumption within Wireless Sensor Networks</title>
<abstract>The use of sensor networks should keep developing, mainly in such fields as scientific, logistic, military or health-care applications. However, sensor size represents a significant limitation mainly in terms of energy autonomy and therefore of life period, for the batteries have to be too tiny. This is the reason why intensive research is being conducted nowadays on how to control sensor energy consumption within a network, taking communications into account as a priority. For this purpose we have proposed a method to calculate energy consumption within the wireless sensor networks, according to the number of information packets sent, the number of nodes. Furthermore, we have succeeded in reducing energy consumption within the sensor networks made up with nodes featuring differing data flow rates.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Biocomm-a cross-layer medium access control (MAC) and routing protocol co-design for biomedical sensor networks</title>
<abstract>One of the most novel applications of wireless sensor networks in recent years has been in the field of biomedical research. Biomedical sensor networks are formed by tiny wireless sensor nodes, embedded inside the body. The communication protocol used in such networks must prevent the formation of hotspots in the network and at the same time route data efficiently, while conserving energy. In this paper we propose a cross-layer medium access control (MAC) protocol and routing protocol co-design for biomedical sensor networks. The cross-layer interactions among the network and MAC layers help optimise the overall performance of the in vivo network. Extensive simulations have been done to show that the proposed Biocomm protocol performs much better than the other existing MAC and routing protocols in terms of preventing the formation of hotspots, reducing energy consumption of nodes and preventing network congestion when used in an in vivo network. A variation of Biocomm, Biocomm-D has been proposed for delay-sensitive biomedical sensor network applications.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Towards real-time wireless sensor networks</title>
<abstract>Wireless sensor networks are poised to change the way computer systems interact with the physical world. We plan on entrusting sensor systems to collect medical data from patients, monitor the safety of our infrastructure, and control manufacturing processes in our factories. To date, the focus of the sensor network community has been on developing best-effort services. This approach is insufficient for many applications since it does not enable developers to determine if a system's requirements in terms of communication latency, bandwidth utilization, reliability, or energy consumption are met. The focus of this thesis is to develop real-time network support for such critical applications.
The first part of the thesis focuses on developing a power management solution for the radio subsystem which addresses both the problem of idle-listening and power control. In contrast to traditional power management solutions which focus solely on reducing energy consumption, the distinguishing feature of our approach is that it achieves both energy efficiency and real-time communication. A solution to the idle-listening problem is proposed in Energy Efficient Sleep Scheduling based on Application Semantics (ESSAT). The novelty of ESSAT lies in that it takes advantage of the common features of data collection applications to determine when to turn on and off a node's radio without affecting real-time performance. A solution to the power control problem is proposed in Real-time Power Aware-Routing (RPAR). RPAR tunes the transmission power for each packet based on its deadline such that energy is saved without missing packet deadlines.
The main theoretical contribution of this thesis is the development of novel transmission scheduling techniques optimized for data collection applications. This work bridges the gap between wireless sensor networks and real-time scheduling theory, which have traditionally been applied to processor scheduling. The proposed approach has significant advantages over existing design methodologies: (1) it provides predictable performance allowing for the performance of a system to be estimated upon its deployment, (2) it is possible to detect and handle overload conditions through simple rate control mechanisms, and (3) it easily accommodates workload changes. I developed this framework under a realistic interference model by coordinating the activities at the MAC, link, and routing layers.
The last component of this thesis focuses on the development of a real-time patient monitoring system for general hospital units. The system is designed to facilitate the detection of clinical deterioration, which is a key factor in saving lives and reducing healthcare costs. Since patients in general hospital wards are often ambulatory, a key challenge is to achieve high reliability even in the presence of mobility. To support patient mobility, I developed the Dynamic Relay Association Protocol – a simple and effective mechanism for dynamically discovering the right relays for forwarding patient data – and a Radio Mapping Tool – a practical tool for ensuring network coverage in 802.15.4 networks. We show that it is feasible to use low-power and low-cost wireless sensor networks for clinical monitoring through an in-depth clinical study. The study was performed in a step-down cardiac care unit at Barnes-Jewish Hospital. This is the first long-term study of such a patient monitoring system.
</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>SOCCP: Self Organize Coverage and Connectivity Protocol</title>
<abstract>Recent developments in wireless communication and embedded computing technologies have led to the advent of wireless sensor network technology. Hundreds of thousands of these micro sensors can be deployed in many areas including health, environment and battlefield in order to monitor the domain with desired level of accuracy. When wireless sensors are deployed in an area, the lifetime of the network should last as long as possible according to the original amount of energy. Therefore, reducing energy consumption in WSNs is of primary concern. In this work, we have presented a protocol to maintain coverage and connectivity in wireless sensor networks, while switching off sensing or communication subsystem of some nodes to increase the network lifetime. The protocol presented here is a type of cluster based approach in which only a special set of nodes -- coordinators -- participate in packet routing but still each node decide independently to remain sensor, goes to sleep or become a coordinator. All nodes of the network generate a local coverage bitmap and based on this bitmap they decide to remain sensor or go to sleep. Using this local bitmap, the network will be capable of producing desired percentage of coverage. By using a local asynchronous scheduling, we have devised a mechanism to reduce interference between coordinator message broadcasts.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Grid-based for energy conservation around obstacles in wireless sensor networks</title>
<abstract>Reducing energy consumption has been a recent focus of wireless sensor network research. Topology control explores the potential that a dense network has for energy savings. We propose that the sensor field be overlaid with a honeycomb virtual mesh[1] based on a tessellation. Then with the help of greedy forwarding Algorithm and perimeter forwarding Algorithm[2-4], we can bypass a void or obstacles easily. And we can save energy to prolong the life time of the Wireless Sensor Networks.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>A Methodology for the Systematic Evaluation of ANN Classifiers for BSN Applications</title>
<abstract>While many BSN applications require that sensor nodes be able to operate for extended periods of time, they also often require the wireless transmission of copious amounts of sensor data to a data aggregator or base station, where the raw data is processed into application-relevant information. The energy requirements of such streaming can be prohibitive, given the competing considerations of form factor and battery life requirements. Making intelligent decisions on the node about which data to store or transmit, and which to ignore, is a promising method of reducing energy consumption. Artificial neural network (ANN) classifiers are among several competitive techniques for such data selection. However, no systematic metrics exist for determining if an ANN classifier is suited for a particular resource constrained computing environment of a typical BSN node. An especially difficult task is assessing, at the design stage, which classifier architectures are feasible on a given resource-constrained node, what computational resources are required to execute a given classifier, and what classification performance might be achieved by a particular classifier on a given set of resources. This paper describes techniques for quantifying and predicting the performance of ANN classifiers on wearable sensor nodes using scalable synthetic test data. Additionally, the paper shows a comparison of synthetic data with gait data collected using an inertial BSN node, and classification results of the gait data using a cerebellar model arithmetic computer (CMAC) architecture show excellent agreement with theoretical predictions.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Energy efficiency clustering algorithm in wireless sensor network application</title>
<abstract>Wireless sensor networks have attracted much research attention in recent years and can be used in many different applications. In this paper, we analyze the impact of Energy efficiency in Wireless sensor networks, as a result is Clustering technique has been proven to be an effective approach for reducing energy consumption. It also can increase the scalability and lifetime of the network. we propose a novel cluster formation algorithm for wireless sensor networks according to considering the energy as an optimization parameter. Compared to other algorithms, the Clustering algorithm increase the cluster head election mechanism, and the simulation results show that Clustering algorithm achieves its intention of consuming less energy.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Dynamic Power Management for Sensor Node in WSN Using Average Reward MDP</title>
<abstract>Reducing energy consumption is one of the key challenges in sensor networks. One technique to reduce energy consumption is dynamic power management. In this paper we model power management problem in a sensor node as an average reward Markov Decision Process and solve it using dynamic programming. We achieve an optimal policy that maximizes long-term average of utility per energy consumption. Simulation results show our approach has the ability of reaching to the same amount of utility as always on policy while consuming less energy than always on policy.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>A Low Power Grid-Based Cluster Routing Algorithm of Wireless Sensor Networks</title>
<abstract>In wireless sensor networks, the energy of nodes is limited. so designing efficient routing for reducing energy consumption is important. In this paper we proposed A Low Power Grid-based Cluster Routing Algorithm of Wireless Sensor Networks (LPGCRA). The characteristic of this algorithm is that the WSN is divided into different grids according to information of the node location, and then the nodes are organized within the grid by the clustering way. The clustering head is chose dynamiclly according to energy dissipation of the cluster nodes, then communicates with BS node through a relaying node. This algorithm is useful to reduce node energy consumption and prolonged life of the system, also enhanced the balance of network load. The simulation results shows that this algorithm balanced the energy consumption of the nodes effectively and improved the network lifetime.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>A Power Control Mechanism for Sensor Node Based on Dynamic Programming</title>
<abstract>Reducing energy consumption is one of the key challenges in sensor networks. Lots of researches have tried to turn the nodes off when not needed to reduce energy consumption. In this paper, we propose a mechanism which dynamically turns on/off different components of the node to conserve energy while maintaining required performance. We model power management problem in a sensor node as an average reward MDP and solve it using dynamic programming. We prove that the MDP is communicating and we use communicating policy iteration algorithm to compute the optimal policy. Simulation results show our approach conserves more energy than always on policy while reaches to the same amount of utility.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>End-to-end delay and network lifetime analysis in a wireless sensor network performing data aggregation</title>
<abstract>Data aggregation is a promising approach for reducing energy consumption and relaying of redundant data in wireless sensor networks for monitoring applications. However data aggregation implies, on the one hand, an increase in network processing at certain nodes, referred to as aggregators, possibly leading to a higher delivery delay to the sink. On the other hand, the advantages of using aggregation are numerous, especially under the perspective of energy consumption reduction and network lifetime increase. In this paper we provide a closed form expression for the end-to-end delay and network lifetime distributions in a wireless sensor network employing aggregation. These derivations can be used to provide some insights on the expected performance in terms of the metrics above mentioned, when data aggregation is used. For example, if a certain constraint on the maximum tolerable delivery delay is given, τ*, this can be useful to characterize what is the probability to satisfy this constraint.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Adaptive Distributed Compression Algorithm for Wireless Sensor Networks</title>
<abstract>In this paper we proposed a novel Adaptive Distributed Wavelet Compression (ADWC) algorithm for reducing energy consumption in a wireless sensor network, where each of the sensors has limited power. This algorithm is characterized by a distributed lifting factorization, which matching well with the transmission strategy employed in wireless sensor networks., it also present an adaptive algorithm to selects the optimal wavelet compression parameters to minimize total energy dissipation. The simulation results showed that these approaches can achieve significant energy savings without sacrificing the quality of the data reconstruction.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Development of the Temperature Sensor for Measuring Heat Based on the Difference between the Specific Enthalpy</title>
<abstract>Reducing energy consumption, improving the heat efficiency is one of the key work of energy conservation and emission reduction. The measuring heat instrument take the principle of the difference between specific enthalpy and use Pt resistance as temperature sensor, the curve of two temperature sensors do matching pairs. This new technology and new technique can solve some difficult problems in the developing process of heat meter temperature sensor, and make the products meet the technical requirements of EN1434 as batch production, reaching the international advanced level, improve the market competitiveness of products.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Unified power management in wireless sensor networks</title>
<abstract>Radio power management is of paramount concern in wireless sensor networks (WSNs) that must achieve long lifetimes on scarce amount of energy. Previous work has treated communication and sensing separately, which is insufficient for a common class of sensor networks that must satisfy both sensing and communication requirements. Furthermore, previous approaches focused on reducing energy consumption in individual radio states resulting in suboptimal solutions. Finally, existing power management protocols often assume simplistic models that cannot accurately reflect the sensing and communication properties of real-world WSNs.
We develop a unified power management approach to address these issues. We first analyze the relationship between sensing and communication performance of WSNs. We show that sensing coverage often leads to good network connectivity and geographic routing performance, which provides insights into unified power management under both sensing and communication performance requirements. We then develop a novel approach called Minimum Power Configuration that integrates the power consumption in different radio states into a unified optimization framework. Finally, we develop two power management protocols that account for realistic communication and sensing properties of WSNs. Configurable Topology Control can configure a network topology to achieve desired path quality in presence of asymmetric and lossy links. Co-Grid is a coverage maintenance protocol that adopts a probabilistic sensing model. Co-Grid can satisfy desirable sensing QoS requirements (i.e., detection probability and false alarm rate) based on a distributed data fusion model.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Energy saving strategy for video-based Wireless Sensor Networks under field coverage preservation</title>
<abstract>The demand for video-based Wireless Sensor Networks (WSN) applications has significantly increased. Different from conventional sensor networks, video WSN imply a directional sensing model, complex in-node processing, and large data transfer, thus high clock frequencies and significant radio transmission time. Reducing energy consumption is a key requirement for reliable applications. We propose in this paper a novel energy saving method based on redundant node deactivation while preserving field coverage.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Low-complexity aggregation of collected images with correlated fields of view in wireless video sensor networks</title>
<abstract>Wireless video sensor networks (WVSNs) require video data from sensor nodes to be delivered efficiently. Cameras in adjacent video nodes tend to have correlated fields of view (FoVs) or overlapping part when a sufficient number of video sensors are deployed. This paper proposes a data aggregation technique for WVSNs to remove the spatial redundancy, thereby reducing energy consumption and response time. Our approach exploits the correlation between discrete cosine transform (DCT) coefficients pairs of two intra-coded images from cameras with overlapping FoVs. Experiments show that an intermediate node en route to the base station achieves bit-rate savings up to 18.9%. This scheme is less complicated than other video and image coding techniques that exploit correlated FoV, allowing resource-constrained video sensors to operate more reliably and longer.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Walking pattern modification using manipulability ellipsoid for biped robot</title>
<abstract>This paper suggests the new approach for reducing energy consumption during the walking cycle of the biped robot. After the stable walking trajectory is manually designed, the trajectory can be modified to reduce the energy consumption and increase walking stability based on the measured angular velocities of the robot during the walk combined with the dexterity information from the manipulability ellipsoid. The modified trajectory was tested on the biped robot and compared to the original trajectory. The gyro sensor and the current sensor were used to measured angular velocity and energy consumption of the robot. The modified walking trajectory can successfully reduce the energy consumption and increase the stability of the robot during its walking cycle.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>TRENS: A Tracking-Assisted Routing Scheme for Wireless Sensor Networks</title>
<abstract>Wireless sensor networks have been proposed as a promising technology employed for tracking target and observing environment. Since the targets and sinks could move anywhere at any time, how to efficiently trace the positions of targets and sinks and route the sensing data from the targets to the sinks poses significant design challenges of a well-behavior routing scheme in WSNs. Moreover, sensor nodes are often operated under energy-constrained situation, thus, reducing energy consumption of each node to extend network lifetime is also an important issue. In this paper, we present a novel routing scheme, named TRENS, to solve the above problem. TRENS is based on the visual-grid structure and uses tracking technology to assist the routing procedure for routing traffic efficiently while reducing power dissipation. Additionally, TRENS introduces a shortcutting scheme to optimize the routing paths for decreasing communication cost and latency. We compare the performance of TRENS with previous schemes by performing extensive simulation. The simulation results show that the proposed architecture conserves energy efficiently and achieves much better performance.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>An Energy-Efficient and Traffic-Aware CSMA/CA Algorithm for LR-WPAN</title>
<abstract>In this paper, we propose an energy-efficient and traffic-aware CSMA/CA algorithm, referred to as ETCA, that aims to reduce redundant transmission of IEEE 802.15.4 Standard for Low-Rate Wireless Personal Area Networks (LR-WPAN). In Wireless Sensor Networks (WSNs), reducing energy consumption is one of the most important issues because it makes the network life-time longer. Sensor nodes consume most of energy for computing and transmitting operations [1]. Unlike IEEE 802.11, IEEE 802.15.4 MAC doesn't use Request To Send (RTS) and Clear To Send (CTS) frames, and it still has hidden terminal problems. Thus, if there are some hidden nodes, there are many collisions and retransmissions [2]. To reduce retransmission, we improve the carrier sensing features in MAC layer of IEEE 802.15.4. When the network is busy, the MAC detects it and adaptively chooses parameters of CSMA/CA mechanism to minimize collisions and the number of retransmission based on ETCA. We evaluated the proposed scheme with original 802.15.4 MAC on the ns-2 simulator. The results showed that our scheme outperforms the original by 15% in average when there is a retransmission activity.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>ETC: Energy-Driven Tree Construction in Wireless Sensor Networks</title>
<abstract>Continuous queries in Wireless Sensor Networks (WSNs) are founded on the premise of Query Routing Tree structures (denoted as T), which provide sensors with a path to the querying node. Predominant data acquisition systems for WSNs construct such structures in an ad-hoc manner and therefore there is no guarantee that a given query workload will be distributed equally among all sensors. That leads to data collisions which represent a major source of energy waste. In this paper we present the Energy-driven Tree Construction (ETC) algorithm, which balances the workload among nodes and minimizes data collisions, thus reducing energy consumption, during data acquisition in WSNs. We show through real micro-benchmarks on the CC2420 radio chip and trace-driven experimentation with real datasets from Intel Research and UC-Berkeley that ETC can provide significant energy reductions under a variety of conditions prolonging the longevity of a wireless sensor network.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Resource allocation and routing in multi-hop wireless networks</title>
<abstract>Multi-hop wireless networks have been widely deployed in different application areas. These include wireless sensor networks, wireless mesh networks, and wireless vehicular ad-hoc networks. Resource allocation and routing are two critical issues for multi-hop wireless networks. The appropriate allocation of the limited resources, such as energy or channel, can usually improve the network performance dramatically. On the other hand, routing is usually coupled with resource allocation, which affects the network topology. The problem itself can even be regarded as a high level resource allocation, because it is equivalent to allocating the set of wireless links and bandwidth for each particular application. Due to the difference in hardware characteristics and application background, different networks should be given special design considerations. In this dissertation, we discuss several research topics related with resource allocation and routing in multi-hop wireless networks.
First, we investigate the problem of sleep scheduling of sensors in dense wireless sensor networks with the goal of reducing energy consumption. The basic idea is to partition sensors into groups such that a connected backbone network can be maintained by keeping only one arbitrary node from each group in active status. Unlike previous approaches that use geographic partitions, we propose to partition nodes based on their measured connectivity. The proposed scheme not only ensures K-vertex connectivity of the backbone network, but also outperforms other approaches in irregular radio environments.
Second, we study the channel assignment with partially overlapping channels in wireless mesh networks. Unlike previous studies that focus on the channel assignment with non-overlapping channels only, we propose efficient channel assignment algorithms that use both non-overlapping and overlapping channels. We show that the network capacity can be dramatically increased by using partially overlapping channels, and the proposed algorithms find better solutions than existing algorithms.
Third, as both static channel assignment and dynamic channel assignment have their own strengths and drawbacks, we propose a hybrid wireless mesh networking architecture, which combines the advantages of both channel assignment approaches. We discuss both the channel assignment algorithms and routing protocols used in this hybrid architecture. We demonstrate that our proposed approach achieves better adaptivity to the changing traffic and lower data delivery delay.
Fourth, we study the application of multi-source video on-demand streaming in wireless mesh networks. We focus on the problem of finding the maximum number of high-quality and independent paths from the user to the multiple video sources by considering the effect of wireless interference. We propose efficient routing algorithms that aim at minimizing the network congestion caused by each new video session. We show that it not only improves the average video streaming performance, but also increase the network's capacity of satisfying video requests.
Finally, we present a routing algorithm for vehicular ad-hoc networks with the goal of efficient data delivery under low and median vehicle densities. We consider the deployment of static nodes at road intersections to help relay data. We propose an algorithm that determines whether a vehicle forwards a packet to the static node and when the static node forwards the packet to vehicles. We show that our proposed routing algorithm achieves lower data delivery delay than the previous algorithms.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Application research on network coding in WSN</title>
<abstract>This paper introduces the characteristics of the wireless network, including self-organizing, dynamic topology, wireless communications and so on, which make the network nodes have limited energy, the same as processing power, storage capacity and communication capabilities, therefore, the efficient use of energy, maximizing its network cycle is the wireless sensor network's core issue. Network coding integrates the concept of routing and coding, which has been proven to be able to approach the theoretical transfer limit of the network capacity. The unreliability of wireless link and the broadcasting characteristics of physical layer are ideal for the use of network coding. This paper reviews the applications of network coding technology in wireless sensor network, including improving network throughput, reducing energy consumption, increasing the reliability and security of the network link, and improving the efficiency of data aggregation.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>

<item>
<title>Efficient clustering for improving network performance in wireless sensor networks</title>
<abstract>Clustering is an important mechanism in large multi-hop wireless sensor networks for obtaining scalability, reducing energy consumption and achieving better network performance. Most of the research in this area has focused on energy-efficient solutions, but has not thoroughly analyzed the network performance, e.g. in terms of data collection rate and time.
The main objective of this paper is to provide a useful fully-distributed inference algorithm for clustering, based on belief propagation. The algorithm selects cluster heads, based on a unique set of global and local parameters, which finally achieves, under the energy constraints, improved network performance. Evaluation of the algorithm implementation shows an increase in throughput in more than 40% compared to HEED scheme. This advantage is expressed in terms of network reliability, data collection quality and transmission cost.</abstract>
<search_task_number>19</search_task_number>
<query>"reducing energy consumption" sensor</query>
<relevance>0</relevance>
</item>




  <item>
    <title>
      Structural user preferences of interfaces and time orientation
    </title>
    <abstract>
      Today the user orientation within the development process of user interfaces in production environment is concentrated on tasks. This is realized by focusing on user groups. To enhance the usability of user interfaces, the development process is expanded by the personalization of user interfaces. Thus user preferences and their attributes e.g. individual differences concerning the structure of interfaces have to be examined for being able to develop appropriate interfaces for specific users. Different test methods to gain these preferences and attributes are described within this paper. The found structural preferences can be connected to the concept of time orientation: it classifies people in two different categories: polychrons and monochrons. The test results confirm that these characteristic are rather individual differences than intercultural variables.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Cultural user interfaces: a silver lining in cultural diversity
    </title>
    <abstract>
      Many software applications marketed outside the country of origin are internationalised and/or localised. In this article, I propose a strategy to localise the software by creating Cultural User Interface a (CUI) for each of the target cultures. A CUI is a user interface that is intuitive to a particular culture. The CUI takes advantage of the shared or common knowledge of a culture which could be defined by country boundaries, language, cultural conventions, race, shared activities or workplace. An application that is CUI-enabled allows the use of many different CUIs. These different CUIs are developed collaboratively with the target cultures, thus problems associated with localisation such as misinterpretation of elements in the CUIs, are unlikely to occur. A CUI can be used not only for one application but for a range of applications.Most software developers have accepted the fact that it is worthwhile economically to internationalise their software. This trend is evident in the growing number of companies that provide internationalisation and translation services for software marketed outside the United States (US). Also, many resources are now available on internationalisation and localisation of software. These resources include books (Kano (1995), Fernandes (1995), O'Donnell (1994), Uren et al. (1993), Apple (1992b), Digital (1992), Madell et al. (1992), Taylor (1992), Nielsen, (1990)), mailing lists (INSOFTL, intercultural.CHI), newsgroups (comp.software.international, comp.std.internat) and FAQs (ISO 8859-1 National Character Set FAQ, Programming for Internationalization FAQ, Globalizing Applications for Windows FAQ). Some examples of current articles written on these topics include Yeo and Barbour (1996), Karat and Karat (1996), Belge (1995), Chris Miller (1994), Hall (1994), and Nakakoji (1994). CHI Workshops have also been conducted (Kellogg and Thomas, 1993). Presently, there is an even further need for internationalised software that allows the use of non-Romanised characters given the popularity of WWW (see. http://www.w3.org/pub/WWW/International/).Current internationalisation and localisation of software has mainly focused on modifications of the language/character sets, collating sequence, the date, time, number and currency formats. As pointed out by Russo and Boor (1993) and Marcus (1993) there are many aspects that need to be addressed. One of these aspects is making provisions for the different perceptions of the diverse cultures.In this article, I discuss the differences in perception of user interface elements across cultures. A proposal and justification for many individually unique Cultural User Interfaces (CUIs) will be put forward. Lastly, technical implications of this proposal and a strategy to develop CUIs will be discussed.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Instructional Design Frameworks and Intercultural Models
    </title>
    <abstract>
      The global technological marketplace has brought with it the need to address international and local target audiences. To remain competitive, companies have begun to design ICTs with a focus on generic and specialized users and learners. Instructional Design Frameworks and Intercultural Models meets the needs of practitioners and researchers by providing frameworks for integrating culture into design. This book offers practical applications for the construction of user interfaces, products, services, and other online environments useful in the development of culture-based designs.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Intercultural Socializing via Mobile Games for Backpackers
    </title>
    <abstract>
      Mobile phones are currently shipped with pre-installed games all around the world. In most cases, these pre-installed games contain very little elements of sharing local heritage, traditional values and beliefs. Naturally, such pre-installed mobile games are provided with a different intent and under various commercial contexts. This paper looks at the design and development of intercultural mobile games aimed for backpackers. This is to allow backpackers to socialize, to share and to learn new cultures during their travels. User evaluation studies were conducted to collect the backpacker's feedback and to improve on conceptualization during the design process. The results were used collectively to provide input for improving the design concepts and interfaces. Screenshots of the mobile games are provided. In general, the results highlight important considerations when deploying an intercultural mobile game to backpackers.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Mobile Phones: Technology, Networks and User Issues
    </title>
    <abstract>
      The fast evolution of the technology, social network services and mobile platforms have transformed the traditional notions of community and intercultural communication. This book presents topical research in the study of the technology, network and user issues in mobile phones today. Topics discussed include increased genetic damage due to mobile telephone radiation; mobile phone interfaces for collaborative learning in everyday life; the internationalisation process of European operators and mobile health services improving healthcare through mobile technology.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Dark Fiber (Electronic Culture: History, Theory, and Practice Series): Tracking Critical Internet Culture
    </title>
    <abstract>
      From the Publisher:According to media critic Geert Lovink, the Internet is being closed off by corporations and governments intent on creating a business and information environment free of dissent. Calling himself a radical media pragmatist, Lovink envisions an Internet culture that goes beyond the engineering culture that spawned it to bring the humanities, user groups, social movements, non-governmental organizations (NGOs), artists, and cultural critics into the core of Internet development.In Dark Fiber, Lovink combines aesthetic and ethical concerns and issues of navigation and usability without ever losing sight of the cultural and economic agendas of those who control hardware, software, content, design, and delivery. He examines the unwarranted faith of the cyber-libertarians in the ability of market forces to create a decentralized, accessible communication system. He studies the inner dynamics of hackers' groups, Internet activists, and artists, seeking to understand the social laws of online life. Finally, he calls for the injection of political and economic competence into the community of freedom-loving cyber-citizens, to wrest the Internet from corporate and state control.The topics include the erosion of email, bandwidth for all, the rise and fall of doctom mania, techno-mysticism, sustainable social networks, the fight for a public Internet time standard, and collaborative text filtering. Stressing the importance of intercultural collaboration, Lovink includes reports from Albania, where NGOs and artists use new media to combat the country's poverty and isolation; from Taiwan, where the September 1999 earthquake highlighted the cultural politics of the Internet; and from Delhi, where a new media center explores free software, public access, and Hindi interfaces.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Reaching a global online audience, First edition
    </title>
    <abstract>
      Internationalizing web sites used to focus on adapting currency formats, time zones, and translation. Although these are important technical considerations, much more is required to adapt a web site for global users. This guide explains several of the deepest dimensions of culture and then uses these intercultural human factors for adapting the interface, multimedia, architecture, and search system. Along the way, we'll look at the impact of uncertainty, communication context, social fabric, and concepts of time, and then use these dimensions to create better web sites for global users. If you design, develop, or deploy web sites for a global audience, then this guide will get you started in the right direction.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      A ubiquitous device to prevent unauthorized use
    </title>
    <abstract>
      In this paper, a ubiquitous device to prevent the unauthorized usage of electronic objects is described. The features, architecture, basic algorithms and application will be presented. The approach offers ways to support universal design embracing accessibility and usability and simultaneously improving security.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Re-framing HCI through local and indigenous perspectives
    </title>
    <abstract>
      This one-day workshop aims to present different local and indigenous perspectives from all over the world in order to lead into an international dialogue on re-framing concepts and models in HCI/Interaction Design. The target audience is HCI researchers and practitioners who have experience with working with culture and HCI. The expected outcome of the workshop is a) network building among the participants, b) a shortlist of papers that can be basis for a proposal for a special issue of the UAIS journal, and c) identify opportunities to develop a funded network or research proposal.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Adding voice to whisper using a simple heuristic algorithm inferred from empirical observation
    </title>
    <abstract>
      The aim of the work described in this paper is to allow people that are enforced to use &quot;whispery voice&quot; to be endowed with &quot;voiced voice&quot;. A very simple method and algorithm obtained by empirical observation of corresponding speech signals is presented and discussed. Only well-known methods like FFT, interpolation, normalization, and formant shifting are used. It turned out that the quality of the processed speech signal is adequate to be understood by hearers. The approach can also be used to replace normal speaking or singing voices.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      From Cultural to Individual Adaptive End-User Interfaces: Helping People with Special Needs
    </title>
    <abstract>
      Culture heavily influences human---computer interaction (HCI) since the end-user is always within a cultural context. Cultural and informational factors correlate to jointly influence the look and feel of interactive systems, e.g. widget position or information density. Every single individual also develops a specific culture (eating style, walking style etc.), i.e. characteristics and behavior as well as attitudes and values. Consequently, individual adaptability can be essential to cover individual needs of the culturally but uniquely imprinted end-users with special needs e.g. reducing the workload by recognizing and knowing the individual expectances of the end-user. This improves usability and leads to shorter training and improves universal access.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Identification of the User by Analyzing Human Computer Interaction
    </title>
    <abstract>
      This paper describes a study analyzing the interaction of users with a computer system to show that user identification is possible only by analyzing the user interaction behavior. The identification of the user can be done with a precision of up to 99.1% within one working session. This classification rate can be improved using additional interaction indicators. Moreover, this kind of protection method using the analysis of the interaction of the user with the system cannot be betrayed because of the uniqueness of the user interaction patterns. The method and the results of the study will be presented and discussed.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Improving performance, perceived usability, and aesthetics with culturally adaptive user interfaces
    </title>
    <abstract>
      When we investigate the usability and aesthetics of user interfaces, we rarely take into account that what users perceive as beautiful and usable strongly depends on their cultural background. In this paper, we argue that it is not feasible to design one interface that appeals to all users of an increasingly global audience. Instead, we propose to design culturally adaptive systems, which automatically generate personalized interfaces that correspond to cultural preferences. In an evaluation of one such system, we demonstrate that a majority of international participants preferred their personalized versions over a nonadapted interface of the same Website. Results show that users were 22&amp;percnt; faster using the culturally adapted interface, needed fewer clicks, and made fewer errors, in line with subjective results demonstrating that they found the adapted version significantly easier to use. Our findings show that interfaces that adapt to cultural preferences can immensely increase the user experience.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Towards culture-centred design
    </title>
    <abstract>
      This paper addresses culturally rooted factors within user interface design. The design implications of globalisation are discussed, together with the related processes of internationalisation, localisation, 'glocalisation', iconisation and culturalisation, in order to establish a basis for a new approach to HCI design. The potential for a more diverse culture-centred, design-based system-'Culture-Centred Design' (CCD) is introduced, and a CCD process developed. A redesigned computer interface, incorporating a consistent and culturally rooted metaphor for a Chinese user target group is discussed. A culturally specific 'garden' metaphor is developed and applied as an alternative to the current global 'office' or 'desktop' metaphor. A working demonstration of the interface is piloted with a group of Chinese users to assess its success in terms of interactivity, usability and cultural significance. The overall results of the first two evaluation phases have shown very positive outcomes for the use of the CCD system and Chinese garden metaphor.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      The role of spatial contextual factors in mobile personalization at large sports events
    </title>
    <abstract>
      This paper presents three field studies undertaken at large sports events in UK and China, with the aim of improving the user experience at these types of events through the design of personally relevant mobile services. These field studies investigated: which aspects of spatial context were relevant within the confines of a large sporting event, how their relevance differed according to sports event and language/culture, and how they could be used to prescribe the behaviour of a personalizable/adaptive mobile device. Spatial aspects of context were found to be highly significant within the large sports arena. They can be used to maximize the relevance of information and communication services delivered to a spectator over a mobile device. A range of design implications are discussed.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Language issues in cross cultural usability testing: a pilot study in china
    </title>
    <abstract>
      Language effect (Chinese vs. English), and power distance between evaluator and user in usability test were investigated. 12 participants from China, Swede, and Denmark formed 7 evaluator-test user pairs. Test users were asked to use a software. Evaluators were asked to conduct the usability test, and try to find usability problems. Participants' conversation, behaviour, and screen operation were recorded by behaviour observation system. Results showed that Speaking Chinese made evaluator giving more help in detail, and encouraging users more frequently; Speaking English asked evaluator and user look at each other more often to make themselves understood, and evaluators paid more attention to check task list. Power distance also had effect on evaluators and users. When evaluator's title were higher than users, evaluator would pay more attention to users' doing, not like to give user detailed instruction, usually loose communication with user, and spent less for task management. In contrast, talking to evaluators with higher rank, users tend to use more gesture to express themselves.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      The next revolution: vehicle user interfaces
    </title>
    <abstract>
      Imagine having to think about safety, usability, and aesthetics issues for the user interface of a two-ton mobile device hurtling through space at 100 km/hr. Now you get the picture.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Using metaphors to explore cultural perspectives in cross-cultural design
    </title>
    <abstract>
      We have proposed five cultural viewpoint metaphors to help designers that wish to encourage and support cross-cultural HCI contacts. In this paper we present the main results of an experiment carried out to assess the potential of these metaphors in designing cross-cultural systems. Six HCI designers, with different cultural backgrounds, were then asked to create re-design alternatives for a real website guided by the metaphors. As a result, the experiment showed the epistemic effect of the metaphors on cross-cultural design, i. e. as a means to build new knowledge and understanding.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Cross-cultural user-interface design for work, home, play, and on the way
    </title>
    <abstract>
      User interface design requires good visual design of metaphors, mental models, navigation, appearance, and interaction to represent data, functions, tasks, roles, organizations, and people. Techniques of simplicity, clarity, and consistency can improve the communication effectiveness of user interfaces for the Web, mobile devices and information appliances, and performance (productivity) tools. In particular, the use of appropriate typography, layout, color, animation, and symbolism can assist developers to achieve more efficient, effective communication to more diverse user communities.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      User-interface design and China: a great leap forward
    </title>
    <abstract>
      If user-interface design seeks to encompass human-experience design, then computer-based communication and interaction designers need to keep Asian, specifically Chinese, users in mind. China, with approximately one-fifth of the world's population, an economy that is growing quickly, and a manufacturing system that exports a significant percentage of the goods imported into the world's countries, needs to be considered in revising concepts of user-interface and user-experience design.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Vers la plénitude de l'expérience utilisateur
    </title>
    <abstract>
      The HCI domain has registered a rapid growth since its inception (1978--1983). The result is that one can take advantage of a body of knowledge, methods, and tools to design high quality interfaces which can meet the requirements of activities to be carried out and satisfy the users' needs and expectations. Nonetheless, the challenges to take up are numerous to continue to innovate and develop our domain. The goal is not only to realize useful and usable interfaces that can stand out thanks to the intelligence of their interactivity, but also to create interactive objects that are remarkable by the values they carry out (e.g., hedonism) and by the capacity to generate a good user experience. In this paper, we analyze the transition occurring in the HCI, where one moves from a focus on interface usability on the more including concept of user experience with the interactive object. The idea from now on is to make room to the pleasure of interacting with technology, to intellectual stimulation, to the strength of the flow experience with the activity, to the pride of being successful, to the enrichment due to exchanges with others, to the attachment for the object, and to aesthetics.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Assistive technology for vision-impairments: anagenda for the ICTD community
    </title>
    <abstract>
      In recent years, ICTD (Information Communications Technology and Development) has grown in significance as an area of engineering research that has focused on low-cost appropriate technologies for the needs of a developing world largely underserved by the dominant modes of technology design. Assistive Technologies (AT) used by people with disabilities facilitate greater equity in the social and economic public sphere. However, by and large such technologies are designed in the industrialized world, for people living in those countries. This is especially true in the case of AT for people with vision impairments -- market-prevalent technologies are both very expensive and are built to support the language and infrastructure typical in the industrialized world. While the community of researchers in the Web Accessibility space have made significant strides, the operational concerns of networks in the developing world, as well as challenges in support for new languages and contexts raises a new set of challenges for technologists in this space. We discuss the state of various technologies in the context of the developing world and propose directions in scientific and community-contributed efforts to increase the relevance and access to AT and accessibility in the developing world.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Usability challenges in designing foreign language keypads for handheld devices
    </title>
    <abstract>
      This paper discusses the importance of language and culture in the effective design and widespread acceptance of handheld devices in foreign markets. To this end, key challenges that usability experts and interaction designers face while designing foreign language keypads for handheld devices are discussed and analyzed. The discussion presupposes English as the point of reference for design decisions but focuses on the challenges faced when considering foreign language devices. For the context of this paper, Arabic is cited as the 'foreign' language in the design of BlackBerry devices.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Incorporating the cultural dimensions into the theoretical framework of website information architecture
    </title>
    <abstract>
      Information Architecture (IA) has emerged as a discipline that is concerned with the development of systematic approaches to the presentation and organization of online information. The IA discipline has commanded significant attention from professional practitioners but lacks in the theoretical perspective. In our effort to formalize the knowledge of the discipline, we report on the extension of our initial work of formalizing the architectural framework for understanding website IA. Since the web is not a culturally neutral medium, we sought to delineate the cultural dimensions within our formed framework of website IA with the incorporation of the cultural dimensions of Hofstede and Hofstede's (2005), Hall's (1966), Hall and Hall's (1990) and Trompenaar's (1997). This attempt contributes towards the progress of putting a sense of cultural localization to the IA augmentation for local and international website design. In addition, to avoid theoretical aloofness and arbitrariness, practical design presumptions are also reflected.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Non-universal usability?: a survey of how usability is understood by Chinese and Danish users
    </title>
    <abstract>
      Most research assumes that usability is understood similarly by users in different cultures, implying that the notion of usability, its aspects, and their interrelations are constant across cultures. The present study shows that this is not the case for a sample of 412 users from China and Denmark, who differ in how they understand and prioritize different aspects of usability. Chinese users appear to be more concerned with visual appearance, satisfaction, and fun than Danish users; Danish users prioritize effectiveness, efficiency, and lack of frustration higher than Chinese users. The results suggest that culture influences perceptions of usability. We discuss implications for usability research and for usability practice.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Using Hofstede's cultural dimensions to interpret cross-cultural blended teaching and learning
    </title>
    <abstract>
      This article reflects on the cross-cultural communicative experiences of professors from South Africa and students from Sudan, during a two-year Internet-supported Masters' course in Computers in Education. Four of Hofstede's cultural dimensions were considered as categories of interpretation. The purpose of the research was to determine the extent to which Hofstede's static quantitative, research could be used as a basis for an essentially qualitative dynamic interpretation. While Hofstede's work focuses on cultural differences, this article tries to uncover what commonalities were constructed in the process. It was found that in this case, dimensions such as power distance and uncertainty avoidance tended to amplify each other, while together they resulted in a movement away from individualism towards collectivism. Accommodating across cultures did not mean that one should move into the other culture. Three elements seem to play a role when cultures meet: Reduction of communicative uncertainty, construction of shared meaning, and appropriate use of technology. More research should be conducted to uncover the elements that are common to cultures because emphasising commonality seems more useful than trying to overcome differences.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Board-based collaboration in cross-cultural pairs
    </title>
    <abstract>
      This paper reports on an ongoing study of cross-cultural collaboration mediated by board-based collaborative systems. Twenty-one pairs (American-Chinese and American-American pairs) performed collaborative design tasks either face-to-face or remotely. Survey data, video recording, and design products were collected to examine the impact of Culture (American-American vs. American-Chinese), Medium (Face-to-Face vs. Computer-Supported), and Board-based System (Mimio™ vs. SMART Board™) on the process and outcomes of collaboration. Results from the survey showed significant effects of these variables on measures of common ground, cognitive consensus building, perceived performance, and satisfaction. The effects on perceived performance were robust. American-Chinese pairs reported a significantly lower level of consensus when using a system that supports unidirectional (Mimio™) rather than bi-directional (SMART Board™) interaction on the board.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Notations around the world: census and exploitation
    </title>
    <abstract>
      Mathematical notations around the world are diverse. Not as much as requiring computing machines' makers to adapt to each culture, but as much as to disorient a person landing on a web-page with a text in mathematics.In order to understand better this diversity, we are building a census of notations: it should allow any content creator or mathematician to grasp which mathematical notation is used in which language and culture. The census is built collaboratively, collected in pages with a given semantic and presenting observations of the widespread notations being used in existing materials by a graphical extract. We contend that our approach should dissipate the fallacies found here and there about the notations in &quot;other cultures&quot; so that a better understanding of the cultures can be realized.The exploitation of the census in the math-bridge project is also presented: this project aims at taking learners &quot;where they are in their math-knowledge&quot; and bring them to a level ready to start engineering studies. The census serves as definitive reference for the transformation elements that generate the rendering of formulæ in web-browsers.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Using card sorts for understanding website information architectures: technological, methodological and cultural issues
    </title>
    <abstract>
      The card sort technique has many uses in HCI research and practice. Card sorts have traditionally been conducted with physical cards but now programs are available for this task. It is unclear if results from an online version of this technique are as reliable as the &quot;oncard&quot; version. This paper presents a study comparing oncard and online versions of the card sort technique for card set reflecting the information architecture (IA) of two website domains (museum and news sites). No differences were found between the two versions. However, the online version took significantly longer for participants than the oncard version, particularly for non-native English speakers. The card sort technique was also able to reveal cultural differences between mental models of British, Chinese and Indian participants of the IAs of both museum and news websites and showed that all participants have mental models that differ substantially from the typical IAs of websites in these domains.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Cross-cultural user-experience design
    </title>
    <abstract>
      User interfaces for desktop, Web, mobile, and vehicle platforms extend across culturally diverse user communities, sometimes within a single country or language group, and certainly across the globe. If user interfaces are to be usable, useful, and appealing to such a wide range of users, user-interface/user-experience developers must account for cultural aspects in globalizing/localizing products and services. In this course, participants will learn practical principles and techniques that are immediately useful for both analysis and design tasks. Where time permits, they will have an opportunity to put their understanding into practice through a series of group exercises. Some handout materials are available in Mandarin.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Designing for all users: including the odd users
    </title>
    <abstract>
      The field of HCI has played an important role in broadening the spectrum of users of computational artifacts. However, users with extreme preferences are mostly ignored by the designers and researchers because they do not constitute a large portion of the market and the users lack generalizable characteristics. In order to further discuss these concerns, this paper introduces a case about the extreme users and the challenges they face. The paper ends with discussing future directions and challenges in designing for all users in the field of HCI.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      A tool for cross-cultural human computer interaction analysis
    </title>
    <abstract>
      This paper describes a tool for analyzing cross-cultural human computer interaction (HCI). From literature and reasoning possible cultural HCI indicators have been identified and measured with this tool to compare them in respect to the different culture of the users. Concept, implementation, usage, benefit and implications of this tool will be presented. Two online studies using this tool concerning cultural adaptability exemplified by use cases of navigation systems revealed differences in interaction behavior that depend on the cultural background of the users (e.g. attitude, preference, skill etc.) and proved that the tool is working properly.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Enhancing bilingual electronic group meeting comprehension with round-trip translations
    </title>
    <abstract>
      Multilingual groups can use electronic meeting technology to automatically translate typed comments into several languages, but participants cannot be sure how well their comments are understood by those speaking other languages unless a translation preview is provided. Here, we describe a multilingual meeting system that presents to each user a round-trip translation (RTT) after each comment is typed, allowing them to revise a comment if the preview is poor. In a study of how English speakers use the system with automatic translation to German, results showed that the participants thought the system and the preview feature were useful in a multilingual meeting. Those using the feature better estimated the accuracy of the German translations than those who did not use RTT, and there was a significant, positive correlation between the forward translations to German and the back translations to English, indicating that the accuracy can be predicted and comprehension can be enhanced in a bilingual meeting.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      User-Centered Design Stories: Real-World UCD Case Studies
    </title>
    <abstract>
      Intended for both the student and the practitioner, this is the first user-centered design casebook. It follows the Harvard Case study method, where the reader is placed in the role of the decision-maker in a real-life professional situation. In this book, the reader is asked to perform analysis of dozens of UCD work situations and propose solutions for the problem set.The problems posed in the cases cover a wide variety of key tasks and issues facing practitioners today, including those that are related to organizational/managerial topics, UCD methods and processes, and technical/ project issues. The benefit of the casebook and its organization is that it offers the new practitioner (as well as experienced practitioners working in new settings) the valuable practice in decision-making that one cannot get by reading a book or attending a seminar.*The first User-Centered Design Casebook, with cases covering the key tasks and issues facing UCD practitioners today.*Each chapter based on real world cases with complex problems, giving readers as close to a real-world experience as possible.* Offers &quot;the things you don't learn in school,&quot; such as innovative and hybrid solutions that were actually used on the problems discussed.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Text Entry Systems: Mobility, Accessibility, Universality
    </title>
    <abstract>
      Text entry has never been so important as it is today. This is in large part due to the phenomenal, relatively recent success of mobile computing, text messaging on mobile phones, and the proliferation of small devices like the Blackberry and Palm Pilot. Compared with the recent past, when text entry was primarily through the standard &quot;qwerty&quot; keyboard, people today use a diverse array of devices with the number and variety of such devices ever increasing.The variety is not just in the devices, but also in the technologies used: Entry modalities have become more varied and include speech recognition and synthesis, handwriting recognition, and even eye-tracking using image processing on web-cams. Statistical language modeling has advanced greatly in the past ten years and so therein is potential to facilitate and improve text entry--increasingly, the way people communicate.This book consists of four parts, and covers these areas: Guidelines for Designing Better Entry Systems (including research methodologies, measurement, and language modelling); Devices and Modalities; Languages of the world and entry systems in those languages; and variety in users and their difficulties with text entry--and the possible design and guideline solutions for those individual user groups.This book covers different aspects of text entry systems and offers prospective researchers and developers* global guidelines for conducting research on text entry, in terms of design strategy, evaluation methodology, and requirements;* history and current state of the art of entry systems, including coverage of recent research topics;* specific guidelines for designing entry systems for a specific target, depending on devices, modalities, language, and different physical conditions of users
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Proceedings of the 2011 annual conference extended abstracts on Human factors in computing systems
    </title>
    <abstract>
      Over the last year or so, we have been blessed with the challenge, the opportunity, and the distinct pleasure of organizing the CHI 2011 Conference on Human Factors in Computing Systems, the premier international conference for the field of human-computer interaction.CHI 2011 takes place in gorgeous, energetic, sophisticated Vancouver BC, a city renowned for its innovation in entertainment, sustainability, accessibility, and inclusivity. The New York Times calls it, &quot;a liquid city, a tomorrow city, equal parts India, China, England, France and the Pacific Northwest.&quot; Vancouver lays a beautiful backdrop for our conference, which boasts nearly 30 years of wonderful work.Behind the success of the conference is our diverse community of faculty and students, of researchers and practitioners, of young and, well, also of experienced. It is a community of designers, technologists, psychologists, social scientists, biologists, artists, engineers, anthropologists, musicians; the list goes on. Wherever we are, we are always a community of near and far. Most impressively, ours is a community that cares deeply about innovating, learning, sharing, and interacting; all with the common goal of using technology to shape the way people around the world live and play.Returning attendees will recognize the general conference format - 2 days of small intimate workshops, followed by 4 days of technical content, all surrounded by social and intellectual exchanges. In addition to the familiar venues that form the core of the conference, we have also arranged various special events, such an keynotes by Howard Rheingold and Ethan Zuckerman; invited talks by ACM SIGCHI award winners Terry Winograd, Larry Tesler, Alan Newell, and Clayton Lewis; an HCI museum exhibit hosted by Bill Buxton; and a panel celebrating Stu Card's achievements and contributions to the field of HCI.With the record number of submissions and accepted content this year, we hope that you will utilize the print and electronic programs, but also the daily CHI Madness presentations that provide a glimpse of the day ahead. In the interest of continuing to evolve the conference to best serve our needs, we will experiment with shorter talks this year (20 minute slots for long pieces of content and 10 for shorter ones) to infuse even more energy into the program. We will also have a pretty full slate of social media applications to help you connect with other attendees and to provide you with the fullest experience possible.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Proceedings of the 27th international conference extended abstracts on Human factors in computing systems
    </title>
    <abstract>
      Welcome to CHI 2009! CHI comprises many events, ranging from archival material stored in the ACM digital library, to transient interactions such as the presentations, panels, and poster discussions, to the many social interactions and activities that make CHI a collegial and intimate experience. All parts are important, but it is the archival material -- especially the papers and notes -- that establishes CHI as the leading academic conference in Human Computer Interaction. Yet there are significant challenges in managing paper and notes within CHI. The HCI field has been very successful at creating new generations of research and practitioners over the years. The many people who are part of this community see CHI as the place to share their knowledge and experiences with others, primarily by publishing and presenting papers and notes. This has stressed the system in many ways. As submissions increase, so do the difficulties in managing the review process, finding good reviewers and other volunteers, matching papers to those competent in the subject matter, deciding which papers to accept or reject, maintaining consistent standards across both paper and notes, and not falling into the trap of overly narrowing our view of what is an 'acceptable' CHI paper.This year, we introduced several large changes to the CHI papers/notes process to mitigate these challenges, most which will be transparent to attendees. First, we reorganized the CHI program committee into nine topical subcommittees - each a mini program committee - comprising sub-committee chairs (SCs) and various associate chairs (ACs) knowledgeable on the topic. Authors could select the subcommittee that he or she felt could best handle their submission. We did this to improve the match of a submission to AC and ultimately to reviewers, to have more focused and relevant discussions in the program committee meeting, and to minimize the load on individual volunteers. Second, we combined papers and notes, where all were handled in exactly the same way. We did this to ensure a consistent decision standard across both submission types. Third, we introduced contribution types, where each type described a different way that a CHI submission could contribute to the field as well as typical questions such a contribution should address. Authors identified their submission by contribution type, and (hopefully) used the information to help structure their paper. The idea is that we wanted to encourage a broad variety of submissions from authors (rather than 'formula' papers), while also providing guidance to referees by supplying criteria appropriate to the type of contribution the submission was making.It will likely take several years before the full impact of these changes are known. We know that subcommittees did help us manage the large number of submissions. We also believe there was an overall better match between referees and submissions, and that papers and notes were handled consistently. We don't yet know about the effect of contribution types: this is a cultural change where we are hoping that authors will be more willing to write papers that don't match a particular formula, and that reviewers will be more accepting of those submissions.Now for the numbers. This year, there were 1130 submissions, comprising 711 full papers and 419 notes. This is the highest number of submissions ever to CHI. Of these, we accepted 24.5%. The papers/notes committee involved 107 volunteers: the 2 co-chairs, 10 sub-committee chairs, and 95 associate chairs (ACs). Each AC managed 10-14 submissions, and personally recruited at least three -- sometimes more -- referees knowledgeable in the paper's topic. Refereeing was through blind review. Each referee returned a recommendation along with a detailed review, and authors had opportunity to rebut these reviews. Additional reviews were sometimes solicited. Almost all program committee members then attended a two day meeting in Boston in December. Rigorous discussions took place at the PC meeting, and the majority of papers were read by a second AC as well. The decision process was highly visible so that the committee could calibrate itself.Finally, the various committees nominated 5% of the submissions as potential best papers. A separate committee deliberated over these papers, where only 1% of papers and notes received a best paper award. In total, as you will see in the program, 32 papers and four notes were designated as honorable mentions, while seven papers and four notes honored as best papers. Congratulations to all authors who achieved this significant status!
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Proceedings of the 2011 annual conference on Human factors in computing systems
    </title>
    <abstract>
      Over the last year or so, we have been blessed with the challenge, the opportunity, and the distinct pleasure of organizing the CHI 2011 Conference on Human Factors in Computing Systems, the premier international conference for the field of human-computer interaction.CHI 2011 takes place in gorgeous, energetic, sophisticated Vancouver BC, a city renowned for its innovation in entertainment, sustainability, accessibility, and inclusivity. The New York Times calls it, &quot;a liquid city, a tomorrow city, equal parts India, China, England, France and the Pacific Northwest.&quot; Vancouver lays a beautiful backdrop for our conference, which boasts nearly 30 years of wonderful work.Behind the success of the conference is our diverse community of faculty and students, of researchers and practitioners, of young and, well, also of experienced. It is a community of designers, technologists, psychologists, social scientists, biologists, artists, engineers, anthropologists, musicians; the list goes on. Wherever we are, we are always a community of near and far. Most impressively, ours is a community that cares deeply about innovating, learning, sharing, and interacting; all with the common goal of using technology to shape the way people around the world live and play.Returning attendees will recognize the general conference format - 2 days of small intimate workshops, followed by 4 days of technical content, all surrounded by social and intellectual exchanges. In addition to the familiar venues that form the core of the conference, we have also arranged various special events, such an keynotes by Howard Rheingold and Ethan Zuckerman; invited talks by ACM SIGCHI award winners Terry Winograd, Larry Tesler, Alan Newell, and Clayton Lewis; an HCI museum exhibit hosted by Bill Buxton; and a panel celebrating Stu Card's achievements and contributions to the field of HCI.With the record number of submissions and accepted content this year, we hope that you will utilize the print and electronic programs, but also the daily CHI Madness presentations that provide a glimpse of the day ahead. In the interest of continuing to evolve the conference to best serve our needs, we will experiment with shorter talks this year (20 minute slots for long pieces of content and 10 for shorter ones) to infuse even more energy into the program. We will also have a pretty full slate of social media applications to help you connect with other attendees and to provide you with the fullest experience possible.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Implicit: a multi-agent recommendation system for web search
    </title>
    <abstract>
      For people with non-ordinary interests, it is hard to search for information on the Internet because search engines are impersonalized and are more focused on &quot;average&quot; individuals with &quot;standard&quot; preferences. In order to improve web search for a community of people with similar but specific interests, we propose to use the implicit knowledge contained in the search behavior of groups of users. We developed a multi-agent recommendation system called Implicit, which supports web search for groups or communities of people. In Implicit, agents observe behavior of their users to learn about the &quot;culture&quot; of the community with specific interests. They facilitate sharing of knowledge about relevant links within the community by means of recommendations. The agents also recommend contacts, i.e., who in the community is the right person to ask for a specific topic. Experimental evaluation shows that Implicit improves the quality of the web search in terms of precision and recall.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Mobile Technology for Children: Designing for Interaction and Learning
    </title>
    <abstract>
      With the goal of improving the design of mobile technology for children, On the Move brings together contributions from HCI leaders in research, and industry, and technology and education based policy experts to analyze and evaluate and present solutions.  To show readers how they can apply each design problem and case study to their HCI professional or academic work, each chapter will contain best practice advice.In HCI, social implications, in addition to interface design, usability, and performance, are all part of an informed design solution. Chauncey Wilson, Senior User Researcher, Autodesk, Inc., and forthcoming MK author, states, “The design of mobile devices is not an algorithmic process, it is must be considered in a social context that examines culture, changing trends, and other factors. This proposal provides a solid foundation of the social and cultural factors that are critical in the design of mobile products for children.”There are many technological solutions to consider, many contexts to explore user scenarios and many goals for supporting learning. It contains the work of 43 authors from 9 countries, each deeply invested in improving and analyzing design of childrens mobile products.  These authors have diverse points of view, but it is a subject that deserves debate.  The trends, design and use of these products has been both lauded and criticized, and the debate is far from over.  The need has never been greater for an evaluation of the design and the affects of the design mobile technology as it pertains to children's products and learning – the good and the bad – especially for and by the people who conduct research, develop and design the products.*First book for HCI practitioners and researchers to present a multitude of voices on the design, technology, and impact of mobile devices for children from global perspective*Features contributions from leading HCI academics, professionals, and childrens technology policy leaders from nine countries*Each contribution and case study is followed by a best practice overview to help readers improve future research and design and for a quick reference at a later date
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Future Directions of Knowledge Systems Environments for Web 3.0
    </title>
    <abstract>
      The internet and web applications have changed business and human life. Nowadays everybody is used to obtain data through the internet. Most applications are still Web 1.0 applications. Web 2.0 community collaboration and annotated data on the basis of Web 3.0 technologies supports new businesses and applications. The quality dimension of the web is however one of the main challenges. Knowledge systems target at high-quality data on safe grounds, with a good reference to established science and technology and with data adaptation to user's needs and demands. Knowledge system can be build based on existing and novel technologies. This paper discusses the challenges, two solutions and the fundamentals of knowledge system environments.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Software Development Cultures and Cooperation Problems: A Field Study of the Early Stages of Development of Software for a Scientific Community
    </title>
    <abstract>
      In earlier work, I identified a particular class of end-user developers, who include scientists and whom I term `professional end-user developers', as being of especial interest. Here, I extend this work by articulating a culture of professional end-user development, and illustrating by means of a field-study how the influence of this culture causes cooperation problems in an inter-disciplinary team developing a software system for a scientific community. My analysis of the field study data is informed by some recent literature on multi-national work cultures. Whilst acknowledging that viewing a scientific development through a lens of software development culture does not give a full picture, I argue that it nonetheless provides deep insights.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Measuring data warehousing success: an empirical investigation applying the DeLone and McLean model
    </title>
    <abstract>
      The purpose of this study is to examine the factors that affect data warehousing success. The DeLone and McLean (1992) original model has been used as a backbone to construct an evaluation instrument which has been operationalised using data warehousing and IS success literature review and an exploratory case study conducted in a large European financial company. Two contextual variables: user participation and user involvement are introduced in the model and their roles on individual impacts of the data warehouse are tested. Our results confirm the determining power of information quality and user satisfaction as antecedents to the system use. Concerning the roles played by the contextual factors, only the direct effect of user involvement on individual impacts is supported.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Gesture activated mobile edutainment (GAME): intercultural training of nonverbal behavior with mobile phones
    </title>
    <abstract>
      An approach to intercultural training of nonverbal behavior is presented that draws from research on role-plays with virtual agents and ideas from situated learning. To this end, a mobile serious game is realized where the user acquires knowledge about German emblematic gestures and tries them out in role-plays with virtual agents. Gesture performance is evaluated making use of build-in acceleration sensors of smart phones. After an account of the theoretical background covering diverse areas like virtual agents, situated learning and intercultural training, the paper presents the GAME approach along with details on the gesture recognition and content authoring. By its experience-based role-plays with virtual characters, GAME brings together ideas from situated learning and intercultural training in an integrated approach and paves the way for new m-learning concepts.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Wave like an Egyptian: accelerometer based gesture recognition for culture specific interactions
    </title>
    <abstract>
      The user's behavior and his interpretation of interactions with others is influenced by his cultural background, which provides a number of heuristics or patterns of behavior and interpretation. This cultural influence on interaction has largely been neglected in HCI research due to two challenges: (i) grasping culture as a computational term and (ii) infering the user's cultural background by observable measures. In this paper, we describe how the Wiimote can be utilized to uncover the user's cultural background by analyzing his patterns of gestural expressivity in a model based on cultural dimensions. With this information at hand, the behavior of an interactive system can be adapted to culture-dependent patterns of interaction.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Discovering eye gaze behavior during human-agent conversation in an interactive storytelling application
    </title>
    <abstract>
      In this paper, we investigate the user's eye gaze behavior during the conversation with an interactive storytelling application. We present an interactive eye gaze model for embodied conversational agents in order to improve the experience of users participating in Interactive Storytelling. The underlying narrative in which the approach was tested is based on a classical XIXthcentury psychological novel: Madame Bovary, by Flaubert. At various stages of the narrative, the user can address the main character or respond to her using free-style spoken natural language input, impersonating her lover. An eye tracker was connected to enable the interactive gaze model to respond to user's current gaze (i.e. looking into the virtual character's eyes or not). We conducted a study with 19 students where we compared our interactive eye gaze model with a non-interactive eye gaze model that was informed by studies of human gaze behaviors, but had no information on where the user was looking. The interactive model achieved a higher score for user ratings than the non-interactive model. In addition we analyzed the users' gaze behavior during the conversation with the virtual character.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Cultural differences and design methods for user experience research: Dutch and Korean participants compared
    </title>
    <abstract>
      As business competition globalizes, understanding user experience from various cultures plays a crucial role in design process. However, because most user research methods were developed in Western area, one may question if the expected result can be obtained when applying them to totally different culture. The paper explores cultural effects on the feedback collected in the process and result of user experience research conducted in two countries, the Netherlands and Korea. We presumed four factors which influence user research process: spontaneity of participation, uncertainty avoidance, tendency of problem criticism, and attitude within a group. After the two sets of user research in two countries, actual differences of results were revealed. Consequently, guidelines of user experience research in Korea were suggested based on discovered differences.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Cultural user experience issues in e-government: designing for a multi-cultural society
    </title>
    <abstract>
      This paper examines the influences of culture on the user experience of local e-government services. It investigates the hypothesis that citizens with different cultural backgrounds experience different problems when using e-government applications. Thirty participants with Moroccan, Surinamese and Dutch cultural backgrounds completed a short questionnaire for demographic purposes. Then, they were observed while using a local e-government website. After tasks were completed, a short interview investigated user experience issues in more depth. By referring to existing literature on cross-cultural values and norms, the possible origins of differences in user experience problems for the cultural groups were explored. The findings suggest that differences in user problems coincide with expectations about cultural characteristics derived from previous literature. The findings of this paper support the notion that users with different cultural backgrounds experience different user problems.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      1
    </relevance>
  </item>
  <item>
    <title>
      Requirements analysis of presence: Insights from a RPG game
    </title>
    <abstract>
      Virtual worlds are computer-based simulations intended to give its users the impression of being in another place.Presence, or the sense of “being there,” is a major design requirement for virtual environments where users inhabit an artificial reality in the form of two or three-dimensional graphical representations. Promoting this subjective experience has always been one of the major concerns of designers, but this complex and difficult task requires the awareness of other design requirements and their effects on presence. This article aims to define various psychological and technological aspects of presence based on virtual environment design requirements defined by Stuart [2001].Previous research tried to define hypothesized factors of presence by using subjective user responses obtained from questionnaires. This study incorporates a different approach to define potential components of presence, specifying the individual design requirements for virtual worlds based on the conceptual framework designed by Stuart. This framework has not been applied to the analysis of the concept of presence before, and it defines possible factors that contribute to a sense of presence, some of which have not been included in previous work. In order to decompose presence into its components, researchers should also be aware of the design requirements delineated in this framework.Detailed analysis of design requirements will focus on a computer role-playing game (RPG), giving examples from one of the best titles in the genre. Since role-playing games are social and interactive worlds where players assume the role of a virtual character that can be subjectively defined as a second-self, they are highly relevant to presence research. Thus, selected design requirements will be discussed from a computer-gaming perspective by defining how each relevant requirement is addressed on the selected RPG, and how they should be addressed by game designers.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>
  <item>
    <title>
      Mobile personalization at large sports events user experience and mobile device personalization
    </title>
    <abstract>
      Mobile personalization is frequently discussed, and has been shown in relation to a number of usage scenarios. However, this research has focused mainly on technology development. There have been few studies of mobile user experience, and personalization in sports. This paper is devoted to the new field of studying the user experience related to mobile personalization at large sports events (LSE). In order to support and enrich the user experience at LSE with mobile personalization, this study investigates the current audience experience at stadiums and derives the usage patterns that device personalization could usefully support in this context.
    </abstract>
    <search_task_number>
      10
    </search_task_number>
    <query>
      intercultural user interfaces
    </query>
    <relevance>
      0
    </relevance>
  </item>



  <item>
    <title>Analysing cache effects in distribution sorting</title>
	<abstract>We study cache effects in distribution sorting algorithms for sorting keys drawn independently at random from a uniform distribution ('uniform keys'). We note that the performance of a recently-published distribution sorting algorithm, Flashsort1, which sorts <i>n</i> uniform floating-point keys in <i>O(n)</i> expected time, does not scale well with the input size due to poor cache utilisation. We present an approximate analysis for distribution sorting uniform keys which, as validated by simulation results, predicts the expected cache misses of Flashsort1 quite well. Using this analysis, we design a multiple-pass variant of Flashsort1 which outperforms Flashsort1 and comparison-based algorithms on uniform floating-point keys for moderate to large values of <i>n</i>. Using experimental results we also show that the integer distribution sorting algorithm MSB radix sort performs well on both uniform integer and uniform floating-point keys.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Sorting improves word-aligned bitmap indexes</title>
	<abstract>Bitmap indexes must be compressed to reduce input/output costs and minimize CPU usage. To accelerate logical operations (AND, OR, XOR) over bitmaps, we use techniques based on run-length encoding (RLE), such as Word-Aligned Hybrid (WAH) compression. These techniques are sensitive to the order of the rows: a simple lexicographical sort can divide the index size by 9 and make indexes several times faster. We investigate row-reordering heuristics. Simply permuting the columns of the table can increase the sorting efficiency by 40%. Secondary contributions include efficient algorithms to construct and aggregate bitmaps. The effect of word length is also reviewed by constructing 16-bit, 32-bit and 64-bit indexes. Using 64-bit CPUs, we find that 64-bit indexes are slightly faster than 32-bit indexes despite being nearly twice as large. </abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Reconfigurable Computing: The Theory and Practice of FPGA-Based Computation</title>
	<abstract>The main characteristic of Reconfigurable Computing is the presence of hardware that can be reconfigured to implement specific functionality more suitable for specially tailored hardware than on a simple uniprocessor. Reconfigurable computing systems join microprocessors and programmable hardware in order to take advantage of the combined strengths of hardware and software and have been used in applications ranging from embedded systems to high performance computing. Many of the fundamental theories have been identified and used by the Hardware/Software Co-Design research field. Although the same background ideas are shared in both areas, they have different goals and use different approaches.This book is intended as an introduction to the entire range of issues important to reconfigurable computing, using FPGAs as the context, or "computing vehicles" to implement this powerful technology. It will take a reader with a background in the basics of digital design and software programming and provide them with the knowledge needed to be an effective designer or researcher in this rapidly evolving field.

    Treatment of FPGAs as computing vehicles rather than glue-logic or ASIC substitutes
    Views of FPGA programming beyond Verilog/VHDL
    Broad set of case studies demonstrating how to use FPGAs in novel and efficient ways</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Algorithms and data structures for external memory</title>
	<abstract>Data sets in large applications are often too massive to fit completely inside the computer's internal memory. The resulting input/output communication (or I/O) between fast internal memory and slower external memory (such as disks) can be a major performance bottleneck. In this manuscript, we survey the state of the art in the design and analysis of algorithms and data structures for external memory (or EM for short), where the goal is to exploit locality and parallelism in order to reduce the I/O costs. We consider a variety of EM paradigms for solving batched and online problems efficiently in external memory.

    For the batched problem of sorting and related problems like permuting and fast Fourier transform, the key paradigms include distribution and merging. The paradigm of disk striping offers an elegant way to use multiple disks in parallel. For sorting, however, disk striping can be nonoptimal with respect to I/O, so to gain further improvements we discuss distribution and merging techniques for using the disks independently. We also consider useful techniques for batched EM problems involving matrices, geometric data, and graphs.

    In the online domain, canonical EM applications include dictionary lookup and range searching. The two important classes of indexed data structures are based upon extendible hashing and B-trees. The paradigms of filtering and bootstrapping provide convenient means in online data structures to make effective use of the data accessed from disk. We also re-examine some of the above EM problems in slightly different settings, such as when the data items are moving, when the data items are variable-length such as character strings, when the data structure is compressed to save space, or when the allocated amount of internal memory can change dynamically.

    Programming tools and environments are available for simplifying the EM programming task. We report on some experiments in the domain of spatial databases using the TPIE system (Transparent Parallel I/O programming Environment). The newly developed EM algorithms and data structures that incorporate the paradigms we discuss are significantly faster than other methods used in practice.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>New algorithms for join and grouping operations</title>
	<abstract>Traditional database query processing relies on three types of algorithms for join and for grouping operations. For joins, index nested loops join exploits an index on its inner input, merge join exploits sorted inputs, and hash join exploits differences in the sizes of the join inputs. For grouping, an index-based algorithm has been used in the past whereas today sort- and hash-based algorithms prevail. Cost-based query optimization chooses the most appropriate algorithm for each query and for each operation. Unfortunately, mistaken algorithm choices during compile-time query optimization are common yet expensive to investigate and to resolve.

    Our goal is to end mistaken choices among join algorithms and among grouping algorithms by replacing the three traditional types of algorithms with a single one. Like merge join, this new join algorithm exploits sorted inputs. Like hash join, it exploits different input sizes for unsorted inputs. In fact, for unsorted inputs, the cost functions for recursive hash join and for hybrid hash join have guided our search for the new join algorithm. In consequence, the new join algorithm can replace both merge join and hash join in a database management system.

    The in-memory components of the new join algorithm employ indexes. If the database contains indexes for one (or both) of the inputs, the new join can exploit persistent indexes instead of temporary in-memory indexes. Using database indexes to find matching input records, the new join algorithm can also replace index nested loops join.

    In addition to join operations, a very similar algorithm supports grouping ("group by" queries in SQL) and duplicate elimination. For unsorted inputs, candidate output records take on the role of one of the inputs in a join operation. Our goal is to define a single grouping algorithm that can replace grouping by repeated index searches, by sorting, and by hashing. In other words, our goal is to end mistaken algorithm choices not only for joins and other binary matching operations but also for grouping and other unary matching operations in database query processing.

    Finally, these new algorithms can be instrumental for efficient and robust data processing in a map-reduce environment, because `map' and `reduce' operations are similar in essentials to join and grouping operations.

    Results from an implementation of the core algorithm are reported.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Visualization in Medicine: Theory, Algorithms, and Applications</title>
	<abstract>Visualization in Medicine is the first book on visualization and its application to problems in medical diagnosis, education, and treatment. The book describes the algorithms, the applications and their validation (how reliable are the results?), and the clinical evaluation of the applications (are the techniques useful?). It discusses visualization techniques from research literature as well as the compromises required to solve practical clinical problems.

    The book covers image acquisition, image analysis, and interaction techniques designed to explore and analyze the data. The final chapter shows how visualization is used for planning liver surgery, one of the most demanding surgical disciplines. The book is based on several years of the authors' teaching and research experience. Both authors have initiated and lead a variety of interdisciplinary projects involving computer scientists and medical doctors, primarily radiologists and surgeons.

    * A core field of visualization and graphics missing a dedicated book until now
    * Written by pioneers in the field and illustrated in full color
    * Covers theory as well as practice</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Join operations in temporal databases</title>
	<abstract>Joins are arguably the most important relational operators. Poor implementations are tantamount to computing the Cartesian product of the input relations. In a temporal database, the problem is more acute for two reasons. First, conventional techniques are designed for the evaluation of joins with equality predicates rather than the inequality predicates prevalent in valid-time queries. Second, the presence of temporally varying data dramatically increases the size of a database. These factors indicate that specialized techniques are needed to efficiently evaluate temporal joins.We address this need for efficient join evaluation in temporal databases. Our purpose is twofold. We first survey all previously proposed temporal join operators. While many temporal join operators have been defined in previous work, this work has been done largely in isolation from competing proposals, with little, if any, comparison of the various operators. We then address evaluation algorithms, comparing the applicability of various algorithms to the temporal join operators and describing a performance study involving algorithms for one important operator, the temporal equijoin. Our focus, with respect to implementation, is on non-index-based join algorithms. Such algorithms do not rely on auxiliary access paths but may exploit sort orderings to achieve efficiency.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Proceedings of the 11th Annual conference on Genetic and evolutionary computation</title>
	<abstract>These proceedings contain the papers presented at the 11th Annual Genetic and Evolutionary Computation Conference (GECCO-2009), held in Montreal, Canada, July 8-12, 2009.

    After 2007, when GECCO was held in London, UK, this is the second time GECCO has been held outside the U.S. The generally high number of submissions of previous events has been maintained: 531 papers have been submitted for review, which is an increase of about 18% when compared to last year. Of these 531 papers, 220 were accepted as eight-page publications and 25 minutes presentations at the conference, yielding an acceptance ratio of 41,4%. In addition, 137 submissions (25,8%) have been accepted for poster presentations with two-page abstracts included in the proceedings. Last year, GECCO successfully moved over to electronic proceedings, and we continued with this publishing strategy as it greatly facilitates the handling of all conference materials.

    GECCO has lived up to its motto of one conference, many mini-conferences. This year, there were 15 separate tracks that operated independently from each other. Each track had its own track chair(s) and individual program committee. A member of one track's program committee was not allowed to simultaneously be a member of another track's committee. To reduce any bias reviewers might have, all reviews were conducted double blind, no authors' names were included in the reviewed papers. About 600 researchers participated in the reviewing process. We want to thank them for all their work, which is highly appreciated and absolutely vital for the quality of the conference.

    Track chairs have been asked to not accept more than 50% of their submissions as full papers. An appropriate acceptance rate is important in order to preserve the quality of the conference. Even though we were not bound by strong physical or environmental limitations on the number of accepted papers, we strove to keep our acceptance rate at the lower end. The scientific quality of the conference as well as that of the proceedings also is ensured by principles laid down in the GECCO by-laws of SIGEVO: (i) The GECCO conference shall be a broad-based conference encompassing the whole field of genetic and evolutionary computation. (ii) Papers will be published and presented as part of the main conference proceedings only after being peer reviewed. No invited papers shall be published (except for those of up to three invited plenary speakers). (iii) The peer review process shall be conducted consistent with the principle of division of powers performed by a multiplicity of independent program committees, each with expertise in the area of the paper being reviewed. (iv) The determination of the policy for the peer review process for each of the conference's independent program committees and the reviewing of papers for each program committee shall be performed by persons who occupy their positions by virtue of meeting objective and explicitly stated qualifications based on their previous scientific research activity or applications activity. (v) Emerging areas within the field of genetic and evolutionary computation shall be actively encouraged and incorporated in the activities of the conference by providing a semi-automatic method for their inclusion into the activities of the conference (with some procedural flexibility being extended to such emerging new areas). (vi) The percentage of submitted papers that are accepted as regular papers (i.e., papers other than poster papers) shall not exceed 50%.

    In addition to the presentation of the papers contained in these proceedings, GECCO-2009 also included free tutorials, workshops, a series of sessions on Evolutionary Computation in Practice, various competitions, late-breaking papers, and a job shop.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Low-Power Design of Nanometer FPGAs: Architecture and EDA</title>
	<abstract>Low-Power Design of Nanometer FPGAs Architecture and EDA is an invaluable reference for researchers and practicing engineers concerned with power-efficient, FPGA design. State-of-the-art power reduction techniques for FPGAs will be described and compared. These techniques can be applied at the circuit, architecture, and electronic design automation levels to describe both the dynamic and leakage power sources and enable strategies for codesign.


    Low-power techniques presented at key FPGA design levels for circuits, architectures, and electronic design automation, form critical, "bridge" guidelines for codesign

    Comprehensive review of leakage-tolerant techniques empowers designers to minimize power dissipation

    Provides valuable tools for estimating power efficiency/savings of current, low-power FPGA design techniques</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>System Level Design with Rosetta</title>
	<abstract>The steady and unabated increase in the capacity of silicon has brought the semiconductor industry to a watershed challenge. Now a single chip can integrate a radio transceiver, a network interface, multimedia functions, all the "glue" needed to hold it together as well as a design that allows the hardware and software to be reconfigured for future applications. Such complex heterogeneous systems demand a different design methodology. A consortium of industrial and government labs have created a new language and a new design methodology to support this effort. Rosetta permits designers to specify requirements and constraints independent of their low level implementation and to integrate the designs of domains as distinct as digital and analog electronics, and the mechanical, optical, fluidic and thermal subsystems with which they interact.

    In this book, Perry Alexander, one of the developers of Rosetta, provides a tutorial introduction to the language and the system-level design methodology it was designed to support.

    * The first commercially published book on this system-level design language

    * Teaches you all you need to know on how to specify, define, and generate models in Rosetta

    * A presentation of complete case studies analyzing design trade-offs for power consumption, security requirements in a networking environment, and constraints for hardware/software co-design</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Disappearing Cryptography: Information Hiding: Steganography &amp; Watermarking</title>
	<abstract>Cryptology is the practice of hiding digital information by means of various obfuscatory and steganographic techniques. The application of said techniques facilitates message confidentiality and sender/receiver identity authentication, and helps to ensure the integrity and security of computer passwords, ATM card information, digital signatures, DVD and HDDVD content, and electronic commerce. Cryptography is also central to digital rights management (DRM), a group of techniques for technologically controlling the use of copyrighted material that is being widely implemented and deployed at the behest of corporations that own and create revenue from the hundreds of thousands of mini-transactions that take place daily on programs like iTunes.

    This new edition of our best-selling book on cryptography and information hiding delineates a number of different methods to hide information in all types of digital media files. These methods include encryption, compression, data embedding and watermarking, data mimicry, and scrambling. During the last 5 years, the continued advancement and exponential increase of computer processing power have enhanced the efficacy and scope of electronic espionage and content appropriation. Therefore, this edition has amended and expanded outdated sections in accordance with new dangers, and includes 5 completely new chapters that introduce newer more sophisticated and refined cryptographic algorithms and techniques (such as fingerprinting, synchronization, and quantization) capable of withstanding the evolved forms of attack.

    Each chapter is divided into sections, first providing an introduction and high-level summary for those who wish to understand the concepts without wading through technical explanations, and then presenting concrete examples and greater detail for those who want to write their own programs. This combination of practicality and theory allows programmers and system designers to not only implement tried and true encryption procedures, but also consider probable future developments in their designs, thus fulfilling the need for preemptive caution that is becoming ever more explicit as the transference of digital media escalates.


    * Includes 5 completely new chapters that delineate the most current and sophisticated cryptographic algorithms, allowing readers to protect their information against even the most evolved electronic attacks.

    * Conceptual tutelage in conjunction with detailed mathematical directives allows the reader to not only understand encryption procedures, but also to write programs which anticipate future security developments in their design.

    * Grants the reader access to online source code which can be used to directly implement proven cryptographic procedures such as data mimicry and reversible grammar generation into their own work.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>External memory algorithms and data structures: dealing with massive data</title>
	<abstract>Data sets in large applications are often too massive to fit completely inside the computers internal memory. The resulting input/output communication (or I/O) between fast internal memory and slower external memory (such as disks) can be a major performance bottleneck. In this article we survey the state of the art in the design and analysis of external memory (or EM) algorithms and data structures, where the goal is to exploit locality in order to reduce the I/O costs. We consider a variety of EM paradigms for solving batched and online problems efficiently in external memory. For the batched problem of sorting and related problems such as permuting and fast Fourier transform, the key paradigms include distribution and merging. The paradigm of disk striping offers an elegant way to use multiple disks in parallel. For sorting, however, disk striping can be nonoptimal with respect to I/O, so to gain further improvements we discuss distribution and merging techniques for using the disks independently. We also consider useful techniques for batched EM problems involving matrices (such as matrix multiplication and transposition), geometric data (such as finding intersections and constructing convex hulls), and graphs (such as list ranking, connected components, topological sorting, and shortest paths). In the online domain, canonical EM applications include dictionary lookup and range searching. The two important classes of indexed data structures are based upon extendible hashing and B-trees. The paradigms of filtering and bootstrapping provide a convenient means in online data structures to make effective use of the data accessed from disk. We also reexamine some of the above EM problems in slightly different settings, such as when the data items are moving, when the data items are variable-length (e.g., text strings), or when the allocated amount of internal memory can change dynamically. Programming tools and environments are available for simplifying the EM programming task. During the course of the survey, we report on some experiments in the domain of spatial databases using the TPIE system (transparent parallel I/O programming environment). The newly developed EM algorithms and data structures that incorporate the paradigms we discuss are significantly faster than methods currently used in practice.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>GPGPU: general purpose computation on graphics hardware</title>
	<abstract>The graphics processor (GPU) on today's commodity video cards has evolved into an extremely powerful and flexible processor. The latest graphics architectures provide tremendous memory bandwidth and computational horsepower, with fully programmable vertex and pixel processing units that support vector operations up to full IEEE floating point precision. High level languages have emerged for graphics hardware, making this computational power accessible. Architecturally, GPUs are highly parallel streaming processors optimized for vector operations, with both MIMD (vertex) and SIMD (pixel) pipelines. Not surprisingly, these processors are capable of general-purpose computation beyond the graphics applications for which they were designed. Researchers have found that exploiting the GPU can accelerate some problems by over an order of magnitude over the CPU.However, significant barriers still exist for the developer who wishes to use the inexpensive power of commodity graphics hardware, whether for in-game simulation of physics of for conventional computational science. These chips are designed for and driven by video game development; the programming model is unusual, the programming environment is tightly constrained, and the underlying architectures are largely secret. The GPU developer must be an expert in computer graphics and its computational idioms to make effective use of the hardware, and still pitfalls abound. This course provides a detailed introduction to general purpose computation on graphics hardware (GPGPU). We emphasize core computational building blocks, ranging from linear algebra to database queries, and review the tools, perils, and tricks of the trade in GPU programming. Finally we present some interesting and important case studies on general-purpose applications of graphics hardware.The course presenters are experts on general-purpose GPU computation from academia and industry, and have presented papers and tutorials on the topic at SIGGRAPH, Graphics Hardware, Game Developers Conference, and elsewhere.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Revised6 report on the algorithmic language scheme</title>
	<abstract></abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Revised6 report on the algorithmic language scheme</title>
	<abstract>Programming languages should be designed not by piling feature on top of feature, but by removing the weaknesses and restrictions that make additional features appear necessary. Scheme demonstrates that a very small number of rules for forming expressions, with no restrictions on how they are composed, suffice to form a practical and efficient programming language that is flexible enough to support most of the major programming paradigms in use today.

    Scheme was one of the first programming languages to incorporate first-class procedures as in the lambda calculus, thereby proving the usefulness of static scope rules and block structure in a dynamically typed language. Scheme was the first major dialect of Lisp to distinguish procedures from lambda expressions and symbols, to use a single lexical environment for all variables, and to evaluate the operator position of a procedure call in the same way as an operand position. By relying entirely on procedure calls to express iteration, Scheme emphasized the fact that tail-recursive procedure calls are essentially gotos that pass arguments. Scheme was the first widely used programming language to embrace first-class escape procedures, from which all previously known sequential control structures can be synthesized. A subsequent version of Scheme introduced the concept of exact and inexact number objects, an extension of Common Lisp's generic arithmetic. More recently, Scheme became the first programming language to support hygienic macros, which permit the syntax of a block-structured language to be extended in a consistent and reliable manner.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Compactly encoding unstructured inputs with differential compression</title>
	<abstract>The subject of this article is differential compression, the algorithmic task of finding common strings between versions of data and using them to encode one version compactly by describing it as a set of changes from its companion. A main goal of this work is to present new differencing algorithms that (i) operate at a fine granularity (the atomic unit of change), (ii) make no assumptions about the format or alignment of input data, and (iii) in practice use linear time, use constant space, and give good compression. We present new algorithms, which do not always compress optimally but use considerably less time or space than existing algorithms. One new algorithm runs in O(n) time and O(1) space in the worst case (where each unit of space contains [log n] bits), as compared to algorithms that run in O(n) time and O(n) space or in O(n2) time and O(1) space. We introduce two new techniques for differential compression and apply these to give additional algorithms that improve compression and time performance. We experimentally explore the properties of our algorithms by running them on actual versioned data. Finally, we present theoretical results that limit the compression power of differencing algorithms that are restricted to making only a single pass over the data.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A survey of lower bounds for satisfiability and related problems</title>
	<abstract>Ever since the fundamental work of Cook from 1971, satisfiability has been recognized as a central problem in computational complexity. It is widely believed to be intractable, and yet till recently even a linear-time, logarithmic-space algorithm for satisfiability was not ruled out. In 1997 Fortnow, building on earlier work by Kannan, ruled out such an algorithm. Since then there has been a significant amount of progress giving non-trivial lower bounds on the computational complexity of satisfiability. In this article, we survey the known lower bounds for the time and space complexity of satisfiability and closely related problems on deterministic, randomized, and quantum models with random access. We discuss the state-of-the-art results and present the underlying arguments in a unified framework.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Proceedings of the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers</title>
	<abstract>These proceedings contain the papers presented at the 11th Annual Genetic and Evolutionary Computation Conference (GECCO-2009), held in Montreal, Canada, July 8-12, 2009.

    After 2007, when GECCO was held in London, UK, this is the second time GECCO has been held outside the U.S. The generally high number of submissions of previous events has been maintained: 531 papers have been submitted for review, which is an increase of about 18% when compared to last year. Of these 531 papers, 220 were accepted as eight-page publications and 25 minutes presentations at the conference, yielding an acceptance ratio of 41,4%. In addition, 137 submissions (25,8%) have been accepted for poster presentations with two-page abstracts included in the proceedings. Last year, GECCO successfully moved over to electronic proceedings, and we continued with this publishing strategy as it greatly facilitates the handling of all conference materials.

    GECCO has lived up to its motto of one conference, many mini-conferences. This year, there were 15 separate tracks that operated independently from each other. Each track had its own track chair(s) and individual program committee. A member of one track's program committee was not allowed to simultaneously be a member of another track's committee. To reduce any bias reviewers might have, all reviews were conducted double blind, no authors' names were included in the reviewed papers. About 600 researchers participated in the reviewing process. We want to thank them for all their work, which is highly appreciated and absolutely vital for the quality of the conference.

    Track chairs have been asked to not accept more than 50% of their submissions as full papers. An appropriate acceptance rate is important in order to preserve the quality of the conference. Even though we were not bound by strong physical or environmental limitations on the number of accepted papers, we strove to keep our acceptance rate at the lower end. The scientific quality of the conference as well as that of the proceedings also is ensured by principles laid down in the GECCO by-laws of SIGEVO: (i) The GECCO conference shall be a broad-based conference encompassing the whole field of genetic and evolutionary computation. (ii) Papers will be published and presented as part of the main conference proceedings only after being peer reviewed. No invited papers shall be published (except for those of up to three invited plenary speakers). (iii) The peer review process shall be conducted consistent with the principle of division of powers performed by a multiplicity of independent program committees, each with expertise in the area of the paper being reviewed. (iv) The determination of the policy for the peer review process for each of the conference's independent program committees and the reviewing of papers for each program committee shall be performed by persons who occupy their positions by virtue of meeting objective and explicitly stated qualifications based on their previous scientific research activity or applications activity. (v) Emerging areas within the field of genetic and evolutionary computation shall be actively encouraged and incorporated in the activities of the conference by providing a semi-automatic method for their inclusion into the activities of the conference (with some procedural flexibility being extended to such emerging new areas). (vi) The percentage of submitted papers that are accepted as regular papers (i.e., papers other than poster papers) shall not exceed 50%.

    In addition to the presentation of the papers contained in these proceedings, GECCO-2009 also included free tutorials, workshops, a series of sessions on Evolutionary Computation in Practice, various competitions, late-breaking papers, and a job shop.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A Dynamically Tuned Sorting Library</title>
	<abstract>Empirical search is a strategy used during the installation oflibrary generators such as ATLAS, FFTW, and SPIRAL to identify the algorithm or the version of an algorithm that delivers thebest performance. In the past, empirical search has been appliedalmost exclusively to scientific problems. In this paper, we discuss the application of empirical search to sorting, which is oneof the best understood symbolic computing problems. When contrasted with the dense numerical computations of ATLAS, FFTW,and SPIRAL, sorting presents a new challenge, namely that the relative performance of the algorithms depend not only on the characteristics of the target machine and the size of the input data but also on the distribution of values in the input data set.Empirical search is applied in the study reported here as partof a sorting library generator. The resulting routines dynamicallyadapt to the characteristics of the input data by selecting the bestsorting algorithm from a small set of alternatives. To generate therun time selection mechanism our generator makes use of machinelearning to predict the best algorithm as a function of the characteristics of the input data set and the performance of the differentalgorithms on the target machine. This prediction is based on thedata obtained through empirical search at installation time.Our results show that our approach is quite effective. Whensorting data inputs of 12M keys with various standard deviations,our adaptive approach selected the best algorithm for all the inputdata sets and all platforms that we tried in our experiments. Thewrong decision could have introduced a performance degradationof up to 133%, with an average value of 44%.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Implementing sorting in database systems</title>
	<abstract>Most commercial database systems do (or should) exploit many sorting techniques that are publicly known, but not readily available in the research literature. These techniques improve both sort performance on modern computer systems and the ability to adapt gracefully to resource fluctuations in multiuser operations. This survey collects many of these techniques for easy reference by students, researchers, and product developers. It covers in-memory sorting, disk-based external sorting, and considerations that apply specifically to sorting in database systems.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Describing shapes by geometrical-topological properties of real functions</title>
	<abstract>Differential topology, and specifically Morse theory, provide a suitable setting for formalizing and solving several problems related to shape analysis. The fundamental idea behind Morse theory is that of combining the topological exploration of a shape with quantitative measurement of geometrical properties provided by a real function defined on the shape. The added value of approaches based on Morse theory is in the possibility of adopting different functions as shape descriptors according to the properties and invariants that one wishes to analyze. In this sense, Morse theory allows one to construct a general framework for shape characterization, parametrized with respect to the mapping function used, and possibly the space associated with the shape. The mapping function plays the role of a lens through which we look at the properties of the shape, and different functions provide different insights.

    In the last decade, an increasing number of methods that are rooted in Morse theory and make use of properties of real-valued functions for describing shapes have been proposed in the literature. The methods proposed range from approaches which use the configuration of contours for encoding topographic surfaces to more recent work on size theory and persistent homology. All these have been developed over the years with a specific target domain and it is not trivial to systematize this work and understand the links, similarities, and differences among the different methods. Moreover, different terms have been used to denote the same mathematical constructs, which often overwhelm the understanding of the underlying common framework.

    The aim of this survey is to provide a clear vision of what has been developed so far, focusing on methods that make use of theoretical frameworks that are developed for classes of real functions rather than for a single function, even if they are applied in a restricted manner. The term geometrical-topological used in the title is meant to underline that both levels of information content are relevant for the applications of shape descriptions: geometrical, or metrical, properties and attributes are crucial for characterizing specific instances of features, while topological properties are necessary to abstract and classify shapes according to invariant aspects of their geometry. The approaches surveyed will be discussed in detail, with respect to theory, computation, and application. Several properties of the shape descriptors will be analyzed and compared. We believe this is a crucial step to exploit fully the potential of such approaches in many applications, as well as to identify important areas of future research.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>An experimental study of sorting and branch prediction</title>
	<abstract>Sorting is one of the most important and well-studied problems in computer science. Many good algorithms are known which offer various trade-offs in efficiency, simplicity, memory use, and other factors. However, these algorithms do not take into account features of modern computer architectures that significantly influence performance. Caches and branch predictors are two such features and, while there has been a significant amount of research into the cache performance of general purpose sorting algorithms, there has been little research on their branch prediction properties. In this paper, we empirically examine the behavior of the branches in all the most common sorting algorithms. We also consider the interaction of cache optimization on the predictability of the branches in these algorithms. We find insertion sort to have the fewest branch mispredictions of any comparison-based sorting algorithm, that bubble and shaker sort operate in a fashion that makes their branches highly unpredictable, that the unpredictability of shellsort's branches improves its caching behavior, and that several cache optimizations have little effect on mergesort's branch mispredictions. We find also that optimizations to quicksort, for example the choice of pivot, have a strong influence on the predictability of its branches. We point out a simple way of removing branch instructions from a classic heapsort implementation and also show that unrolling a loop in a cache-optimized heapsort implementation improves the predicitability of its branches. Finally, we note that when sorting random data two-level adaptive branch predictors are usually no better than simpler bimodal predictors. This is despite the fact that two-level adaptive predictors are almost always superior to bimodal predictors, in general.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Distributed learning with data reduction</title>
	<abstract>The work deals with the distributed machine learning. Distributed learning from data is considered to be an important challenge faced by researchers and practice in the domain of the distributed data mining and distributed knowledge discovery from databases. Currently, learning from data is recognized as one of the most widely investigated paradigms of machine learning. At the same time it is perceived as a difficult and demanding computational problem. Even more complex and still to a large extent open is learning from the distributed data. One of the approaches suitable for learning from the geographically distributed data is to select from the local databases relevant local patterns, called also prototypes. Such prototypes are selected using some specialized data reduction methods. The dissertation contains an overview of the problem of learning classifiers from data, followed by a discussion of the distributed learning. The above includes the problem formulation and the state-of-the-art review. Next, data reduction, approaches, techniques and algorithms are discussed. The central part of the dissertation proposes an agent-based distributed learning framework. The idea is to carry-out data reduction in parallel in separate locations, employing specialized software agents. The process ends when locally selected prototypes are moved to a central site and merged into the global knowledge model. The following part of the work contains the results of an extensive computational experiment aiming at validation of the proposed approach. Finally, conclusions and suggestions for further research are formulated.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>An experimental analysis of self-adjusting computation</title>
	<abstract>Recent work on adaptive functional programming (AFP) developed techniques for writing programs that can respond to modifications to their data by performing change propagation. To achieve this, executions of programs are represented with dynamic dependence graphs (DDGs) that record data dependences and control dependences in a way that a change-propagation algorithm can update the computation as if the program were from scratch, by re-executing only the parts of the computation affected by the changes. Since change-propagation only re-executes parts of the computation, it can respond to certain incremental modifications asymptotically faster than recomputing from scratch, potentially offering significant speedups. Such asymptotic speedups, however, are rare: for many computations and modifications, change propagation is no faster than recomputing from scratch.

    In this article, we realize a duality between dynamic dependence graphs and memoization, and combine them to give a change-propagation algorithm that can dramatically increase computation reuse. The key idea is to use DDGs to identify and re-execute the parts of the computation that are affected by modifications, while using memoization to identify the parts of the computation that remain unaffected by the changes. We refer to this approach as self-adjusting computation. Since DDGs are imperative, but (traditional) memoization requires purely functional computation, reusing computation correctly via memoization becomes a challenge. We overcome this challenge with a technique for remembering and reusing not just the results of function calls (as in conventional memoization), but their executions represented with DDGs. We show that the proposed approach is realistic by describing a library for self-adjusting computation, presenting efficient algorithms for realizing the library, and describing and evaluating an implementation. Our experimental evaluation with a variety of applications, ranging from simple list primitives to more sophisticated computational geometry algorithms, shows that the approach is effective in practice: compared to recomputing from-scratch; self-adjusting programs respond to small modifications to their data orders of magnitude faster.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>GPUTeraSort: high performance graphics co-processor sorting for large database management</title>
	<abstract>We present a novel external sorting algorithm using graphics processors (GPUs) on large databases composed of billions of records and wide keys. Our algorithm uses the data parallelism within a GPU along with task parallelism by scheduling some of the memory-intensive and compute-intensive threads on the GPU. Our new sorting architecture provides multiple memory interfaces on the same PC -- a fast and dedicated memory interface on the GPU along with the main memory interface for CPU computations. As a result, we achieve higher memory bandwidth as compared to CPU-based algorithms running on commodity PCs. Our approach takes into account the limited communication bandwidth between the CPU and the GPU, and reduces the data communication between the two processors. Our algorithm also improves the performance of disk transfers and achieves close to peak I/O performance. We have tested the performance of our algorithm on the SortBenchmark and applied it to large databases composed of a few hundred Gigabytes of data. Our results on a 3 GHz Pentium IV PC with $300 NVIDIA 7800 GT GPU indicate a significant performance improvement over optimized CPU-based algorithms on high-end PCs with 3.6 GHz Dual Xeon processors. Our implementation is able to outperform the current high-end PennySort benchmark and results in a higher performance to price ratio. Overall, our results indicate that using a GPU as a co-processor can significantly improve the performance of sorting algorithms on large databases.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Cache-oblivious databases: Limitations and opportunities</title>
	<abstract>Cache-oblivious techniques, proposed in the theory community, have optimal asymptotic bounds on the amount of data transferred between any two adjacent levels of an arbitrary memory hierarchy. Moreover, this optimal performance is achieved without any hardware platform specific tuning. These properties are highly attractive to autonomous databases, especially because the hardware architectures are becoming increasingly complex and diverse.

    In this article, we present our design, implementation, and evaluation of the first cache-oblivious in-memory query processor, EaseDB. Moreover, we discuss the inherent limitations of the cache-oblivious approach as well as the opportunities given by the upcoming hardware architectures. Specifically, a cache-oblivious technique usually requires sophisticated algorithm design to achieve a comparable performance to its cache-conscious counterpart. Nevertheless, this development-time effort is compensated by the automaticity of performance achievement and the reduced ownership cost. Furthermore, this automaticity enables cache-oblivious techniques to outperform their cache-conscious counterparts in multi-threading processors.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Empirical hardness models: Methodology and a case study on combinatorial auctions</title>
	<abstract>Is it possible to predict how long an algorithm will take to solve a previously-unseen instance of an NP-complete problem? If so, what uses can be found for models that make such predictions? This article provides answers to these questions and evaluates the answers experimentally.

    We propose the use of supervised machine learning to build models that predict an algorithm's runtime given a problem instance. We discuss the construction of these models and describe techniques for interpreting them to gain understanding of the characteristics that cause instances to be hard or easy. We also present two applications of our models: building algorithm portfolios that outperform their constituent algorithms, and generating test distributions that emphasize hard problems.

    We demonstrate the effectiveness of our techniques in a case study of the combinatorial auction winner determination problem. Our experimental results show that we can build very accurate models of an algorithm's running time, interpret our models, build an algorithm portfolio that strongly outperforms the best single algorithm, and tune a standard benchmark suite to generate much harder problem instances.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Join algorithm costs revisited</title>
	<abstract>A method of analysing join algorithms based upon the time required to access, transfer and perform the relevant CPU-based operations on a disk page is proposed. The costs of variations of several of the standard join algorithms, including nested block, sort-merge, GRACE hash and hybrid hash, are presented. For a given total buffer size, the cost of these join algorithms depends on the parts of the buffer allocated for each purpose. For example, when joining two relations using the nested block join algorithm, the amount of buffer space allocated for the outer and inner relations can significantly affect the cost of the join. Analysis of expected and experimental results of various join algorithms show that a combination of the optimal nested block and optimal GRACE hash join algorithms usually provide the greatest cost benefit, unless the relation size is a small multiple of the memory size. Algorithms to quickly determine a buffer allocation producing the minimal cost for each of these algorithms are presented. When the relation size is a small multiple of the amount of main memory available (typically up to three to six times), the hybrid hash join algorithm is preferable.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Average-case complexity</title>
	<abstract>We survey the average-case complexity of problems in NP.

    We discuss various notions of good-on-average algorithms, and present completeness results due to Impagliazzo and Levin. Such completeness results establish the fact that if a certain specific (but somewhat artificial) NP problem is easy-on-average with respect to the uniform distribution, then all problems in NP are easy-on-average with respect to all samplable distributions. Applying the theory to natural distributional problems remain an outstanding open question. We review some natural distributional problems whose average-case complexity is of particular interest and that do not yet fit into this theory.

    A major open question is whether the existence of hard-on-average problems in NP can be based on the P ? NP assumption or on related worst-case assumptions. We review negative results showing that certain proof techniques cannot prove such a result. While the relation between worst-case and average-case complexity for general NP problems remains open, there has been progress in understanding the relation between different "degrees" of average-case complexity. We discuss some of these "hardness amplification" results.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>FAST: Flash-aware external sorting for mobile database systems</title>
	<abstract>Recently, flash memory has gained its popularity as storage on wide spectrum of computing devices such as cellular phones, digital cameras, digital audio players and PDAs. The integration of high-density flash memory has been accelerated twice every year for past few years. As flash memory's capacity increases and its price drops, it is expected that flash memory will be more competitive with magnetic disk drives. Therefore, it is desirable to adapt disk-based algorithms to take advantage of the flash memory technology. In this paper, we propose a novel Flash-Aware external SorTing algorithm, FAST, that overcomes the limitation of larger writing cost for flash memory to improve both overall execution time and response time. In FAST, we reduce the write operations with additional read operations. We provide the analysis for both traditional and our flash-aware algorithms by comparing the detailed cost formulas. Experimental results with synthetic and real-life data sets show that FAST can result in faster execution time as well as smaller response time than traditional external sorting algorithms. </abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Two-way replacement selection</title>
	<abstract>The performance of external sorting using merge sort is highly dependent on the length of the runs generated. One of the most commonly used run generation strategies is Replacement Selection (RS) because, on average, it generates runs that are twice the size of the memory available. However, the length of the runs generated by RS is downsized for data with certain characteristics, like inputs sorted inversely with respect to the desired output order.

    The goal of this paper is to propose and analyze two-way replacement selection (2WRS), which is a generalization of RS obtained by implementing two heaps instead of the single heap implemented by RS. The appropriate management of these two heaps allows generating runs larger than the memory available in a stable way, i.e. independent from the characteristics of the datasets. Depending on the changing characteristics of the input dataset, 2WRS assigns a new data record to one or the other heap, and grows or shrinks each heap, accommodating to the growing or decreasing tendency of the dataset. On average, 2WRS creates runs of at least the length generated by RS, and longer for datasets that combine increasing and decreasing data subsets. We tested both algorithms on large datasets with different characteristics and 2WRS achieves speedups at least similar to RS, and over 2.5 when RS fails to generate large runs.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Practical methods for constructing suffix trees</title>
	<abstract>Sequence datasets are ubiquitous in modern life-science applications, and querying sequences is a common and critical operation in many of these applications. The suffix tree is a versatile data structure that can be used to evaluate a wide variety of queries on sequence datasets, including evaluating exact and approximate string matches, and finding repeat patterns. However, methods for constructing suffix trees are often very time-consuming, especially for suffix trees that are large and do not fit in the available main memory. Even when the suffix tree fits in memory, it turns out that the processor cache behavior of theoretically optimal suffix tree construction methods is poor, resulting in poor performance. Currently, there are a large number of algorithms for constructing suffix trees, but the practical tradeoffs in using these algorithms for different scenarios are not well characterized.In this paper, we explore suffix tree construction algorithms over a wide spectrum of data sources and sizes. First, we show that on modern processors, a cache-efficient algorithm with O(n2) worst-case complexity outperforms popular linear time algorithms like Ukkonen and McCreight, even for in-memory construction. For larger datasets, the disk I/O requirement quickly becomes the bottleneck in each algorithm's performance. To address this problem, we describe two approaches. First, we present a buffer management strategy for the O(n2) algorithm. The resulting new algorithm, which we call "Top Down Disk-based" (TDD), scales to sizes much larger than have been previously described in literature. This approach far outperforms the best known disk-based construction methods. Second, we present a new disk-based suffix tree construction algorithm that is based on a sort-merge paradigm, and show that for constructing very large suffix trees with very little resources, this algorithm is more efficient than TDD.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Evaluating holistic aggregators efficiently for very large datasets</title>
	<abstract>In data warehousing applications, numerous OLAP queries involve the processing of holistic aggregators such as computing the "top n", median, quantiles, etc. In this paper, we present a novel approach called dynamic bucketing to efficiently evaluate these aggregators. We partition data into equiwidth buckets and further partition dense buckets into subbuckets as needed by allocating and reclaiming memory space. The bucketing process dynamically adapts to the input order and distribution of input datasets. The histograms of the buckets and subbuckets are stored in our new data structure called structure trees. A recent selection algorithm based on regular sampling is generalized and its analysis extended. We have also compared our new algorithms with this generalized algorithm and several other recent algorithms. Experimental results show that our new algorithms significantly outperform prior ones not only in the runtime but also in accuracy</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Faster suffix sorting</title>
	<abstract>We propose a fast and memory-efficient algorithm for lexicographically sorting the suffixes of a string, a problem that has important applications in data compression as well as string matching. Our algorithm eliminates much of the overhead of previous specialized approaches while maintaining their robustness for degenerate inputs. For input size n, our algorithm operates in only two integer arrays of size n, and has worst-case time complexity O(nlogn). We demonstrate experimentally that our algorithm has stable performance compared with other approaches. </abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Cache-efficient string sorting using copying</title>
	<abstract>Burstsort is a cache-oriented sorting technique that uses a dynamic trie to efficiently divide large sets of string keys into related subsets small enough to sort in cache. In our original burstsort, string keys sharing a common prefix were managed via a bucket of pointers represented as a list or array; this approach was found to be up to twice as fast as the previous best string sorts, mostly because of a sharp reduction in out-of-cache references. In this paper, we introduce C-burstsort, which copies the unexamined tail of each key to the bucket and discards the original key to improve data locality. On both Intel and PowerPC architectures, and on a wide range of string types, we show that sorting is typically twice as fast as our original burstsort and four to five times faster than multikey quicksort and previous radixsorts. A variant that copies both suffixes and record pointers to buckets, CP-burstsort, uses more memory, but provides stable sorting. In current computers, where performance is limited by memory access latencies, these new algorithms can dramatically reduce the time needed for internal sorting of large numbers of strings.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Sorting networks on FPGAs</title>
	<abstract>Computer architectures are quickly changing toward heterogeneous many-core systems. Such a trend opens up interesting opportunities but also raises immense challenges since the efficient use of heterogeneous many-core systems is not a trivial problem. Software-configurable microprocessors and FPGAs add further diversity but also increase complexity. In this paper, we explore the use of sorting networks on field-programmable gate arrays (FPGAs). FPGAs are very versatile in terms of how they can be used and can also be added as additional processing units in standard CPU sockets. Our results indicate that efficient usage of FPGAs involves non-trivial aspects such as having the right computation model (a sorting network in this case); a careful implementation that balances all the design constraints in an FPGA; and the proper integration strategy to link the FPGA to the rest of the system. Once these issues are properly addressed, our experiments show that FPGAs exhibit performance figures competitive with those of modern general-purpose CPUs while offering significant advantages in terms of power consumption and parallel stream evaluation.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Compression techniques for fast external sorting</title>
	<abstract>External sorting of large files of records involves use of disk space to store temporary files, processing time for sorting, and transfer time between CPU, cache, memory, and disk. Compression can reduce disk and transfer costs, and, in the case of external sorts, cut merge costs by reducing the number of runs. It is therefore plausible that overall costs of external sorting could be reduced through use of compression.In this paper, we propose new compression techniques for data consisting of sets of records. The best of these techniques, based on building a trie of variable-length common strings, provides fast compression and decompression and allows random access to individual records. We show experimentally that our trie-based compression leads to significant reduction in sorting costs; that is, it is faster to compress the data, sort it, and then decompress it than to sort the uncompressed data. While the degree of compression is not quite as great as can be obtained with adaptive techniques such as Lempel-Ziv methods, these cannot be applied to sorting. Our experiments show that, in comparison to approaches such as Huffman coding of fixed-length substrings, our novel trie-based method is faster and provides greater size reductions.</abstract>
        <search_task_number>7</search_task_number>
        <query>input size effect sorting algorithms</query>
        <relevance>0</relevance>
  </item>
  <item>
    <title>Building Intelligent Interactive Tutors: Student-centered strategies for revolutionizing e-learning</title>
	<abstract>Computers have transformed every facet of our culture, most dramatically communication, transportation, finance, science, and the economy. Yet their impact has not been generally felt in education due to lack of hardware, teacher training, and sophisticated software. Another reason is that current instructional software is neither truly responsive to student needs nor flexible enough to emulate teaching. The more instructional software can reason about its own teaching process, know what it is teaching, and which method to use for teaching, the greater is its impact on education.

    Building Intelligent Interactive Tutors discusses educational systems that assess a student's knowledge and are adaptive to a student's learning needs. Dr. Woolf taps into 20 years of research on intelligent tutors to bring designers and developers a broad range of issues and methods that produce the best intelligent learning environments possible, whether for classroom or life-long learning. The book describes multidisciplinary approaches to using computers for teaching, reports on research, development, and real-world experiences, and discusses intelligent tutors, web-based learning systems, adaptive learning systems, intelligent agents and intelligent multimedia.

    *Combines both theory and practice to offer most in-depth and up-to-date treatment of intelligent tutoring systems available
    *Presents powerful drivers of virtual teaching systems, including cognitive science, artificial intelligence, and the Internet
    *Features algorithmic material that enables programmers and researchers to design building compo</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Fast and approximate stream mining of quantiles and frequencies using graphics processors</title>
	<abstract>We present algorithms for fast quantile and frequency estimation in large data streams using graphics processors (GPUs). We exploit the high computation power and memory bandwidth of graphics processors and present a new sorting algorithm that performs rasterization operations on the GPUs. We use sorting as the main computational component for histogram approximation and construction of e-approximate quantile and frequency summaries. Our algorithms for numerical statistics computation on data streams are deterministic, applicable to fixed or variable-sized sliding windows and use a limited memory footprint. We use GPU as a co-processor and minimize the data transmission between the CPU and GPU by taking into account the low bus bandwidth. We implemented our algorithms on a PC with a NVIDIA GeForce FX 6800 Ultra GPU and a 3.4 GHz Pentium IV CPU and applied them to large data streams consisting of more than 100 million values. We also compared the performance of our GPU-based algorithms with optimized implementations of prior CPU-based algorithms. Overall, our results demonstrate that the graphics processors available on a commodity computer system are efficient stream-processor and useful co-processors for mining data streams.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>FPGA Design Automation: A Survey</title>
	<abstract>Design automation or computer-aided design (CAD) for field programmable gate arrays (FPGAs) has played a critical role in the rapid advancement and adoption of FPGA technology over the past two decades. The purpose of this paper is to meet the demand for an up-to-date comprehensive survey/tutorial for FPGA design automation, with an emphasis on the recent developments within the past 5-10 years. The paper focuses on the theory and techniques that have been, or most likely will be, reduced to practice. It covers all major steps in FPGA design flow which includes: routing and placement, circuit clustering, technology mapping and architecture-specific optimization, physical synthesis, RT-level and behavior-level synthesis, and power optimization. We hope that this paper can be used both as a guide for beginners who are embarking on research in this relatively young yet exciting area, and a useful reference for established researchers in this field.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Cache-conscious sorting of large sets of strings with dynamic tries</title>
	<abstract>Ongoing changes in computer architecture are affecting the efficiency of string-sorting algorithms. The size of main memory in typical computers continues to grow but memory accesses require increasing numbers of instruction cycles, which is a problem for the most efficient of the existing string-sorting algorithms as they do not utilize cache well for large data sets. We propose a new sorting algorithm for strings, burstsort, based on dynamic construction of a compact trie in which strings are kept in buckets. It is simple, fast, and efficient. We experimentally explore key implementation options and compare burstsort to existing string-sorting algorithms on large and small sets of strings with a range of characteristics. These experiments show that, for large sets of strings, burstsort is almost twice as fast as any previous algorithm, primarily due to a lower rate of cache miss.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Algorithms and analyses for maximal vector computation</title>
	<abstract>The maximal vector problem is to identify the maximals over a collection of vectors. This arises in many contexts and, as such, has been well studied.The problem recently gained renewed attention with skyline queries for relational databases and with work to develop skyline algorithms that are external and relationally well behaved. While many algorithms have been proposed, how they perform has been unclear. We study the performance of, and design choices behind, these algorithms. We prove runtime bounds based on the number of vectors N and the dimensionality K. Early algorithms based on divide and conquer established seemingly good average and worst-case asymptotic runtimes. In fact, the problem can be solved in $$\mathcal{O}(KN)$$ average-case (holding K as fixed). We prove, however, that the performance is quite bad with respect to K. We demonstrate that the more recent skyline algorithms are better behaved, and can also achieve $$\mathcal{O}(KN)$$ average-case. While K matters for these, in practice, its effect vanishes in the asymptotic. We introduce a new external algorithm, LESS, that is more efficient and better behaved. We evaluate LESS's effectiveness and improvement over the field, and prove that its average-case running time is $$\mathcal{O}(KN)$$.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Evolving compare-exchange networks using grammars</title>
	<abstract>We apply a genetic algorithm whose genotypes represent simple grammars to search for families of compare-exchange networks that are merging networks. The grammars are restricted to be of a simple form based on prior knowledge about the search domain. Finding merging networks in this fashion leads to the discovery of networks that also sort if the data is cycled through them a small number of times. Random network and grammar generation results show that the problem is difficult and suggest some conjectures about how the genetic algorithm is operating for this problem. The genetic algorithm finds the best-known network of the kind for which we search, and further finds a novel, slightly suboptimal network that turns out to be an interesting combination of ideas from known, theoretically derived networks.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Learning to Rank for Information Retrieval</title>
	<abstract>Learning to rank for Information Retrieval (IR) is a task to automatically construct a ranking model using training data, such that the model can sort new objects according to their degrees of relevance, preference, or importance. Many IR problems are by nature ranking problems, and many IR technologies can be potentially enhanced by using learning-to-rank techniques. The objective of this tutorial is to give an introduction to this research direction. Specifically, the existing learning-to-rank algorithms are reviewed and categorized into three approaches: the pointwise, pairwise, and listwise approaches. The advantages and disadvantages with each approach are analyzed, and the relationships between the loss functions used in these approaches and IR evaluation measures are discussed. Then the empirical evaluations on typical learning-to-rank methods are shown, with the LETOR collection as a benchmark dataset, which seems to suggest that the listwise approach be the most effective one among all the approaches. After that, a statistical ranking theory is introduced, which can describe different learning-to-rank algorithms, and be used to analyze their query-level generalization abilities. At the end of the tutorial, we provide a summary and discuss potential future work on learning to rank.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Real-Time Performance of Sorting Algorithms</title>
	<abstract>In hard real-time systems tasks must meet their deadlines under guarantee. Soft real-time tasks may miss deadlines occasionally, as long as the entire system can provide the specified quality of service.

    In this paper we investigate the hard and soft real-time performance of sorting algorithms and compare it to their average performance. We show in which way the adequacy of an algorithm depends on the demanded performance criterium (hard, soft, or non real-time). The results provide a guideline to select the right sorting algorithm for a given application.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Fast and accurate computation of equi-depth histograms over data streams</title>
	<abstract>Equi-depth histograms represent a fundamental synopsis widely used in both database and data stream applications, as they provide the cornerstone of many techniques such as query optimization, approximate query answering, distribution fitting, and parallel database partitioning. Equi-depth histograms try to partition a sequence of data in a way that every part has the same number of data items. In this paper, we present a new algorithm to estimate equi-depth histograms for high speed data streams over sliding windows. While many previous methods were based on quantile computations, we propose a new method called BAr Splitting Histogram (BASH) that provides an expected e-approximate solution to compute the equi-depth histogram. Extensive experiments show that BASH is at least four times faster than one of the best existing approaches, while achieving similar or better accuracy and in some cases using less memory. The experimental results also indicate that BASH is more stable on data affected by frequent concept shifts.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Termination analysis and specialization-point insertion in offline partial evaluation</title>
	<abstract>Recent research suggests that the goal of fully automatic and reliable program generation for a broad range of applications is coming nearer to feasibility. However, several interesting and challenging problems remain to be solved before it becomes a reality. Solving them is also necessary, if we hope ever to elevate software engineering from its current state (a highly developed handiwork) into a successful branch of engineering, capable of solving a wide range of new problems by systematic, well-automated and well-founded methods.A key problem in all program generation is termination of the generation process. This article focuses on off-line partial evaluation and describes recent progress towards automatically solving the termination problem, first for individual programs, and then for specializers and "generating extensions", the program generators that most offline partial evaluators produce.The technique is based on size-change graphs that approximate the changes in parameter sizes at function calls. We formulate a criterion, bounded anchoring, for detecting parameters known to be bounded during specialization: a bounded parameter can act as an anchor for other parameters. Specialization points necessary for termination are computed by adding a parameter that tracks call depth, and then selecting a specialization point in every call loop where it is unanchored. By generalizing all unbounded parameters, we compute a binding-time division which together with the set of specialization points guarantees termination.Contributions of this article include a proof, based on the operational semantics of partial evaluation with memoization, that the analysis guarantees termination; and an in-depth description of safety of the increasing size approximation operator required for termination analysis in partial evaluation.Initial experiments with a prototype shows that the analysis overall yields binding-time divisions that can achieve a high degree of specialization, while still guaranteeing termination.The article ends with a list of challenging problems whose solution would bring the community closer to the goal of broad-spectrum, fully automatic and reliable program generation.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Engineering a cache-oblivious sorting algorithm</title>
	<abstract>This paper is an algorithmic engineering study of cache-oblivious sorting. We investigate by empirical methods a number of implementation issues and parameter choices for the cache-oblivious sorting algorithm Lazy Funnelsort and compare the final algorithm with Quicksort, the established standard for comparison-based sorting, as well as with recent cache-aware proposals. The main result is a carefully implemented cache-oblivious sorting algorithm, which, our experiments show, can be faster than the best Quicksort implementation we are able to find for input sizes well within the limits of RAM. It is also at least as fast as the recent cache-aware implementations included in the test. On disk, the difference is even more pronounced regarding Quicksort and the cache-aware algorithms, whereas the algorithm is slower than a careful implementation of multiway Mergesort, such as TPIE.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Lineage tracing for general data warehouse transformations</title>
	<abstract>Data warehousing systems integrate information from operational data sources into a central repository to enable analysis and mining of the integrated information. During the integration process, source data typically undergoes a series of transformations, which may vary from simple algebraic operations or aggregations to complex "data cleansing" procedures. In a warehousing environment, the data lineage problem is that of tracing warehouse data items back to the original source items from which they were derived. We formally define the lineage tracing problem in the presence of general data warehouse transformations, and we present algorithms for lineage tracing in this environment. Our tracing procedures take advantage of known structure or properties of transformations when present, but also work in the absence of such information. Our results can be used as the basis for a lineage tracing tool in a general warehousing setting, and also can guide the design of data warehouses that enable efficient lineage tracing.</abstract>
	<search_task_number>7</search_task_number>
	<query>input size effect sorting algorithms</query>
	<relevance>0</relevance>
  </item>




  <item>
    <title>An experimental study of sorting and branch prediction</title>
    <abstract>Sorting is one of the most important and well-studied problems in computer science. Many good algorithms are known which offer various trade-offs in efficiency, simplicity, memory use, and other factors. However, these algorithms do not take into account features of modern computer architectures that significantly influence performance. Caches and branch predictors are two such features and, while there has been a significant amount of research into the cache performance of general purpose sorting algorithms, there has been little research on their branch prediction properties. In this paper, we empirically examine the behavior of the branches in all the most common sorting algorithms. We also consider the interaction of cache optimization on the predictability of the branches in these algorithms. We find insertion sort to have the fewest branch mispredictions of any comparison-based sorting algorithm, that bubble and shaker sort operate in a fashion that makes their branches highly unpredictable, that the unpredictability of shellsort's branches improves its caching behavior, and that several cache optimizations have little effect on mergesort's branch mispredictions. We find also that optimizations to quicksort, for example the choice of pivot, have a strong influence on the predictability of its branches. We point out a simple way of removing branch instructions from a classic heapsort implementation and also show that unrolling a loop in a cache-optimized heapsort implementation improves the predicitability of its branches. Finally, we note that when sorting random data two-level adaptive branch predictors are usually no better than simpler bimodal predictors. This is despite the fact that two-level adaptive predictors are almost always superior to bimodal predictors, in general.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>On the adaptiveness of Quicksort</title>
    <abstract>Quicksort was first introduced in 1961 by Hoare. Many variants have been developed, the best of which are among the fastest generic-sorting algorithms available, as testified by the choice of Quicksort as the default sorting algorithm in most programming libraries. Some sorting algorithms are adaptive, i.e., they have a complexity analysis that is better for inputs, which are nearly sorted, according to some specified measure of presortedness. Quicksort is not among these, as it uses Ω(n log n) comparisons even for sorted inputs. However, in this paper, we demonstrate empirically that the actual running time of Quicksort is adaptive with respect to the presortedness measure Inv. Differences close to a factor of two are observed between instances with low and high Inv value. We then show that for the randomized version of Quicksort, the number of element swaps performed is provably adaptive with respect to the measure Inv. More precisely, we prove that randomized Quicksort performs expected O(n(1 + log(1 + Inv/n))) element swaps, where Inv denotes the number of inversions in the input sequence. This result provides a theoretical explanation for the observed behavior and gives new insights on the behavior of Quicksort. We also give some empirical results on the adaptive behavior of Heapsort and Mergesort.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Data-specific analysis of string sorting</title>
    <abstract>We consider the complexity of sorting strings in the model that counts comparisons between symbols and not just comparisons between strings. We show that for any set of strings S the complexity of sorting S can naturally be expressed in terms of the trie induced by S. This holds not only for lower bounds but also for the running times of various algorithms. Thus this "data-specific" analysis allows a direct comparison of different algorithms running on the same data. We give such "data-specific" analyses for various versions of quicksort and versions of mergesort. As a corollary we arrive at a very simple analysis of quicksorting random strings, which so far required rather sophisticated mathematical tools. As part of this we provide insights in the analysis of tries of random strings which may be interesting in their own right.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Recursion patterns and time-analysis</title>
    <abstract>This paper explores some ideas concerning the time-analysis of functional programs defined by instantiating typical recursion patterns such as folds, unfolds, and hylomorphisms. The concepts in this paper are illustrated through a rich set of examples in the Haskell programming language. We concentrate on unfolds and folds (also known as anamorphisms and catamorphisms respectively) of recursively defined types, as well as the more general hylomorphism pattern. For the latter, we use as case-studies two famous sorting algorithms, mergesort and quicksort. Even though time analysis is not compositional, we argue that splitting functions to expose the explicit construction of the recursion tree and its later consumption helps with this analysis.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Remarks on turbo ASMs for functional equations and recursion schemes</title>
    <abstract>The question raised in [15] is answered how to naturally model widely used forms of recursion by abstract machines. We show that turbo ASMs as defined in [7] allow one to faithfully reflect the common intuitive single-agent understanding of recursion. The argument is illustrated by turbo ASMs for Mergesort and Quicksort. Using turbo ASMs for returning function values allows one to seamlessly integrate functional description and programming techniques into the high-level 'abstract programming' by state transforming ASM rules.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Engineering a cache-oblivious sorting algorithm</title>
    <abstract>This paper is an algorithmic engineering study of cache-oblivious sorting. We investigate by empirical methods a number of implementation issues and parameter choices for the cache-oblivious sorting algorithm Lazy Funnelsort and compare the final algorithm with Quicksort, the established standard for comparison-based sorting, as well as with recent cache-aware proposals. The main result is a carefully implemented cache-oblivious sorting algorithm, which, our experiments show, can be faster than the best Quicksort implementation we are able to find for input sizes well within the limits of RAM. It is also at least as fast as the recent cache-aware implementations included in the test. On disk, the difference is even more pronounced regarding Quicksort and the cache-aware algorithms, whereas the algorithm is slower than a careful implementation of multiway Mergesort, such as TPIE.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Improving memory performance of sorting algorithms</title>
    <abstract>Memory hierarchy considerations during sorting algorithm design and implementation play an important role in significantly improving execution performance. Existing algorithms mainly attempt to reduce capacity misses on direct-mapped caches. To reduce other types of cache misses that occur in the more common set-associative caches and the TLB, we restructure the mergesort and quicksort algorithms further by integrating tiling, padding, and buffering techniques and by repartitioning the data set. Our study shows that substantial performance improvements can be obtained using our new methods.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Parallelization of Recursive Procedures</title>
    <abstract>Parallelizing compilers have traditionally focussed mainly on parallelizing loops. This paper presents a new framework for automatically parallelizing recursive procedures that typically appear in divide-and-conquer algorithms. We present compile-time analysis to detect the independence of multiple recursive calls in a procedure. This allows exploitation of a scalable form of nested parallelism, where each parallel task can further spawn off parallel work in subsequent recursive calls.We describe a run-time system which efficiently supports this kind of nested parallelism without unnecessarily blocking tasks. We have implemented this framework in a parallelizing compiler, which is able to automatically parallelize programs like quicksort and mergesort, written in C.For cases where even the advanced symbolic analysis and array section analysis we describe are not able to prove the independence of procedure calls, we propose novel techniques for speculative run-time parallelization, which are more efficient and powerful in this context than analogous techniques proposed previously for speculatively parallelizing loops. Our experimental results on an IBM G30 SMP machine show good speedups obtained by following our approach.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>In-place algorithms for sorting problems</title>
    <abstract>An algorithm is said to operate in-place if it uses only a constant amount of extra memory for storing local variables besides the memory reserved for the input elements. In other words, the size of the extra memory does not grow as the number of input elements, n, gets larger, but it is bounded by a constant. An algorithm reorders the input elements stably if the original relative order of equal elements is retained. In this thesis, we devise in-place algorithms for sorting and related problems. We measure the efficiency of the algorithms by calculating the number of element comparisons and element moves performed in the worst case in the following. The amount of index manipulation operations is closely related to these quantities, so it is omitted in our calculations. When no precise figures are needed, we denote the sum of all operations by a general expression "time". The thesis consists of five separate articles, the main contributions of which are described below. We construct algorithms for stable partitioning and stable selection which are the first linear-time algorithms being both stable and in-place concurrently. Moreover, we define problems stable unpartitioning and restoring selection and devise linear-time algorithms for these problems. The algorithm for stable un-partitioning is in-place while that for restoring selection uses O(n) extra bits. By using these algorithms as subroutines we construct an adaption of Quicksort that sorts a multiset stably in O(&amp;Sigma;ki = 1 mi log(n/mi)) time where mi is the multiplicity of ith distinct element for i = 1,.., k. This is the first in-place algorithm that sorts a multiset stably in asymptotically optimal time. We present in-place algorithms for unstable and stable merging. The algorithms are asymptotically more efficient than earlier ones: the number of moves is 3(n + m)+o(m) for the unstable algorithm, 5n+12m+o(m) for the stable algorithm, and the number of comparisons at most m(t + 1) + n/2t + o(m) comparisons where m &amp;le; n and t = [log(n/m)]. The previous best results were 1.125(n + m) + o(n) comparisons and 5(n + in) + o(n) moves for unstable merging, and 16.5(n + in) + o(n) moves for stable merging. Finally, we devise two in-place algorithms for sorting. Both algorithms are adaptions of Mergesort. The first performs n log2 n + O(n) comparisons and &amp;#949; n loge n + O(n log log n) moves for any fixed 0 &amp;lt; &amp;#949; &amp;le; 2. Our experiments show that this algorithm performs well in practice The second requires n loge n + O(n) comparisons and fewer than O(n log n/log log n) moves. This is the first in-place sorting algorithm that performs o(n log n) moves in the worst case while guaranteeing O(n log n) comparisons.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Symbolic bounds analysis of pointers, array indices, and accessed memory regions</title>
    <abstract>This article presents a novel framework for the symbolic bounds analysis of pointers, array indices, and accessed memory regions. Our framework formulates each analysis problem as a system of inequality constraints between symbolic bound polynomials. It then reduces the constraint system to a linear program. The solution to the linear program provides symbolic lower and upper bounds for the values of pointer and array index variables and for the regions of memory that each statement and procedure accesses. This approach eliminates fundamental problems associated with applying standard fixed-point approaches to symbolic analysis problems. Experimental results from our implemented compiler show that the analysis can solve several important problems, including static race detection, automatic parallelization, static detection of array bounds violations, elimination of array bounds checks, and reduction of the number of bits used to store computed values.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Using the divide and conquer strategy to teach Java framework design</title>
    <abstract>All programmers should understand the concept of program families and know the techniques for constructing them. This paper describes a case study that can be used to introduce students in a Java software design course to the construction of program families using software frameworks. The example is the family of programs that use the well-known divide and conquer algorithmic strategy.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Space profiling for parallel functional programs</title>
    <abstract>This paper presents a semantic space profiler for parallel functional programs. Building on previous work in sequential profiling, our tools help programmers to relate runtime resource use back to program source code. Unlike many profiling tools, our profiler is based on a cost semantics. This provides a means to reason about performance without requiring a detailed understanding of the compiler or runtime system. It also provides a specification for language implementers. This is critical in that it enables us to separate cleanly the performance of the application from that of the language implementation. Some aspects of the implementation can have significant effects on performance. Our cost semantics enables programmers to understand the impact of different scheduling policies while hiding many of the details of their implementations. We show applications where the choice of scheduling policy has asymptotic effects on space use. We explain these use patterns through a demonstration of our tools. We also validate our methodology by observing similar performance in our implementation of a parallel extension of Standard ML.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Contemplate sorting with columnsort</title>
    <abstract>The efficiency of Internet search engines has made it trivial for students to find implementations of standard algorithms. This fact has led computer science educators to be more creative with their assignments to encourage students to create their own implementations. Unfortunately, excessive creativity can obscure learning objectives, particularly for less insightful students. We demonstrate that recasting the parallel sorting algorithm Columnsort for a uniprocessor environment provides the foundation for a variety of sorting assignments that can engage students while not obscuring educational objectives.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Improving reuse of component families by generating component hierarchies</title>
    <abstract>Feature-oriented software development (FOSD) enables developers to generate families of similar components. However, current FOSD approaches degrade component reuse because they do not allow a developer to combine multiple components of the same family in a larger program. This is because individual family members cannot be distinguished from each other. We present an approach to model and generate component hierarchies that allow a programmer to combine multiple component variants. A component hierarchy structures the components of a family according to their functionality. Due to subtyping between the components of a hierarchy, client developers can write generic code that works with different component variants.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Scalable parallel suffix array construction</title>
    <abstract>Suffix arrays are a simple and powerful data structure for text processing that can be used for full text indexes, data compression, and many other applications in particular in bioinformatics. We describe the first implementation and experimental evaluation of a scalable parallel algorithm for suffix array construction. The implementation works on distributed memory computers using MPI, Experiments with up to 128 processors show good constant factors and make it look likely that the algorithm would also scale to considerably larger systems. This makes it possible to build suffix arrays for huge inputs very quickly. Our algorithm is a parallelization of the linear time DC3 algorithm.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>A framework for adaptive algorithm selection in STAPL</title>
    <abstract>Writing portable programs that perform well on multiple platforms or for varying input sizes and types can be very difficult because performance is often sensitive to the system architecture, the run-time environment, and input data characteristics. This is even more challenging on parallel and distributed systems due to the wide variety of system architectures. One way to address this problem is to adaptively select the best parallel algorithm for the current input data and system from a set of functionally equivalent algorithmic options. Toward this goal, we have developed a general framework for adaptive algorithm selection for use in the Standard Template Adaptive Parallel Library (STAPL). Our framework uses machine learning techniques to analyze data collected by STAPL installation benchmarks and to determine tests that will select among algorithmic options at run-time. We apply a prototype implementation of our framework to two important parallel operations, sorting and matrix multiplication, on multiple platforms and show that the framework determines run-time tests that correctly select the best performing algorithm from among several competing algorithmic options in 86-100% of the cases studied, depending on the operation and the system.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>PetaBricks: a language and compiler for algorithmic choice</title>
    <abstract>It is often impossible to obtain a one-size-fits-all solution for high performance algorithms when considering different choices for data distributions, parallelism, transformations, and blocking. The best solution to these choices is often tightly coupled to different architectures, problem sizes, data, and available system resources. In some cases, completely different algorithms may provide the best performance. Current compiler and programming language techniques are able to change some of these parameters, but today there is no simple way for the programmer to express or the compiler to choose different algorithms to handle different parts of the data. Existing solutions normally can handle only coarse-grained, library level selections or hand coded cutoffs between base cases and recursive cases. We present PetaBricks, a new implicitly parallel language and compiler where having multiple implementations of multiple algorithms to solve a problem is the natural way of programming. We make algorithmic choice a first class construct of the language. Choices are provided in a way that also allows our compiler to tune at a finer granularity. The PetaBricks compiler autotunes programs by making both fine-grained as well as algorithmic choices. Choices also include different automatic parallelization techniques, data distributions, algorithmic parameters, transformations, and blocking. Additionally, we introduce novel techniques to autotune algorithms for different convergence criteria. When choosing between various direct and iterative methods, the PetaBricks compiler is able to tune a program in such a way that delivers near-optimal efficiency for any desired level of accuracy. The compiler has the flexibility of utilizing different convergence criteria for the various components within a single algorithm, providing the user with accuracy choice alongside algorithmic choice.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>SmartApps: middle-ware for adaptive applications on reconfigurable platforms</title>
    <abstract>One general avenue to obtain optimized performance on large and complex systems is to approach optimization from a global perspective of the complete system in a customized manner for each application, i.e., application-centric optimization. Lately, there have been encouraging developments in reconfigurable operating systems and hardware that will enable customized optimization. For example, machines built with PIM's and FPGA's can be quickly reconfigured to better fit a certain application and operating systems, such as IBM's K42, can have their services customized to fit the needs and characteristics of an application. While progress in operating system and hardware and hardware has made re-configuration possible, we still need strategies and techniques to exploit them for improved application performance.In this paper, we describe the approach we are using in our smart application (SMARTAPPS) project. In the SMARTAPP executable, the compiler embeds most run-time system services and a feedback loop to monitor performance and trigger run-time adaptations. At run-time, after incorporating the code's input and determining the system's state, the SMARTAPP performs an instance specific optimization. During execution, the application continually monitors its performance and the available resources to determine if restructuring should occur. The framework includes mechanisms for performing the actual restructuring at various levels including: algorithmic adaptation, tuning reconfigurable OS services (scheduling policy, page size, etc.), and system configuration (e.g., number of processors). This paper concentrates on the techniques for providing customized system services for communication, thread scheduling, memory management, and performance monitoring and modeling.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>CoSaMP: iterative signal recovery from incomplete and inaccurate samples</title>
    <abstract>Compressive sampling (CoSa) is a new paradigm for developing data sampling technologies. It is based on the principle that many types of vector-space data are compressible, which is a term of art in mathematical signal processing. The key ideas are that randomized dimension reduction preserves the information in a compressible signal and that it is possible to develop hardware devices that implement this dimension reduction efficiently. The main computational challenge in CoSa is to reconstruct a compressible signal from the reduced representation acquired by the sampling device. This extended abstract describes a recent algorithm, called, &lt;code&gt;CoSaMP&lt;/code&gt;, that accomplishes the data recovery task. It was the first known method to offer near-optimal guarantees on resource usage.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Generic multiset programming for language-integrated querying</title>
    <abstract>This paper demonstrates how relational algebraic programming based on efficient symbolic representations of multisets and operations on them can be applied to the query sublanguage of SQL in a type-safe fashion. In essence, it provides a library for naïve programming with multisets in a generalized SQL-style fashion, but avoids many cases of asymptotically inefficient nested iteration through cross-products.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>A Dynamically Tuned Sorting Library</title>
    <abstract>Empirical search is a strategy used during the installation of library generators such as ATLAS, FFTW, and SPIRAL to identify the algorithm or the version of an algorithm that delivers the best performance. In the past, empirical search has been applied almost exclusively to scientific problems. In this paper, we discuss the application of empirical search to sorting, which is one of the best understood symbolic computing problems. When contrasted with the dense numerical computations of ATLAS, FFTW,and SPIRAL, sorting presents a new challenge, namely that the relative performance of the algorithms depend not only on the characteristics of the target machine and the size of the input data but also on the distribution of values in the input data set. Empirical search is applied in the study reported here as part of a sorting library generator. The resulting routines dynamically adapt to the characteristics of the input data by selecting the best sorting algorithm from a small set of alternatives. To generate the run time selection mechanism our generator makes use of machine learning to predict the best algorithm as a function of the characteristics of the input data set and the performance of the different algorithms on the target machine. This prediction is based on the data obtained through empirical search at installation time.Our results show that our approach is quite effective. When sorting data inputs of 12M keys with various standard deviations,our adaptive approach selected the best algorithm for all the input data sets and all platforms that we tried in our experiments. The wrong decision could have introduced a performance degradation of up to 133%, with an average value of 44%.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Scheduling threads for constructive cache sharing on CMPs</title>
    <abstract>In chip multiprocessors (CMPs), limiting the number of offchip cache misses is crucial for good performance. Many multithreaded programs provide opportunities for constructive cache sharing, in which concurrently scheduled threads share a largely overlapping working set. In this paper, we compare the performance of two state-of-the-art schedulers proposed for fine-grained multithreaded programs: Parallel Depth First (PDF), which is specifically designed for constructive cache sharing, and Work Stealing (WS), which is a more traditional design. Our experimental results indicate that PDF scheduling yields a 1.3--1.6X performance improvement relative to WS for several fine-grain parallel benchmarks on projected future CMP configurations; we also report several issues that may limit the advantage of PDF in certain applications. These results also indicate that PDF more effectively utilizes off-chip bandwidth, making it possible to trade-off on-chip cache for a larger number of cores. Moreover, we find that task granularity plays a key role in cache performance. Therefore, we present an automatic approach for selecting effective grain sizes, based on a new working set profiling algorithm that is an order of magnitude faster than previous approaches. This is the first paper demonstrating the effectiveness of PDF on real benchmarks, providing a direct comparison between PDF and WS, revealing the limiting factors for PDF in practice, and presenting an approach for overcoming these factors.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Efficient implementation of sorting on multi-core SIMD CPU architecture</title>
    <abstract>Sorting a list of input numbers is one of the most fundamental problems in the field of computer science in general and high-throughput database applications in particular. Although literature abounds with various flavors of sorting algorithms, different architectures call for customized implementations to achieve faster sorting times. This paper presents an efficient implementation and detailed analysis of MergeSort on current CPU architectures. Our SIMD implementation with 128-bit SSE is 3.3X faster than the scalar version. In addition, our algorithm performs an efficient multiway merge, and is not constrained by the memory bandwidth. Our multi-threaded, SIMD implementation sorts 64 million floating point numbers in less than0.5 seconds on a commodity 4-core Intel processor. This measured performance compares favorably with all previously published results. Additionally, the paper demonstrates performance scalability of the proposed sorting algorithm with respect to certain salient architectural features of modern chip multiprocessor (CMP) architectures, including SIMD width and core-count. Based on our analytical models of various architectural configurations, we see excellent scalability of our implementation with SIMD width scaling up to 16X wider than current SSE width of 128-bits, and CMP core-count scaling well beyond 32 cores. Cycle-accurate simulation of Intel's upcoming x86 many-core Larrabee architecture confirms scalability of our proposed algorithm.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>User notification in taxonomy based digital libraries</title>
    <abstract>We consider a publish/subscribe system for digital libraries which continuously evaluates queries over a large repository containing document descriptions. The subscriptions, the query expressions and the document descriptions, all rely on a taxonomy that is a hierarchically organized set of keywords, or terms. The digital library supports insertion, update and removal of a document. Each of these operations is seen as an event that must be notified only to those users whose subscriptions match the document's description. The paper addresses the problem of efficiently supporting the notification process, and makes contributions in two directions: (a) definition of a formal model for the publish/subscribe process; (b) proposal of a semi-lattice structure for subscriptions allowing the filtering out of non matching subscriptions. Experimental results that show the cost benefits obtained by our approach are presented in the full paper [6]</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Constraint-based termination analysis of logic programs</title>
    <abstract>Current norm-based automatic termination analysis techniques for logic programs can be split up into different components: inference of mode or type information, derivation of models, generation of well-founded orders, and verification of the termination conditions themselves. Although providing high-precision results, these techniques suffer from an efficiency point of view, as several of these analyses are often performed through abstract interpretation. We present a new termination analysis which integrates the various components and produces a set of constraints that, when solvable, identifies successful termination proofs. The proposed method is both efficient and precise. The use of constraint sets enables the propagation on information over all different phases while the need for multiple analyses is considerably reduced.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Cache-efficient string sorting using copying</title>
    <abstract>Burstsort is a cache-oriented sorting technique that uses a dynamic trie to efficiently divide large sets of string keys into related subsets small enough to sort in cache. In our original burstsort, string keys sharing a common prefix were managed via a bucket of pointers represented as a list or array; this approach was found to be up to twice as fast as the previous best string sorts, mostly because of a sharp reduction in out-of-cache references. In this paper, we introduce C-burstsort, which copies the unexamined tail of each key to the bucket and discards the original key to improve data locality. On both Intel and PowerPC architectures, and on a wide range of string types, we show that sorting is typically twice as fast as our original burstsort and four to five times faster than multikey quicksort and previous radixsorts. A variant that copies both suffixes and record pointers to buckets, CP-burstsort, uses more memory, but provides stable sorting. In current computers, where performance is limited by memory access latencies, these new algorithms can dramatically reduce the time needed for internal sorting of large numbers of strings.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Memory management for self-adjusting computation</title>
    <abstract>The cost of reclaiming space with traversal-based garbage collection is inversely proportional to the amount of free memory, i.e., O(1/(1-f)), where f is the fraction of memory that is live. Consequently, the cost of garbage collection can be very high when the size of the live data remains large relative to the available free space. Intuitively, this is because allocating a small amount of memory space will require the garbage collector to traverse a significant fraction of the memory only to discover little garbage. This is unfortunate because in some application domains the size of the memory-resident data can be generally high. This can cause high GC overheads, especially when generational assumptions do not hold. One such application domain is self-adjusting computation, where computations use memory-resident execution traces in order to respond to changes to their state (e.g., inputs) efficiently. This paper proposes memory-management techniques for self-adjusting computation that remain efficient even when the size of the live data is large. More precisely, the proposed techniques guarantee O(1) amortized cost for each reclaimed memory object. We propose a set of primitives for self-adjusting computation that support the proposed memory management techniques. The primitives provide an operation for allocating memory; we reclaim unused memory automatically. We implement a library for supporting the primitives in the C language and perform an experimental evaluation. Our experiments show that the approach can be implemented with reasonably small constant-factor overheads and that the programs written using the library behave optimally. Compared to previous implementations, we measure up to an order of magnitude improvement in performance and up to a 75% reduction in space usage.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Parallel sorting pattern</title>
    <abstract>A large number of parallel applications contain a computationally intensive phase in which a large list of elements must be ordered based on some common attribute of the elements. How do we sort a sequence of elements on multiple processing units so as to minimize redistribution of keys while allowing processing units to do independent sorting work?</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>List processing: sort again, naturally</title>
    <abstract>This paper discusses a possible student project for use within the Data Structures and Algorithms treatment of linked lists. Students can explicitly compare the recursive list-oriented MergeSort algorithm with iterative list-oriented MergeSort algorithms (with O(n) space overhead) including the "Natural MergeSort." The author's experimental results are shown for implementations in C and in Java.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Eergy aware data management on AVR micro controller based systems</title>
    <abstract>Data management systems comprise various algorithms for efficiently retrieving and managing data. Typically, algorithm efficiency or performance is correlated with execution speed. However, the uptime of battery-powered mobile- and embedded systems strongly depends on the energy consumption of the involved components. This paper reports our results concerning the energy consumption of different implementations of sorting and join algorithms. We demonstrate that high performance algorithms often require more energy than slower ones. Furthermore, we show that dynamically exchanging algorithms at runtime results in a better throughput if energy is limited.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Using SIMD registers and instructions to enable instruction-level parallelism in sorting algorithms</title>
    <abstract>Most contemporary processors offer some version of Single Instruction Multiple Data (SIMD) machinery - vector registers and instructions to manipulate data stored in such registers. The central idea of this paper is to use these SIMD resources to improve the performance of the tail of recursive sorting algorithms. When the number of elements to be sorted reaches a set threshold, data is loaded into the vector registers, manipulated in-register, and the result stored back to memory. Three implementations of sorting with two different SIMD machineries - x86-64's SSE2 and G5's AltiVec - demonstrate that this idea delivers significant speed improvements. The improvements provided are orthogonal to the gains obtained through empirical search for a suitable sorting algorithm [11]. When integrated with the Dynamically Tuned Sorting Library (DTSL) this new code generation strategy reduces the time spent by DTSL up to 22% for moderately-sized arrays, with greater relative reductions for small arrays. Wall-clock performance of d-heaps is improved by up to 39% using a similar technique.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Towards a theory of cache-efficient algorithms</title>
    <abstract>We present a model that enables us to analyze the running time of an algorithm on a computer with a memory hierarchy with limited associativity, in terms of various cache parameters. Our cache model, an extension of Aggarwal and Vitter's I/O model, enables us to establish useful relationships between the cache complexity and the I/O complexity of computations. As a corollary, we obtain cache-efficient algorithms in the single-level cache model for fundamental problems like sorting, FFT, and an important subclass of permutations. We also analyze the average-case cache behavior of mergesort, show that ignoring associativity concerns could lead to inferior performance, and present supporting experimental evidence.We further extend our model to multiple levels of cache with limited associativity and present optimal algorithms for matrix transpose and sorting. Our techniques may be used for systematic exploitation of the memory hierarchy starting from the algorithm design stage, and for dealing with the hitherto unresolved problem of limited associativity.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A novel sorting animation: permuting picture pixels</title>
    <abstract>Algorithm animation has a long history in CS education, and in this paper we describe a novel way to animate basic sorting routines. The idea is to scramble the pixels of a picture, and then use a sorting routine to unscramble them. The resulting animation of moving pixels is both enjoyable to watch, and provides enough clues to figure out what algorithm is doing the unscrambling. We have used this as a class activity in numerous data structures and algorithms courses to test students understanding of different sorting algorithms. In addition to describing how our animation technique works and is implemented we also discuss some of the related work in the field of algorithm visualization, and the importance of student engagement in such visualizations.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Software implementation strategies for power-conscious systems</title>
    <abstract>A variety of systems with possibly embedded computing power, such as small portable robots, hand-held computers, and automated vehicles, have power supply constraints. Their batteries generally last only for a few hours before being replaced or recharged. It is important that all design efforts are made to conserve power in those systems. Energy consumption in a system can be reduced using a number of techniques, such as low-power electronics, architecture-level power reduction, compiler techniques, to name just a few. However, energy conservation at the application software-level has not yet been explored. In this paper, we show the impact of various software implementation techniques on energy saving. Based on the observation that different instructions of a processor cost different amount of energy, we propose three energy saving strategies, namely (i) assigning live variables to registers, (ii) avoiding repetitive address computations, and (iii) minimizing memory accesses. We also study how a variety of algorithm design and implementation techniques affect energy consumption. In particular, we focus on the following aspects: (i) recursive versus iterative (with stacks and without stacks), (ii) different representations of the same algorithm, (iii) different algorithms - with identical asymptotic complexity - for the same problem, and (iv) different input representations. We demonstrate the energy saving capabilities of these approaches by studying a variety of applications related to power-conscious systems, such as sorting, pattern matching, matrix operations, depth-first search, and dynamic programming. From our experimental results, we conclude that by suitably choosing an algorithm for a problem and applying the energy saving techniques, energy savings in excess of 60% can be achieved.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>WISE: Automated test generation for worst-case complexity</title>
    <abstract>Program analysis and automated test generation have primarily been used to find correctness bugs. We present complexity testing, a novel automated test generation technique to find performance bugs. Our complexity testing algorithm, which we call WISE (Worst-case Inputs from Symbolic Execution), operates on a program accepting inputs of arbitrary size. For each input size, WISE attempts to construct an input which exhibits the worst-case computational complexity of the program. WISE uses exhaustive test generation for small input sizes and generalizes the result of executing the program on those inputs into an “input generator.” The generator is subsequently used to efficiently generate worst-case inputs for larger input sizes. We have performed experiments to demonstrate the utility of our approach on a set of standard data structures and algorithms. Our results show that WISE can effectively generate worst case inputs for several of these benchmarks.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>The GNU libstdc++ parallel mode: software engineering considerations</title>
    <abstract>The C++ Standard Library implementation provided with the free GNU C++ compiler, libstdc++, provides a "parallel mode" as of version 4.3. Using this mode enables existing serial code to take advantage of many parallelized STL algorithms, an approach to making use of multi-core processors which are now or will soon will be ubiquitous. This paper describes the software engineering issues discovered during implementation, the results of user testing, and presents possible solutions to outstanding issues. Design issues with configuring the software environment to a wide variety of multi-core hardware options, influencing algorithm and parameter choices at compile and run time, standards compliance, and the interplay between execution speed, the executable size, the library code size, and the compilation time are addressed.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>External sorting for index construction of large semantic web databases</title>
    <abstract>Today's Semantic Web datasets become increasingly larger containing up to several hundred million triples. The performance of index construction is a crucial factor for the success of large Semantic Web databases. In this paper, we propose two new approaches for RDF index construction: External chunks merge sort and Distribution Sort for RDF. The former stores and retrieves chunks from a special chunks heap to speed up replacement selection. The latter leverages the RDF-specific properties to construct RDF indices and significantly improves the performance. Our experimental results show that our approaches significantly speed up RDF index construction, and are important techniques for large Semantic Web databases.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>On Developing New Models, with Paging as a Case Study</title>
    <abstract>As computer science has progressed, numerous models and measures have been developed over the years. Among the most commonly used in theoretical computer science are the RAM model, the I/O model, worst case analysis, space (memory) usage, average case analysis, amortized analysis, adaptive analysis and the competitive ratio. New models are added to this list every few years to re ect varying constraints imposed by novel application or advances in computer architectures. Examples of alternative models are the transdichotomous RAM or word-RAM, the data stream model, the MapReduce model, the cache oblivious model and the smoothed analysis model. New models and measures, when successful expand our understanding of computation and open new avenues of inquiry. As it is to be expected relatively few models and paradigms are introduced every year, and even less are eventually proven successful. In this paper we discuss rst certain shortcomings of the online competitive analysis model particularly as it concerns paging, discuss existing solutions in the literature as well as present recent progress in developing models and measures that better re ect actual practice for the case of paging. From there we proceed to a more general discussion on how to measure and evaluate new models within theoretical computer science and how to contrast them, when appropriate, to existing models. Lastly, we highlight certain \natural" choices and assumptions of the standard worst-case model which are often unstated and rarely explicitly justied. We contrast these choices to those made in the formalization of probability theory.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Introduction to the SIGACT news online algorithms column</title>
    <abstract>Pros and cons of competitive analysis have been debated since its inception in mid 1980s. Much of this discussion centered around the accuracy of performance evaluation methods for paging, which is probably the most central among online optimization problems studied in the literature, and, at the same time, ironically, the most prominent example of shortcomings of competitive analysis. In this quarter's column, Reza Dorrigiv and Alejandro Lopez-Ortiz review the history of the problem, discuss several performance models from the literature, and propose a new model that avoids shortcomings of previous approaches. If you are interested in contributing to the column as a guest writer, feel free to contact me by email. All kinds of contributions related to online algorithms and competitive analysis are of interest: technical articles, surveys, conference reports, opinion pieces, and other.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Bureaucratic protocols for secure two-party sorting, selection, and permuting</title>
    <abstract>In this paper, we introduce a framework for secure two-party (S2P) computations, which we call bureaucratic computing, and we demonstrate its efficiency by designing practical S2P computations for sorting, selection, and random permutation. In a nutshell, the main idea behind bureaucratic computing is to design data-oblivious algorithms that push all knowledge and influence of input values down to small black-box circuits, which are simulated using Yao's garbled paradigm. The practical benefit of this approach is that it maintains the zero-knowledge features of secure two-party computations while avoiding the significant computational overheads that come from trying to apply Yao's garbled paradigm to anything other than simple two-input functions.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Implementing sorting in database systems</title>
    <abstract>Most commercial database systems do (or should) exploit many sorting techniques that are publicly known, but not readily available in the research literature. These techniques improve both sort performance on modern computer systems and the ability to adapt gracefully to resource fluctuations in multiuser operations. This survey collects many of these techniques for easy reference by students, researchers, and product developers. It covers in-memory sorting, disk-based external sorting, and considerations that apply specifically to sorting in database systems.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Sorting hierarchical data in external memory for archiving</title>
    <abstract>Sorting hierarchical data in external memory is necessary for a wide variety of applications including archiving scientific data and dealing with large XML datasets. The topic of sorting hierarchical data, however, has received little attention from the research community so far. In this paper we focus on sorting arbitrary hierarchical data that far exceed the size of physical memory. We propose HErMeS, an algorithm that generalizes the most widely-used techniques for sorting flat data in external memory. HErMeS efficiently exploits the hierarchical structure to minimize the number of disk accesses and optimize the use of available memory. We extract the theoretical bounds of the algorithm with respect to the structure of the hierarchical dataset. We then show how the algorithm can be used to support efficient archiving. We have conducted an experimental study using several workloads and comparing HErMeS to the state-of-the-art approaches. Our results show that our algorithm (a) meets its theoretical expectations, (b) allows for scalable database archiving, and (c) outperforms the competition by a significant factor. These results, we believe, prove our technique to be a viable and scalable solution to the problem of sorting hierarchical data in external memory.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Continuity analysis of programs</title>
    <abstract>We present an analysis to automatically determine if a program represents a continuous function, or equivalently, if infinitesimal changes to its inputs can only cause infinitesimal changes to its outputs. The analysis can be used to verify the robustness of programs whose inputs can have small amounts of error and uncertainty---e.g., embedded controllers processing slightly unreliable sensor data, or handheld devices using slightly stale satellite data. Continuity is a fundamental notion in mathematics. However, it is difficult to apply continuity proofs from real analysis to functions that are coded as imperative programs, especially when they use diverse data types and features such as assignments, branches, and loops. We associate data types with metric spaces as opposed to just sets of values, and continuity of typed programs is phrased in terms of these spaces. Our analysis reduces questions about continuity to verification conditions that do not refer to infinitesimal changes and can be discharged using off-the-shelf SMT solvers. Challenges arise in proving continuity of programs with branches and loops, as a small perturbation in the value of a variable often leads to divergent control-flow that can lead to large changes in values of variables. Our proof rules identify appropriate ``synchronization points'' between executions and their perturbed counterparts, and establish that values of certain variables converge back to the original results in spite of temporary divergence. We prove our analysis sound with respect to the traditional epsilon-delta definition of continuity. We demonstrate the precision of our analysis by applying it to a range of classic algorithms, including algorithms for array sorting, shortest paths in graphs, minimum spanning trees, and combinatorial optimization. A prototype implementation based on the Z3 SMT-solver is also presented.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Double sorting: testing their sorting skills</title>
    <abstract>You're teaching elementary sorting techniques, and you would like your students to do a programming assignment that tests their understanding of the ideas. But all of the code for elementary sorting techniques are in the textbook, easily found on the Web, etc. We suggest the use of two "Double Sorting" techniques whose solution is not standardly available, are fairly straight-forward to code, and offer speed improvements over the "straight" sorts. Double Sorting, the idea of processing two chosen elements simultaneously, applies to both Insertion Sort and Selection Sort, with speedups of 33% and 25%, hence are good enough to justify coding, but not good enough to be in Web collections. Code for this can be written in as little as a dozen lines of C++/Java code, and is easily within the reach of introductory students who understand the basic algorithms. In addition, the ideas used for double sorting are natural first steps in understanding how the N2 sorts can be improved towards the N log N sorts that they will study later. For more advanced students, these double sorts also generate good exercises in the analysis of algorithms.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Sorting and selection in posets</title>
    <abstract>Classical problems of sorting and searching assume an underlying linear ordering of the objects being compared. In this paper, we study these problems in the context of partially ordered sets, in which some pairs of objects are incomparable. This generalization is interesting from a combinatorial perspective, and it has immediate applications in ranking scenarios where there is no underlying linear ordering, e.g., conference submissions. It also has applications in reconstructing certain types of networks, including biological networks. Our results represent significant progress over previous results from two decades ago by Faigle and Turán. In particular, we present the first algorithm that sorts a width-w poset of size n with optimal query complexity O(n(w + log n)). We also describe a variant of Mergesort with query complexity O(wn log n/w) and total complexity O(w2n log n/w); an algorithm with the same query complexity was given by Faigle and Turán, but no efficient implementation of that algorithm is known. Both our sorting algorithms can be applied with negligible overhead to the more general problem of reconstructing transitive relations. We also consider two related problems: finding the minimal elements, and its generalization to finding the bottom k "levels", called the k-selection problem. We give efficient deterministic and randomized algorithms for finding the minimal elements with O(wn) query and total complexity. We provide matching lower bounds for the query complexity up to a factor of 2 and generalize the results to the k-selection problem. Finally, we present efficient algorithms for computing a linear extension of a poset and computing the heights of all elements.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>C is for circuits: capturing FPGA circuits as sequential code for portability</title>
    <abstract>ing common sequential algorithms, captured in a language like C, to FPGA circuits is now well-known to provide dramatic speedups for numerous applications, and to provide tremendous portability and adaptability advantages over circuit implementations of an application. However, many applications targeted to FPGAs are still designed and distributed at the circuit level, due in part to tremendous human ingenuity being exercised at that level to achieve exceptional performance and efficiency. A question then arises as to whether applications for FPGAs will have to be distributed as circuits to achieve desired performance and efficiency, or if instead a more portable language like C might be used. Given a set of common synthesis transformations, we studied the extent to which circuits published in FCCM in the past 6 years could be captured as sequential code and then synthesized back to the published circuit. The study showed that a surprising 82% of the 35 circuits chosen for the study could be re-derived from some form of standard C code, suggesting that standard C code, without extensions, may be an effective means for distributing FPGA applications</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Efficient sorting using registers and caches</title>
    <abstract>Modern computer systems have increasingly complex memory systems. Common machine models for algorithm analysis do not reflect many of the features of these systems, e.g., large register sets, lockup-free caches, cache hierarchies, associativity, cache line fetching, and streaming behavior. Inadequate models lead to poor algorithmic choices and an incomplete understanding of algorithm behavior on real machines.A key step toward developing better models is to quantify the performance effects of features not reflected in the models. This paper explores the effect of memory system features on sorting performance. We introduce a new cache-conscious sorting algorithm, R-MERGE, which achieves better performance in practice over algorithms that are superior in the theoretical models. R-MERGE is designed to minimize memory stall cycles rather than cache misses by considering features common to many system designs.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A new class of nature-inspired algorithms for self-adaptive peer-to-peer computing</title>
    <abstract>We present, and evaluate benefits of, a design methodology for translating natural phenomena represented as mathematical models, into novel, self-adaptive, peer-to-peer (p2p) distributed computing algorithms (protocols). Concretely, our first contribution is a set of techniques to translate discrete sequence equations (also known as difference equations) into new p2p protocols called sequence protocols. Sequence protocols are self-adaptive, scalable, and fault-tolerant, with applicability in p2p settings like Grids. A sequence protocol is a set of probabilistic local and message-passing actions for each process. These actions are translated from terms in a set of source sequence equations. Individual processes do not simulate the source sequence equations completely. Instead, each process executes probabilistic local and message passing actions, so that the emergent round-to-round behavior of the sequence protocol in a p2p system can be probabilistically predicted by the source sequence equations. The article's second contribution is the design and evaluation of a set of sequence protocols for detection of two global triggers in a distributed system: threshold detection and interval detection. This article's third contribution is a new self-adaptive Grid computing protocol called HoneyAdapt. HoneyAdapt is derived from sequence equations modeling adaptive bee foraging behavior in nature. HoneyAdapt is intended for Grid applications that allow Grid clients, at run-time, a choice of algorithms for executing chunks of the application's dataset. HoneyAdapt tells each Grid client how to adaptively select at run-time, for each chunk it receives, a good algorithm for computing the chunk—this selection is based on continuous feedback from other clients. Finally, we design a variant of HoneyAdapt, called HoneySort, for application to Grid parallelized sorting settings using the master-worker paradigm. Our evaluation of these contributions consists of mathematical analysis, large-scale trace-based simulation results, and experimental results from a HoneySort deployment.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>
  <item>
    <title>Type classes in functional logic programming</title>
    <abstract>Type classes provide a clean, modular and elegant way of writing overloaded functions. Functional logic programming languages (FLP in short) like Toy or Curry have adopted the Damas-Milner type system, so it seems natural to adopt also type classes in FLP. However, type classes has been barely introduced in FLP. A reason for this lack of success is that the usual translation of type classes using dictionaries presents some problems in FLP like the absence of expected answers due to a bad interaction of dictionaries with the call-time choice semantics for non-determinism adopted in FLP systems. In this paper we present a type-passing translation of type classes based on type-indexed functions and type witnesses that is well-typed with respect to a new liberal type system recently proposed for FLP. We argue the suitability of this translation for FLP because it improves the dictionary-based one in three aspects. First, it obtains programs which run as fast or faster---with an speedup from 1.05 to 2.30 in our experiments. Second, it solves the mentioned problem of missing answers. Finally, the proposed translation generates shorter and simpler programs.</abstract>
    <search_task_number>7</search_task_number>
    <query>quicksort mergesort</query>
    <relevance>0</relevance>
  </item>



  <item>
    <title>Constructing DEVS models based on experts' knowledge: application to STMicroelectronics' large scale manufacturing processes (Work-in-Progress)</title>
	<abstract>STMicroelectronics has a spirit of innovation in the development of chips, continuously modifying its production processes for improving performance. This is why it is interested in creating 'generic models' of their manufacturing processes and keeping them updated in spite of the high number of modifications demanded. It is specially interested in capitalizing experts' knowledge. In this article we explain ideas for constructing models for STMicroelectronics manufacturing processes using experts' definitions. The importance of our approach resides in the construction of models that constitute a step forwards to the implementation of an alarm control system over processes modifications.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Research on Dynamic Reconfigurable Manufacturing Process Model Based on Model Driven Architecture</title>
	<abstract>With the characteristic analysis of manufacturing process in various manufacture enterprises, Model Driven Architecture methodology is applied to construct dynamic reconfiguration manufacturing process model, which can solve the problems of portability, information integration, and interoperability effectively. A dynamic reconfigurable modeling process framework based on model driven Architecture is introduced for manufacture enterprises in this paper. It separates the manufacturing system functionality specification from its implementation on any specific technology platform, and makes the system reconfigurate the manufacturing progress through the transformation of models and transmission of information. Based on MDA methodology, a PIM was built with UML for dynamic reconfiguration manufacturing process model. Then, the PIM was transformed into several indispensable PSMs. At last, the integrated system was applied to a discreet manufacturing process and the application result proves that the system is practical and effective.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Manufacturing planning and predictive process model integration using software agents</title>
	<abstract>Intelligent agents provide a means to integrate various manufacturing software applications. The agents are typically executed in a computer-based collaborative environment, referred to as a multi-agent system. The National Institute of Standards and Technology (NIST) has developed a prototype multi-agent system supporting the integration of manufacturing planning, predictive machining models, and manufacturing control. The agents within this platform have access to a knowledge base, a manufacturing resource database, a numerical control programming system, a mathematical equation solving system, and a computer-aided design system. Intelligence has been implemented within the agents in rules that are used for process planning, service selection, and job execution. The primary purposes for developing such a platform are to support the integration of predictive models, process planning, and shop floor machining activities and to demonstrate an integration framework to enable the use of machining process knowledge with higher-level manufacturing applications.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A Standardised Data Model for Process and Flow Management: ISO 15531-43 --A Step Towards CE in Manufacturing --</title>
	<abstract>Ready access to information is a critical requirement in concurrent engineering and access to manufacturing information is particularly important to businesses involved in the product production. While a range of research has considered manufacturing information models for process planning, little has been done to formalize an information model to support process flow information. This is necessary for all areas of manufacture where flow is important including process planning, production planning, scheduling and supply chain management. This paper introduces a new standard which is being introduced to support flow management and discusses the key features which it offers.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Pattern recognition of manufacturing process signals using Gaussian mixture models-based recognition systems</title>
	<abstract>Unnatural patterns exhibited in manufacturing processes can be associated with certain assignable causes for process variation. Hence, accurate identification of various process patterns (PPs) can significantly narrow down the scope of possible causes that must be investigated, and speed up the troubleshooting process. This paper proposes a Gaussian mixture models (GMM)-based PP recognition (PPR) model, which employs a collection of several GMMs trained for PPR. By using statistical features and wavelet energy features as the input features, the proposed PPR model provides more simple training procedure and better generalization performance than using single recognizer, and hence is easier to be used by quality engineers and operators. Furthermore, the proposed model is capable of adapting novel PPs through using a dynamic modeling scheme. The simulation results indicate that the GMM-based PPR model shows good detection and recognition of current PPs and adapts further novel PPs effectively. Analysis from this study provides guidelines in developing GMM - based SPC recognition systems.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>On-line analysis of out-of-control signals in multivariate manufacturing processes using a hybrid learning-based model</title>
	<abstract>Advanced automatic data acquisition is now widely adopted in manufacturing industries and it is common to monitor several correlated quality variables simultaneously. Most of multivariate quality control charts are effective in detecting out-of-control signals based upon an overall statistics in multivariate manufacturing processes. The main problem of such charts is that they can detect an out-of-control event but do not directly determine which variable or group of variables has caused the out-of-control signal and what is the magnitude of out of control. This study presents a hybrid learning-based model for on-line analysis of out-of-control signals in multivariate manufacturing processes. This model consists of two modules. In the first module using a support vector machine-classifier, type of unnatural pattern can be recognized. Then by using three neural networks for shift mean, trend and cycle it can be recognized magnitude of mean shift, slope of trend and cycle amplitude for each variable simultaneously in the second module. The performance of the proposed approach has been evaluated using two examples. The output generated by trained hybrid model is strongly correlated with the corresponding actual target value for each quality characteristic. The main contributions of this work are recognizing the type of unnatural pattern and classification major parameters for shift, trend and cycle and for each variable simultaneously by proposed hybrid model.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Affordable Design - A Methodology to Implement Process-Based Manufacturing Cost Models Into the Traditional Performance-Focused Multidisciplinary Design Optimization</title>
	<abstract>The primary objective of this paper is to demonstrate the use of process-based manufacturing and assembly cost models in a traditional performance-focused multidisciplinary design and optimization process. The use of automated cost-performance analysis is an enabling technology that could bring realistic process-based manufacturing and assembly cost into multidisciplinary design and optimization. In this paper, we present a new methodology for incorporating process costing into a standard multidisciplinary design optimization process. Material, manufacturing processes, and assembly processes cost then could be used as the objective function for the optimization methods. A case study involving forty-six different configurations of a simple wind is presented, indicating that a design based on performance criteria alone may not necessarily be the most affordable as far as manufacturing and assembly cost is concerned.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Conceptual model for semantic representation of industrial manufacturing processes</title>
	<abstract>Industrial manufacturing processes representation is a key challenge for leveraging interoperability among business partners. The Semantic representation of information enables the creation of intelligent systems, which can interpret and understand potentially automated tasks, harnessing added-value decision-making processes. Particularly, the Semantic Web can provide a cutting-edge formal representation and knowledge-driven set of technologies to enable automation of industrial manufacturing processes. This paper presents an ontology and a proof-of-concept implementation to describe the automation of decision-making processes which model human behavior, representing the interaction with the overall environment. The model is based on different situations a problem might yield and the correspondent behavioural responses which should be generated. Using the concept of ''Situation'' as the conceptual corner-stone and building block of descriptions, we discuss how semantics provides a natural knowledge representation strategy, which eases the resource-intensive process of acquiring knowledge. The validation milestones of the system come from a real-world company where the system has been in production mode for a remarkably successful time, a mechanical parts factory.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Bringing Manufacturing into Design via Process-Dependent SPICE Models</title>
	<abstract>This paper describes methodology for constructing compact SPICE models as a function of process parameter variations. The methodology involves global extraction of process-dependant SPICE model parameters from silicon calibrated TCAD simulations. The model is validated by comparing device characteristics from the extracted SPICE parameters with those from TCAD simulations. The analysis demonstrates an excellent goodness of fit over the full range of process parameter variations. The process-dependant SPICE models allow direct access to process parameter variations in circuit design. The extracted models are employed in rudimentary digital circuits to investigate the delay variation in response to process deviations. The proposed approach significantly improves design-for-manufacturing (DFM) by allowing for accurate design sensitivity analysis and parametric yield assessment, as a function of statistically independent and measurable process variations.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>HPNS: a hybrid process net simulation environment executing online dynamic models of industrial manufacturing systems</title>
	<abstract>Modelling technical systems nowadays is a great challenge of automation technology. Particular complex manufacturing processes, like the industrial paper production, consists of discrete and continuous signals and dynamic response times. Inspired by Petri nets, this paper proposes a new modelling approach is presented to describe those technical systems on different abstraction layer. The hybrid process net simulator (HPNS) framework allows to specify complex models, typically represented as a system of differential equations mixed with continuous and discrete-event subsystems, based on a bipartite graph structure. In addition, the HPNS execute model specifications with dynamic delays.

This paper focuses on the concept and the modelling approach. We give a short review of this new unified representation model for hybrid technical systems, the presentation of the model formalism is out of scope of this paper. A short summary about capabilities and restrictions of HPNS is presented. Moreover, examples of hybrid systems are presented.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Chip speed prediction model for optimization of semiconductor manufacturing process using neural networks and statistical methods</title>
	<abstract>As the increase of device complexity, prediction of chip performance such as chip speed is crucial as much as yield prediction model in semiconductor manufacturing. In this paper, hybrid of circuit simulation and DOE (design of experiment) are applied to develop speed prediction models for high-speed microprocessor manufacturing process. Speed prediction models are developed through three steps; transistor ratio variation analysis step, electrical test data analysis step, and fabrication test data analysis step. Artificial neural networks are used to find relation between measured data and chip speed and network inputs are selected based on statistical analysis. Electrical test based modeling results showed only 1.2% of RMS error and fabrication level speed prediction model showed 83% of fitness results.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Finite element analysis of the thermoforming manufacturing process using the hyperelastic mooney-rivlin model</title>
	<abstract>Thermoforming is a manufacturing process widely used to produce thin thermoplastic parts. In this process a previously extruded thermoplastic sheet is clamped and then heated and formed into a mold cavity using a differential pressure. In this paper a finite element model of the thermoforming process of an ABS sheet is proposed and numerical results are compared to data from literature. Thermoplastic sheet is modelled according to the membrane formulation. An implicit time scheme has been adopted for the integration algorithm. Mechanical behaviour of the processing material is assumed as hyperelastic, according to the two parameters Mooney-Rivlin model. Mathematical formulation of the mechanical model is exposed. The proposed model allows to evaluate material thinning, stresses, strains and contact status between the processing material and the die.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Model reference adaptive temperature control of the electromagnetic oven process in manufacturing process</title>
	<abstract>Nowadays, the electromagnetic ovens are used for heating the component assembly of electronic manufacturing. The control systems of the electromagnetic ovens are feedback control system and PID controller are used to control their temperature. This process is an importance process in electronics manufacturing. The problems of the electromagnetic oven are the accuracy of the temperature response when they are used in the long period of time. Examples of their problems are overshoot and high value of delay time. When the over rising of temperature occur, it leads to produce damaging. The over temperature problem is suspected that be can taken from physical changing of the electromagnetic oven. Because of this reason, PID controller parameters are not appropriate with a new condition. Therefore, the electromagnetic oven control system will have low efficiency. This paper presents the control method for any electromagnetic oven by using Model Reference Adaptive System (MRAS) to adjust the parameters of PI controller. An approximated transfer function of the electromagnetic oven obtained from closed-loop data by using nonlinear least squares method and the optimization of PID controller design is demonstrated via gradient descent method. The experimental and simulation results showed good performance in actual operations.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>A process-driven computing model for reconfigurable semiconductor manufacturing</title>
	<abstract>Reconfigurability is essential for semiconductor manufacturing systems to remain competitive. Reconfigurable systems avoid costly modifications required to change and adapt to changes in product, production and services. A fully automated, collaborative, and integrated while reconfigurable manufacturing system proves cost-effective in the long term and is a promising strategy for the semiconductor manufacturing industry. However, there is a lack of computing models to facilitate the design and development of control and management systems in a truly reconfigurable manner. This paper presents an innovative computing model for reconfigurable systems and controlled manufacturing processes while allowing for the integration of modern technologies to facilitate reconfiguration, such as radio frequency identification (RFID) and reconfigurable field programmable gate array (FPGA). Shop floor manufacturing activities are modeled as processes from a business perspective. A process-driven formal method that builds on prior research on virtual production lines is proposed for the formation of a reconfigurable cross-facility manufacturing system. The trajectory of the controlled manufacturing systems is optimized for on-demand production services. Reconfigurable process controllers are introduced in support of the essential system reconfigurability of future semiconductor manufacturing systems. Implementation of this approach is also presented.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A neural network ensemble-based model for on-line monitoring and diagnosis of out-of-control signals in multivariate manufacturing processes</title>
	<abstract>In multivariate statistical process control, most multivariate quality control charts are shown to be effective in detecting out-of-control signals based upon an overall statistics. But these charts do not relieve the need for pinpointing source(s) of the out-of-control signals, which would be very useful for quality practitioners to locate the assignable causes that give rise to the out-of-control situation. In this study, a learning-based model has been investigated for monitoring and diagnosing out-of-control signals in a bivariate process. In this model, a selective neural network (NN) ensemble approach (DPSOEN, Discrete Particle Swarm Optimization) was developed for performing these tasks. The simulation results demonstrate that the proposed model outperforms the conventional multivariate control scheme in terms of average run length (ARL), and can accurately classify source(s) of out-of-control signals. Extensive experiment is also carried out to examine the effects of six statistical features on the performance of DPSOEN. Analysis from this study provides guidelines in developing NN ensemble-based Statistical process control recognition systems in multivariate processes.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Fuzzy performance modeling aligned with process and organization model of integrated system in manufacturing</title>
	<abstract>Based on long-term industrial practice, our research team addressed economic view to complement the existing CIM system architecture, which is used to measure the relevant performance and has become the annex C of ISO 15704. However the work on economic view issued before mainly focused on the performance framework, in this paper a fuzzy performance modeling process and method is proposed as the further research. Considering the numerous dynamic and subjective indicators of performance, the related fuzzy method that the research largely relies on here is the best choice for its natural advantages in this area. Before the method for fuzzy performance modeling is put forward, the fuzzy performance modeling framework aligned with process and organization model is discussed firstly. Afterwards, a theorem is presented for conflict and redundancy validation of fuzzy rules that is necessary for performance modeling. Finally the fuzzy performance modeling and measurement method are suggested.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Using ANNs to model hot extrusion manufacturing process</title>
	<abstract>In metal forming processes, automatic selection of forming tools is heavily depended on the estimation of forming forces. Due to complex relationships between processes parameters like die angle, co-efficient of friction, velocity of dies, and temperature of billet for forming products with sound quality and forming forces related, there is a need to develop approximate models to estimate the forming forces without complex mathematical models or time-consuming simulation techniques. In this paper, an Artificial Neural Networks (ANNs) model has been developed for rapid predication of the forming forces based on process parameters. The results obtained are found to correlate well with the finite element simulation data in case of hot extrusion.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Concept of a data exchange agent system for automatic construction of simulation models of manufacturing processes</title>
	<abstract>The simulative agent system is going to be capable of independent collection of information required for construction of a simulation model, monitoring and constant data updating, deciding on the necessity of building new updated simulation models, applying changes to data (based on results of a conducted simulation) into class ERP and PDM systems. In this way will be achieved - apart from faster model construction - an increase in reality reproduction precision, in comparison to simulation models built with traditional methods.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Research on Quality Supervision Model for Cooperative Manufacturing Preparing Process in Virtual Enterprise</title>
	<abstract>On the basis of the analysis of internal and external research on current situation of quality supervision theory for cooperative manufacturing preparing process in virtual enterprise, the quality supervision model for cooperative manufacturing preparing process in virtual enterprise is firstly constructed; and secondly, this paper mainly probes into contents and quantitative method of the supervision and approve of process planning and trial manufacturing quality plans in cooperative manufacturing process by means of ISO 9000 family standards and the method of system reliability analysis; thirdly, the method of appraisal and approve of the stability and capability in initial process is exploited by the methods of data transfer in continuous process and SPC technology ; finally, contents and quantitative method of the appraisal and approve of trial products are presented; Meanwhile, the applied research is proposed in combined with the practice of cooperative manufacturing preparing process of power products in a virtual enterprise.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Evaluating and Selecting Flexible Manufacturing Systems by Integrating Data Envelopment Analysis and Analytical Hierarchy Process Model</title>
	<abstract>Because of continuous changes in the market and the today’s competitiveness world, most of manufacturing firms are competing for meeting demand, increasing quality and decreasing costs. Today, manufacturing firms are required to select a suitable flexible manufacturing system (FMS) to keep their share of the market. We propose a procedure for evaluating the flexible manufacturing systems based on a model incorporating two decision models namely “Data Envelopment Analysis (DEA)” and “Analytical Hierarchy Process (AHP)”. DEA helps us to categorize the Decision Making Units (DMUs) to only two classes of efficient and inefficient units, whereas by using an AHP, we can have a full rank of DMUs. Input and output factors considered for ranking FMSs are Capital and Operating Costs, Throughput Time, Work in Process, Labor Requirements, Required Floor Space, Product Mix Flexibility,Yield, and Volume Flexibility. We implement this approach for a Vehicle Manufacturing Company in Iran.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Validation and verification of the simulation model of a photolithography process in semiconductor manufacturing</title>
	<abstract>An abstract is not available.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>  
    <item>
    <title>Overlapping model of beads and curve fitting of bead section for rapid manufacturing by robotic MAG welding process</title>
	<abstract>As a deposition technology, robotic metal active gas (MAG) welding has shown new promises for rapid prototyping (RP) of metallic parts. During the process of forming metal parts with the robotic MAG welding technology, the sectional geometry of single-pass bead and the overlap of the adjacent beads have critical effects on the dimensional accuracy and quality of metal parts. In this work, Canny edge detection of the robotic MAG beads was carried out and the data were smoothed with a Gaussian filter and fitted with Gaussian function, logistic function, parabola function and sine function, respectively. In addition, a mathematical model of bead section was developed to analyze the bead geometry. Based on ''surfacing of equivalent area'' method, the concept of overlapping coefficient and optimum-overlapping coefficient was put forward, and calculated model of overlapping was analyzed. Optimal overlapping coefficient was calculated to be 63.66% under experimental condition. The conclusion is that the edge detection of bead section with Canny operator is continuous and distinct, and as compared with Gaussian function, logistic function and parabola function, sine function has higher accuracy to fit the measured data, and ''surfacing of equivalent area'' method shows to be rational and feasible by the experiments.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The Research and Application on Process Planning Knowledge Discovery Technology Based on Knowledge Model in Discrete Manufacturing Enterprise</title>
	<abstract>With the development of knowledge economy, knowledge resource in manufacturing enterprise has become the strategic resource. The competition in manufacturing enterprises focuses on the development and management of knowledge resource. CAPP (Computer Aided Processing Planning) system is the supportable system in process planning information for discrete manufacturing enterprise. The systematic methods of process planning knowledge discovery about professional knowledge, experience and criterion in process planning work become the key problem in CAPP system. Process planning knowledge model is analyzed oriented process planning knowledge discovery problem in CAPP system. Process planning knowledge discovery technology based on knowledge model driving is researched and is applied in CAPPFramework system (a CAPP development platform supported by 863/CIMS in China). The practical application of knowledge discovery technology in CAPPFramework is described in detail, it can automatically achieve the knowledge in process planning database by the interaction with knowledge engineer.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>A model-based engineering process for increasing productivity in discrete manufacturing</title>
	<abstract>In a competitive market, manufacturing enterprises require a process of continuos, on-going improvement in order to maintain and enhance productivity and a competitive edge. The use of Information Systems (IS) has been increasingly playing a critical role in any such engineering process. The primary drivers are efficiency and quality increase through automation, facilitation of better business processes and improved decision making. The engineering process involves use of an IS for collection of relevant production data, visualization, analysis and decision making. Many problems and issues relating to the design, development, integration, evolution and maintenance of ISs in large-scale and complex plants have become apparent which are not adequately addressed by the traditional Process Monitoring &amp; Control (PM&amp;C) systems. Model Integrated Computing (MIC) [1] offers a feasible approach towards providing costeffective development, integration, evolution and maintenance of ISs through the extensive use of models during the life cycle. This paper describes the application of MIC in the engineering process for improving productivity in the context of discrete manufacturing operations at Saturn. The Saturn Site Production Flow (SSPF) system is a client-server application, designed to provide an integrated problem-solving environment. It presents consistent and pertinent information, provides analysis and decision support services that are needed for informed decision making by the team members and leaders within Saturn.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Master Model-Driven Process and Fixture Collaborative Design for Aeroengine Blade Manufacturing</title>
	<abstract>In order to solve data inconsistency and low designing efficiency in the process of aeroengine blade manufacturing, the production master model technologies is put forward and described in detail. Integrate framework for aeroengine blade manufacturing has been constructed. The process master model has been described in detail. The process master mode-driven rapid fixture design method has been analyzed and researched to improve efficiency in blade fixture designing. On the base of process master model, the collaborative design mode between process planning and fixture designing is proposed. At last, an engineering application case is used to verify theory and method above mentioned.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>CIMOSA model creation and execution for a casting process and a manufacturing cell</title>
	<abstract>An abstract is not available.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Process/Product Integrated Planning in a Manufacturing System: A Linear Programming Model</title>
	<abstract>An abstract is not available.</abstract>>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A General Model of Auxiliary Processes in Manufacturing</title>
	<abstract>An abstract is not available.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Management of resources in small and medium-sized production enterprises</title>
	<abstract>Enormous loss of production time in machining, handling and assembly operations is caused mainly by resources' unavailability. For this reason companies, both large and small, should optimize the production processes with the detection and elimination in advance of the majority of possible mistakes and disturbances that could cause deadlocks in the production process. The optimization tools which are available on the market are usually too expensive for smaller companies, which search for their competitive position in small quantity production. Therefore, a low cost simulation tool for the optimization of the resource present management is presented in this paper, where resources are treated as one of the main parameters in the production process and which could also enable smaller companies to optimize their production processes from the resource presence viewpoint.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A QoS-based GFAM Scheduling Approach for Manufacturing Grid</title>
	<abstract>In order to resolve resource scheduling problem in Manufacturing Grid (MG) to handle dynamic changes in the availability of manufacturing resources and user requirements, and to provide scalable, controllable, measurable, and easily enforceable policies for the management of manufacturing resources, we propose a QoS (Quality of Service)-based scheduling system - Manufacturing Grid Resource Scheduler (MGRS). In MGRS, we make some necessary definitions, such as the QoS-based manufacturing task/sub-task, QoSbased manufacturing resource, and then deduce the Resource-Task Matching (RTM) matrix for resource scheduling. Based on these definitions and deductions, this paper sets up the Group decision-making Fuzzy Analytic hierarchy process Model (GFAM), with resources' QoS attributes as the evaluation criteria, and the Fuzzy Analytic Hierarchy Process (FAHP) and Group Decision-Making (GDM) as the arithmetic basis. The goal of the GFAM is to select a most suitable resource from the authorized resource lists to match each submitted task with the better QoS performance, such as the higher user satisfaction, product quality and service, as well as the lower failure rate, time-to-market, and cost, etc. Furthermore, the GFAM model is applied in MG, and the results show that it is a very useful solution for resource scheduling during the process of products manufacturing.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Casting Back Errant Stage in Multi-stage Manufacturing Process</title>
	<abstract>Each stage is influence on product quality in multistage manufacturing process. When product have some problem, the traditional statistical process control method was not detected the exact errant reason timely due to the interaction of each stage. On the basis of the variation transmission model, the conception of virtual stage is proposed. Then, by the hypothesis testing, the stage having errant can be judged exactly. Finally, a knuckle pivot manufacturing process with three stages is given for illustration and verification.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>0</relevance>
  </item>
    <item>
    <title>Study on Management System of Energy Information in Manufacturing Processes</title>
	<abstract>In order to manage energy utilization precisely in discrete manufactures and realize energy saving, energy information models were proposed firstly aiming at dispersive energy consumption and the problem of product energy consumption calculation. Secondly an energy information management system including an energy management tool, an energy consumption accounting tool and an energy consumption analysis tool was developed based on these models. Then the corresponding energy databases in discrete manufactures were designed to manage energy utilization precisely and efficiently in discrete manufactures. Thirdly some key technologies were adopted to realize multi-user information interaction and ensure security, stability, and interoperability of the information. Feasibility and effectiveness of the system were verified by a case of enterprise application.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Hybrid discrete event simulation with model predictive control for semiconductor supply-chain manufacturing</title>
	<abstract>Simulation modeling combined with decision control can offer important benefits for analysis, design, and operation of semiconductor supply-chain network systems. Detailed simulation of physical processes provides information for its controller to account for (expected) stochasticity present in the manufacturing processes. In turn, the controller can provide (near) optimal decisions for the operation of the processes and thus handle uncertainty in customer demands. In this paper, we describe an environment that synthesizes Discrete-EVent System specification (DEVS) with Model Predictive Control (MPC) paradigms using a Knowledge Interchange Broker (KIB). This environment uses the KIB to compose discrete event simulation and model predictive control models. This approach to composability affords flexibility for studying semiconductor supply-chain manufacturing at varying levels of detail. We describe a hybrid DEVS/MPC environments via a knowledge interchange broker. We conclude with a comparison of this work with another that employs the Simulink/MATLAB environment.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Improving cost model development process using advanced data generation and data analysis techniques</title>
	<abstract>This paper provides an example of the use of virtual manufacturing and data mining techniques in the development of a model for manufacturing process time estimation and hence the cost. A range of experiments was initially carried out which involved using a virtual model of a vertical end milling machining process. Data generated from the virtual model was then analysed using data mining in order to identify the relationships between processing times and product features, process features and process activities. The results of this methodology are presented in this paper.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Flexible experimentation and analysis for hybrid DEVS and MPC models</title>
	<abstract>Discrete-event simulation and control-theoretic approaches lend themselves to studying semiconductor manufacturing supply-chain systems. In this work, we detail a modeling approach for semiconductor manufacturing supply-chain systems in a hybrid DEVS/MPC testbed that supports experimentations for DEVS and MPC models using KIBDEVS/MPC. This testbed supports detailed analysis and design of interactions between discrete processes and tactical controller. A set of experiments have been devised to illustrate the role of modeling interactions between Discrete Event System Specification and Model Predictive Control models. The testbed offers novel features to methodically identify and analyze complex model interactions and thus support alternative designs based on tradeoffs between model resolutions and execution times.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>A new approach to robust economic design of control charts</title>
	<abstract>Control charts are widely used in industrial practice to maintain manufacturing processes in desired operating conditions. Design of control charts aims at finding the best parameters for the operation of chart. In the case of economic designs, the control chart parameters are chosen in such a fashion that the cost of controlling the process is minimum. For an X@? control chart, the design involves the selection of three parameters namely, the sample size, the sampling interval and the control limit coefficient. The effectiveness of a design relies on the accuracy of estimation of input parameters used in the model. The input parameters are some cost parameters like 'cost of false alarms', and some process parameters like 'process failure rate'. Conventional control chart designs consider point estimates for the input parameters. The point estimates used in the design may not represent the true parameters and some times may be far from true values. This situation may lead to severe cost penalties for not knowing the true values of the parameters. In order to reduce such cost penalties, each cost and process parameter can be expressed in a range such that it covers the true parameter. This in turn, calls for a design procedure, which considers a range for each parameter, and selects the best control chart parameters. Present paper deals with the economic design of an X@? control chart, in which the input parameters are expressed as ranges. A risk-based approach has been employed to find the optimum parameters of an X@? control chart. Genetic algorithm (GA) has been used as a search tool to find the best design (input) parameters with which the control chart has to be designed. Performance of average based and risk-based designs are compared with respect to the risks they produce. Risk-based design methodology has been extended to incorporate statistical constraints also. The proposed method minimizes the risk of not knowing the true parameters to be used in the design, and is robust to the true parameter values.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An intelligent modeling and analysis method of manufacturing process using the first-order predicate logic</title>
	<abstract>Manufacturing process refers to machining sequence from raw materials to final products. Process plan has important effects on manufacturing process. In general, process designer relies on his experience and knowledge to arrange the process plan. For a complex part, it takes long time and effort to determine process plan. In this paper, an intelligent modeling and analysis method using the first-order predicate logic is proposed to evaluate the manufacturing performance. First, the logic predicates used to represent the process plan are defined according to the machining methods, and the predicate variables are discussed in detail. Consequently, the process plan can be represented in the form of the first-order predicate logic. Second, a type of element model composed of four nodes and four links is put forward in order to construct the process model. All components in this element model are respectively explained, and the mapping relationship between element model and predicate logic is described in detail. According to engineering practices, logic inference rules are suggested and the inference process is illustrated. Hence, the manufacturing process model can be constructed. Third, the process simulation is carried out to evaluate the performance of manufacturing system by using measures such as efficiency, the machine utilization, etc. Finally, a case study is given to explain this intelligent modeling method using the first-order predicate logic.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>The Use of the Manufacturing Sensitivity Model Forms to Comprehend Layout Manufacturing Robustness For Use During Device Design</title>
	<abstract>As semiconductor device manufacturing processes are reducing feature sizes ever smaller, the manufacturing processes are becoming ever more complex. This complexity is having significant impacts on data communications between device design teams and manufacturing process teams. With current manufacturing process constraints, the constraints placed on a design team are difficult to conceptualize, communicate and enforce. This study describes a new type of process model, referred to as a Focus Sensitivity Model that is capable of speeding up the model based analysis of design patterns for manufacturing robustness. The FSM is a difference model based on the photolithography process model. The FSM produces information about multiple process states in one pass. It also produces interpreted data, which removes the need to understand the performance of individual process states. Finally, FSM is capable of analyzing drawn patterns without optical proximity correction applied to determine pattern manufacturability.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Understanding ART-based neural algorithms as statistical tools for manufacturing process quality control</title>
	<abstract>Neural networks have recently received a great deal of attention in the field of manufacturing process quality control, where statistical techniques have traditionally been used. In this paper, a neural-based procedure for quality monitoring is discussed from a statistical perspective. The neural network is based on Fuzzy ART, which is exploited for recognising any unnatural change in the state of a manufacturing process. Initially, the neural algorithm is analysed by means of geometrical arguments. Then, in order to evaluate control performances in terms of errors of Types I and II, the effects of three tuneable parameters are examined through a statistical model. Upper bound limits for the error rates are analytically computed, and then numerically illustrated for different combinations of the tuneable parameters. Finally, a criterion for the neural network designing is proposed and validated in a specific test case through simulation. The results demonstrate the effectiveness of the proposed neural-based procedure for manufacturing quality monitoring.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Neutral template libraries for efficient distributed simulation within a manufacturing system engineering platform</title>
	<abstract>The MISSION project develops an environment for integrated applications of simulation tools which can be offered by different vendors. The template library supports the generation of models from the view of the application instead of simulation tool features. The selection of simulation tools applied is performed with the mostly completed, but still neutral model. The template library is a reservoir of neutral re-usable elements incorporating their major attributes, and referencing to implementations of these models in different simulation tools. Within the manufacturing system engineering (MSE) process, the template library is mainly used as a flexible knowledge base. For this purpose, attributes can be defined depending on the design agents applied. Some attributes are predefined according to the requirements of the MISSION modelling platform (MMP) or according to available user requirements. The user has the chance to add templates and attributes of templates. Furthermore, the user can use objects of these templates within the MSE process. Concerning the simulation process, the template library contains for each application template a reference to simulation models. The simulation model implements the content of the template. The paper presents the template library approach and a short introduction to the MISSION platform.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Handling constraints for manufacturing process optimisation using genetic algorithms</title>
	<abstract>Handling constraints is a common challenge to all optimisation methods. To no exception is the planning and optimisation of manufacturing processes that often involves a number of constraints reflecting the complicated reality of manufacturing to which the pursuit of the best operation condition is subject. Mathematical models describing today's manufacturing processes are generally discontinuous, non-explicit, and not analytically differentiable; all of which renders traditional optimisation methods difficult to apply. Genetic Algorithm (GA) is known to provide an optimisation platform method capable of treating highly nonlinear and ill-behaved complex problems, thereby making it an appealing candidate. However, several issues in regard to the handling constraints must be rigorously addressed in order for GA to become a viable and effective method for manufacturing optimisation. In this paper, a new constraint handling strategy combined with (α,μ)-population initialisation is proposed. Twelve numerical test cases and one surface grinding process optimisation are presented to evaluate its optimisation performance.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Interoperating simulations of automatic material handling systems and manufacturing processes</title>
	<abstract>To perform a high fidelity simulation study on a 300 mm wafer fabrication plant, modeling of the manufacturing process (MP) alone is not sufficient. Inclusion of the automated material handling system (AMHS) model is necessary due to the high degree of factory automation. There isn't, however, a single tool that is capable of modeling both the AMHS and MP with sufficient accuracy and granularity. A commercial simulation package such as AutoMod is usually used to model the AMHS while AutoSched AP is usually used to model the MP. These packages can be integrated using the supplied interoperation module but flexibility in optimizing the execution performance for different simulation models is lacking. In this paper, we present an approach to interoperation based on the High Level Architecture standard. We note that the typical characteristics of disparity in the models' time granularity and frequent model interactions are the obstacle to good execution performance.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Model for Silicon Piece Manufacturing Process Planning Based on Polychromatic Sets Theory</title>
	<abstract>Polychromatic sets theory is summarized. Based on the study of process demand for silicon piece manufacturing and process sequence, the polychromatic sets theory was applied to construct the mathematical model of silicon piece process planning. The model was comprised of contour matrix and polychromatic graph, which were respectively used to describe the process demand for silicon piece manufacturing and the interaction among working procedures. According to this model, the logical sum or logical product of sets theory was applied to solve reasonable process routing, and then take a customer order demanding as an example, the established model and solution were confirmed.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Devs and Fuzzy logic to model and simulate a manufacturing process</title>
	<abstract>In this article, we describe the modeling of a manufacturing process of cheese to the milk believed using formalism Devs (Discrete Event System Specification )[Zeigler, 1976]. The approach of B.P. Zeigler makes it possible to model various software applications, of distributed systems, dynamic systems and systems of production, independently of simulation. Some is the model, modeling and simulation is separate. Formalism DEVS makes it possible to use the same technique of simulation the some either applicability of the system and thus the some or model chooses.Because of the types of data and uncertainty of some among, we use the theory of fuzzy subsets developed by Zadeh [Zadeh, 1975]. Moreover, we present a short synopsis of the basic concepts of the fuzzy logic used in our problem, to model dubious data of manufacturing process.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Dynamic Performance Evaluation Model of Collaborative Knowledge Chain in Collaborative Manufacturing Process</title>
	<abstract>Building a dynamic performance evaluation model is an important part in the management of collaborative manufacturing process. By analyzing the current evaluating method, this paper firstly put forward the mathematical model of dynamic performance evaluation of collaborative knowledge chain, and built a performance evaluation indices model. Then, the weight factors of performance evaluation model were calculated by using Delphi method. Thirdly, performance values of each knowledge unit and collaborative knowledge chain were calculated, and accordingly figures for performance values can be built. Finally, a case study was carried out.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Development and application of equipment-specific process models for semiconductor manufacturing</title>
	<abstract>An abstract is not available.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Manufacturing process control through integration of neural networks and fuzzy model</title>
	<abstract>An abstract is not available.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Piecewise deterministic Markov process model for flexible manufacturing systems with preventive maintenance</title>
	<abstract>An abstract is not available.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Time sequence ordering extensions to the Entity-Relationship model and their application to the automated manufacturing process</title>
	<abstract>An abstract is not available.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Manufacturing supply chain applications 2: development of distributed simulation model for the transporter entity in a supply chain process</title>
	<abstract>Transporter is a critical part of Supply Chain integration. An international transporter process involves multiple ground pickup and delivery operations, package sorting and palletizing, airport operations and air transport. This paper describes a successful two-machine implementation of a distributed simulation model for an international transportation system in a supply chain network operation using Run Time Infrastructure of High Level Architecture software developed by the Defense Modeling and Simulation Office, the Distributed Manufacturing Simulation Adapter developed by the National Institute of Standards and Technology, and ARENA simulation tool. By incorporating the capabilities provided by these tools, it was successful to establish the information flow exchange between the machines where one machine houses transporter while the other has suppliers, customers, and distribution centers located in different parts of world. This research tool attempts to facilitate the development of distributed simulations so they can be used to analyze and solve manufacturing related problems.</abstract>
	<search_task_number>8</search_task_number>
	<query>manufacturing process models</query>
	<relevance>1</relevance>
  </item>



  <item>
    <title>A new concept of the industrial process simulation-cybernetic approach using distributed access control schemes</title>
	<abstract></abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Industrial enterprises business processes simulation with BPsim.MAS</title>
	<abstract>Necessity to mind a huge amount of factors while developing modern enterprises models dictates new requirements for modeling software, which needs to process all data to achieve precise results, make use of all possible means for that, such as distributed calculations and introduce original approaches where possible to avoid extra time waste for multiple simulation experiments. The article focuses on the software apparatus used in distributed multi-agent resource conversion processes based tool BPsim. MAS, pointing out its advantages and describing used technologies. Second part describes BPsim.MAS deployment in Urals Industrial Group, CJSC, which allowed income and market share growth. Various pricing strategies are discussed and active/passive competitors' behavior is considered.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Plenary lecture 3: simulation, artificial intelligence and virtual systems applications in industrial processes education</title>
	<abstract>Simulation has become a powerful tool for system's design, analysis and optimization in industrial processes. Every day there are more powerful computers, with cheapest costs and easy for use. Also, software or application programs with highly flexible programming languages, has allowed the wide use of different simulation techniques in processes control.

Artificial intelligence is one of the scientific areas with greater diffusion and application in the last years. Every day is more common to find tools for industrial, commercial or academic use that involve the use of intelligent techniques in the resolution of critical and recurrent problems, due to their simplicity, implantation facilities and design characteristics.

Virtual systems, allow creating similar environments as the ones found in industrial processes, that can be used for accumulating knowledge and experiences that could be later used in real conditions. These virtual systems can support the learning, emulating real situations and have been widely used in engineering field applications.

The joint use of the three previously mentioned areas can be a powerful tool for decision making that allows, among other things: • To predict the result of the actions that has been taken on the process or control system. • To understand the reasons for events occurrence. • To identify conflictive areas before the system installation. • To explore the effects of some modifications given to the system. • To evaluate ideas and their viability, and to identify their problems. • To stimulate the creative and to train personnel. • To optimize processes (energy savings, bottle necks, results improvements, etc.).

In this Plenary Speech, it will be presented diverse applications of Simulation, Artificial Intelligence and Virtual Systems in Industrial Processes Education, It will be also considered the methodological framework for designing this applications.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The simulation engine: a platform for developing industrial process knowledge-based discrete event simulations</title>
	<abstract></abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Manufacturing simulation: a new tool for robotics, FMS and industrial process design</title>
	<abstract></abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Manufacturing Simulation: A New Tool for Robotics, FMS, and Industrial Process Design</title>
	<abstract></abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Identification of Multivariable Industrial Processes: For Simulation, Diagnosis and Control</title>
	<abstract></abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The new design: the changing role of industrial engineers in the design process through the use of simulation</title>
	<abstract></abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Data mining and simulation processes as useful tools for industrial processes</title>
	<abstract>The most common goal of the factory owner is to achieve better quality in the final product by means of an improved process control. The significance and relevance of optimizing the existing control models is even greater in the open-loop control systems or in those governed by computational methods dependent on adjustable parameters. This talk reviews some typical industrial environments and focuses on some parts of them in order to show the real interest of these improvements. We will identify some difficulties in obtaining these improvements and show how the optimal control model for the manufacturing process can be obtained from data provided by sensors. We will also discuss some technical problems that are related to the main goal, and will identify some topics concerning outliers, density and topology. Also, we will show how these techniques can be applied as an instrumental toolbox in addressing some environmental problems.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Modeling and Simulation of a Complex Industrial Process</title>
	<abstract>Numerical interval simulation combines qualitative and quantitative knowledge to predict when a complex process is violating acceptable margins of productivity and risk. A Brazilian steel plant plans to use the system as an adviser to its workers. Complex industrial processes pose special problems for modeling and simulation. On the one hand, precise information for modeling is generally not available or is hard to determine, because such processes are typically complex and experimental. On the other hand, safety, reliability, and productivity are often prime concerns, requiring precise predictions of system behavior. Thus, any effort to develop a simulation system must first create a sufficiently accurate and precise model that reflects the information actually available. It must then determine how to successfully simulate that model within a relatively narrow acceptable operating range. In this article, we describe how we created a semiquantitative model of the sintering process at Companhia Siderúrgica de Tubarao, a Brazilian-Japanese steel company in Vitória, Brazil. A semiquantitative model is one that combines quantitative knowledge, such as a known, observable value, with qualitative knowledge, such as an interval of possible values for "high" pressure. We used this model as input to numerical interval simulation, a method to produce accurate and tight predictions of semiquantitative models. Our goal was to improve productivity while ensuring that the process did not become overly risky. The semiquantitative simulations have been very successful, and work is underway to incorporate the simulator (model and NIS) into operations at the plant. The resulting advisory system will extrapolate process behavior and advise operators when the process might enter either a low-productivity or high-risk region. CST also plans to use the system as a component of its operator-training program.We believe this approach is viable for modeling and simulating a range of complex industrial processes, including chemical, nuclear, and thermal. We hope that our approach will help fill in the information gaps in these processes and enable better monitoring</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Computer simulation of a parametric optimization problem for an industrial process</title>
	<abstract>Results in functional analysis are applied to the computation of the optimal control of a parabolic distributed parameter system. The control term is a coefficient of the state equation, and is computed by the steepest descent method. An a-priori feedback is determined in the case of a constant control on the optimization interval. Numerical results are discussed.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A simulation model for ergonomic design of industrial processes</title>
	<abstract></abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Application of simulation for manufacturing processes improvements: using simulation techniques for continuous process verification in industrial system development</title>
	<abstract>The purpose of this paper is to describe how discrete event simulation should be used as a tool for continuous process verification in industrial system development. Results include a specification of the working procedures to be used in each life cycle phase of a development project, as well as a definition of the areas where efforts are needed in the future. The approach assures continuous verification of the processes, which will lead to better decisions early on. Better decisions imply reduction in time and costs as well as systems with high quality. In conclusion, using simulation techniques for continuous process verification makes us more likely to develop an optimal industrial solution.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>HPNS: a hybrid process net simulation environment executing online dynamic models of industrial manufacturing systems</title>
	<abstract>Modelling technical systems nowadays is a great challenge of automation technology. Particular complex manufacturing processes, like the industrial paper production, consists of discrete and continuous signals and dynamic response times. Inspired by Petri nets, this paper proposes a new modelling approach is presented to describe those technical systems on different abstraction layer. The hybrid process net simulator (HPNS) framework allows to specify complex models, typically represented as a system of differential equations mixed with continuous and discrete-event subsystems, based on a bipartite graph structure. In addition, the HPNS execute model specifications with dynamic delays.

This paper focuses on the concept and the modelling approach. We give a short review of this new unified representation model for hybrid technical systems, the presentation of the model formalism is out of scope of this paper. A short summary about capabilities and restrictions of HPNS is presented. Moreover, examples of hybrid systems are presented.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Modeling and simulation of a column industrial robot type used in mounting processes</title>
	<abstract>The column Industrial Robots type represents a large used category, starting with the period of the beginning of automatic flexible manufacture. These column industrial robots are used largely in manipulation and mounting operations of a different spares with small and medium dimensions, with regulate forms, which is processed in flexible cells ready in star or circular way of the components. Starting from column industrial robot architecture one tries to find a simulation procedure for the designing of robots using polynomial interpolation of three degree method. This work presents some determinations regarding the simulation of positions, speeds and accelerations which exists into a translation or rotation couple from a kind of robot.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Design of freely programmable LabWindows/CVI-based real-time simulators for testing industrial controllers</title>
	<abstract></abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Researches on Digital Industrial Design System and its Application on Air Nailer Design Process</title>
	<abstract>With the combination of the industrial design activity and digital technique, a Digital Industrial Design System is established, with which digital product design evaluation and product digital simulation technologies are integrated into. The human-machine analysis and engineering analysis methods are imported when the modeling results have been finished to give the designer the means to make the design results more reasonable. The system framework and architecture of the digital industrial design system are presented. The application of the digital industrial design system for the air nailer design make the improvement of the means of the air nailer industrial design. With which, the layout design of the air nailer design is ameliorated with the former design instances and the product functional requirements. The former design and color design goes along with the database system. The reasonableness of the design on the various stages is acquired together with the use of digital simulation means. For the modeling result of the air nailer’s body will be affecting the comprehensive performance such as the product nailing force, nailing efficiency and product strength directly, the stress of the body of the air nailer and the air flow characteristic within the cylinder are analyzed in order to improve the modeling design result. The digital industrial design system platform of the air nailer design is developed under the support of the Zhejiang province momentous science and technology project and the system is of good effect for the air nailer design.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A Framework for Verifying SLA Compliance in Composed Services</title>
	<abstract>Service level agreements (SLAs) impose many non-functional requirements on services. Business analysts specify and check these requirements in business process models using tools such as IBM WebSphere Business Modeler. System integrators on the other hand use service composition tools such as IBM WebSphere Integration Developer to create service composition models, which specify the integration of services. However, system integrators rarely verify SLA compliance in their proposed composition designs. Instead, SLA compliance is verified after the composed services are deployed in the field. To improve the quality of the composed services, we propose a framework to verify SLA compliance in composed services at design time. The framework re-uses information in business process models to simulate services and verify the non-functional requirements before the service deployment. To demonstrate our framework, we built a prototype using an industrial process simulation engine from IBM WebSphere Business Modeler and integrate it into an industrial service composition tool. Through a case study, we demonstrate that our framework and the prototype assist system integrators in composing services while considering the non-functional requirements.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Co-simulation in large scale environments using the HPNS framework</title>
	<abstract>Today's manufacturing systems are highly complex and many consist of continuous and discrete parts, called hybrid manufacturing systems. The model identification and the simulation capabilities are both key technologies improving the deep understanding of manufacturing processes like the industrial paper production. <u>D</u>iscrete <u>E</u>vent <u>S</u>imulation (DES) and the efficient handling of the continuous parts based on this DES approach enable the simulation of complex models [6].

This paper focuses on co-simulation aspects of large-scale systems and presents a two-domain approach. Finally, we consider the characteristics of a co-simulation framework and difficulties of model deployment, including simulation output analysis, data maintenance, and model integration. We close with simulation results of performance evaluations related to the two-domain approach.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Parameter validity control in process control systems</title>
	<abstract>Possibility of identifying the (explicit and implicit) invalidity in sensors' indications within the process control systems (automatic control systems for industrial processes) is considered, on the basis of statistical data on industrial processes provided by the control system; the data is represented in the form of simulation model with interval validity estimates of the parameters. In the designed model, validity identification includes verifying all the functional relations specified for the selected industrial process. The issue of implementing the suggested model is studied subject to process control systems in the energy sector.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Quality control of an industrial unit production using qualitative methods</title>
	<abstract>In this work is presented the application of qualitative models and statistical methods in quality control of production procedures of various components of an automatic industrial press machine. The methodology of creating a descriptive model of a manufacturing process is presented using qualitative methods of description and a qualitative modeling and simulation application (QMTOOL). The simulation of the model allows the redefinition of the initial machine part specifications and model structure and aims to achieve the most efficient functionality of the production procedures. The final analysis of the simulation results is accomplished using statistical and mathematical methods and applications for the evaluation of the final machine components as well as the production procedures. The important role of the qualitative modeling methodologies and the reliability of the statistical functions used are documented with practical case studies carried out in a machines constructions company.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Proceedings of the 35th conference on Winter simulation: driving innovation</title>
	<abstract>The 2003 Winter Simulation Conference (WSC) continues a thirty-six year tradition as the premier event in discrete-event and combined discrete-continuous simulation. The conference theme, Driving Innovation, invites you to push the boundaries and find new trails to where simulation technology will lead in the future. Simulation enables companies to be more innovative. It allows us to focus energy and resources on the alternatives that work best and have the best value proposition. This allows us to spend our financial and political capital on the things that matter most: Focused innovation on valued outcomes.

Advances in high performance computing, clever algorithms, and the availability of real-time information are forever transforming every aspect of our business and personal lives in exciting and unprecedented ways. The coupling and integration of computer-based simulation from multiple disciplines offers novel opportunities for major advances and discoveries at all scales. On the molecular scale, simulation helps researchers to understand protein folding and nanotechnology holds promise for breakthroughs in new material design. Simulation-based modeling and computing are playing a role in creating advanced engines for military aircraft and special effects for the entertainment industry through movies and games. On the grand scale, simulation is used to develop environmental ecosystems and to investigate planetary systems and galaxies.

WSC '03 has a high quality program of 264 papers organized into 16 tracks. Nine tracks and seven minitracks spotlight simulation applications in areas as diverse as: military; manufacturing; logistics, transportation, and distribution; supply chain management; semiconductor manufacturing; business process improvement; construction engineering and project management; simulation education; and health-care applications. For the second year in a row the conference features a full track in risk analysis and a mini-track devoted to simulation-based scheduling. The risk analysis track includes sessions on cutting edge developments in simulation applied to risk and insurance, financial engineering, and collateralized debt and real options. The future of simulation mini-track offers exciting topics such as simulation standards, return on investment for simulation, and simulation technology. Recent research in modeling and analysis is described in three methodology tracks. An additional three tutorial tracks present the fundamentals of simulation, provide an in-depth exploration of advanced simulation methodologies, and demonstrate the capabilities and new features of simulation software. The Proceedings of the Winter Simulation Conference is published in both hardcopy and CD-ROM formats, a practice that started in 1997.

The Winter Simulation Conference is a nonprofit event, organized by eight sponsoring organizations. We are very grateful for the support provided by the American Statistical Association, Association for Computing Machinery/ Special Interest Group on Simulation, Institute of Electrical and Electronics Engineers/Computer Society, Institute of Electrical and Electronics Engineers/Systems, Man, and Cybernetics Society, Institute of Industrial Engineers, Institute for Operations Research and the Management Sciences--College on Simulation, National Institute of Standards and Technology, and The Society for Modeling and Simulation International.

A conference of this size and scope requires the work of many dedicated volunteers whose efforts span several years of planning and preparation. It all starts about five years before each conference, when the WSC Board of Directors appoints the General Chair and Program Chair. The General Chair, David Ferrin, selected the site and the conference committee, and then directed the committee's activities in the years leading up to the conference. Each member of the committee has done an exemplary job in organizing, publicizing, and running a successful conference. It has been an incredible honor and pleasure to work with such an extraordinary group of talented, dedicated individuals. The Program Chair, Douglas Morrice, handled the Call for Papers and developed a truly outstanding program that provides a theme and direction for the conference as a whole. Stephen Chick and Paul Sanchez, the Proceedings Co-Editors, had their hands full reviewing papers and working with authors to produce a high-quality final version. Many thanks to Ann Dunkin, the Business Chair, who created and tracked the budget and cash flow to allow us to wisely manage every dollar. Tom Jefferson, the Exhibits Chair, arranged for a full complement of vendor exhibits to provide WSC attendees with the maximum exposure to the latest products and trends. The Publicity Chair, Joe Hugan, not only proactively continued our efforts to bring the word of WSC to more people in more areas using every technology possible, he contributed many creative ideas to give the Preliminary Program a new and improved format. David Graehl, the Publications Chair, handled the numerous publications that make the conference complete. The Corporate Sponsorship Chair, Milind Datar and his committee members developed innovative ways to reach both industrial and academic organizations to improve WSC while providing opportunities for visibility. Korinn Ferrin took the reins as Guest Program Chair to gather and distribute information on the many sights and activities in New Orleans. The entire committee took the concepts of electronic format to new heights beginning with innovative approaches for the Call for Papers and continuing throughout the conference planning process.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Impact of process simulation on software practice: an initial report</title>
	<abstract>Process simulation has become a powerful technology in support of software project management and process improvement over the past decades. This research, inspired by the Impact Project, intends to investigate the technology transfer of software process simulation to the use in industrial settings, and further identify the best practices to release its full potential in software practice. We collected the reported applications of process simulation in software industry, and identified its wide adoption in the organizations delivering various software intensive systems. This paper, as an initial report of the research, briefs a historical perspective of the impact upon practice based on the documented evidence, and also elaborates the research-practice transition by examining one detailed case study. It is shown that research has a significant impact on practice in this area. The analysis of impact trace also reveals that the success of software process simulation in practice highly relies on the association with other software process techniques or practices and the close collaboration between researchers and practitioners.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Report on ProSim'04: the 5th international workshop on software process simulation and modeling</title>
	<abstract>This paper reports on the 5th International Workshop on Software Process Simulation and Modeling (ProSim 2004), held in Edinburgh, Scotland, from the 24th to 25th May 2004, co-hosted with the 26th International Conference on Software Engineering (ICSE 2004). Since 1998, ProSim has been a successful international workshop that show-cases the leading research in the software process simulation and modeling domain and attracts many of the leading researchers and industrial practitioners in these areas. In 2004, ProSim attracted more than 30 participants from America, Asia, Australia/New Zealand and Europe. According to the feedback received from participants, the goals of this workshop were fully achieved. The planning of ProSim 2005 has yet been started.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>How to test your models more effectively: applying agile and automated techniques to simulation testing</title>
	<abstract>In the industrial engineering community, it's a well-known adage that focusing on process can help achieve better results. In this second of a series of papers, we'll focus on the process of simulation testing and outline how improving your testing process can lead to better results for your projects. We'll consider model building as a software development exercise, and discuss how best practices from the broader software testing community can be applied for process improvement. In particular, we'll explore various approaches to automated testing and their applicability toward simulation projects, based on recent explorations in our own projects. Part 1 of our series introduced the "milestones" approach to simulation development -- based on the popular "agile software" philosophy and our own experiences in real-world simulation consulting practice. This time, we'll discuss how thinking agile can help you become a more effective tester, and help ensure the quality of your models.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Transitioning students from simulation mechanics to simulation as a process improvement tool: a multimedia case study approach</title>
	<abstract>Many undergraduate Industrial Engineering programs offer a course in discrete event simulation. While students often grasp the theory behind simulation and can perform the mechanics of model building and analysis, they often have difficulty formulating creative, viable process improvement ideas, which is a need of today's industrial employers. In this paper, a novel multimedia case-based teaching approach is presented that addresses this need. These multimedia cases are presented as course learning modules to the students. The components of the case-based learning modules include not only the requisite problem background, summary and relevant data, but they also include additional streaming video of the real-world process being studied, actual engineering drawings and still photos of the product and process, and a base simulation model. It is these additional components of the proposed multimedia teaching approach that help "bring the factory to the student" and better prepare graduates for the national workforce.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>On-line adaptive principal component extraction algorithms using iteration approach</title>
	<abstract>Two new on-line algorithms for adaptive principal component analysis (APCA) are proposed and discussed in order to solve the problem of on-line industrial process monitoring in this paper. Both the algorithms have the capability of extracting principal component eigenvectors on-line in a fixed size sliding data window with high dimensional input data. The first algorithm is based on the steepest gradient descent approach, which updates the covariance matrix with deflation transformation and on-line iteration. Based on neural networks, the second algorithm constructs the input data sequence with an on-line iteration method and trains the neural network in every data frame. The convergence of the two algorithms is then analyzed and the simulations are given to illustrate the effectiveness of the two algorithms. At last, the applications of the two algorithms are discussed.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Iterative use of simulation and scheduling methodologies to improve productivity</title>
	<abstract>Experienced and wise industrial engineering educators and practitioners have long understood that industrial engineering is a coherent discipline encompassing techniques that work best synergistically, not a motley collection of specialized techniques each isolated in a separate chimney. As an example of the synergies which industrial engineering can bring to process improvement in a production environment, this case study presents the integrated use of process simulation, production scheduling, and detailed analysis of material-handling methods and their improvement. The study undertook the identification and improvement of production and scheduling policies to the benefit of a manufacturing process whose original throughput capacity fell significantly short of high and increasing demand.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A simulation architecture for manufacturing interoperability testing</title>
	<abstract>Manufacturing systems are often costly to develop and operate. Simulation technology has been demonstrated to be an effective tool for improving the efficiency of manufacturing system design, operation, and maintenance. But manufacturing simulations are usually developed to address a narrow set of industrial issues, e.g., the purchase of new equipment or the modification of a manufacturing process. Once the analysis is complete a particular simulation model may not be used again. If simulations could be made more modular and easily integrated, they could have tremendous value as tools for manufacturing interoperability testing. This paper presents a modular reference architecture to facilitate the integration of manufacturing simulation systems with other support and testing applications. Opportunities for testing are also discussed that will be enabled by the implementation of the architecture.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Designing a hierarchical neural network based on fuzzy clustering for fault diagnosis of the Tennessee-Eastman process</title>
	<abstract>This paper proposes a hierarchical artificial neural network (HANN) for isolating the faults of the Tennessee-Eastman process (TEP). The TEP process is the simulation of a chemical plant created by the Eastman Chemical Company to provide a realistic industrial process for evaluating process control and monitoring methods The first step in designing the HANN is to divide the fault patterns space into a few sub-spaces through using fuzzy C-means clustering algorithm. For each sub-space of fault patterns a special neural network has been trained in order to diagnose the faults of that sub-space. A supervisor network has been developed to decide which one of the special neural networks should be triggered. In this regard, each neural network in the proposed HANN has been given a specific duty, so the proposed procedure can be called Duty-Oriented HANN (DOHANN). The neuromorphic structure of the networks is based on multilayer perceptron (MLP) networks. The simulation of Tennessee-Eastman (TE) process has been used to generate the required training and test data. The performance of the developed method has been evaluated and compared to that of a conventional single neural network (SNN) as well as the technique of dynamic principal component analysis (DPCA). The simulation results indicate that the DOHANN diagnoses the TEP faults considerably better than SNN and DPCA methods. Training of each MLP network for the DOHANN model has required less computer time in comparison to SNN model. This is because of structurally simpler MLPs used by the developed DOHANN method.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A redesign framework for call centers</title>
	<abstract>An important shortcoming in the Business Process Redesign (BPR) literature is the lack of concrete guidance on how to improve an existing business process. Our earlier work has aimed at filling this gap by identifying a set of BPR best practices. This paper takes a further step by showing how a set of best practices can be used to derive a redesign framework for a specific domain, in this case for call centers. Such a framework identifies the various available design options and specifies the relevant performance characteristics. To evaluate concrete design configurations (i.e., coherent combinations of design choices) we use a formal modelling approach based on Petri nets and the simulation tool CPN-Tools. An industrial case study is used to gather relevant context data. We expect that this work helps researchers and practitioners to optimize the performance of actual call centers and to set up similar frameworks for other domains.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Neural network-based estimation of indirect aluminum extrusion process parameters</title>
	<abstract>The objective of this paper is to estimate realistic principal extrusion process parameters by means of artificial neural network. Conventionally, finite element analysis is used to derive extrusion process parameters. However, the finite element analysis of the extrusion model does not consider the manufacturing process constraints in its modeling. Therefore, the process parameters obtained through such an analysis remains highly theoretical. Alternatively, process development in industrial extrusion is to a great extent based on trial and error and often involves full-size experiments, which are both expensive and time-consuming. The artificial neural network-based estimation of the extrusion process parameters prior to plant execution helps to make the actual extrusion operation more efficient because more realistic parameters may be obtained. And so, it bridges the gap between simulation results and the parameters required by a real manufacturing execution system. In this work, the feasibility of making use of neural networks to predict realistic principal extrusion process parameters is studied. In the course, a suitable neural network is designed which is trained using an appropriate learning algorithm. The network so trained is used to predict the extrusion process parameters.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Modeling and simulation of hard disk dive final assembly using a HDD template</title>
	<abstract>A HDD template is designed and developed for modeling and simulation for final assembly of hard disk drive (HDD) manufacturing using Arena. The designed HDD template is a high flexibility and good performance at an internal supply chain level and self-development and improves the system performance significantly. It is developed the intelligent based dynamic machine knowledge, which can capture dynamic based activities with fuzzy system. The study shows how modeling and simulation tools can be used and integrated to implement highly automated systems for industrial processes and deal with flexible products. In such context we designed and developed a prototype for the final assembly of hard disk drive with dynamic and static behavior.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Business process based simulation: a powerful tool for demand analysis of business process reengineering and information system implementation</title>
	<abstract>Demand analysis is of fundamentally importance in the implementation of information system. Business process reengineering (BPR) often gets involved in the process of demand analysis and play a crucial role in the achievement of project objectives. Business process based simulation (BPS) provides a precise and visual method to analyze and compare the concerned performances before and after BPR. The paper presents an industrial experience in using the BPS tool to demonstrate the effects of BPR on restraining stocking-up and overdue payments in the distribution management of a supply chain. Before significant investment involved, the related design result of BPR is validated both by the analytical method and simulation experiments. Based on the mutual supportive results, the BPS method approves its correctness and show its nicety, flexibility and the capacity of visualization.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Intelligent supervision of petroleum processes based on multi-agent systems</title>
	<abstract>This work presents a methodology for the design of an intelligent supervisory system that combines the principles of fuzzy logic, the Internal Model Control (IMC) architecture and the paradigm of Multi-Agent Systems (MAS). The methodology has been conceived to be applied in an intelligent supervisory system, specifically for two kinds of complex petroleum industrial processes: the gas-oil separation process and the oil-heating process. The supervision proposal takes into account the fact of using standard local supervisors schemes connected between themselves and to a global supervisor so that local objectives in each process can be met, thereby letting the global or social objective be obtained through the application of basic mechanism of communication, cooperation and coordination; where these objectives have been previously defined and structured in a hierarchical manner. The paper includes some computational simulations performed under MATLAB / SIMULINK and the results obtained show a good overall system performance.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Quantitative simulation approach to ameliorate deadlock concerns in automotive manufacture</title>
	<abstract>Discrete-event process simulation is a decades-long friend of the industrial, process, or production engineer analyzing complex manufacturing process replete with heavy capital investment, complex material-handling requirements, and requirements for flexibility in the face of highly volatile marketplace demands. When these challenges are coupled with the economic gauntlet confronting the automotive industry, particularly within the United States, the importance of achieving efficiency and continuous improvement demands the support of simulation analyses even more urgently. In this case study, we examine the contribution of simulation not only to study key performance metrics of throughput and equipment utilization, but also to predict and ameliorate the risk of expensive, disruptive deadlocks of material-handling equipment. Additionally and innovatively, simulation is used synergistically with closed-form binomial and trinomial distribution analyses.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Multiple fault disambiguations through parameter estimation: a bond graph model-based approach</title>
	<abstract>To ensure safe operation of industrial processes, automated Fault Detection and Isolation (FDI) procedures are implemented in their supervision platforms. In the safety-critical and environmentally hazardous processes, it is impossible to introduce all kinds of faults and then to derive their consequences. Qualitative determination of consequences of different faults can be misleading in complex dynamical systems. Therefore, simulation of a prototype model turns out to be a practical and an economical solution for the development of a complete Knowledge-Base (KB). Consequently, the intelligence acquired by KB from the simulated models is used to fine-tune the Decision Support System (DSS) such that false alarms and misdetections are minimised. A method for model-based multiple FDI by using Analytical Redundancy Relations (ARRs) and parameter estimation is developed in this paper. Parameter estimation is an essential prerequisite for fault accommodation through system reconfiguration or Fault Tolerant Control (FTC). Bond graph modelling is used to describe the process models and then the model is used to derive the ARRs and fault candidates. Parameter values corresponding to the fault-subspace are estimated by minimising a function of the ARRs. Modelling uncertainties arising out of parameter estimation and sensor noise are taken care by using a passive approach for robust FDI. The developed technique is applied to monitor an open-loop non-linear thermo-fluid process.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Using optmization coupled with simulation to construct layout solutions</title>
	<abstract>This study presents how heuristic optimization technologies coupled with modeling and simulation (M&amp;S) can be used to strengthen engineering system analysis in complex industrial settings. In this analysis, we address the warehouse order-picking process, one of the most labor intense activities in the supply chain. An understanding of the impact of the simultaneous effects of customer demands as well as the products and their storage is essential for improving the operational performance. We used a heuristic optimization to obtain an optimal layout that considers the dynamics of the demand. At the same time, we used M&amp;S to investigate the effects of creating and implementing weight zones. Results from combining layout optimization and zone simulation indicate significant opportunities for improving.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Decision tree and first-principles model-based approach for reactor runaway analysis and forecasting</title>
	<abstract>Decision trees (DTs) are effective in extracting linguistically interpretable models from data. This paper shows that DTs can also be used to extract information from process models, e.g. they can be used to represent homogenous operating regions of complex process. To illustrate the usefulness of this novel approach a detailed case study is shown where DTs are used for forecasting the development of runaway in an industrial, fixed bed, tube reactor. Based on first-principles knowledge and historical process data the steady-state simulator of the tube reactor has been identified and validated. The runaway criterion based on Ljapunov's indirect stability analysis has been applied to generate a data base used for DT induction. Finally, the logical rules extracted from the DTs are used in an operator support system (OSS), since they are proven to be useful to describe the safe operating regions. A simulation study based on the dynamical model of the process is also presented. The results confirm that by the synergistic combination of a DT based on expert system and the dynamic simulator a powerful tool for runaway forecasting and analysis is achieved and it can be used to work safe operating strategies out.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Intelligent supervision based on multi-agent systems: application to petroleum industrial processes</title>
	<abstract>This work presents a methodology for the design of an intelligent supervisory system that combines the principles of fuzzy logic, the Internal Model Control (IMC) architecture and the paradigm of Multi-Agent Systems (MAS). The methodology has been conceived to be applied in an intelligent supervisory system, specifically for two kinds of complex petroleum industrial processes: the gas-oil separation process and the oil-heating process. The supervision proposal takes into account the fact of using standard local supervisors schemes connected between themselves and to a global supervisor so that local objectives in each process can be met, thereby letting the global or social objective be obtained through the application of basic mechanism of communication, cooperation and coordination; where these objectives have been previously defined and structured in a hierarchical manner. The paper includes some computational simulations performed under MATLAB / SIMULINK and the results obtained show a good overall system performance.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Emulation models for testing of process control systems</title>
	<abstract>The paper presents the new visual interactive discrete simulation environment intended for the development and execution of simulation models and emulation ones. Main possibilities that are provided by the environment are: the quick building of models from libraries elements, 3D representation, and the communication of models with real control systems. The paper presents three cases of successful development and of the use of emulation models for process control systems in various industrial fields created with the help of this environment. The use of emulation models reduced the time and the cost of testing control systems. Emulation models were also used for training of operating personnel.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Possibilities of the direct implementation of control algorithms from environment MATLAB/Simulink</title>
	<abstract>The implementation of the control algorithms from the simulation environment into industrial controller is necessary to solve effectively in relation to minimization of implementation errors. The capability of algorithm transmission into the industrial controller is important and transmission time should be minimal. Is it obvious that the control algorithms are designed and developed in simulation environment first and they are implemented into the industrial controllers in the real production processes afterwards. The object of this paper is an analysis of possibilities of the control algorithm direct implementation from today's most applicable simulation environment in automation - MATLAB into PLC - programmable logic controller. The advantages of the direct implementation of control algorithms are given first of all by the minimum of implementation errors in comparison with the indirect implementation. The development of heterogeneous control algorithms becomes faster. The choice of optimal control algorithm is relatively simple due to its proven functionality in the real processes.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Development environment of expert system to operate in real time with industrial automation supervisory system</title>
	<abstract>Normally the operator's interaction with Supervisory Systems developed for an industrial automated process is a manual process, which depends on the operator's skills and the constant operation's attention.

This work presents the development, application and tests of the expert system denominated SISES "Sistema de Integração de Sistemas Especialistas e Supervisórios", that means System of Integration Expert Systems and Supervisory Systems. The SISES was developed to operate in real time with Supervisory Systems and to have the ability to analyze the available data in the Supervisory System and to make inference. The SISES's perform was tested by a simulation methodology process and also in a real nylon industrial production.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Simulation of Real-Time Systems: An Object-Oriented Approach Supported by a Virtual Reality- Based Tool</title>
	<abstract>This article intends to demonstrate the applicability and usefulness of Virtual Reality (VR) technology to support Real-Time Systems (RTS) simulations, as a form to evaluate the correctness of such systems. The Virtual Reality SIMulation (VR-SIM), a tool which incorporates VR modeling resources, is presented. This tool offers support to simulate the behavior of RTS, checking the scheduling of processes and timing constraints. The main conceps of RTS, simulation and VR are presented, the VR-SIM architecture and functionality are described and a RTS example from the industrial area is specified and simulated. Aspects of design and implementation of the tool, following an object-oriented approach, are discussed.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Fault diagnosis using dynamic trend analysis: A review and recent developments</title>
	<abstract>Dynamic trend analysis is an important technique for fault detection and diagnosis. Trend analysis involves hierarchical representation of signal trends, extraction of the trends, and their comparison (estimation of similarity) to infer the state of the process. In this paper, an overview of some of the existing methods for trend extraction and similarity estimation is presented. A novel interval-halving method for trend extraction and a fuzzy-matching-based method for similarity estimation and inferencing are also presented. The effectiveness of the interval halving and trend matching is shown through simulation studies on the fault diagnosis of the Tennessee Eastman process. Industrial experiences on the application of trend analysis technique for fault detection and diagnosis is also presented followed by a discussion on outstanding issues and solution approaches.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Design and simulation of a fuzzy substrate feeding controller for an industrial scale fed-batch baker yeast fermentor</title>
	<abstract>Conventional control systems can not give satisfactory results in fermentation systems due to process non-linearity and long delay time,. This paper presents design and simulation a fuzzy controller for industrial fed-batch baker's yeast fermentation system in order to maximize the cell-mass production and to minimize ethanol formation. Designed fuzzy controller determines an optimal substrate feeding strategy for an industrial scale fed-batch fermentor relating to status of estimated specific growth rate, elapsed time and ethanol concentration. The proposed controller uses an error in specific growth rate (e), fermentation time (t) and concentration of ethanol (Ce) as controller inputs and produces molasses feeding rate (F) as control output. The controller has been tested on a simulated fed-batch industrial scaled fermenter and resulted in higher productivity than the conventional controller.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Unsupervised learning techniques for fine-tuning fuzzy cognitive map causal links</title>
	<abstract>Fuzzy Cognitive Maps (FCMs) constitute an attractive knowledge-based methodology, combining the robust properties of fuzzy logic and neural networks. FCMs represent causal knowledge as a signed directed graph with feedback and provide an intuitive framework which incorporates the experts' knowledge. FCMs handle available information and knowledge from an abstract point of view. They develop behavioural model of the system exploiting the experience and knowledge of experts. The construction of FCMs is based mainly on experts who determine the structure of FCM, i.e. concepts and weighted interconnections among concepts. But this methodology may not be a sufficient model of the system because the human factor is not always reliable. Thus the FCM model of the system may requires restructuring which is achieved through adjustment the weights of FCM interconnections using specific learning algorithms for FCMs. In this article, two unsupervised learning algorithms are presented and compared for training FCMs; how they define, select or fine-tuning weights of the causal interconnections among concepts. The implementation and results of these unsupervised learning techniques for an industrial process control problem are discussed. The simulations results of training the process system verify the effectiveness, validity and advantageous characteristics of those learning techniques for FCMs.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Computer-supported competence management: Evolution of industrial processes as life cycles of organizations</title>
	<abstract>The examination of human performance within industrial processes increasingly extends beyond the matter of resource implementation. In addition to traditional approaches to personnel assignment planning, new management methods for the preservation and further development of personnel are becoming established. The following paper discusses the effects and challenges related to computed-aided competence management. For this purpose it is indispensable to examine the notion of competence more in depth as well as the resulting competence management. Since as of yet no universally valid definition of competence exists, the following article discusses various facets of competence management and illustrates these by considering individual simulation-aided decision and planning tools for industrial process optimization.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Numerical simulation of 3D bubbles rising in viscous liquids using a front tracking method</title>
	<abstract>The rise of bubbles in viscous liquids is not only a very common process in many industrial applications, but also an important fundamental problem in fluid physics. An improved numerical algorithm based on the front tracking method, originally proposed by Tryggvason and his co-workers, has been validated against experiments over a wide range of intermediate Reynolds and Bond numbers using an axisymmetric model [J. Hua, J. Lou, Numerical simulation of bubble rising in viscous liquid, J. Comput. Phys. 22 (2007) 769-795]. In the current paper, this numerical algorithm is further extended to simulate 3D bubbles rising in viscous liquids with high Reynolds and Bond numbers and with large density and viscosity ratios representative of the common air-water two-phase flow system. To facilitate the 3D front tracking simulation, mesh adaptation is implemented for both the front mesh on the bubble surface and the background mesh. On the latter mesh, the governing Navier-Stokes equations for incompressible, Newtonian flow are solved in a moving reference frame attached to the rising bubble. Specifically, the equations are solved using a finite volume scheme based on the Semi-Implicit Method for Pressure-Linked Equations (SIMPLE) algorithm, and it appears to be robust even for high Reynolds numbers and high density and viscosity ratios. The 3D bubble surface is tracked explicitly using an adaptive, unstructured triangular mesh. The numerical model is integrated with the software package PARAMESH, a block-based adaptive mesh refinement (AMR) tool developed for parallel computing. PARAMESH allows background mesh adaptation as well as the solution of the governing equations in parallel on a supercomputer. Further, Peskin distribution function is applied to interpolate the variable values between the front and the background meshes. Detailed sensitivity analysis about the numerical modeling algorithm has been performed. The current model has also been applied to simulate a number of cases of 3D gas bubbles rising in viscous liquids, e.g. air bubbles rising in water. Simulation results are compared with experimental observations both in aspect of terminal bubble shapes and terminal bubble velocities. In addition, we applied this model to simulate the interaction between two bubbles rising in a liquid, which illustrated the model's capability in predicting the interaction dynamics of rising bubbles.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Modeling and Simulation of a Fuzzy Supervisory Controller for an Industrial Boiler</title>
	<abstract>In this paper we compare and discuss the performance of a boiler evaporator system when the system is controlled by a traditional PID-type strategy and when the system is enhanced by using fuzzy logic blocks to provide set-points for the system. The strategy used in fuzzy logic controllers (FLCs) is called fuzzy supervisory control and it generates set-points for the conventional controllers. The boiler under test is a VU-60 industrial system that produces 180,000 pounds of steam per hour. The mathematical model of the plant is a scaled version model of that obtained for a thermoelectric unit. The new model simplifies the large-scale thermoelectric boiler model to an industrial small-scale type VU-60 boiler model based upon first principle mass and energy balance equations. The main change consists of representing only the behavior of the drum&amp;#x2014;evaporator system, having a partial model of the combustion process, with a simplified combustion control system and a three-element boiler feed-water controller. The control system for combustion and boiler feed-water receives a supervisory signal (or set-point tracking signal) that comes from the FLC to improve the performance of the overall control system. The behavior of the supervisory controller brings some advantages to the system performance, compared with the traditional control schemes. The comparison reflects fuel improvements from 2.5% to 6.5% depending upon the steam load ramp regime. The simulations are performed using the SIMULINK&amp;#x00AE; shell running under the MATLAB &amp;#x00AE; platform.</abstract>
	<search_task_number>8</search_task_number>
	<query>industrial process simulation</query>
	<relevance>0</relevance>
  </item>



  <item>
    <title>Natural language understanding and speech recognition</title>
    <abstract>Natural language understanding must be an integral part of any automatic speech recognition system that attempts to deal with interactive problem solving. The methods for representing and integrating knowledge from different sources may be valuable for the understanding process as well as speech recognition.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Detecting and correcting speech recognition errors during natural language understanding</title>
    <abstract>The focus of this work is to improve the ability of a spoken dialog system to identify and compensate for speech recognition errors. Rather than trying to eradicate errors coming from the speech recognizer, our work focuses on detecting, locating and correcting the errors that occur, within the natural language understanding component of a spoken dialog system.
Current spoken dialog systems operate within narrow domains, and many work by filling in slots for the information they need to achieve a specific task. Such simple systems do not require a syntactic analysis of what the user said; they can accomplish their mission by recognizing only a few key phrases. We believe that as spoken language systems become more sophisticated, they will require a more thorough analysis of the user input, rendering many current robustness strategies ineffective. By identifying implausible speech recognition hypotheses, the spoken dialog system can attempt to repair the communication breakdown, either by using stochastic methods to predict what was actually said or by using an appropriate dialog strategy.

We show that by describing the expected structure of spoken turns in human-computer practical dialog and formalizing the structure by means of context-free grammar rules used by a traditional bottom-up chart parser, we can achieve 92.1% accuracy in the task of detecting erroneous speech recognizer output based solely on the chart generated during parsing, an improvement of 18.2 percentage points over the majority-class baseline. Furthermore, we can reliably locate the start index of errors within misrecognized strings using the chart and domain-specific word bigram models. We developed and implemented algorithms that use the predicted error start location together with the word bigram models, phonetic similarity and the recognized string to generate correction hypotheses.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Towards natural language understanding of partial speech recognition results in dialogue systems</title>
    <abstract>We investigate natural language understanding of partial speech recognition results to equip a dialogue system with incremental language processing capabilities for more realistic human-computer conversations. We show that relatively high accuracy can be achieved in understanding of spontaneous utterances before utterances are completed.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition</title>
    <abstract>From the Publisher:

This book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora.Methodology boxes are included in each chapter. Each chapter is built around one or more worked examples to demonstrate the main idea of the chapter. Covers the fundamental algorithms of various fields, whether originally proposed for spoken or written language to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation. Emphasis on web and other practical applications. Emphasis on scientific evaluation. Useful as a reference for professionals in any of the areas of speech and language processing.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A Robust Loose Coupling for Speech Recognition and Natural Language Understanding</title>
    <abstract>The focus of this thesis proposal is to improve the ability of a computational system to understand spoken utterances in a dialogue with a human. Available computational methods for word recognition do not perform as well on spontaneous speech as we would hope. Even a state of the art recognizer achieves slightly worse than 70\% word accuracy on (nearly) spontaneous speech in a conversation about a specific problem. .pp To address this problem, I will explore novel methods for post-processing the output of a speech recognizer in order to correct errors. I adopt statistical techniques for modeling the noisy channel from the speaker to the listener in order to correct some of the errors introduced there. The statistical model accounts for frequent errors such as simple word/word confusions and short phrasal problems (one-to-many word substitutions and many-to-one word concatenations). To use the model, a search algorithm is required to find the most likely correction of a given word sequence from the speech recognizer. The post-processor output should contain fewer errors, thus making interpretation by higher levels, such as parsing, more reliable. .pp Spontaneous speech is also challenging to process because it is more incremental than written language. Utterances frequently form brief phrases and fragments rather than full sentences; they tend to come in installments and refinements. Known methods for parsing do not perform as well as we would like in the face of these linguistic ambiguities and idiosyncrasies. Even state of the art algorithms for parsing spontaneous language sustain high error rates. .pp To address the incrementality of spontaneously spoken utterances, I will develop methods for segmenting a given utterance into ``chunks'''' representing individual thoughts. Given an utterance of spontaneous speech, a tool for automatic prosodic feature extraction will analyze the output of the error-correcting post-processor and the acoustic waveform to generate prosodic cues. These cues will aid a robust parser using a prosody-wise grammar to identify the incremental phrases in the utterance and to provide a syntactic analysis. .pp These components will augment the {\sc Trains-95} conversational planning assistant.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Integration of speech recognition and natural language processing in the MIT VOYAGER system</title>
    <abstract>The MIT VOYAGER speech understanding system is an urban exploration and navigation system that interacts with the user through spoken dialogue, text, and graphics. The authors describe recent attempts at improving the integration between the speech recognition and natural language components. They used the generation capability of the natural language component to produce a word-pair language model to constrain the recognizer's search space, thus improving the coverage of the overall system. They also implemented a strategy in which the recognizer generates the top N word strings and passes them along to the natural language component for filtering. Results on performance evaluation are presented.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A Cache-Based Natural Language Model for Speech Recognition</title>
    <abstract>Speech-recognition systems must often decide between competing ways of breaking up the acoustic input into strings of words. Since the possible strings may be acoustically similar, a language model is required; given a word string, the model returns its linguistic probability. Several Markov language models are discussed. A novel kind of language model which reflects short-term patterns of word use by means of a cache component (analogous to cache memory in hardware terminology) is presented. The model also contains a 3g-gram component of the traditional type. The combined model and a pure 3g-gram model were tested on samples drawn from the Lancaster-Oslo/Bergen (LOB) corpus of English text. The relative performance of the two models is examined, and suggestions for the future improvements are made.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Speech recognition and the frequency of recently used words: a modified Markov model for natural language</title>
    <abstract>Speech recognition systems incorporate a language model which, at each stage of the recognition task, assigns a probability of occurrence to each word in the vocabulary. A class of Markov language models identified by Jelinek has achieved considerable success in this domain. A modification of the Markov approach, which assigns higher probabilities to recently used words, is proposed and tested against a pure Markov model. Parameter calculation and comparison of the two models both involve use of the LOB Corpus of tagged modern English.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Speech recognition in a dialog system for patient health monitoring</title>
    <abstract>We describe CARDIAC, a prototype for an intelligent conversational assistant that provides health monitoring for chronic heart failure patients. CARDIAC supports user initiative through its ability to understand natural language and connect it to intention recognition. The spoken language interface allows patients to interact with CARDIAC without special training. We present speech recognition results obtained during an evaluation with fourteen chronic heart failure patients.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Cooperative Agents and Recognition Systems (CARS) for Drivers and Passengers</title>
    <abstract>In this paper we present SRI's vision of the human-machine interface for a car environment. This interface leverages our work in human-computer interaction, speech, speaker and gesture recognition, natural language understanding, and intelligent agents architecture. We propose a natural interface that allows the driver to interact with the navigation system, control electronic devices, and communicate with the rest of the world much as would be possible in the office environment. Passengers would be able to use the system to watch TV or play games in their private spaces. The final prototype will be fully configurable (languages, voice output, and so forth), and will include speaker recognition technology for resetting preferences and/or for security.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Use of Line Spectral Frequencies for Emotion Recognition from Speech</title>
    <abstract>We propose the use of the line spectral frequency (LSF) features for emotion recognition from speech, which have not been been previously employed for emotion recognition to the best of our knowledge. Spectral features such as mel-scaled cepstral coefficients have already been successfully used for the parameterization of speech signals for emotion recognition. The LSF features also offer a spectral representation for speech, moreover they carry intrinsic information on the formant structure as well, which are related to the emotional state of the speaker [4]. We use the Gaussian mixture model (GMM) classifier architecture, that captures the static color of the spectral features. Experimental studies performed over the Berlin Emotional Speech Database and the FAU Aibo Emotion Corpus demonstrate that decision fusion configurations with LSF features bring a consistent improvement over the MFCC based emotion classification rates.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A Multi-Agent Reinforcement Learning Algorithm for Disambiguation in a Spoken Dialogue System</title>
    <abstract>A spoken dialogue system (SDS) communicates with its user(s) in a spoken natural language. It responds to user speech input for answering questions, providing advice, and so on. Correctly understanding user input is very important to system performance. A key issue in understanding user input is handling ambiguity since any natural language is ambiguous. In our research, we develop a novel multi-agent reinforcement learning algorithm for disambiguation in a spoken dialogue system. In the algorithm, multiple agents learn knowledge about user behavior in activities and language use, and the knowledge is used to handle ambiguity. In this paper, we introduce the multi-agent reinforcement learning algorithm, and describe a spoken dialogue system for mathematics tutoring that we build to implement and experiment the algorithm.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>In the News</title>
    <abstract>AI use is increasing in many areas of commercial technology, including business intelligence, speech and handwriting recognition, and security monitoring. At the same time, AI use is also growing in other areas, including technologies for assisting the disabled. This article explores ways researchers are using AI and computer-vision techniques to read sign language and improve mobility aids such as wheelchairs and walkers. This issue also includes news brief articles on shape-changing robots and using AI to simplify mobile recommendation technology.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Ambient Intelligence—the Next Step for Artificial Intelligence</title>
    <abstract>Ambient intelligence (AmI) deals with a new world of ubiquitous computing devices, where physical environments interact intelligently and unobtrusively with people. These environments should be aware of people's needs, customizing requirements and forecasting behaviors. AmI environments can be diverse, such as homes, offices, meeting rooms, schools, hospitals, control centers, vehicles, tourist attractions, stores, sports facilities, and music devices. Artificial intelligence research aims to include more intelligence in AmI environments, allowing better support for humans and access to the essential knowledge for making better decisions when interacting with these environments. This article, which introduces a special issue on AmI, views the area from an artificial intelligence perspective.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>An extended system for labeling graphical documents using statistical language models</title>
    <abstract>This paper describes a proposed extended system for the recognition and labeling of graphical objects within architectural and engineering documents that integrates Statistical Language Models (SLMs) with shape classifiers. Traditionally used for Natural Language Processing, SLMS have been successful in such fields as Speech Recognition and Information Retrieval. There exist similarities between natural language and technical graphical data that suggest that adapting SLMs for use with graphical data is a worthwhile approach. Statistical Graphical Language Models (SGLMs) are applied to graphical documents based on associations between different classes of shape in a drawing to automate the structuring and labeling of graphical data. The SGLMs are designed to be combined with other classifiers to improve their recognition performance. SGLMs perform best when the graphical domain being examined has an underlying semantic system, that is; graphical objects have not been placed randomly within the data. A system which combines a Shape Classifier with SGLMS is described.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Speech repairs, intonational phrases, and discourse markers: modeling speakers' utterances in spoken dialogue</title>
    <abstract>Interactive spoken dialogue provides many new challenges for natural language understanding systems. One of the most critical challenges is simply determining the speaker's intended utterances: both segmenting a speaker's turn into utterances and determining the intended words in each utterance. Even assuming perfect word recognition, the latter problem is complicated by the occurrence of speech repairs, which occur where speakers go back and change (or repeat) something they just said. The words that are replaced or repeated are no longer part of the intended utterance, and so need to be identified. Segmenting turns and resolving repairs are strongly interwined with a third task: identifying discourse markers. Because of the interactions, and interactions with POS tagging and speech recognition, we need to address these tasks together and early on in the processing stream. This paper presents a statistical language model in which we redefine the speech recognition problem so that it includes the identification of POS tags, discourse markers, speech repairs, and intonational phrases. By solving these simultaneously, we obtain better results on each task than addressing them separately. Our model is able to identify 72% of turn-internal intonational boundaries with a precision of 71%, 97% of discourse markers with 96% precision, and detect and correct 66% of repairs with 74% precision.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Context dependent modeling of phones in continuous speech using decision trees</title>
    <abstract>In a continuous speech recognition system it is important to model the context dependent variations in the pronunciations of words. In this paper we present an automatic method for modeling phonological variation using decision trees. For each phone we construct a decision tree that specifies the acoustic realization of the phone as a function of the context in which it appears. Several thousand sentences from a natural language corpus spoken by several talkers are used to construct these decision trees. Experimental results on a 5000-word vocabulary natural language speech recognition task are presented.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Graphical Object Recognition using Statistical Language Models</title>
    <abstract>This paper describes a proposed system for the recognition and labeling of graphical objects within architectural and engineering documents that integrates Statistical Language Models (SLMs) with traditional classifiers. SLMs are techniques used with success in Natural Language Processing (NLP) for use in such tasks as Speech Recognition and Information Retrieval. This research proposes the adaptation of SLMs for use with graphical notation i.e. Statistical Graphical Language Model (SGLMs). Reasoning of the similarities between natural language and technical graphics is presented and the proposed use of SGLM for graphical object recognition is described.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>The design for the wall street journal-based CSR corpus</title>
    <abstract>The DARPA Spoken Language System (SLS) community has long taken a leadership position in designing, implementing, and globally distributing significant speech corpora widely used for advancing speech recognition research. The Wall Street Journal (WSJ) CSR Corpus described here is the newest addition to this valuable set of resources. In contrast to previous corpora, the WSJ corpus will provide DARPA its first general-purpose English, large vocabulary, natural language, high perplexity, corpus containing significant quantities of both speech data (400 hrs.) and text data (47M words), thereby providing a means to integrate speech recognition and natural language processing in application domains with high potential practical value. This paper presents the motivating goals, acoustic data design, text processing steps, lexicons, and testing paradigms incorporated into the multi-faceted WSJ CSR Corpus.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Intelligent human computer interfaces and the case study of building English-to-Maori talking dictionary</title>
    <abstract>The paper presents a general engineering model of a hybrid system for speech recognition and information retrieval. The speech recognition part consists of a low level neural network module for phoneme recognition and a higher level fuzzy reasoning module for word recognition and language modelling. There are several features such systems are characterised by, namely: adaptability, i.e. the system is adaptable to new speakers and accents; dealing with ambiguity during the recognition process; extendability, i.e. new modules can be introduced to the system at a later stage and the dictionary is extendable. A case study of building an experimental English to Maori talking dictionary for a small dictionary of words and speaker independent mode is reported. Subject Terms: natural language interfaces; natural languages; speech recognition; language translation; word processing; information retrieval; neural nets; intelligent human computer interfaces; case study; English to Maori talking dictionary; engineering model; hybrid system; speech recognition; information retrieval; low level neural network module; phoneme recognition; higher level fuzzy reasoning module; word recognition; language modelling; extendability; speaker independent mode</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Fast speaker adaptation: some experiments on different techniques for codebook and HMM parameters estimation</title>
    <abstract>A set of techniques to perform fast speaker adaptation for a large vocabulary, natural-language, speech recognition system are presented. The experimentation has been carried out using a 20000-word, real-time, natural-language speech recognizer for the Italian language. To perform speaker adaptation within the framework of the probabilistic approach to speech recognition two different problems must be addressed: codebook adaptation and hidden Markov model parameters adaptation. The basic idea is to use a set of data collected from several different speakers as a source of a priori knowledge with a small speech sample provided by the new speaker to perform the adaptation task. Several different techniques for codebook adaptation have been tried and discussed.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Decision trees for phonological rules in continuous speech</title>
    <abstract>The authors present an automatic method for modeling phonological variation using decision trees. For each phone they construct a decision tree that specifies the acoustic realization of the phone as a function of the context in which it appears. Several-thousand sentences from a natural language corpus spoken by several speakers are used to construct these decision trees. Experimental results on a 5000-word vocabulary natural language speech recognition task are presented.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Spoken dialogue technology: enabling the conversational user interface</title>
    <abstract>Spoken dialogue systems allow users to interact with computer-based applications such as databases and expert systems by using natural spoken language. The origins of spoken dialogue systems can be traced back to Artificial Intelligence research in the 1950s concerned with developing conversational interfaces. However, it is only within the last decade or so, with major advances in speech technology, that large-scale working systems have been developed and, in some cases, introduced into commercial environments. As a result many major telecommunications and software companies have become aware of the potential for spoken dialogue technology to provide solutions in newly developing areas such as computer-telephony integration. Voice portals, which provide a speech-based interface between a telephone user and Web-based services, are the most recent application of spoken dialogue technology. This article describes the main components of the technology---speech recognition, language understanding, dialogue management, communication with an external source such as a database, language generation, speech synthesis---and shows how these component technologies can be integrated into a spoken dialogue system. The article describes in detail the methods that have been adopted in some well-known dialogue systems, explores different system architectures, considers issues of specification, design, and evaluation, reviews some currently available dialogue development toolkits, and outlines prospects for future development.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Extracting key semantic terms from Chinese speech query for web searches</title>
    <abstract>This paper discusses the challenges and proposes a solution to performing information retrieval on the Web using Chinese natural language speech query. The main contribution of this research is in devising a divide-and-conquer strategy to alleviate the speech recognition errors. It uses the query model to facilitate the extraction of main core semantic string (CSS) from the Chinese natural language speech query. It then breaks the CSS into basic components corresponding to phrases, and uses a multi-tier strategy to map the basic components to known phrases in order to further eliminate the errors. The resulting system has been found to be effective.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>An Efficient Compositional Semantics for Natural-Language Database Queries with Arbitrarily-Nested Quantification and Negation</title>
    <abstract>A novel and efficient implementation of a compositional semantics for a small natural-language query processor has been developed. The approach is based on a set-theoretic version of Montague Semantics in which sets that are constructed as part of denotations of negative constructs are represented by enumerating the members of their complements with respect to the universe of discourse. The semantics accommodates arbitrarily-nested quantifiers and various forms of negation, nouns, transitive and intransitive verbs, conjunction, and disjunction. Queries containing the word "is" and passive verb constructs can be evaluated. However, the approach for these two constructs is ad hoc and inefficient. The approach has been implemented in a syntax-directed evaluator, constructed as an executable specification of an attribute grammar, with a user-independent speech-recognition front-end.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A multimodal guide for the augmented campus</title>
    <abstract>The use of Personal Digital Assistants (PDAs) with ad-hoc built-in information retrieval and auto-localization functionalities can help people navigating an environment in a more natural manner compared to traditional audio/visual pre-recorded guides. In this work we propose and discuss a user-friendly, multi-modal guide system for pervasive context-aware service provision within augmented environments. The proposed system is adaptable to the user needs of mobility within a given environment; it is usable on different mobile devices and in particular on PDAs, which are used as advanced adaptive HEI (human-environment interaction) interfaces. An information retrieval service is provided that is easily accessible through spoken language interaction in cooperation with an auto-localization service. The interaction is enabled by speech recognition and synthesis technologies, and by a ChatBot system, endowed with common sense reasoning capabilities to properly interpret user speech and provide him with the requested information. This interaction mode turns to be more natural, and users are required to have only basic skills on the use of PDAs. The auto-localization service relies on a RFID-based framework, which resides partly in the mobile side of the entire system (PDAs), and partly in the environment side. In particular, RFID technology allows the system to provide users with context-related information. An implemented case study is showed that illustrates service provision in an augmented environment within university campus settings (termed "Augmented Campus"). Lastly, a discussion about user experiences while using trial services within the Augmented Campus is given.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Exploiting speech recognition transcripts for narrative peak detection in short-form documentaries</title>
    <abstract>Narrative peaks are points at which the viewer perceives a spike in the level of dramatic tension within the narrative flow of a video. This paper reports on four approaches to narrative peak detection in television documentaries that were developed by a joint team consisting of members from Delft University of Technology and the University of Twente within the framework of the VideoCLEF 2009 Affect Detection task. The approaches make use of speech recognition transcripts and seek to exploit various sources of evidence in order to automatically identify narrative peaks. These sources include speaker style (word choice), stylistic devices (use of repetitions), strategies strengthening viewers' feelings of involvement (direct audience address) and emotional speech. These approaches are compared to a challenging baseline that predicts the presence of narrative peaks at fixed points in the video, presumed to be dictated by natural narrative rhythm or production convention. Two approaches deliver top narrative peak detection results. One uses counts of personal pronouns to identify points in the video where viewers feel most directly involved. The other uses affective word ratings to calculate scores reflecting emotional language.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation</title>
    <abstract>This comprehensive handbook, written by leading experts in the field, details the groundbreaking research conducted under the breakthrough GALE program--The Global Autonomous Language Exploitation within the Defense Advanced Research Projects Agency (DARPA), while placing it in the context of previous research in the fields of natural language and signal processing, artificial intelligence and machine translation.The most fundamental contrast between GALE and its predecessor programs was its holistic integration of previously separate or sequential processes. In earlier language research programs, each of the individual processes was performed separately and sequentially: speech recognition, language recognition, transcription, translation, and content summarization. The GALE program employed a distinctly new approach by executing these processes simultaneously. Speech and language recognition algorithms now aid translation and transcription processes and vice versa. This combination of previously distinct processes has produced significant research and performance breakthroughs and has fundamentally changed the natural language processing and machine translation fields.This comprehensive handbook provides an exhaustive exploration into these latest technologies in natural language, speech and signal processing, and machine translation, providing researchers, practitioners and students with an authoritative reference on the topic.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Dragon</title>
    <abstract>Dragon Systems is developing and building high performance, computationally-efficient interactive speech workstations, to support adaptive speech recognition of large vocabulary, natural language speech input in real-time. These systems are based on the results of the multi-knowledge source (MKS) algorithm architecture and multi-processor accelerator hardware studies previously undertaken by Dragon under DARPA auspices, and on Dragon's separately developed speech recognition systems (VoiceScribe, DragonWriter, and DragonDictate) and related work.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Industrially oriented voice control system</title>
    <abstract>The purpose of my research was to develop a novel voice control system for the use in the robotized manufacturing cells as well as to create tools providing its simple integration into manufacturing. A comprehensive study of existing problems and their possible solutions has been performed. Unlike some other works, it focused on the specific requirements that should be fulfilled by industrially oriented voice control systems. Analysis of existing solutions related to the natural language processing and those related to various voice control applications has been performed. Its goal was to establish the optimal method of voice command analysis for industrially oriented systems. Finally, a voice control system for manufacturing cells has been developed, implemented and practically verified in the laboratory. Unlike many other solutions, it takes into consideration almost all aspects of voice command processing (speech recognition, syntactic and semantic analysis and spontaneous speech effects) and - most importantly - their mutual influence. To provide the simple system customization (integration into any particular manufacturing cells), a special format for quasi-natural sublanguage syntax definition has been developed. A novel algorithm for semantic analysis, using specific features of voice commands used for controlling industrial devices and machines, has been incorporated into the system. Successful implementation in the educational robotized machining cell shows that industrial applications should be possible in the very next future.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Word Topic Models for Spoken Document Retrieval and Transcription</title>
    <abstract>Statistical language modeling (LM), which aims to capture the regularities in human natural language and quantify the acceptability of a given word sequence, has long been an interesting yet challenging research topic in the speech and language processing community. It also has been introduced to information retrieval (IR) problems, and provided an effective and theoretically attractive probabilistic framework for building IR systems. In this article, we propose a word topic model (WTM) to explore the co-occurrence relationship between words, as well as the long-span latent topical information, for language modeling in spoken document retrieval and transcription. The document or the search history as a whole is modeled as a composite WTM model for generating a newly observed word. The underlying characteristics and different kinds of model structures are extensively investigated, while the performance of WTM is thoroughly analyzed and verified by comparison with the well-known probabilistic latent semantic analysis (PLSA) model as well as the other models. The IR experiments are performed on the TDT Chinese collections (TDT-2 and TDT-3), while the large vocabulary continuous speech recognition (LVCSR) experiments are conducted on the Mandarin broadcast news collected in Taiwan. Experimental results seem to indicate that WTM is a promising alternative to the existing models.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>From ubgs to cfgs a practical corpus-driven approach</title>
    <abstract>We present a simple and intuitive unsound corpus-driven approximation method for turning unification-based grammars, such as HPSG, CLE, or PATR-II into context-free grammars (CFGs). Our research is motivated by the idea that we can exploit (large-scale), hand-written unification grammars not only for the purpose of describing natural language and obtaining a syntactic structure (and perhaps a semantic form), but also to address several other very practical topics. Firstly, to speed up deep parsing by having a cheap recognition pre-flter (the approximated CFG). Secondly, to obtain an indirect stochastic parsing model for the unification grammar through a trained PCFG, obtained from the approximated CFG. This gives us an efficient disambiguation model for the unification-based grammar. Thirdly, to generate domain-specific subgrammars for application areas such as information extraction or question answering. And finally, to compile context-free language models which assist the acoustic model of a speech recognizer. The approximation method is unsound in that it does not generate a CFG whose language is a true superset of the language accepted by the original unification-based grammar. It is a corpus-driven method in that it relies on a corpus of parsed sentences and generates broader CFGs when given more input samples. Our open approach can be fine-tuned in different directions, allowing us to monotonically come close to the original parse trees by shifting more information into the context-free symbols. The approach has been fully implemented in JAVA.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A case-based reasoning approach for speech corpus generation</title>
    <abstract>Corpus-based stochastic language models have achieved significant success in speech recognition, but construction of a corpus pertaining to a specific application is a difficult task. This paper introduces a Case-Based Reasoning system to generate natural language corpora. In comparison to traditional natural language generation approaches, this system overcomes the inflexibility of template-based methods while avoiding the linguistic sophistication of rule-based packages. The evaluation of the system indicates our approach is effective in generating users' specifications or queries as 98% of the generated sentences are grammatically correct. The study result also shows that the language model derived from the generated corpus can significantly outperform a general language model or a dictation grammar.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Realization of natural language interfaces using lazy functional programming</title>
    <abstract>The construction of natural language interfaces to computers continues to be a major challenge. The need for such interfaces is growing now that speech recognition technology is becoming more readily available, and people cannot speak those computer-oriented formal languages that are frequently used to interact with computer applications. Much of the research related to the design and implementation of natural language interfaces has involved the use of high-level declarative programming languages. This is to be expected as the task is extremely difficult, involving syntactic and semantic analysis of potentially ambiguous input. The use of LISP and Prolog in this area is well documented. However, research involving the relatively new lazy functional programming paradigm is less well known. This paper provides a comprehensive survey of that research.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Speech and Language Processing for Multimodal Human-Computer Interaction</title>
    <abstract>In this paper, we describe our recent work at Microsoft Research, in the project codenamed Dr. Who, aimed at the development of enabling technologies for speech-centric multimodal human-computer interaction. In particular, we present in detail MiPad as the first Dr. Who's application that addresses specifically the mobile user interaction scenario. MiPad is a wireless mobile PDA prototype that enables users to accomplish many common tasks using a multimodal spoken language interface and wireless-data technologies. It fully integrates continuous speech recognition and spoken language understanding, and provides a novel solution to the current prevailing problem of pecking with tiny styluses or typing on minuscule keyboards in today's PDAs or smart phones. Despite its current incomplete implementation, we have observed that speech and pen have the potential to significantly improve user experience in our user study reported in this paper. We describe in this system-oriented paper the main components of MiPad, with a focus on the robust speech processing and spoken language understanding aspects. The detailed MiPad components discussed include: distributed speech recognition considerations for the speech processing algorithm design; a stereo-based speech feature enhancement algorithm used for noise-robust front-end speech processing; Aurora2 evaluation results for this front-end processing; speech feature compression (source coding) and error protection (channel coding) for distributed speech recognition in MiPad; HMM-based acoustic modeling for continuous speech recognition decoding; a unified language model integrating context-free grammar and N-gram model for the speech decoding; schema-based knowledge representation for the MiPad's personal information management task; a unified statistical framework that integrates speech recognition, spoken language understanding and dialogue management; the robust natural language parser used in MiPad to process the speech recognizer's output; a machine-aided grammar learning and development used for spoken language understanding for the MiPad task; Tap &amp; Talk multimodal interaction and user interface design; back channel communication and MiPad's error repair strategy; and finally, user study results that demonstrate the superior throughput achieved by the Tap &amp; Talk multimodal interaction over the existing pen-only PDA interface. These user study results highlight the crucial role played by speech in enhancing the overall user experience in MiPad-like human-computer interaction devices.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>MARS: A Statistical Semantic Parsing and Generation-Based Multilingual Automatic tRanslation System</title>
    <abstract>We present MARS (Multilingual Automatic tRanslation System), a research prototype speech-to-speech translation system. MARS is aimed at two-way conversational spoken language translation between English and Mandarin Chinese for limited domains, such as air travel reservations. In MARS, machine translation is embedded within a complex speech processing task, and the translation performance is highly effected by the performance of other components, such as the recognizer and semantic parser, etc. All components in the proposed system are statistically trained using an appropriate training corpus. The speech signal is first recognized by an automatic speech recognizer (ASR). Next, the ASR-transcribed text is analyzed by a semantic parser, which uses a statistical decision-tree model that does not require hand-crafted grammars or rules. Furthermore, the parser provides semantic information that helps further re-scoring of the speech recognition hypotheses. The semantic content extracted by the parser is formatted into a language-independent tree structure, which is used for an interlingua based translation. A Maximum Entropy based sentence-level natural language generation (NLG) approach is used to generate sentences in the target language from the semantic tree representations. Finally, the generated target sentence is synthesized into speech by a speech synthesizer.

Many new features and innovations have been incorporated into MARS: the translation is based on understanding the meaning of the sentence; the semantic parser uses a statistical model and is trained from a semantically annotated corpus; the output of the semantic parser is used to select a more specific language model to refine the speech recognition performance; the NLG component uses a statistical model and is also trained from the same annotated corpus. These features give MARS the advantages of robustness to speech disfluencies and recognition errors, tighter integration of semantic information into speech recognition, and portability to new languages and domains. These advantages are verified by our experimental results.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Towards best practice in the development and evaluation of speech recognition components of a spoken language dialog system</title>
    <abstract>This article provides a global overview of the main aspects of current practice in the design, implementation and evaluation of speech recognition components for Spoken Language Dialog Systems (SLDSs), and presents the results of the DISC European project related to speech recognition. DISC and its successor DISC-2 are efforts towards the definition of best practice guidelines for SLDS development and evaluation. SLDSs aim at using natural spoken input for performing an information processing task such as automated standards, call routing or travel planning and reservations. The main functionality of an SLDS are speech recognition, natural language understanding, dialog management, database access and interpretation, response generation and speech synthesis. Speech recognition, which transforms the acoustic signal into a string of words, is a key technology in any SLDS.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>From Word Form Surfaces to Communication</title>
    <abstract>The starting point of this paper is the external surface of a word form, for example the agent-external acoustic perturbations constituting a language sign in speech or the dots on paper in the case of written language. The external surfaces are modality-dependent tokens which the hearer recognizes by means of (i) pattern-matching and (ii) a mapping into modality-independent types, and which the speaker produces by an inverse mapping from modality-independent types into tokens synthesized in a modality of choice.

The types are provided by a lexicon stored in the agent's memory. They include not only the necessary as opposed to accidental (kata sumbebêkos), as used in the philosophical tradition of Aristotle. properties of the surface shape, but also the associated morphosyntactic properties and the meaning. The question addressed by this paper is how to design the lexical analysis of word form types as a data structure (abstract data type), suitable for the purpose of Database Semantics (DBS), i.e., for a computational model of natural language communication. Database Semantics describes the procedural aspects of the SLIM theory of language[1, p. 1]. As an acronym, SLIM stands for the principles of Surface compositional, time Linear, Internal Matching. As a word, SLIM stands for low (linear) mathematical complexity.

After discussing the conditions of automatic word form recognition and production in a talking robot, we turn to the question of what format the analyzed word forms should have. The requirements are an easy coding of lexical details, a simple detection and representation of semantic relations, suitability for storage and retrieval in a database, support of a computationally straightforward matching procedure for relating the levels of language and context, and compatibility with a suitable algorithm.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Speech to sign language translation system for Spanish</title>
    <abstract>This paper describes the development of and the first experiments in a Spanish to sign language translation system in a real domain. The developed system focuses on the sentences spoken by an official when assisting people applying for, or renewing their Identity Card. The system translates official explanations into Spanish Sign Language (LSE: Lengua de Signos Espanola) for Deaf people. The translation system is made up of a speech recognizer (for decoding the spoken utterance into a word sequence), a natural language translator (for converting a word sequence into a sequence of signs belonging to the sign language), and a 3D avatar animation module (for playing back the hand movements). Two proposals for natural language translation have been evaluated: a rule-based translation module (that computes sign confidence measures from the word confidence measures obtained in the speech recognition module) and a statistical translation module (in this case, parallel corpora were used for training the statistical model). The best configuration reported 31.6% SER (Sign Error Rate) and 0.5780 BLEU (BiLingual Evaluation Understudy). The paper also describes the eSIGN 3D avatar animation module (considering the sign confidence), and the limitations found when implementing a strategy for reducing the delay between the spoken utterance and the sign sequence animation.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>My science tutor: A conversational multimedia virtual tutor for elementary school science</title>
    <abstract>This article describes My Science Tutor (MyST), an intelligent tutoring system designed to improve science learning by students in 3rd, 4th, and 5th grades (7 to 11 years old) through conversational dialogs with a virtual science tutor. In our study, individual students engage in spoken dialogs with the virtual tutor Marni during 15 to 20 minute sessions following classroom science investigations to discuss and extend concepts embedded in the investigations. The spoken dialogs in MyST are designed to scaffold learning by presenting open-ended questions accompanied by illustrations or animations related to the classroom investigations and the science concepts being learned. The focus of the interactions is to elicit self-expression from students. To this end, Marni applies some of the principles of Questioning the Author, a proven approach to classroom conversations, to challenge students to think about and integrate new concepts with prior knowledge to construct enriched mental models that can be used to explain and predict scientific phenomena. In this article, we describe how spoken dialogs using Automatic Speech Recognition (ASR) and natural language processing were developed to stimulate students' thinking, reasoning and self explanations. We describe the MyST system architecture and Wizard of Oz procedure that was used to collect data from tutorial sessions with elementary school students. Using data collected with the procedure, we present evaluations of the ASR and semantic parsing components. A formal evaluation of learning gains resulting from system use is currently being conducted. This paper presents survey results of teachers' and children's impressions of MyST.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>multithreaded context for robust conversational interfaces: Context-sensitive speech recognition and interpretation of corrective fragments</title>
    <abstract>We focus on the issue of robustness of conversational interfaces that are flexible enough to allow natural "multithreaded" conversational flow. Our main advance is to use context-sensitive speech recognition in a general way, with a representation of dialogue context that is rich and flexible enough to support conversation about multiple interleaved topics, as well as the interpretation of corrective fragments. We explain, by use of worked examples, the use of our "Conversational Intelligence Architecture" (CIA) to represent conversational threads, and how each thread can be associated with a language model (LM) for more robust speech recognition. The CIA uses fine-grained dynamic representations of dialogue context, which supersede those used in finite-state or form-based dialogue managers. In an evaluation of a dialogue system built using this architecture we found that 87.9% of recognized utterances were recognized using a context-specific language model, resulting in an 11.5% reduction in the overall utterance recognition error rate, and a 13.4% reduction in concept error rate. Thus we show that by using context-sensitive recognition based on the predicted type of the user's next dialogue move, a more flexible dialogue system can also exhibit an improvement in speech recognition performance.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Nine Issues in Speech Translation</title>
    <abstract>This paper sketches research in nine areas related to spoken language translation: interactive disambiguation (two demonstrations of highly interactive, broad-coverage speech translation are reported); system architecture; data structures; the interface between speech recognition and analysis; the use of natural pauses for segmenting utterances; example-based machine translation; dialogue acts; the tracking of lexical co-occurrences; and the resolution of translation mismatches.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Confidence estimation for NLP applications</title>
    <abstract>Confidence measures are a practical solution for improving the usefulness of Natural Language Processing applications. Confidence estimation is a generic machine learning approach for deriving confidence measures. We give an overview of the application of confidence estimation in various fields of Natural Language Processing, and present experimental results for speech recognition, spoken language understanding, and statistical machine translation.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Rapid bootstrapping of statistical spoken dialogue systems</title>
    <abstract>Rapid deployment of statistical spoken dialogue systems poses portability challenges for building new applications. We discuss the challenges that arise and focus on two main problems: (i) fast semantic annotation for statistical speech understanding and (ii) reliable and efficient statistical language modeling using limited in-domain resources. We address the first problem by presenting a new bootstrapping framework that uses a majority-voting based combination of three methods for the semantic annotation of a ''mini-corpus'' that is usually manually annotated. The three methods are a statistical decision tree based parser, a similarity measure and a support vector machine classifier. The bootstrapping framework results in an overall cost reduction of about a factor of two in the annotation effort compared to the baseline method. We address the second problem by devising a method to efficiently build reliable statistical language models for new spoken dialog systems, given limited in-domain data. This method exploits external text resources that are collected for other speech recognition tasks as well as dynamic text resources acquired from the World Wide Web. The proposed method is applied to a spoken dialog system in a financial transaction domain and a natural language call-routing task in a package shipment domain. The experiments demonstrate that language models built using external resources, when used jointly with the limited in-domain language model, result in relative word error rate reductions of 9-18%. Alternatively, the proposed method can be used to produce a 3-to-10 fold reduction for the in-domain data requirement to achieve a given performance level.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Natural Language Processing (Almost) from Scratch</title>
    <abstract>We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Development of a personalized integrated voice recognition and synthesis system</title>
    <abstract>Successful and effective man-machine communication is a major milestone for human beings in transferring their abilities to machines. As language is the most natural means of communication, speech recognition and synthesis are best means for communicating with the computer. Reliable speech recognition is a difficult problem, requiring a combination of many techniques. Recent progress in speech synthesis has produced synthesizers with very high intelligibility but the sound quality and naturalness still remain a major problem to be addressed. This paper aims to look at the problem in an altogether different perspective; speech recognition and synthesis for an individual is much simpler problem to tackle and yet presents a wide range of applications that serve the basic purpose of human computer interaction.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>DialogStudio: A workbench for data-driven spoken dialog system development and management</title>
    <abstract>Recently, data-driven speech technologies have been widely used to build speech user interfaces. However, developing and managing data-driven spoken dialog systems are laborious and time consuming tasks. Spoken dialog systems have many components and their development and management involves numerous tasks such as preparing the corpus, training, testing and integrating each component for system development and management. In addition, data annotation for natural language understanding and speech recognition is quite burdensome. This paper describes the development of a tool, DialogStudio, to support the development and management of data-driven spoken dialog systems. Desirable aspects of the data-driven spoken dialog system workbench tool are identified, and architectures and concepts are proposed that make DialogStudio efficient in data annotation and system development in a domain and methodology neutral manner. The usability of DialogStudio was validated by developing dialog systems in three different domains with two different dialog management methods. Objective evaluations of each domain show that DialogStudio is a feasible solution as a workbench for data-driven spoken dialog systems.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Designing and evaluating a wizarded uncertainty-adaptive spoken dialogue tutoring system</title>
    <abstract>We describe the design and evaluation of two different dynamic student uncertainty adaptations in wizarded versions of a spoken dialogue tutoring system. The two adaptive systems adapt to each student turn based on its uncertainty, after an unseen human ''wizard'' performs speech recognition and natural language understanding and annotates the turn for uncertainty. The design of our two uncertainty adaptations is based on a hypothesis in the literature that uncertainty is an ''opportunity to learn''; both adaptations use additional substantive content to respond to uncertain turns, but the two adaptations vary in the complexity of these responses. The evaluation of our two uncertainty adaptations represents one of the first controlled experiments to investigate whether substantive dynamic responses to student affect can significantly improve performance in computer tutors. To our knowledge we are the first study to show that dynamically responding to uncertainty can significantly improve learning during computer tutoring. We also highlight our ongoing evaluation of our uncertainty-adaptive systems with respect to other important performance metrics, and we discuss how our corpus can be used by the wider computer speech and language community as a linguistic resource supporting further research on effective affect-adaptive spoken dialogue systems in general.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>A fast re-scoring strategy to capture long-distance dependencies</title>
    <abstract>A re-scoring strategy is proposed that makes it feasible to capture more long-distance dependencies in the natural language. Two pass strategies have become popular in a number of recognition tasks such as ASR (automatic speech recognition), MT (machine translation) and OCR (optical character recognition). The first pass typically applies a weak language model (n-grams) to a lattice and the second pass applies a stronger language model to N best lists. The stronger language model is intended to capture more long-distance dependencies. The proposed method uses RNN-LM (recurrent neural network language model), which is a long span LM, to re-score word lattices in the second pass. A hill climbing method (iterative decoding) is proposed to search over islands of confusability in the word lattice. An evaluation based on Broadcast News shows speedups of 20 over basic N best re-scoring, and word error rate reduction of 8% (relative) on a highly competitive setup.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>
  <item>
    <title>Similarity-Based Models of Word Cooccurrence Probabilities</title>
    <abstract>In many applications of natural language processing (NLP) it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations “eat a peach” and ”eat a beach” is more likely. Statistical NLP methods determine the likelihood of a word combination from its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in any given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on “most similar” words.

We describe probabilistic word association models based on distributional word similarity, and apply them to two tasks, language modeling and pseudo-word disambiguation. In the language modeling task, a similarity-based model is used to improve probability estimates for unseen bigrams in a back-off language model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error.

We also compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency to avoid giving too much weight to easy-to-disambiguate high-frequency configurations. The similarity-based methods perform up to 40% better on this particular task.</abstract>
    <search_task_number>9</search_task_number>
    <query>natural language speech recognition</query>
    <relevance>1</relevance>
  </item>



    <item>
        <title>Detecting and connecting disjoint sub-networks in wireless sensor and actor networks</title>
        <abstract>Wireless sensor and actor networks (WSANs) can be considered as a combination of a sensor network and an actor network in which powerful and mobile actor nodes can perform application specific actions based on the received data from the sensors. As most of these actions are performed collaboratively among the actors, inter-actor connectivity is one of the desirable features of WSANs. In this paper, we propose a novel distributed algorithm for establishing a connected inter-actor network topology. Considering initially disjoint sets of actors, our algorithm first initiates a search process by using the underlying sensor network in order to detect the possible sub-networks of actors in the region. After these sub-networks are detected, our algorithm pursues a coordinated actor movement in order to connect the sub-networks and thus achieve inter-actor connectivity for all the actors. This coordinated movement approach exploits the minimum connected dominating set of each sub-network when picking the appropriate actor to move so that the connectivity of each sub-network is not violated. In addition, the approach strives to minimize the total travel distance of actors and the messaging cost on both sensors and actors in order to extend the lifetime of WSAN. We analytically study the performance of our algorithm. Extensive simulation experiments validate the analytical results and confirm the effectiveness of our approach. </abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>The value of handhelds in smart environments</title>
        <abstract>The severe resource restrictions of computer-augmented everyday artifacts imply substantial problems for the design of applications in smart environments. Some of these problems can be overcome by exploiting the resources, I/O interfaces, and computing capabilities of nearby mobile devices in an ad-hoc fashion. We identify the means by which smart objects can make use of handheld devices such as PDAs and mobile phones, and derive the following major roles of handhelds in smart environments: (1) mobile infrastructure access point; (2) user interface; (3) remote sensor; (4) mobile storage medium; (5) remote resource provider; and (6) weak user identifier. We present concrete applications that illustrate these roles, and describe how handhelds can serve as mobile mediators between computer-augmented everyday artifacts, their users, and background infrastructure services. The presented applications include a remote interaction scenario, a smart medicine cabinet, and an inventory monitoring application.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Exploiting mobility for energy efficient data collection in wireless sensor networks</title>
        <abstract>We analyze an architecture based on mobility to address the problem of energy efficient data collection in a sensor network. Our approach exploits mobile nodes present in the sensor field as forwarding agents. As a mobile node moves in close proximity to sensors, data is transferred to the mobile node for later depositing at the destination. We present an analytical model to understand the key performance metrics such as data transfer, latency to the destination, and power. Parameters for our model include: sensor buffer size, data generation rate, radio characteristics, and mobility patterns of mobile nodes. Through simulation we verify our model and show that our approach can provide substantial savings in energy as compared to the traditional ad-hoc network approach.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>A green wireless sensor network for environmental monitoring and risk identification</title>
        <abstract>A sensor-based community network for environmental data gathering and predictive analysis has been developed. A mesh network of wireless sensors reports data to a central site for environmental monitoring and risk identification. Data analysis and visual presentation is provided in a geographical and temporal context. This network is considered green due to decreased energy usage by the overall network as well as its actual application, which permits environmental information to be contextually presented and communicated with concerned urban community as well as decision makers. Periodic data reporting from the sensor network, in contrast with the usual timestamp synchronisation, reduces the amount of communication required between network nodes, resulting in an overall energy saving, while not compromising the nature of the data gathered. The sensor network applications provide an outstanding representation of green networking as sparse but sufficient environmental monitoring, accompanied by real-time data analysis, and historical pattern identification permits risk identification in support of public safety and protection.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Communications of the ACM: Volume 52 Issue 5</title>
        <abstract></abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>A novel agent-based user-network communication model in wireless sensor networks</title>
        <abstract>Wireless sensor networks generally have three kinds of objects: sensor nodes, sinks, and users that send queries and receive data via the sinks. In addition, the user and the sinks are mostly connected to each other by infrastructure networks. The users, however, should receive the data from the sinks through multi-hop communications between disseminating sensor nodes if such users move into the sensor networks without infrastructure networks. To support mobile users, previous work has studied various user mobility models. Nevertheless, such approaches are not compatible with the existing data-centric routing algorithms, and it is difficult for the mobile users to gather data efficiently from sensor nodes due to their mobility. To improve the shortcomings, we propose a view of mobility and propose a model to support a user mobility that is independent of sinks. The proposed model, finally, is evaluated by simulation of delivery ratio, latency, and network lifetime.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Smart-Its Friends: A Technique for Users to Easily Establish Connections between Smart Artefacts</title>
        <abstract>Ubiquitous computing is associated with a vision of everything being connected to everything. However, for successful applications to emerge, it will not be the quantity but the quality and usefulness of connections that will matter. Our concern is how qualitative relations and more selective connections can be established between smart artefacts, and how users can retain control over artefact interconnection. We propose context proximity for selective artefact communication, using the context of artefacts for matchmaking. We further suggest to empower users with simple but effective means to impose the same context on a number of artefacts. To prove our point we have implemented Smart-Its Friends, small embedded devices that become connected when a user holds them together and shakes them.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Communications of the ACM: Volume 53 Issue 5</title>
        <abstract></abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Communications of the ACM: Volume 54 Issue 2</title>
        <abstract></abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Event-based applications and enabling technologies</title>
        <abstract>Event processing has become the paradigm of choice in many monitoring and reactive applications. However, the understanding of events, their composition and level of abstraction, the style of processing and the quality of service requirements vary drastically across application domains. We introduce the basic notions of event processing to create a common understanding, present the enabling technologies that are used for the implementation of event-based systems, survey a wide range of applications identifying their main features, and discuss open research issues.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>On target coverage in wireless heterogeneous sensor networks with multiple sensing units</title>
        <abstract>The paper considers the target coverage problem in wireless heterogeneous sensor networks (WHSNs) with multiple sensing units. This kind of target coverage problem can be reduced to a set cover problem and be further formulated as integer programming (IP) constraints. However, to solve the IP problem is an NP-complete problem. Therefore, two heuristic but distributed schemes, REFS and EEFS, are proposed in the paper to solve the target coverage problem. In REFS (remaining energy first scheme), each sensor considers its remaining energy and neighbors' decisions to enable its sensing units as well as to ensure every target being covered by the sensing attributes which are required to be covered at each target. The advantages of REFS are its simplicity and less communication overhead incurred. However, in order to make the best use of the sensing units on each sensor, another scheme, called EEFS (energy efficient first scheme), is proposed as well. Different from REFS, a sensor in EEFS considers its sensing capabilities and remaining energy as well as those of its neighbors to make a better decision to turn on its sensing units and to ensure each target being covered by required attributes. To our best knowledge, this paper is the first paper to solve the problem for WHSNs with multiple sensing units. Simulation results show that REFS and EEFS can prolong the network lifetime effectively. Furthermore, EEFS outperforms REFS in network lifetime. On the other hand, compared with the performance obtained from the IP solution, the difference between EEFS and the IP solution can be confined within 10%. </abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Localized routing trees for query processing in sensor networks</title>
        <abstract>In this paper, we propose a novel energy-efficient approach, a localized routing tree (LRT) coupled with a route redirection (RR) strategy, to support various types of queries. LRTs take care of the sensors near the sink and reduce the energy consumption of these sensors, and RR reduces the energy cost of data receptions. Compared to the existing approaches, simulation studies show that LRT together with RR has significant improvement on the query capacity.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Analysis of Deterministic Tracking of Multiple Objects Using a Binary Sensor Network</title>
        <abstract>Let consider a set of anonymous moving objects to be tracked in a binary sensor network. This article studies the problem of associating deterministically a track revealed by the sensor network with the trajectory of an unique anonymous object, namely the multiple object tracking and identification (MOTI) problem. In our model, the network is represented by a sparse connected graph where each vertex represents a binary sensor and there is an edge between two sensors if an object can pass from one sensed region to another one without activating any other sensor. The difficulty of MOTI lies in the fact that the trajectories of two or more objects can be so close that the corresponding tracks on the sensor network can no longer be distinguished (track merging), thus confusing the deterministic association between an object trajectory and a track.

    The article presents several results. We first show that MOTI cannot be solved on a general graph of ideal binary sensors even by an omniscient external observer if all the objects can freely move on the graph. Then we describe restrictions that can be imposed a priori either on the graph, on the object movements, or on both, to make the MOTI problem always solvable. In the absence of an omniscient observer, we show how our results can lead to the definition of distributed algorithms that are able to detect when the system is in a state where MOTI becomes unsolvable.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Tangible User Interfaces: Past, Present, and Future Directions</title>
        <abstract>In the last two decades, Tangible User Interfaces (TUIs) have emerged as a new interface type that interlinks the digital and physical worlds. Drawing upon users' knowledge and skills of interaction with the real non-digital world, TUIs show a potential to enhance the way in which people interact with and leverage digital information. However, TUI research is still in its infancy and extensive research is required in order to fully understand the implications of tangible user interfaces, to develop technologies that further bridge the digital and the physical, and to guide TUI design with empirical knowledge.

    This monograph examines the existing body of work on Tangible User Interfaces. We start by sketching the history of tangible user interfaces, examining the intellectual origins of this field. We then present TUIs in a broader context, survey application domains, and review frameworks and taxonomies. We also discuss conceptual foundations of TUIs including perspectives from cognitive sciences, psychology, and philosophy. Methods and technologies for designing, building, and evaluating TUIs are also addressed. Finally, we discuss the strengths and limitations of TUIs and chart directions for future research.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Dynamic parking negotiation and guidance using an agent-based platform</title>
        <abstract>Modern prosperous cities strongly need advanced parking assistant systems, intelligent transportation systems providing drivers with parking information. Existing parking information systems usually ignore the parking price factor and do not automatically provide optimal car parks matching drivers' demand. Currently, the parking price has no negotiable space; consumers lose their bargaining position to obtain better and cheaper parking. This study uses an intelligent agent system, and considering negotiable parking prices, selects the optimal car park for the driver. The autonomous coordination activities challenge traditional approaches and call for new paradigms and supporting middleware. An agent-based coordination network is proposed to bring true benefit to drivers and car park operators. These modern intelligent agents have capabilities including planning, mobility, execution monitoring and coordination. These properties can be used to construct the integrated parking assistant system. </abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>A survey on service-oriented middleware for wireless sensor networks</title>
        <abstract>Wireless sensor networks (WSN) are used for many applications such as environmental monitoring, infrastructure security, healthcare applications, and traffic control. The design and development of such applications must address many challenges dictated by WSN characteristics on one hand and the targeted applications on the other. One of the emerging approaches used for relaxing these challenges is using service-oriented middleware (SOM). Service-oriented computing, in general, aims to make services available and easily accessible through standardized models and protocols without having to worry about the underlying infrastructures, development models, or implementation details. SOM could play an important role in facilitating the design, development, and implementation of service-oriented systems. This will help achieve interoperability, loose coupling, and heterogeneity support. Furthermore, SOM approaches will provision non-functional requirements like scalability, reliability, flexibility, and Quality of Service (QoS) assurance. This paper surveys the current work in SOM and the trends and challenges to be addressed when designing and developing these solutions for WSN.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>A survey of autonomic computing—degrees, models, and applications</title>
        <abstract>Autonomic Computing is a concept that brings together many fields of computing with the purpose of creating computing systems that self-manage. In its early days it was criticised as being a “hype topic” or a rebadging of some Multi Agent Systems work. In this survey, we hope to show that this was not indeed ‘hype’ and that, though it draws on much work already carried out by the Computer Science and Control communities, its innovation is strong and lies in its robust application to the specific self-management of computing systems. To this end, we first provide an introduction to the motivation and concepts of autonomic computing and describe some research that has been seen as seminal in influencing a large proportion of early work. Taking the components of an established reference model in turn, we discuss the works that have provided significant contributions to that area. We then look at larger scaled systems that compose autonomic systems illustrating the hierarchical nature of their architectures. Autonomicity is not a well defined subject and as such different systems adhere to different degrees of Autonomicity, therefore we cross-slice the body of work in terms of these degrees. From this we list the key applications of autonomic computing and discuss the research work that is missing and what we believe the community should be considering.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Run-time energy consumption estimation based on workload in server systems</title>
        <abstract>This paper proposes to develop a system-wide energy consumption model for servers by making use of hardware performance counters and experimental measurements. We develop a real-time energy prediction model that relates server energy consumption to its overall thermal envelope. While previous studies have attempted system-wide modeling of server power consumption through subsystem models, our approach is different in that it uses a small set of tightly correlated parameters to create a model relating system energy input to subsystem energy consumption. We develop a linear regression model that relates processor power, bus activity, and system ambient temperatures into real-time predictions of the power consumption of long jobs and as result controlling their thermal impact. Using the HyperTransport bus model as a case study and through electrical measurements on example server subsystems, we develop a statistical model for estimating run-time power consumption. Our model is accurate within an error of four percent(4%) as verified using a set of common processor benchmarks.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Computing needs time</title>
        <abstract>The passage of time is essential to ensuring the repeatability and predictability of software and networks in cyber-physical systems.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>A multi-agent approach to controlling a smart environment</title>
        <abstract>The goal of the MavHome (Managing An Intelligent Versatile Home) project is to create a home that acts as a rational agent. The agent seeks to maximize inhabitant comfort and minimize operation cost. In order to achieve these goals, the agent must be able to predict the mobility patterns and device usages of the inhabitants. Because of the size of the problem, controlling a smart environment can be effectively approached as a multi-agent task. Individual agents can address a portion of the problem but must coordinate their actions to accomplish the overall goals of the system. In this chapter, we discuss the application of multi-agent systems to the challenge of controlling a smart environment and describe its implementation in the MavHome project.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
        <item>
        <title>The flooding time synchronization protocol</title>
        <abstract>Wireless sensor network applications, similarly to other distributed systems, often require a scalable time synchronization service enabling data consistency and coordination. This paper describes the Flooding Time Synchronization Protocol (FTSP), especially tailored for applications requiring stringent precision on resource limited wireless platforms. The proposed time synchronization protocol uses low communication bandwidth and it is robust against node and link failures. The FTSP achieves its robustness by utilizing periodic flooding of synchronization messages, and implicit dynamic topology update. The unique high precision performance is reached by utilizing MAC-layer time-stamping and comprehensive error compensation including clock skew estimation. The sources of delays and uncertainties in message transmission are analyzed in detail and techniques are presented to mitigate their effects. The FTSP was implemented on the Berkeley Mica2 platform and evaluated in a 60-node, multi-hop setup. The average per-hop synchronization error was in the one microsecond range, which is markedly better than that of the existing RBS and TPSN algorithms.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>SENST*: approaches for reducing the energy consumption of smartphone-based context recognition</title>
        <abstract>Modern smartphones provide sensors that can be used to describe the current context of the device and its user. Contextual knowledge allows software systems to adapt to personal preferences of users and to make data processing context-aware. Different sensors or measurement approaches used for recognizing the values of particular context elements vary greatly in their energy consumption. This paper presents approaches for reducing the energy consumption of utilizing smartphone sensors. We discuss sensor substitution strategies as well as logical dependencies among sensor measurements. The paper describes the first milestone towards a generalization of such strategies. Furthermore,We show that energy awareness benefits from a more abstract view on context elements.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Managing Adaptive Versatile environments</title>
        <abstract>The goal of the MavHome project is to develop technologies to Manage Adaptive Versatile environments. In this paper, we present a complete agent architecture for a single inhabitant intelligent environment and discuss the development, deployment, and techniques utilized in our working intelligent environments. Empirical evaluation of our approach has proven its effectiveness at reducing inhabitant interactions in simulated and real environments. </abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Comparison of energy intake prediction algorithms for systems powered by photovoltaic harvesters</title>
        <abstract>Small size photovoltaic modules can harvest enough energy to power many personal devices and wireless sensor nodes. The prediction of solar energy intake is possible thanks to the periodical availability of the sunlight and its cyclic behavior. Thus, smart and innovative power management strategies can take advantage from intake prediction algorithms to optimize the energy usage by keeping the system in low power state as long as possible. On the other hand, very accurate predictions need time and energy because of complex calculations, thus an algorithm that can provide the optimal trade-off between computational effort and accuracy is a breakthrough for systems with tight power constraints. In this paper we introduce an innovative, efficient and reliable solar prediction algorithm, the weather conditioned moving average (WCMA). The algorithm has been further enhanced to increase performance using a phase displacement regulator (PDR) which reduces the average error to less than 9.2% at a minimum energy cost. The proposed new algorithm compares favorably with several competing approaches. </abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Clustering of wireless sensor and actor networks based on sensor distribution and connectivity</title>
        <abstract>Wireless Sensor and Actor Networks (WSANs) employ significantly more capable actor nodes that can collect data from sensors and perform application specific actions. To take these actions collaboratively at any spot in the monitored regions, maximal actor coverage along with inter-actor connectivity is desirable. In this paper, we propose a distributed actor positioning and clustering algorithm which employs actors as cluster-heads and places them in such a way that the coverage of actors is maximized and the data gathering and acting times are minimized. Such placement of actors is done by determining the k-hop Independent Dominating Set (IDS) of the underlying sensor network. Basically, before the actors are placed, the sensors pick the cluster-heads based on IDS. The actors are then placed at the locations of such cluster-heads. We further derive conditions to guarantee inter-actor connectivity after the clustering is performed. If inter-connectivity does not exist, the actors coordinate through the underlying sensors in their clusters to adjust their locations so that connectivity can be established. The performances of the proposed approaches are validated through simulations. </abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>A compositional framework for real-time embedded systems</title>
        <abstract>While component technology has been widely accepted as a methodology for designing complex systems, there are few component technologies that have been developed to accommodate the characteristics of embedded systems. Embedded systems are often subject to resource constraints as well as timing constraints. Typical scarce resources include memory for cost-sensitive systems. Many techniques, developed for reducing code size, often yield code size vs. execution time tradeoffs. Our goal is to develop a framework for supporting the compositionality of resource and timing properties. The proposed framework allows component-level resource and timing properties, which include the resource/time tradeoffs, to be independently analyzed, abstracted, and composed into the system-level resource and timing properties. In this paper, we focus on the problem of composing the collective task-level code size vs. execution time tradeoffs into a component-level code size vs. execution time tradeoff.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Data quality ontology: an ontology for imperfect knowledge</title>
        <abstract>Data quality and ontology are two of the dominating research topics in GIS, influencing many others. Research so far investigated them in isolation. Ontology is concerned with perfect knowledge of the world and ignores so far imperfections in our knowledge. An ontology for imperfect knowledge leads to a consistent classification of imperfections of data (i.e., data quality), and a formalizable description of the influence of data quality on decisions. If we want to deal with data quality with ontological methods, then reality and the information model stored in the GIS must be represented in the same model. This allows to use closed loops semantics to define "fitness for use" as leading to correct, executable decisions. The approach covers knowledge of physical reality as well as personal (subjective) and social constructions. It lists systematically influences leading to imperfections in data in logical succession.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Monitoring heritage buildings with wireless sensor networks: The Torre Aquila deployment</title>
        <abstract>Wireless sensor networks are untethered infrastructures that are easy to deploy and have limited visual impact—a key asset in monitoring heritage buildings of artistic interest. This paper describes one such system deployed in Torre Aquila, a medieval tower in Trento (Italy). Our contributions range from the hardware to the graphical front-end. Customized hardware deals efficiently with high-volume vibration data, and specially-designed sensors acquire the building's deformation. Dedicated software services provide: i) data collection, to efficiently reconcile the diverse data rates and reliability needs of heterogeneous sensors; ii) data dissemination, to spread configuration changes and enable remote tasking; iii) time synchronization, with low memory demands. Unlike most deployments, built directly on the operating system, our entire software layer sits atop our TeenyLIME middleware. Based on 4 months of operation, we show that our system is an effective tool for assessing the tower's stability, as it delivers data reliably (with loss ratios ≪0.01%) and has an estimated lifetime beyond one year.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Multimode locomotion via SuperBot reconfigurable robots</title>
        <abstract>One of the most challenging issues for a self-sustaining robotic system is how to use its limited resources to accomplish a large variety of tasks. The scope of such tasks could include transportation, exploration, construction, inspection, maintenance,in-situ resource utilization, and support for astronauts. This paper proposes a modular and reconfigurable solution for this challenge by allowing a robot to support multiple modes of locomotion and select the appropriate mode for the task at hand. This solution relies on robots that are made of reconfigurable modules. Each locomotion mode consists of a set of characteristics for the environment type, speed, turning-ability, energy-efficiency, and recoverability from failures. This paper demonstrates a solution using the SuperBot robot that combines advantages from M-TRAN, CONRO, ATRON, and other chain-based and lattice-based robots. At the present, a single real SuperBot module can move, turn, sidewind, maneuver, and travel on batteries up to 500 m on carpet in an office environment. In physics-based simulation, SuperBot modules can perform multimodal locomotions such as snake, caterpillar, insect, spider, rolling track, H-walker, etc. It can move at speeds of up to 1.0 m/s on flat terrain using less than 6 W per module, and climb slopes of no less 40 degrees.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Compositional framework for real-time embedded systems</title>
        <abstract>An embedded system consists of a collection of components that interact with each other and with their environment through sensors and actuators. Two key characteristics of embedded systems are real-time and resource-constrained. As embedded systems become more complex due to increased functionalities, it is desirable to achieve the compositional design and analysis of resource-constrained real-time systems, i.e., the system-level design and analysis on the timing and resource aspects can be achieved by composing independently obtained component-level design and analysis results. In this dissertation, we propose a framework for this problem. In the proposed framework, we develop techniques for the compositional schedulability analysis of real-time systems through real-time component interfaces. We also develop techniques for supporting the compositional design and analysis of resource-constrained real-time systems by determining the resource use of each task within individual components such that a total cost on collective resource use is minimized subject to the system's timing and resource constraints.

    In the real-time systems community, compositional schedulability analysis has not been adequately addressed except trivial cases. In this dissertation, we extend the results of traditional real-time scheduling theories by including a notion of real-time resource model into schedulability analysis. We propose a periodic resource model that can specify periodic behavior in resource allocations and develop exact schedulability conditions with the worst-case resource supply scenario of the proposed periodic resource model. Based on this result, we derive a periodic component interface that specifies the minimum periodic resource requirements necessary to guarantee the schedulability of individual components. We then achieve the compositional schedulability analysis of real-time systems through the periodic component interface.

    Typical scarce resources for real-time embedded systems include energy for battery-operated systems and memory for cost-sensitive systems. Many techniques have been proposed to reduce energy consumption and program code size, respectively. These techniques often produce tradeoffs between reducing resource consumption and increasing program execution time. Given such tradeoffs for resource-constrained real-time systems, we consider a multidimensional optimization problem that is to determine the resource use of individual workloads while a total cost on the resource uses, in terms of processor utilization, code size, and processor energy consumption, subject to the system's real-time and resource constraints. Showing the NP-hardness of this problem, we develop a framework for exploring tradeoff space to find sub-optimal solutions efficiently and extend the framework for addressing the problem compositionally.

    In this dissertation, we propose a framework for supporting component-based design and analysis on timing and resource aspects. Our proposed framework lays a groundwork for future advances of component-based design and analysis techniques for real-time embedded systems.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>1</relevance>
    </item>
    <item>
        <title>Standardization on Body Area Network and a Prototype System Based on UWB</title>
        <abstract>Body area network (BAN) is a promising wireless technology that realizes wireless connectivity among vital signal sensors deployed on human body. Monitoring various vital signals collected through BAN provides an efficient way to lower disease occurrence rate and reduce medical expenditure. Task Group 6 (TG6) within the IEEE 802 Local and Metropolitan Area Network Standards Committee is developing a BAN standard, i.e., IEEE 802.15.6. In which, specifications of three physical layers (PHYs) and a single common medium access control (MAC) are being drafted. The standardization process has been continuing for several years in the Task Group 6 (TG6) under Working Group 15 (WG). In this paper, we describe the up-to-date status of IEEE 802.15.6 standardization. Some main specifications under drafting are presented. Moreover, as an effort of implementing a BAN model, a prototype BAN system based on the high band of ultra-wideband (UWB) is demonstrated.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>In-network data estimation for sensor-driven scientific applications</title>
        <abstract>Sensor networks employed by scientific applications often need to support localized collaboration of sensor nodes to perform in-network data processing. This includes new quantitative synthesis andhypothesis testing in near real time, as data streaming from distributedinstruments, to transform raw data into high level domain-dependent information. This paper investigates in-network data processing mechanismswith dynamic data requirements in resource constrained heterogeneoussensor networks. Particularly, we explore how the temporaland spatial correlation of sensor measurements can be used to trade off between the complexity of coordination among sensor clusters and the savings that result from having fewer sensors involved in in-network processing,while maintaining an acceptable error threshold. Experimental results show that the proposed in-network mechanisms can facilitate the efficient usage of resources and satisfy data requirement in the presence of dynamics and uncertainty.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Intelligent software agent framework for customized mobile services</title>
        <abstract>In this paper, a framework for customized mobile services is presented to ease and speed up new Internet service development and deployment in mobile platforms. An intelligent software agent approach is used in framework realization together with the blackboard style. Agents are used because of their capabilities like autonomous, situated, reactive, proactive, flexible, robust and social. The presented framework consists of an extensible set of intelligent software agents managing connectivity to the networks, configuration, QoS, power consumption, and context-awareness including location-based information, intelligent handover, presence, security, firewalls and many others.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Software engineering for health education and care delivery systems: The Smart Condo project</title>
        <abstract>Providing affordable, high-quality healthcare to the elderly while enabling them to live independently longer is of critical importance, as this is an increasing and expensive demographic to treat. Sensor-network technologies are essential to developing assisted living environments. In our Smart Condo project, we have deployed a sensor network with a variety of sensor types in an 850 square-foot condominium. The sensor network records a variety of events and environmental parameters and feeds the related data into our web-based system. This system is responsible for inferring higher-order information about the activities of the condo's occupant and supporting the visualization of the collected information in a 2D Geographic Information System (GIS) and a 3D virtual world, namely Second Life (SL).</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Uncertainty-aware Wireless Sensor Networks</title>
        <abstract>The characterisation of uncertainty and the management of Quality of Service are important issues in mobile communications. In a Wireless Sensor Network, there is a high probability of redundancy, correlation and noise in the sensor features since data is often collected from a large array of densely deployed neighbouring sensors. This article proposes a soft computing approach to manage uncertainty by reasoning over inconsistent, incomplete, and fragmentary information using classical rough set and dominance-based rough set theories. A methodological and computational basis is provided and is illustrated in a real world sensor network application of aquatic biodiversity mapping under uncertainty.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>Rendezvous Layer Protocols for Bluetooth-Enabled Smart Devices</title>
        <abstract>Communication platforms for ubiquitous computing need to be flexible, self-organizing, highly scalable and energy efficient, because in the envisioned scenarios a large number of autonomous entities communicate in potentially unpredictable ways. Short-range wireless technologies form the basis of such communication platforms. In this paper we investigate device discovery in Bluetooth, a candidate wireless technology for ubiquitous computing. Detecting new devices accounts for a significant portion of the total energy consumption in Bluetooth. It is argued that the standard Bluetooth rendezvous protocols for device detection are not well suited for ubiquitous computing scenarios, because they do not scale to a large number of devices, take too long to complete, and consume too much energy. Based on theoretical considerations, practical experiments and simulation results, recommendations for choosing inquiry parameters that optimize discovery performance are given. We propose an adaptive rendezvous protocol that significantly increases the performance of the inquiry procedure by implementing cooperative device discovery. Also higher level methods to optimize discovery performance, specifically the use of sensory data and context information, are considered.</abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>
    <item>
        <title>An improved transport layer protocol for wireless sensor networks</title>
        <abstract>Wireless sensor networks (WSNs) are a kind of communication networks having independent sensor nodes that form multi-hop ad hoc network to transfer data. In the past few years, various transport control protocols in wireless sensor networks (WSNs) have been developed and proposed in the literature. In this paper, we have analyzed pump slowly, fetch quickly (PSFQ) protocol and presented an improved transport layer protocol for wireless sensor networks. The improved protocol has been analyzed based on various factors such as average latency and average error tolerance and it is found that the proposed protocol is better than PSFQ in terms of these factors. </abstract>
        <search_task_number>19</search_task_number>
        <query>("through sensors") and ("energy consumption" or "save energy")</query>
        <relevance>0</relevance>
    </item>



  <item>
    <title>New Challenges for Cross-Language Information Retrieval: Multimedia Data and the User Experience</title>
	<abstract>Evaluation exercises in Cross-Language Information Retrieval (CLIR) have so far been limited to the location of potentially relevant documents from within electronic text collections. Although there has been considerable progress in recent years much further research is required in CLIR, and clearly one focus of future research must continue to address fundamental retrieval issues. However, CLIR is now sufficiently mature to broaden the investigation to consider some new challenges. Two interesting further areas of investigation are the user experience of accessingin formation from retrieved documents in CLIR, and the extension of existing research to cross-language methods for multimedia retrieval.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Evaluating Interactive Cross-Language Information Retrieval: Document Selection</title>
	<abstract>The problem of finding documents that are written in a language that the searcher cannot read is perhaps the most challenging application of Cross-Language Information Retrieval (CLIR) technology. The first Cross-Language Evaluation Forum (CLEF) provided an excellent venue for assessing the performance of automated CLIR techniques, but little is known about how searchers and systems might interact to achieve better cross-language search results than automated systems alone can provide. This paper explores the question of how interactive approaches to CLIR might be evaluated, suggesting an initial focus on evaluation of interactive document selection. Important evaluation issues are identified, the structure of an interactive CLEF evaluation is proposed, and the key research communities that could be brought together by such an evaluation are introduced.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>The Domain-Specific Task of CLEF - Specific Evaluation Strategies in Cross-Language Information Retrieval</title>
	<abstract>This paper describes the domain-specific cross-language information retrieval (CLIR) task of CLEF, why and how it is important and how it differs from general cross-language retrieval problem associated with the general CLEF collections. The inclusion of a domainspecific document collection and topics has both advantages and disadvantages.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Cross-Language Information Retrieval Using Dutch Query Translation</title>
	<abstract>This paper describes an elementary bilingual information retrieval experiment. The experiment takes Dutch topics to retrieve relevant English documents using Microsoft SQL Server version 7.0. In order to cross the language barrier between query and document, the researchers use query translation by means of a machine-readable dictionary. The Dutch run was void of the typical natural language processing techniques such as parsing, stemming, or part of speech tagging. A monolingual run was carried out for comparison purposes. Due to limitations in time, retrieval system, translation method, and test collection, there is only a preliminary analysis of the results.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Translation Resources, Merging Strategies, and Relevance Feedback for Cross-Language Information Retrieval</title>
	<abstract>This paper describes the official runs of the Twenty-One group for the first CLEF workshop. The Twenty-One group participated in the monolingual, bilingual and multilingual tasks. The following new techniques are introduced in this paper. In the bilingual task we experimented with different methods to estimate translation probabilities. In the multilingual task we experimented with refinements on raw-score merging techniques and with a new relevance feedback algorithm that re-estimates both the model's translation probabilities and the relevance weights. Finally, we performed preliminary experiments to exploit the web to generate translation probabilities and bilingual dictionaries, notably for English-Italian and English-Dutch.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Cross-language information retrieval using meta-language index construction and structural queries</title>
	<abstract>Structural Query Language allows expert users to richly represent its information needs but unfortunately, the complexity of SQLs make them impractical in the Web search engines. Automatically detecting the concepts in an unstructured user's information need and generating a richly structured, multilingual equivalent query is an ideal solution. We utilize Wikipedia as a great concept repository and also some state of the art algorithms for extracting Wikipedia's concepts from the user's information need. This process is called "Query Wikification". Our experiments on the TEL corpus at CLEF2009 achieves +23% and +17% improvement in Mean Average Precision and Recall against the baseline. Our approach is unique in that, it does improve both precision and recall; two pans that often improving one, hurt the another.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>User-assisted query translation for interactive cross-language information retrieval</title>
	<abstract>Interactive Cross-Language Information Retrieval (CLIR), a process in which searcher and system collaborate to find documents that satisfy an information need regardless of the language in which those documents are written, calls for designs in which synergies between searcher and system can be leveraged so that the strengths of one can cover weaknesses of the other. This paper describes an approach that employs user-assisted query translation to help searchers better understand the system's operation. Supporting interaction and interface designs are introduced, and results from three user studies are presented. The results indicate that experienced searchers presented with this new system evolve new search strategies that make effective use of the new capabilities, that they achieve retrieval effectiveness comparable to results obtained using fully automatic techniques, and that reported satisfaction with support for cross-language searching increased. The paper concludes with a description of a freely available interactive CLIR system that incorporates lessons learned from this research.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>A Hybrid Technique for English-Chinese Cross Language Information Retrieval</title>
	<abstract>In this article we describe a hybrid technique for dictionary-based query translation suitable for English-Chinese cross language information retrieval. This technique marries a graph-based model for the resolution of candidate term ambiguity with a pattern-based method for the translation of out-of-vocabulary (OOV) terms. We evaluate the performance of this hybrid technique in an experiment using several NTCIR test collections. Experimental results indicate a substantial increase in retrieval effectiveness over various baseline systems incorporating machine- and dictionary-based translation.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Dictionary-Based Cross-Language Information Retrieval: Problems, Methods, and Research Findings</title>
	<abstract>This paper reviews literature on dictionary-based cross-language information retrieval (CLIR) and presents CLIR research done at the University of Tampere (UTA). The main problems associated with dictionary-based CLIR, as well as appropriate methods to deal with the problems are discussed. We will present the structured query model by Pirkola and report findings for four different language pairs concerning the effectiveness of query structuring. The architecture of our automatic query translation and construction system is presented.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Technical issues of cross-language information retrieval: a review</title>
	<abstract>This paper reviews state-of-the-art techniques and methods for enhancing effectiveness of cross-language information retrieval (CLIR). The following research issues are covered: (1) matching strategies and translation techniques, (2) methods for solving the problem of translation ambiguity, (3) formal models for CLIR such as application of the language model, (4) the pivot language approach, (5) methods for searching multilingual document collection, (6) techniques for combining multiple language resources, etc.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Translation enhancement: a new relevance feedback method for cross-language information retrieval</title>
	<abstract>As an effective technique for improving retrieval effectiveness, relevance feedback (RF) has been widely studied in both monolingual and cross-language information retrieval (CLIR) settings. The studies of RF in CLIR have been focused on query expansion (QE), in which queries are reformulated before and/or after they are translated. However, RF in CLIR actually not only can help select better query terms, but also can enhance query translation by adjusting translation probabilities and even resolve some out-of-vocabulary terms. In this paper, we propose a novel RF method called translation enhancement (TE), which uses the extracted translation relationships from relevant documents to revise the translation probabilities of query terms and to identify extra translation alternatives if available so that the translated queries are more tuned to the current search. We studied TE using pseudo relevance feedback (PRF) and interactive relevance feedback (IRF). Our results show that TE can significantly improve CLIR with both types of RF methods, and that the improvement is comparable to that of QE. More importantly, the effects of TE and QE are complementary. Their integration can produce further improvement, and makes CLIR more robust for a variety of queries.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>
Retrieval effectiveness of cross language information retrieval search engines</title>
	<abstract>This study evaluates the retrieval effectiveness of English-Chinese (EC) cross-language information retrieval (CLIR) on four common search engines along the dimensions of recall and precision. We formulated a set of simple and complex queries on different topics including queries with translation ambiguity. Three independent bilingual proficient evaluators reviewed a total of 960 returned web pages each to assess document relevance. Findings showed that CLIR effectiveness is poor with average recall and precision values of 0.165 and 0.539 for monolingual EE/CC searches, and 0.078 and 0.282 for cross lingual CE/EC searches. Google outperformed Yahoo! in the experiments, and EC and EE searches returned better results than CE and CC results respectively. As this is the first set CLIR retrieval effectiveness measurements reported in literature, these findings can serve as a benchmark and provide a better understanding of the current CLIR capabilities of Web search engines.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Translation disambiguation for cross-language information retrieval using context-based translation probability</title>
	<abstract>Disambiguation between multiple translation choices is very important in dictionary-based cross-language information retrieval. In prior work, disambiguation techniques have used term co-occurrence statistics from the collection being searched. Experimentally these techniques have worked well but are based upon heuristic assumptions. In this paper, a theoretically grounded alternative is proposed, one which uses sense disambiguation based upon context terms within the source text. Specifically this paper introduces the concept of translation probabilities incorporating a context term and extends the IBM Model 1 for estimating context-based translation probabilities from a sentence-aligned bilingual corpus. Experimental results in English to Italian bilingual searches show significant performance improvement of the context-based translation probabilities over the case without any disambiguation.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>A maximum coherence model for dictionary-based cross-language information retrieval</title>
	<abstract>One key to cross-language information retrieval is how to efficiently resolve the translation ambiguity of queries given their short length. This problem is even more challenging when only bilingual dictionaries are available, which is the focus of this paper. In the previous research of cross-language information retrieval using bilingual dictionaries, the word co-occurrence statistics is used to determine the most likely translations of queries. In this paper, we propose a novel statistical model, named ``maximum coherence model'', which estimates the translation probabilities of query words that are consistent with the word co-occurrence statistics. Unlike the previous work, where a binary decision is made for the selection of translations, the new model maintains the uncertainty in translating query words when their sense ambiguity is difficult to resolve. Furthermore, this new model is able to estimate translations of multiple query words simultaneously. This is in contrast to many previous approaches where translations of individual query words are determined independently. Empirical studies with TREC datasets have shown that the maximum coherence model achieves a relative 10% - 40% improvement in cross-language information retrieval, comparing to other approaches that also use word co-occurrence statistics for sense disambiguation.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Using comparable corpora to improve the effectiveness of cross-language information retrieval</title>
	<abstract>Large-scale comparable corpora became more abundant and accessible than parallel corpora, with the explosive growth of the World Wide Web. From the Cross-Language Information Retrieval point of view, limitation of translation resources as well as ambiguity arising due to failure to translate query terms is largely responsible for large drops in the effectiveness below monolingual performance. Therefore, strategies on bilingual terminology extraction from comparable texts must be given more attention in order to enrich existing bilingual lexicons and thesauri and to enhance Cross-Language Information Retrieval. In the present paper, we focus on the enhancement of Cross-Language Information Retrieval using a two-stage corpus-based translation model that includes bi-directional extraction of bilingual terminology from comparable corpora and selection of best translation alternatives on the basis of their morphological knowledge. The impact of comparable corpora on the performance of the Cross-Language Information Retrieval process is evaluated in this study and the results indicate that the effect is clearly positive, especially when using the linear combination with bilingual dictionaries and Japanese-English pair of languages.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Using Web resources to construct multilingual medical thesaurus for cross-language medical information retrieval</title>
	<abstract>Due to the language barrier, non-English users are unable to retrieve the most updated medical information from the U.S. authoritative medical websites, such as PubMed and MedlinePlus. However, currently, there is no any cross-language medical information retrieval (CLMIR) system that can help Chinese-speaking consumers cross the language barrier in finding useful English medical information. A few CLMIR systems utilize MeSH (Medical Subject Headings) to help overcome the language barrier. Unfortunately, the traditional Chinese version of MeSH is currently unavailable. In this paper, we employ a semi-automatic term translation method to construct a Chinese-English MeSH by exploiting abundant multilingual Web resources, including Web anchor texts and search-result pages. Through this method, we have developed a Chinese-English Mesh Compilation System to assist knowledge engineers in compiling a Chinese-English medical thesaurus with more than 19,000 entries. Furthermore, this thesaurus has been used to develop a prototypical system for cross-language medical information retrieval, MMODE, which can help consumers retrieve top-quality English medical information using Chinese terms.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Bootstrapping dictionaries for cross-language information retrieval</title>
	<abstract>The bottleneck for dictionary-based cross-language information retrieval is the lack of comprehensive dictionaries, in particular for many different languages. We here introduce a methodology by which multilingual dictionaries (for Spanish and Swedish) emerge automatically from simple seed lexicons. These seed lexicons are automatically generated, by cognate mapping, from (previously manually constructed) Portuguese and German as well as English sources. Lexical and semantic hypotheses are then validated and new ones iteratively generated by making use of co-occurrence patterns of hypothesized translation synonyms in parallel corpora. We evaluate these newly derived dictionaries on a large medical document collection within a cross-language retrieval setting.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Research on English-Chinese bi-directional cross-language information retrieval</title>
	<abstract>With the rapid growing amount of information available to us, the situations that a user needs to use a retrieval system to perform querying a multilingual document collection are becoming increasingly emerging and common. Thus an important problem is formed, to match the user queries specified in one language against documents in another different language, i.e. Cross-Language Information Retrieval (CLIR). Based on the work in CLIR evaluation task in the 9th Text Retrieval Conference (TREC-9), we have constructed an English-Chinese bi-directional CLIR system. In this system, we adopt English-Chinese bi-directional query translation as the dominant strategy, use English and Chinese queries as translation objects, and utilize English and Chinese machine readable dictionaries as the important knowledge source to acquire correct translations. By combining English and Chinese monolingual IR systems constructed by us, the complete English-Chinese bi-directional CLIR process can be implemented successfully.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Using Statistical Term Similarity for Sense Disambiguationin Cross-Language Information Retrieval</title>
	<abstract>With the increasing availability of machine-readable bilingual dictionaries, dictionary-based automatic query translation has become a viable approach to Cross-Language Information Retrieval (CLIR). In this approach, resolving term ambiguity is a crucial step. We propose a sense disambiguation technique based on a term-similarity measure for selecting the right translation sense of a query term. In addition, we apply a query expansion technique which is also based on the term similarity measure to improve the effectiveness of the translation queries. The results of our Indonesian to English and English to Indonesian CLIR experiments demonstrate the effectiveness of the sense disambiguation technique. As for the query expansion technique, it is shown to be effective as long as the term ambiguity in the queries has been resolved. In the effort to solve the term ambiguity problem, we discovered that differences in the pattern of word-formation between the two languages render query translations from one language to the other difficult.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>A progressive algorithm for cross-language information retrieval based on dictionary translation</title>
	<abstract>Query translation is the mainstream in cross-language information retrieval, but ambiguity must be resolved by methods based on dictionary translation. In this paper, we propose a progressive algorithm for disambiguation which is derived from another algorithm we propose called the max-sum model. The new algorithm take a strategy called weighted-average probability distribution to redistribute the probabilities. Moreover, the new algorithm can be computed in a more direct way by solving an equation system. All the resource our method requires is a bilingual dictionary and a monolingual corpus. Experiments show it outperforms four other methods.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Automatic processing of multilingual medical terminology: applications to thesaurus enrichment and cross-language information retrieval</title>
	<abstract>Objectives:: We present in this article experiments on multi-language information extraction and access in the medical domain. For such applications, multilingual terminology plays a crucial role when working on specialized languages and specific domains. Material and methods:: We propose firstly a method for enriching multilingual thesauri which extracts new terms from parallel corpora, and secondly, a new approach for bilingual lexicon extraction from comparable corpora, which uses a bilingual thesaurus as a pivot. We illustrate their use in multi-language information retrieval (English/German) in the medical domains. Results:: Our experiments show that these automatically extracted bilingual lexicons are accurate enough (85% precision for term extraction) for semi-automatically enriching mono- or bi-lingual thesauri such as the universal medical language system, and that their use in cross-language information retrieval significantly improves the retrieval performance (from 22 to 40% average precision) and clearly outperforms existing bilingual lexicon resources (both general lexicons and specialized ones). Conclusion:: We show in this paper first that bilingual lexicon extraction from parallel corpora in the medical domain could lead to accurate, specialized lexicons, which can be used to help enrich existing thesauri and second that bilingual lexicons extracted from comparable corpora outperform general bilingual resources for cross-language information retrieval.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Statistical transliteration for english-arabic cross language information retrieval</title>
	<abstract>Out of vocabulary (OOV) words are problematic for cross language information retrieval. One way to deal with OOV words when the two languages have different alphabets, is to transliterate the unknown words, that is, to render them in the orthography of the second language. In the present study, we present a simple statistical technique to train an English to Arabic transliteration model from pairs of names. We call this a selected n-gram model because a two-stage training procedure first learns which n-gram segments should be added to the unigram inventory for the source language, and then a second stage learns the translation model over this inventory. This technique requires no heuristics or linguistic knowledge of either language. We evaluate the statistically-trained model and a simpler hand-crafted model on a test set of named entities from the Arabic AFP corpus and demonstrate that they perform better than two online translation sources. We also explore the effectiveness of these systems on the TREC 2002 cross language IR task. We find that transliteration either of OOV named entities or of all OOV words is an effective approach for cross language IR.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Exploiting comparable corpora for cross-language information retrieval</title>
	<abstract>Large-scale comparable corpora became more abundant and accessible than parallel corpora, with the explosive growth of the World Wide Web. Therefore, strategies on bilingual terminology extraction from comparable texts must be given more attention in order to enrich existing bilingual lexicons and thesauri and to enhance Cross-Language Information Retrieval. In the present paper, we focus on the enhancement of Cross-Language Information Retrieval using a two-stage corpus-based translation model that includes bi-directional extraction of bilingual terminology from comparable corpora and selection of best translation alternatives on the basis of their morphological knowledge. The impact of comparable corpora on the performance of the Cross-Language Information Retrieval process is evaluated in this study and the results indicate that the effect is clearly positive, especially when using the linear combination with bilingual dictionaries and Japanese-English pair of languages.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Translating unknown queries with web corpora for cross-language information retrieval</title>
	<abstract>It is crucial for cross-language information retrieval (CLIR) systems to deal with the translation of unknown queries due to that real queries might be short. The purpose of this paper is to investigate the feasibility of exploiting the Web as the corpus source to translate unknown queries for CLIR. We propose an online translation approach to determine effective translations for unknown query terms via mining of bilingual search-result pages obtained from Web search engines. This approach can alleviate the problem of the lack of large bilingual corpora, translate many unknown query terms, provide flexible query specifications, and extract semantically-close translations to benefit CLIR tasks -- especially for cross-language Web search.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Named entity transliteration for cross-language information retrieval using compressed word format mapping algorithm</title>
	<abstract>Transliteration of named entities in user queries is a vital step in any Cross-Language Information Retrieval (CLIR) system. Several methods for transliteration have been proposed till date based on the nature of the languages considered. In this paper, we present a transliteration algorithm for mapping English named entities to their proper Tamil equivalents. Our algorithm employs a grapheme-based model, in which transliteration equivalents are identified by mapping the source language names to their equivalents in a target language database, instead of generating them. The basic principle is to compress the source word into its minimal form and align it across an indexed list of target language words to arrive at the top n-equivalents based on the edit distance. We compare the performance of our approach with a statistical generation approach using Microsoft Research India (MSRI) transliteration corpus. Our approach has proved very effective in terms of accuracy and time.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Web-based pattern learning for named entity translation in Korean-Chinese cross-language information retrieval</title>
	<abstract>Named entity (NE) translation plays an important role in many applications, such as information retrieval and machine translation. In this paper, we focus on translating NEs from Korean to Chinese in order to improve Korean-Chinese cross-language information retrieval (KCIR). The ideographic nature of Chinese makes NE translation difficult because one syllable may map to several Chinese characters. We propose a hybrid NE translation system. First, we integrate two online databases to extend the coverage of our bilingual dictionaries. We use Wikipedia as a translation tool based on the inter-language links between the Korean edition and the Chinese or English editions. We also use Naver.com's people search engine to find a query name's Chinese or English translation. The second component of our system is able to learn Korean-Chinese (K-C), Korean-English (K-E), and English-Chinese (E-C) translation patterns from the web. These patterns can be used to extract K-C, K-E and E-C pairs from Google snippets. We found KCIR performance using this hybrid configuration over five times better than that a dictionary-based configuration using only Naver people search. Mean average precision was as high as 0.3385 and recall reached 0.7578. Our method can handle Chinese, Japanese, Korean, and non-CJK NE translation and improve performance of KCIR substantially.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>
Learning bilingual translations from comparable corpora to cross-language information retrieval: hybrid statistics-based and linguistics-based approach</title>
	<abstract>Recent years saw an increased interest in the use and the construction of large corpora. With this increased interest and awareness has come an expansion in the application to knowledge acquisition and bilingual terminology extraction. The present paper will seek to present an approach to bilingual lexicon extraction from non-aligned comparable corpora, combination to linguistics-based pruning and evaluations on Cross-Language Information Retrieval. We propose and explore a two-stages translation model for the acquisition of bilingual terminology from comparable corpora, disambiguation and selection of best translation alternatives on the basis of their morphological knowledge. Evaluations using a large-scale test collection on Japanese-English and different weighting schemes of SMART retrieval system confirmed the effectiveness of the proposed combination of two-stages comparable corpora and linguistics-based pruning on Cross-Language Information Retrieval.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>User-centered interface design for cross-language information retrieval</title>
	<abstract>This paper reports on the user-centered design methodology and techniques used for the elicitation of user requirements and how these requirements informed the first phase of the user interface design for a Cross-Language Information Retrieval System. We describe a set of factors involved in analysis of the data collected and, finally discuss the implications for user interface design based on the findings.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Cross-language information retrieval: the way ahead</title>
	<abstract>This introductory paper covers not only the research content of the articles in this special issue of IP&amp;M but attempts to characterize the state-of-the-art in the Cross-Language Information Retrieval (CLIR) domain. We present our view of some major directions for CLIR research in the future. In particular, we find that insufficient attention has been given to the Web as a resource for multilingual research, and to languages which are spoken by hundreds of millions of people in the world but have been mainly neglected by the CLIR research community. In addition, we find that most CLIR evaluation has focussed narrowly on the news genre to the exclusion of other important genres such as scientific and technical literature. The paper concludes by describing an ambitious 5-year research plan proposed by James Mayfield and Paul McNamee.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Improving query translation in English-Korean cross-language information retrieval</title>
	<abstract>Query translation is a viable method for cross-language information retrieval (CLIR), but it suffers from translation ambiguities caused by multiple translations of individual query terms. Previous research has employed various methods for disambiguation, including the method of selecting an individual target query term from multiple candidates by comparing their statistical associations with the candidate translations of other query terms. This paper proposes a new method where we examine all combinations of target query term translations corresponding to the source query terms, instead of looking at the candidates for each query term and selecting the best one at a time. The goodness value for a combination of target query terms is computed based on the association value between each pair of the terms in the combination. We tested our method using the NTCIR-3 English Korean-CLIR test collection. The results show some improvements regardless of the association measures we used.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Mining OOV translations from mixed-language web pages for cross language information retrieval</title>
	<abstract>Translating Out-Of-Vocabulary (OOV) terms is crucial for Cross Language Information Retrieval (CLIR). In this paper, we propose a method that automatically acquires a large quantity of OOV translations from the web. Different from previous approaches that rely on a finite set of hand-crafted extraction rules, our method adaptively learns translation extraction patterns based on the observation that translation pairs on the same page tend to appear following similar layout patterns. The learned patterns are leveraged in a discriminative translation extraction model that treats translation extraction from a mixed language bilingual web page as a sequence labeling task in order to exploit useful relations among translation pairs on the page. Experiments demonstrate that our proposed method out-performs earlier work with marked improvement on OOV translation mining quality.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Dictionary-based techniques for cross-language information retrieval</title>
	<abstract>Cross-language information retrieval (CLIR) systems allow users to find documents written in different languages from that of their query. Simple knowledge structures such as bilingual term lists have proven to be a remarkably useful basis for bridging that language gap. A broad array of dictionary-based techniques have demonstrated utility, but comparison across techniques has been difficult because evaluation results often span only a limited range of conditions. This article identifies the key issues in dictionary-based CLIR, develops unified frameworks for term selection and term translation that help to explain the relationships among existing techniques, and illustrates the effect of those techniques using four contrasting languages for systematic experiments with a uniform query translation architecture. Key results include identification of a previously unseen dependence of pre- and post-translation expansion on orthographic cognates and development of a query-specific measure for translation fanout that helps to explain the utility of structured query methods.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Embedding web-based statistical translation models in cross-language information retrieval</title>
	<abstract>Although more and more language pairs are covered by machine translation (MT) services, there are still many pairs that lack translation resources. Cross-language information retrieval (CLIR) is an application that needs translation functionality of a relatively low level of sophistication, since current models for information retrieval (IR) are still based on a bag of words. The Web provides a vast resource for the automatic construction of parallel corpora that can be used to train statistical translation models automatically. The resulting translation models can be embedded in several ways in a retrieval model. In this article, we will investigate the problem of automatically mining parallel texts from the Web and different ways of integrating the translation models within the retrieval process. Our experiments on standard test collections for CLIR show that the Web-based translation models can surpass commercial MT systems in CLIR tasks. These results open the perspective of constructing a fully automatic query translation device for CLIR at a very low cost.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Statistical query translation models for cross-language information retrieval</title>
	<abstract>Query translation is an important task in cross-language information retrieval (CLIR), which aims to determine the best translation words and weights for a query. This article presents three statistical query translation models that focus on the resolution of query translation ambiguities. All the models assume that the selection of the translation of a query term depends on the translations of other terms in the query. They differ in the way linguistic structures are detected and exploited. The co-occurrence model treats a query as a bag of words and uses all the other terms in the query as the context for translation disambiguation. The other two models exploit linguistic dependencies among terms. The noun phrase (NP) translation model detects NPs in a query, and translates each NP as a unit by assuming that the translation of a term only depends on other terms within the same NP. Similarly, the dependency translation model detects and translates dependency triples, such as verb-object, as units. The evaluations show that linguistic structures always lead to more precise translations. The experiments of CLIR on TREC Chinese collections show that all three models have a positive impact on query translation and lead to significant improvements of CLIR performance over the simple dictionary-based translation method. The best results are obtained by combining the three models.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Dictionary-Based Cross-Language Information Retrieval: Learning Experiences from CLEF 2000–2002</title>
	<abstract>In this study the basic framework and performance analysis results are presented for the three year long development process of the dictionary-based UTACLIR system. The tests expand from bilingual CLIR for three language pairs Swedish, Finnish and German to English, to six language pairs, from English to French, German, Spanish, Italian, Dutch and Finnish, and from bilingual to multilingual. In addition, transitive translation tests are reported. The development process of the UTACLIR query translation system will be regarded from the point of view of a learning process. The contribution of the individual components, the effectiveness of compound handling, proper name matching and structuring of queries are analyzed. The results and the fault analysis have been valuable in the development process. Overall the results indicate that the process is robust and can be extended to other languages. The individual effects of the different components are in general positive. However, performance also depends on the topic set and the number of compounds and proper names in the topic, and to some extent on the source and target language. The dictionaries used affect the performance significantly.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Creating and exploiting a comparable corpus in cross-language information retrieval</title>
	<abstract>We present a method for creating a comparable text corpus from two document collections in different languages. The collections can be very different in origin. In this study, we build a comparable corpus from articles by a Swedish news agency and a U.S. newspaper. The keys with best resolution power were extracted from the documents of one collection, the source collection, by using the relative average term frequency (RATF) value. The keys were translated into the language of the other collection, the target collection, with a dictionary-based query translation program. The translated queries were run against the target collection and an alignment pair was made if the retrieved documents matched given date and similarity score criteria. The resulting comparable collection was used as a similarity thesaurus to translate queries along with a dictionary-based translator. The combined approaches outperformed translation schemes where dictionary-based translation or corpus translation was used alone.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Iterative translation disambiguation for cross-language information retrieval</title>
	<abstract>Finding a proper distribution of translation probabilities is one of the most important factors impacting the effectiveness of a cross-language information retrieval system. In this paper we present a new approach that computes translation probabilities for a given query by using only a bilingual dictionary and a monolingual corpus in the target language. The algorithm combines term association measures with an iterative machine learning approach based on expectation maximization. Our approach considers only pairs of translation candidates and is therefore less sensitive to data-sparseness issues than approaches using higher n-grams. The learned translation probabilities are used as query term weights and integrated into a vector-space retrieval system. Results for English-German cross-lingual retrieval show substantial improvements over a baseline using dictionary lookup without term weighting.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>The effect of named entities on effectiveness in cross-language information retrieval evaluation</title>
	<abstract>The large number of experiments carried out within evaluation initiatives for information retrieval has led to an invaluable source for further research and meta-analysis. In this study, an analysis of the results of the Cross Language Evaluation Forum (CLEF) campaigns for the years 2000 to 2003 is presented. This study considers the performance of the systems for each individual topic. It is dedicated to the influence of named entities on retrieval performance. Named entities in topics lead to significant improvement of the retrieval quality in general and for most systems and tasks. The performance of systems varies for topics without, with one or two and with three or more named entities. This knowledge gained by data mining on the evaluation results can be exploited for the improvement of retrieval systems as well as for the design of topics for future CLEF campaigns.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>
Effective arabic-english cross-language information retrieval via machine-readable dictionaries and machine translation</title>
	<abstract>In Cross-Language Information Retrieval (CLIR), queries in one language retrieve relevant documents in other languages Machine-Readable Dictionary (MRD) and Machine Translation (MT) are important resources for query translation in CLIR. We investigate MT and MRD to Arabic-English CLIR. The translation ambiguity associated with these resources is the key problem. We present three methods of query translation using a bilingual dictionary for Arabic-English CLIR. First, we present the Every-Match (EM) method. This method yields ambiguous translations since many extraneous terms are added to the original query. To disambiguate the query translation, we present the First-Match (FM) method that considers the first match in the dictionary as the candidate term. Finally, we present the Two-Phase (TP) method. We show that good retrieval effectiveness can be achieved without complex resources using the Two-Phase method for Arabic-English CLIR. We also empirically evaluate the effectiveness of the MT-based method using short, medium, and long queries from TREC. The effects of the query length on the quality of the MT-based CLIR are investigated.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Implicit ambiguity resolution using incremental clustering in Korean-to-English cross-language information retrieval</title>
	<abstract>This paper presents a method to implicitly resolve ambiguities using dynamic incremental clustering in Korean-to-English cross-language information retrieval. In the framework we propose, a query in Korean is first translated into English by looking up Korean-English dictionary, then documents are retrieved based on the vector space retrieval for the translated query terms. For the top-ranked retrieved documents, query-oriented document clusters are incrementally created and the weight of each retrieved document is re-calculated by using clusters. In experiment on TREC-6 CLIR test collection, our method achieved 28.29% performance improvement for translated queries without ambiguity resolution for queries. This corresponds to 97.27% of the monolingual performance for original queries. When we combine our method with query ambiguity resolution, our method even outperforms the monolingual retrieval.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>GeoCLEF 2008: the CLEF 2008 cross-language geographic information retrieval track overview</title>
	<abstract>GeoCLEF is an evaluation task running under the scope of the Cross Language Evaluation Forum (CLEF). The purpose of GeoCLEF is to test and evaluate cross-language geographic information retrieval (GIR). The GeoCLEF 2008 task presented twenty-five geographically challenging search topics for English, German and Portuguese. Eleven participants submitted 131 runs, based on a variety of approaches, including sample documents, named entity extraction and ontology based retrieval. The evaluation methodology and results are presented in the paper.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>The effect of translation quality in MT-based cross-language information retrieval</title>
	<abstract>This paper explores the relationship between the translation quality and the retrieval effectiveness in Machine Translation (MT) based Cross-Language Information Retrieval (CLIR). To obtain MT systems of different translation quality, we degrade a rule-based MT system by decreasing the size of the rule base and the size of the dictionary. We use the degraded MT systems to translate queries and submit the translated queries of varying quality to the IR system. Retrieval effectiveness is found to correlate highly with the translation quality of the queries. We further analyze the factors that affect the retrieval effectiveness. Title queries are found to be preferred in MT-based CLIR. In addition, dictionary-based degradation is shown to have stronger impact than rule-based degradation in MT-based CLIR.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Improving query translation for cross-language information retrieval using statistical models</title>
	<abstract>Dictionaries have often been used for query translation in cross-language information retrieval (CLIR). However, we are faced with the problem of translation ambiguity, i.e. multiple translations are stored in a dictionary for a word. In addition, a word-by-word query translation is not precise enough. In this paper, we explore several methods to improve the previous dictionary-based query translation. First, as many as possible, noun phrases are recognized and translated as a whole by using statistical models and phrase translation patterns. Second, the best word translations are selected based on the cohesion of the translation words. Our experimental results on TREC English-Chinese CLIR collection show that these techniques result in significant improvements over the simple dictionary approaches, and achieve even better performance than a high-quality machine translation system.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Combining bidirectional translation and synonymy for cross-language information retrieval</title>
	<abstract>This paper introduces a general framework for the use of translation probabilities in cross-language information retrieval based on the notion that information retrieval fundamentally requires matching what the searcher means with what the author of a document meant. That perspective yields a computational formulation that provides a natural way of combining what have been known as query and document translation. Two well-recognized techniques are shown to be a special case of this model under restrictive assumptions. Cross-language search results are reported that are statistically indistinguishable from strong monolingual baselines for both French and Chinese documents.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Using the web for automated translation extraction in cross-language information retrieval</title>
	<abstract>There have been significant advances in Cross-Language Information Retrieval (CLIR) in recent years. One of the major remaining reasons that CLIR does not perform as well as monolingual retrieval is the presence of out of vocabulary (OOV) terms. Previous work has either relied on manual intervention or has only been partially successful in solving this problem. We use a method that extends earlier work in this area by augmenting this with statistical analysis, and corpus-based translation disambiguation to dynamically discover translations of OOV terms. The method can be applied to both Chinese-English and English-Chinese CLIR, correctly extracting translations of OOV terms from the Web automatically, and thus is a significant improvement on earlier work.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Influence of WSD on cross-language information retrieval</title>
	<abstract>Translation ambiguity is a major problem in dictionary-based cross-language information retrieval. This paper proposes a statistical word sense disambiguation (WSD) approach for translation ambiguity resolution. Then, with respect to CLIR effectiveness, the pure effect of a disambiguation module will be explored on the following issues: contribution of disambiguation weight to target term weighting, influences of WSD performance on CLIR retrieval effectiveness. In our investigation, we do not use pre-translation or post-translation methods to exclude any mixing effects on CLIR.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Using ontological chain to resolve the translation ambiguity of cross-language information retrieval</title>
	<abstract>In this paper we proposed an ontology-based approach for reducing the ambiguity of bilingual translation. A metric based on the ontological chain for co-occurrence concept evaluation has also been proposed. Based on the ontology chain we can calculate the best translation of all possible translated terms. The experimental results show that the proposed approach can effectively remove the irrelevant results and improve the precision.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Statistical cross-language information retrieval using n-best query translations</title>
	<abstract>This paper presents a novel statistical model for cross-language information retrieval. Given a written query in the source language, documents in the target language are ranked by integrating probabilities computed by two statistical models: a query-translation model, which generates most probable term-by-term translations of the query, and a query-document model, which evaluates the likelihood of each document and translation. Integration of the two scores is performed over the set of N most probable translations of the query. Experimental results with values N=1, 5, 10 are presented on the Italian-English bilingual track data used in the CLEF 2000 and 2001 evaluation campaigns.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Cross-language information retrieval using PARAFAC2</title>
	<abstract>A standard approach to cross-language information retrieval (CLIR) uses Latent Semantic Analysis (LSA) in conjunction with a multilingual parallel aligned corpus. This approach has been shown to be successful in identifying similar documents across languages - or more precisely, retrieving the most similar document in one language to a query in another language. However, the approach has severe drawbacks when applied to a related task, that of clustering documents "language-independently", so that documents about similar topics end up closest to one another in the semantic space regardless of their language. The problem is that documents are generally more similar to other documents in the same language than they are to documents in a different language, but on the same topic. As a result, when using multilingual LSA, documents will in practice cluster by language, not by topic.

We propose a novel application of PARAFAC2 (which is a variant of PARAFAC, a multi-way generalization of the singular value decomposition [SVD]) to overcome this problem. Instead of forming a single multilingual term-by-document matrix which, under LSA, is subjected to SVD, we form an irregular three-way array, each slice of which is a separate term-by-document matrix for a single language in the parallel corpus. The goal is to compute an SVD for each language such that V (the matrix of right singular vectors) is the same across all languages. Effectively, PARAFAC2 imposes the constraint, not present in standard LSA, that the "concepts" in all documents in the parallel corpus are the same regardless of language. Intuitively, this constraint makes sense, since the whole purpose of using a parallel corpus is that exactly the same concepts are expressed in the translations.

We tested this approach by comparing the performance of PARAFAC2 with standard LSA in solving a particular CLIR problem. From our results, we conclude that PARAFAC2 offers a very promising alternative to LSA not only for multilingual document clustering, but also for solving other problems in cross-language information retrieval.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>
    <item>
    <title>Term disambiguation techniques based on target document collection for cross-language information retrieval: an empirical comparison of performance between techniques</title>
	<abstract>Dictionary-based query translation for cross-language information retrieval often yields various translation candidates having different meanings for a source term in the query. This paper examines methods for solving the ambiguity of translations based on only the target document collections. First, we discuss two kinds of disambiguation technique: (1) one is a method using term co-occurrence statistics in the collection, and (2) a technique based on pseudo-relevance feedback. Next, these techniques are empirically compared using the CLEF 2003 test collection for German to Italian bilingual searches, which are executed by using English language as a pivot. The experiments showed that a variation of term co-occurrence based techniques, in which the best sequence algorithm for selecting translations is used with the Cosine coefficient, is dominant, and that the PRF method shows comparable high search performance, although statistical tests did not sufficiently support these conclusions. Furthermore, we repeat the same experiments for the case of French to Italian (pivot) and English to Italian (non-pivot) searches on the same CLEF 2003 test collection in order to verity our findings. Again, similar results were observed except that the Dice coefficient outperforms slightly the Cosine coefficient in the case of disambiguation based on term co-occurrence for English to Italian searches.</abstract>
	<search_task_number>5</search_task_number>
	<query>cross-language information retrieval</query>
	<relevance>1</relevance>
  </item>



  <item>
    <title>Online multi-label active annotation: towards large-scale content-based video search</title>
	<abstract>Existing video search engines have not taken the advantages of video content analysis and semantic understanding. Video search in academia uses semantic annotation to approach content-based indexing. We argue this is a promising direction to enable real content-based video search. However, due to the complexity of both video data and semantic concepts, existing techniques on automatic video annotation are still not able to handle large-scale video set and large-scale concept set, in terms of both annotation accuracy and computation cost. To address this problem, in this paper, we propose a scalable framework for annotation-based video search, as well as a novel approach to enable large-scale semantic concept annotation, that is, online multi-label active learning. This framework is scalable to both the video sample dimension and concept label dimension. Large-scale unlabeled video samples are assumed to arrive consecutively in batches with an initial pre-labeled training set, based on which a preliminary multi-label classifier is built. For each arrived batch, a multi-label active learning engine is applied, which automatically selects and manually annotates a set of unlabeled sample-label pairs. And then an online learner updates the original classifier by taking the newly labeled sample-label pairs into consideration. This process repeats until all data are arrived. During the process, new , even without any pre-labeled training samples, can be incorporated into the process anytime. Experiments on TRECVID dataset demonstrate the effectiveness and efficiency of the proposed framework.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An annotation method and application for video contents based on a semantic graph</title>
	<abstract>The progress of digitization at broadcasting stations enables the intellectual production of high-quality contents from video materials. Metadata plays an important role in describing the semantics of multimedia contents by aiding in the semantic structuralization of video material. In this paper, we propose an annotation method for video contents based on the concept of a semantic graph, which allows semantic queries on video contents. This method incorporates three ideas: an annotation for metadata on video scenes; time-series links between video scenes; and semantic links to external data. In the proposed annotation method, metadata are provided as human annotations practical for TV productions. We also present the application of a semantic graph to real baseball game contents and evaluate this application to show the usefulness of the proposed annotation method.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Hidden Markov models for automatic annotation and content-based retrieval of images and video</title>
	<abstract>This paper introduces a novel method for automatic annotation of images with keywords from a generic vocabulary of concepts or objects for the purpose of content-based image retrieval. An image, represented as sequence of feature-vectors characterizing low-level visual features such as color, texture or oriented-edges, is modeled as having been stochastically generated by a hidden Markov model, whose states represent concepts. The parameters of the model are estimated from a set of manually annotated (training) images. Each image in a large test collection is then automatically annotated with the a posteriori probability of concepts present in it. This annotation supports content-based search of the image-collection via keywords. Various aspects of model parameterization, parameter estimation, and image annotation are discussed. Empirical retrieval results are presented on two image-collections | COREL and key-frames from TRECVID. Comparisons are made with two other recently developed techniques on the same datasets.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Video Annotation for Content-based Retrieval using Human Behavior Analysis and Domain Knowledge</title>
	<abstract>This paper proposes an automatic annotation method of sports video for content-based retrieval. Conventional methods using position information of objects such as locus, relative positions, their transitions, etc. as indices, have drawbacks that tracking errors of a certain object due to occlusions cause recognition failures, and that representation by position information essentially has limited number of recognizable events in the retrieval. Our approach incorporates human behavior analysis and specific domain knowledge with conventional methods, to develop integrated reasoning module for richer expressiveness of events and robust recognition.Based on the proposed method, we implemented content-based retrieval system which can identify several actions on real tennis video. We select court and net lines, players' positions, ball positions, and players' actions, as indices. Court and net lines are extracted using court model and hough transforms. Players and ball positions are tracked by adaptive template matching and particular predictions against sudden changes of motion direction. Players' actions are analyzed by 2-d appearance based matching using the transition of players' silhouettes and hidden markov model. The results using two sets of tennis video is presented demonstrating the performance and the validity of our approach.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Knowledge-Supported Segmentation and Semantic Contents Extraction from MPEG Videos for Highlight-Based Annotation, Indexing and Retrieval</title>
	<abstract>Automatic recognition of highlights from videos is a fundamental and challenging problem for content-based indexing and retrieval applications. In this paper, we propose techniques to solve this problem by using knowledge supported extraction of semantic contents, and compressed-domain processing is employed for efficiency. Firstly, video shots are detected by using knowledge-supported rules. Then, human objects are detected via statistical skin detection. Meanwhile, camera motion like zoom in is identified. Finally, highlights of zooming in human objects are extracted and used for annotation, indexing and retrieval of the whole videos. Results from large data of test videos have demonstrated the accuracy and robustness of the proposed techniques.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Video-Based sign language content annotation by incorporation of MPEG-7 standard</title>
	<abstract>The advanced progress in multimedia technology increases the demand on delivering effective content in term of quality with the ability to describe content. From the W3C initiative into the web accessibility (WAI), there is a dedicated effort to make data accessible by every person even by people with disabilities. Accordingly, this paper balances the portion between the minimum bandwidth and the optimum required data to display customized video-based sign language. It also describes a systematic approach derived from the MPEG-7 multimedia content description standard to annotate sign language information. A new approach is proposed by this paper. It makes use of an “intermediary” signage object rather than immediate transmission of sign language video clips. Based on the signage object, this research analyses the components in order to enhance the display quality for video-based sign language with less data consumption by determining the accurate display parameters.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Graph aggregation based image modeling and indexing for video annotation</title>
	<abstract>With the rapid growth of video multimedia databases and the lack of textual descriptions for many of them, video annotation became a highly desired task. Conventional systems try to annotate a video query by simply finding its most similar videos in the database. Although the video annotation problem has been tackled in the last decade, no attention has been paid to the problem of assembling video keyframes in a sensed way to provide an answer of the given video query when no single candidate video turns out to be similar to the query. In this paper, we introduce a graph based image modeling and indexing system for video annotation. Our system is able to improve the video annotation task by assembling a set of graphs representing different keyframes of different videos, to compose the video query. The experimental results demonstrate the effectiveness of our system to annotate videos that are not possibly annotated by classical approaches.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Semantic Web for Content Based Video Retrieval</title>
	<abstract>This paper aims to provide a semantic web based video search engine. Currently, we do not have scalable integration platforms to represent extracted features from videos, so that they could be indexed and searched. The task of indexing extracted features from videos is a difficult challenge, due to the diverse nature of the features and the temporal dimensions of videos. We present a semantic web based framework for automatic feature extraction, storage, indexing and retrieval of videos. Videos are represented as interconnected set of semantic resources. Also, we suggest a new ranking algorithm for finding related resources which could be used in a semantic web based search engine.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Extracting Motion Annotations from MPEG-2 Compressed Video for HDTV Content Management Applications</title>
	<abstract>Our research concentrates on developing a novel HDTV content management system that enables end users to search, retrieve, and browse archived standard definition (SD) and high definition (HD) television material for program production and content repurposing in a digital television studio. We have developed the first system that automatically analyzes motion occurring in MPEG-2 coded SD and HD videos within the compressed domain itself, and produces descriptors characterizing the global visual motion in videos, for content-based search and retrieval applications. In this paper, we describe our robust and efficient scheme for automatically generating a flow characterization of a video bitstream without decompression, which is a frame-type-independent uniform motion representation amenable for consistent interpretation and computed from the raw motion vectors encoded in the MPEG-2 bitstreams. We propose novel techniques to handle all the different prediction schemes that are employed with different frame types and picture structures of MPEG-2 during the motion compensation process to deal with interlaced and progressive modes. Experiments with thousands of frames from SD and HD video streams demonstrate the accuracy of our flow estimation process and the effectiveness of utilizing flow vectors for annotation of perceived global motion in video streams.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Film Video Modeling</title>
	<abstract>In this paper we present our concept and design of a general film model that represents structural, semantic, and syntactic elements of film. The purpose of this model is to serve as a basis for film video annotation and subsequent retrieval. It is informed by contemporary film theory and analysis to adequately represent the film content and its attributes. The implementation of such a model enables users to access a much vaster array of multimodal film content than is presently possible.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Automatic Classification of Tennis Video for High-level Content-based Retrieval</title>
	<abstract>This paper presents our techniques and results on automatic analysis of tennis video to facilitate content-based retrieval. Our approach is based on the generation of an image model for the tennis court-lines. We derive this model by using the knowledge about dimensions and connectivity (form) of a tennis court and typical camera geometry used when capturing a tennis video. We use this model to develop (i) a court line detection algorithm and (ii) a robust player-tracking algorithm to track the tennis players over the images sequence. We also present a color-based algorithm to select tennis court clips from an input raw footage of tennis video. Automatically extracted tennis court lines and the players' location information are analyzed in a high-level reasoning module and related to useful high-level tennis play events. Results on real tennis video data are presented demonstrating the validity and performance of the approach.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Active learning in multimedia annotation and retrieval: A survey</title>
	<abstract>Active learning is a machine learning technique that selects the most informative samples for labeling and uses them as training data. It has been widely explored in multimedia research community for its capability of reducing human annotation effort. In this article, we provide a survey on the efforts of leveraging active learning in multimedia annotation and retrieval. We mainly focus on two application domains: image/video annotation and content-based image retrieval. We first briefly introduce the principle of active learning and then we analyze the sample selection criteria. We categorize the existing sample selection strategies used in multimedia annotation and retrieval into five criteria: risk reduction, uncertainty, diversity, density and relevance. We then introduce several classification models used in active learning-based multimedia annotation and retrieval, including semi-supervised learning, multilabel learning and multiple instance learning. We also provide a discussion on several future trends in this research direction. In particular, we discuss cost analysis of human annotation and large-scale interactive multimedia annotation.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Optimizing multi-graph learning: towards a unified video annotation scheme</title>
	<abstract>Learning based semantic video annotation is a promising approach for enabling content-based video search. However, severe difficulties, such as insufficiency of training data and curse of dimensionality, are frequently encountered. This paper proposes a novel unified scheme, Optimized Multi-Graph-based Semi-Supervised Learning (OMG-SSL), to simultaneously attack these difficulties. Instead of only using a single graph, OMG-SSL integrates multiple graphs into a regularization and optimization framework to sufficiently explore their complementary nature. We then show that various crucial factors in video annotation, including multiple modalities, multiple distance metrics, and temporal consistency, in fact all correspond to different correlations among samples, and hence they can be represented by different graphs. Therefore, OMG-SSL is able to simultaneously deal with these factors within a unified framework. Experiments on the TRECVID benchmark demonstrate the effectiveness of our proposed approach.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Visual video retrieval system using MPEG-7 descriptors</title>
	<abstract>Video is one of the most popular media these days. However, the volume of the video content grows very fast, e.g. about 24 hours of video content is uploaded every minute to the most famous web site YouTube, and the necessity to search in this content is apparent. Generally, most of the video search systems are based on annotations or use additional text information, which is not always of high quality or lack precision. We present a system that allows user to search videos according to their visual content.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A scalable and extensible segment-event-object-based sports video retrieval system</title>
	<abstract>Sport video data is growing rapidly as a result of the maturing digital technologies that support digital video capture, faster data processing, and large storage. However, (1) semi-automatic content extraction and annotation, (2) scalable indexing model, and (3) effective retrieval and browsing, still pose the most challenging problems for maximizing the usage of large video databases. This article will present the findings from a comprehensive work that proposes a scalable and extensible sports video retrieval system with two major contributions in the area of sports video indexing and retrieval. The first contribution is a new sports video indexing model that utilizes semi-schema-based indexing scheme on top of an Object-Relationship approach. This indexing model is scalable and extensible as it enables gradual index construction which is supported by ongoing development of future content extraction algorithms. The second contribution is a set of novel queries which are based on XQuery to generate dynamic and user-oriented summaries and event structures. The proposed sports video retrieval system has been fully implemented and populated with soccer, tennis, swimming, and diving video. The system has been evaluated against 20 users to demonstrate and confirm its feasibility and benefits. The experimental sports genres were specifically selected to represent the four main categories of sports domain: period-, set-point-, time (race)-, and performance-based sports. Thus, the proposed system should be generic and robust for all types of sports.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>State-of-the-art on spatio-temporal information-based video retrieval</title>
	<abstract>Video retrieval is increasingly based on image content. A number of studies on video retrieval have used low-level pixel content related to statistical moments, shape, colour and texture. However, it is well recognised that such information is not enough for uniquely discriminating across different multimedia content. The use of semantic information, especially which derived from spatio-temporal analysis is of great value in multimedia annotation, archiving and retrieval. In this review paper, we detail how the use of spatiotemporal semantic knowledge is changing the way in which modern research the conducted. In this paper we review a number of studies and concepts related to such analysis, and draw important conclusions on where future research is headed.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Content redundancy in YouTube and its application to video tagging</title>
	<abstract>The emergence of large-scale social Web communities has enabled users to share online vast amounts of multimedia content. An analysis of YouTube reveals a high amount of redundancy, in the form of videos with overlapping or duplicated content. We use robust content-based video analysis techniques to detect overlapping sequences between videos. Based on the output of these techniques, we present an in-depth study of duplication and content overlap in YouTube, and analyze various dependencies between content overlap and meta data such as video titles, views, video ratings, and tags. As an application, we show that content-based links provide useful information for generating new tag assignments. We propose different tag propagation methods for automatically obtaining richer video annotations. Experiments on video clustering and classification as well as a user evaluation demonstrate the viability of our approach.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>CLOVIS: towards precision-oriented text-based video retrieval through the unification of automatically-extracted concepts and relations of the visual and audio/speech contents</title>
	<abstract>Traditional multimedia (video) retrieval systems use the keyword-based approach in order to make the search process fast although this approach has several shortcomings and limitations related to the way the user is able to formulate her/his information need. Typical Web multimedia retrieval systems illustrate this paradigm in the sense that the result of a search consists of a collection of thousands of multimedia documents, many of which would be irrelevant or not fully exploited by the typical user. Indeed, according to studies related to users' behavior, an individual is mostly interested in the initial documents returned during a search session and therefore a multimedia retrieval system is to model the multimedia content as precisely as possible to allow for the first retrieved images to be fully relevant to the user's information need. For this, the keyword-based approach proves to be clearly insufficient and the need for a high-level index and query language, addressing the issue of combining modalities within expressive frameworks for video indexing and retrieval is of huge importance and the only solution for achieving significant retrieval performance. This paper presents a multi-facetted conceptual framework integrating multiple characterizations of the visual and audio contents for automatic video retrieval. It relies on an expressive representation formalism handling high-level video descriptions and a full-text query framework in an attempt to operate video indexing and retrieval beyond trivial low-level processes, keyword-annotation frameworks and state-of-the art architectures loosely-coupling visual and audio descriptions. Experiments on the multimedia topic search task of the TRECVID evaluation campaign validate our proposal.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Towards effective video annotation: An approach to automatically link notes with video content</title>
	<abstract>The characteristics of annotations, such as highlighting, context-based notes, and organization are difficult to translate from the traditional paper-based medium to the digital format. An added challenge is how to facilitate annotations on a digital video in a collaborative distance learning environment. To explore issues in video annotation, we developed a tool called Interactive Shared Education Environment (ISEE). ISEE automatically generates hyperlinked timestamps, which we called Smartlinks, to associate the notes with their video contents. A usability study with 59 participants, following up by a small-scale eye-tracking study, was conducted to explore users' video note-taking behaviors and to examine the effect of the new Smartlink design. Our results showed that participants with Smartlink took fewer notes, focused less on video controls and more on video content than those without Smartlink. We believe the main benefit of Smartlink is that it may offload non-learning related cognitive loads and allow users to take better notes. Findings from this study on users' video annotation behaviors shed light on the future design of video annotation systems in both individual and collaborative environments.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>VFerret: content-based similarity search tool for continuous archived video</title>
	<abstract>This paper describes VFerret, a content-based similarity search tool for continuous archived video. Instead of depending on attributes or annotations to search desired data from long-time archived video, our system allows users to perform content-based similarity search using visual and audio features, and to combine content-based similarity search with traditional search methods. Our preliminary experience and evaluation shows that content-based similarity search is easy to use and can achieve 0.79 average precision on our simple benchmark. The system is constructed using Ferret toolkit and its memory footprint for metadata is quite small, requiring about 1.4Gbytes for one year of continuous archived video data.</abstract>
	<search_task_number>3</search_task_number>
	<query>Content based video annotation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Experiential meeting system</title>
	<abstract>We are developing experiential meeting systems to allow people to be tele-present in a remote meeting and to be able to review proceedings of a meeting or of several meetings using all the data recorded in a meeting. We consider this as a problem in management and experiential access to all multimedia data acquired in a meeting. The data includes video, audio, presentations, text material, databases and websites related to people and the discussions in the meeting, and any other data or information that could be obtained related to the events in the meeting. For experiential access to live and archived meetings, we propose detecting and storing events at three levels, domain, elemental, and data. We address issues in organizing information at domain level and using current signal processing algorithms for detecting events at data level. We show that to provide better specifications for processing algorithms for video and audio, it is essential to identify what is expected from them and define the environment and expectations very clearly. We also believe that while data processing algorithms are being developed for automatic detection of events, it may be essential to build tagging environments that will allow rapid semiautomatic tagging of data at all three levels so practical meeting systems could be implemented. Use of tagging environment is not only required to enable development of meeting systems in near future, but for defining environments for automatic detection of events in multifarious data. In this paper we present our ideas and experience with different techniques and outline our future directions.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A survey of semantic image and video annotation tools</title>
	<abstract>The availability of semantically annotated image and video assets constitutes a critical prerequisite for the realisation of intelligent knowledge management services pertaining to realistic user needs. Given the extend of the challenges involved in the automatic extraction of such descriptions, manually created metadata play a significant role, further strengthened by their deployment in training and evaluation tasks related to the automatic extraction of content descriptions. The different views taken by the two main approaches towards semantic content description, namely the Semantic Web and MPEG-7, as well as the traits particular to multimedia content due to the multiplicity of information levels involved, have resulted in a variety of image and video annotation tools, adopting varying description aspects. Aiming to provide a common framework of reference and furthermore to highlight open issues, especially with respect to the coverage and the interoperability of the produced metadata, in this chapter we present an overview of the state of the art in image and video annotation tools.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Visualization in Medicine: Theory, Algorithms, and Applications</title>
	<abstract>Visualization in Medicine is the first book on visualization and its application to problems in medical diagnosis, education, and treatment. The book describes the algorithms, the applications and their validation (how reliable are the results?), and the clinical evaluation of the applications (are the techniques useful?). It discusses visualization techniques from research literature as well as the compromises required to solve practical clinical problems. The book covers image acquisition, image analysis, and interaction techniques designed to explore and analyze the data. The final chapter shows how visualization is used for planning liver surgery, one of the most demanding surgical disciplines. The book is based on several years of the authors' teaching and research experience. Both authors have initiated and lead a variety of interdisciplinary projects involving computer scientists and medical doctors, primarily radiologists and surgeons.A core field of visualization and graphics missing a dedicated book until now.Written by pioneers in the field and illustrated in full color. Covers theory as well as practice</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Vlogging: A survey of videoblogging technology on the web</title>
	<abstract>In recent years, blogging has become an exploding passion among Internet communities. By combining the grassroots blogging with the richness of expression available in video, videoblogs (vlogs for short) will be a powerful new media adjunct to our existing televised news sources. Vlogs have gained much attention worldwide, especially with Google's acquisition of YouTube. This article presents a comprehensive survey of videoblogging (vlogging for short) as a new technological trend. We first summarize the technological challenges for vlogging as four key issues that need to be answered. Along with their respective possibilities, we give a review of the currently available techniques and tools supporting vlogging, and envision emerging technological directions for future vlogging. Several multimedia technologies are introduced to empower vlogging technology with better scalability, interactivity, searchability, and accessability, and to potentially reduce the legal, economic, and moral risks of vlogging applications. We also make an in-depth investigation of various vlog mining topics from a research perspective and present several incentive applications such as user-targeted video advertising and collective intelligence gaming. We believe that vlogging and its applications will bring new opportunities and drives to the research in related fields.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Beyond search: Event-driven summarization for web videos</title>
	<abstract>The explosive growth of Web videos brings out the challenge of how to efficiently browse hundreds or even thousands of videos at a glance. Given an event-driven query, social media Web sites usually return a large number of videos that are diverse and noisy in a ranking list. Exploring such results will be time-consuming and thus degrades user experience. This article presents a novel scheme that is able to summarize the content of video search results by mining and threading �key� shots, such that users can get an overview of main content of these videos at a glance. The proposed framework mainly comprises four stages. First, given an event query, a set of Web videos is collected associated with their ranking order and tags. Second, key-shots are established and ranked based on near-duplicate keyframe detection and they are threaded in a chronological order. Third, we analyze the tags associated with key-shots. Irrelevant tags are filtered out via a representativeness and descriptiveness analysis, whereas the remaining tags are propagated among key-shots by random walk. Finally, summarization is formulated as an optimization framework that compromises relevance of key-shots and user-defined skimming ratio. We provide two types of summarization: video skimming and visual-textual storyboard. We conduct user studies on twenty event queries for over hundred hours of videos crawled from YouTube. The evaluation demonstrates the feasibility and effectiveness of the proposed solution.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Keeping Found Things Found: The Study and Practice of Personal Information Management: The Study and Practice of Personal Information Management</title>
	<abstract>WE ARE ADRIFT IN A SEA OF INFORMATION. We need information to make good decisions, to get things done, to learn, and to gain better mastery of the world around us. But we do not always have good control of our information - not even in the "home waters" of an office or on the hard drive of a computer. Instead, information may be controlling us - keeping us from doing the things we need to do, getting us to waste money and precious time. The growth of available information, plus the technologies for its creation, storage, retrieval, distribution and use, is astonishing and sometimes bewildering. Can there be a similar growth in our understanding for how best to manage information and informational tools?. This book provides a comprehensive overview of personal information management (PIM) as both a study and a practice of the activities people do and need to be doing so that information can work for them in their daily lives. Introductory chapters of Keeping Found Things Found: The Study and Practice of Personal Information Management provide an overview of PIM and a sense for its many facets. The next chapters look more closely at the essential challenges of PIM, including finding, keeping, organizing, maintaining, managing privacy, and managing information flow. The book also contains chapters on search, email, mobile PIM, web-based support, and other technologies relevant to PIM.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A survey of traceability in requirements engineering and model-driven development</title>
	<abstract>Traceability--the ability to follow the life of software artifacts--is a topic of great interest to software developers in general, and to requirements engineers and model-driven developers in particular. This article aims to bring those stakeholders together by providing an overview of the current state of traceability research and practice in both areas. As part of an extensive literature survey, we identify commonalities and differences in these areas and uncover several unresolved challenges which affect both domains. A good common foundation for further advances regarding these challenges appears to be a combination of the formal basis and the automated recording opportunities of MDD on the one hand, and the more holistic view of traceability in the requirements engineering domain on the other hand.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Smart meeting systems: A survey of state-of-the-art and open issues</title>
	<abstract>Smart meeting systems, which record meetings and analyze the generated audio--visual content for future viewing, have been a topic of great interest in recent years. A successful smart meeting system relies on various technologies, ranging from devices and algorithms to architecture. This article presents a condensed survey of existing research and technologies, including smart meeting system architecture, meeting capture, meeting recognition, semantic processing, and evaluation methods. It aims at providing an overview of underlying technologies to help understand the key design issues of such systems. This article also describes various open issues as possible ways to extend the capabilities of current smart meeting systems.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Experiences of semantic tagging with Tilkut</title>
	<abstract>There are two main approaches for adding metadata to web content: free user-generated tags and keywords based on taxonomies or semantic ontologies. There are pros and cons in both approaches, so benefits may be gained by combining these approaches. This paper presents a solution that utilizes both freely defined tags and predefined ontologies. To study suggestionbased semantic tagging we created a social bookmarking application called Tilkut. This paper describes the system and experiences from a small scale user study. Suggestions for more intelligent tagging solutions will be given.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>GAT: a Graphical Annotation Tool for semantic regions</title>
	<abstract>This article presents GAT, a Graphical Annotation Tool based on a region-based hierarchical representation of images. The proposed solution uses Partition Trees to navigate through the image segments which are automatically defined at different spatial scales. Moreover, the system focuses on the navigation through ontologies for a semantic annotation of objects and of the parts that compose them. The tool has been designed under usability criteria to minimize the user interaction by trying to predict the future selection of regions and semantic classes. The implementation uses MPEG-7/XML input and output data to allow interoperability with any type of Partition Tree. This tool is publicly available and its source code can be downloaded under a free software license.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Text mining and probabilistic language modeling for online review spam detection</title>
	<abstract>In the era of Web 2.0, huge volumes of consumer reviews are posted to the Internet every day. Manual approaches to detecting and analyzing fake reviews (i.e., spam) are not practical due to the problem of information overload. However, the design and development of automated methods of detecting fake reviews is a challenging research problem. The main reason is that fake reviews are specifically composed to mislead readers, so they may appear the same as legitimate reviews (i.e., ham). As a result, discriminatory features that would enable individual reviews to be classified as spam or ham may not be available. Guided by the design science research methodology, the main contribution of this study is the design and instantiation of novel computational models for detecting fake reviews. In particular, a novel text mining model is developed and integrated into a semantic language model for the detection of untruthful reviews. The models are then evaluated based on a real-world dataset collected from amazon.com. The results of our experiments confirm that the proposed models outperform other well-known baseline models in detecting fake reviews. To the best of our knowledge, the work discussed in this article represents the first successful attempt to apply text mining methods and semantic language models to the detection of fake consumer reviews. A managerial implication of our research is that firms can apply our design artifacts to monitor online consumer reviews to develop effective marketing or product design strategies based on genuine consumer feedback posted to the Internet.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Automatic creation of photo books from stories in social media</title>
	<abstract>Photos are a special way to tell stories of our best memories and moments. The representation of those photos in appealing physical photo books is highly appreciated by many people. Today, many photos are shared via social networking sites, where people upload their photos and share their stories with their friends. The members of social networks comment on each other's photos, add tags or descriptions and upload new photos of the same events to their albums. While the media of different personal events are available on the social network, there is no easy way to collect and bundle them into a story and print this story as a photo book. We propose an approach to automatically detect media elements that match a query (where, when, what, who) in the user's social network and intelligently arrange and compose them into a printable photo book. We combine content analysis of text and images to automatically and semi-automatically select photos of a specific story. We calculate the probabilities of each two photos to belong to the same event using an Expectation-Maximization algorithm that we propose in order to be able to retrieve them easily when receiving the user queries, and we address the differences between our model and other models that use similar proposed algorithms. People's tags and the interaction between the users and the photos as well as other semantic information are exploited to select important photos that are suitable to create the photo book. The selected photos and derived semantics are then employed to automatically create an appealing layout for the photo book.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Parsed use case descriptions as a basis for object-oriented class model generation</title>
	<abstract>Object-oriented analysis and design has become a major approach in the design of software systems. Recent developments in CASE tools provide help in documenting the analysis and design stages and in detecting incompleteness and inconsistency in analysis. However, these tools do not contribute to the initial and difficult stage of the analysis process of identifying the objects/classes, attributes and relationships used to model the problem domain. This paper presents a tool, Class-Gen, which can partially automate the identification of objects/classes from natural language requirement specifications for object identification. Use case descriptions (UCDs) provide the input to Class-Gen which parses and analyzes the text written in English. A parsed use case description (PUCD) is generated which is then used as the basis for the construction of an initial UML class model representing object classes and relationships identified in the requirements. PUCDs enable the extraction of nouns, verbs, adjectives and adverbs from traditional UCDs for the identification process. Finally Class-Gen allows the initial class model to be refined manually. Class-Gen has been evaluated against a collection of unseen requirements. The results of the evaluation are encouraging as they demonstrate the potential for such tools to assist with the software development process.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Building a Usable and Accessible Semantic Web Interaction Platform</title>
	<abstract>Semantic Web applications take off is being slower than expected, at least with respect to "real-world" applications and users. One of the main reasons for this lack of adoption is that most Semantic Web user interfaces are still immature from the usability and accessibility points of view. This is due to the novelty of these technologies, but this also motivates the exploration of alternative interaction paradigms, different from the "traditional" Web or Desktop applications ones. Our proposal is realized in the Rhizomer platform, which explores the possibilities of the object---action interaction paradigm at the Web scale. This paradigm is well suited for heterogeneous resource spaces such as those common in the Semantic Web. Resources, described by metadata, correspond to the objects in the paradigm. Semantic web services, which are dynamically associated to these objects, correspond to the actions. The platform is being put into practice in the context of a research project in order to build an open application for media distribution based on Semantic Web technologies. Moreover, its usability and accessibility have been evaluated in this real setting and compared to similar systems.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>MonuAnno: automatic annotation of georeferenced landmarks images</title>
	<abstract>Uploading tourist photographs is a popular activity on photo sharing platforms. The manual annotation of these images is a tedious process and the users often upload their images with no associated textual information. Automating the annotation process has received a lot of attention but the problem remains a hard one, especially when dealing with large and heterogeneous databases. Here we focus on landmarks images, very frequent among tourism pictures, and propose a new automatic technique for annotating this type of pictures. Our system, called MonuAnno, relies on the joint exploitation of localization information and of image content analysis in an efficient and scalable framework. The annotation is performed using a two steps k Nearest Neighbors (k-NN). First, only neighboring landmarks of a new unlabeled georeferenced image will be considered as potential annotations and the image will be attributed to the landmark that is visually closest. Then, we introduce a verification step that eliminates false positives (images taken near a landmark that represent something else). The technique was tested on Web images and the results show that the precision of the labeling process in MonuAnno exceeds 80%, when annotating around 50% of the images in the test set.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Model-based analysis of flow-mediated dilation and intima-media thickness</title>
	<abstract>We present an end-to-end system for the automatic measurement of flow-mediated dilation (FMD) and intima-media thickness (IMT) for the assessment of the arterial function. The video sequences are acquired from a B-mode echographic scanner. A spline model (deformable template) is fitted to the data to detect the artery boundaries and track them all along the video sequence. The a priori knowledge about the image features and its content is exploited. Preprocessing is performed to improve both the visual quality of video frames for visual inspection and the performance of the segmentation algorithm without affecting the accuracy of the measurements. The system allows real-time processing as well as a high level of interactivity with the user. This is obtained by a graphical user interface (GUI) enabling the cardiologist to supervise the whole process and to eventually reset the contour extraction at any point in time. The system was validated and the accuracy, reproducibility, and repeatability of the measurements were assessed with extensive in vivo experiments. Jointly with the user friendliness, low cost, and robustness, this makes the system suitable for both research and daily clinical use.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Automatic tag expansion using visual similarity for photo sharing websites</title>
	<abstract>In this paper we present an automatic photo tag expansion method designed for photo sharing websites. The purpose of the method is to suggest tags that are relevant to the visual content of a given photo at upload time. Both textual and visual cues are used in the process of tag expansion. When a photo is to be uploaded, the system asks for a couple of initial tags from the user. The initial tags are used to retrieve relevant photos together with their tags. These photos are assumed to be potentially content related to the uploaded target photo. The tag sets of the relevant photos are used to form the candidate tag list, and visual similarities between the target photo and relevant photos are used to give weights to these candidate tags. Tags with the highest weights are suggested to the user. The method is applied on Flickr ( http://www.flickr.com ). Results show that including visual information in the process of photo tagging increases accuracy with respect to text-based methods.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Frankenrigs: building character rigs from multiple sources</title>
	<abstract>We present a new rigging and skinning method which uses a database of partial rigs extracted from a set of source characters. Given a target mesh and a set of joint locations, our system can automatically scan through the database to find the best-fitting body parts, tailor them to match the target mesh, and transfer their skinning information onto the new character. For the cases where our automatic procedure fails, we provide an intuitive set of tools to fix the problems. When used fully automatically, the system can generate results of much higher quality than a standard smooth bind, and with some user interaction, it can create rigs approaching the quality of artist-created manual rigs in a small fraction of the time.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>  
  <item>
    <title>Medical Image Analysis: Progress over Two Decades and the Challenges Ahead</title>
	<abstract>The analysis of medical images has been woven into the fabric of the Pattern Analysis and Machine Intelligence (PAMI) community since the earliest days of these Transactions. Initially, the efforts in this area were seen as applying pattern analysis and computer vision techniques to another interesting dataset. However, over the last two to three decades, the unique nature of the problems presented within this area of study have led to the development of a new discipline in its own right. Examples of these include: the types of image information that are acquired, the fully three-dimensional image data, the nonrigid nature of object motion and deformation, and the statistical variation of both the underlying normal and abnormal ground truth. In this paper, we look at progress in the field over the last 20 years and suggest some of the challenges that remain for the years to come.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Assisted animated production creation and programme generation</title>
	<abstract>The creation of animated productions is a labour intensive process. Whether the end result is a large-budget motion picture, or a small-scale internet production, there is invariably a large amount of time spent in creating the timeline, arranging assets, previewing and editing. This iterative process is necessary in large-scale productions but can become repetitive and frustrating when the end result is a small production that may have similar elements to previous work. We present a workflow system and framework that are able to both greatly facilitate animated programme production and introduce an element of procedural generation. We further present the Programme Editor, an application designed to be a powerful front end for the framework. The principal contribution of this work is the creation of an XML-based scripting engine that can be used to create an animated production. This permits several techniques, tools and workflows to interchange information, allows rapid incorporation of further tools, and furthermore facilitates the complete automatisation of the production process.</abstract>
	<search_task_number>3</search_task_number>
	<query>Automatic or semiautomatic video tagging</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Enhancing search in a geospatial multimedia annotation system</title>
	<abstract>The development of numerous information sharing platforms have led to the emergence of multimedia user-generated content. With the prevalence of networking mobile devices and Global Positioning System (GPS) functionality, these contents could also be tagged with their location (geo-tagged) and visualised on a map through a geospatial information system. This in turn raises new challenges to manage the information retrieval process due to the potentially large amounts of data presented on the map. In this paper, we present a spatial clustering approach to enhance the searching feature of MobiTOP, a geospatial annotation system. The technique, which is a modification of DBSCAN (Density based spatial clustering applications with noise) coupled with our ranking schemes, is compared against other techniques. The evaluation results suggest the viability of our approach, and implications are also discussed.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>3</relevance>
  </item>
  <item>
    <title>Towards the semantic and context-aware management of mobile multimedia</title>
	<abstract>Users of mobile devices can nowadays easily create large quantities of mobile multimedia documents tracing significant events attended, places visited or, simply, moments of their everyday life. However, they face the challenge of organizing these documents in order to facilitate searching through them at a later time and sharing them with other users. We propose using context awareness and semantic technologies in order to improve and facilitate the organization, annotation, retrieval and sharing of personal mobile multimedia documents. Our approach combines metadata extracted and enriched automatically from the users' context with annotations provided manually by the users and with annotations inferred by applying user-defined rules to context features. These new contextual metadata are integrated into the processes of annotation, sharing and keyword-based retrieval.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A hybrid ontology and visual-based retrieval model for cultural heritage multimedia collections</title>
	<abstract>This paper introduces a hybrid multimedia retrieval model that is capable of retrieving cultural heritage multimedia content, based on their semantic annotation with the help of an ontology and on low level visual features with a view to finding similar content. The main novelty is the way in which these techniques cooperate transparently during the evaluation of a single query in a hybrid fashion, making recommendations to the user. A search engine has been developed implementing this model, which is capable of searching through cultural heritage multimedia collections, and indicative examples are discussed, along with insights into its performance.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A web-based system for collaborative annotation of large image and video collections: an evaluation and user study</title>
	<abstract>Annotated collections of images and videos are a necessary basis for the successful development of multimedia retrieval systems. The underlying models of such systems rely heavily on quality and availability of large training collections. The annotation of large collections, however, is a time-consuming and error prone task as it has to be performed by human annotators. In this paper we present the IBM Efficient Video Annotation (EVA) system, a server-based tool for semantic concept annotation of large video and image collections. It is optimised for collaborative annotation and includes features such as workload sharing and support in conducting inter-annotator analysis. We discuss initial results of an ongoing user-evaluation of this system. The results are based on data collected during the 2005 TRECVID Annotation Forum, where more than 100 annotators have been using the system.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Multimedia ontology learning for automatic annotation and video browsing</title>
	<abstract>In this work, we offer an approach to combine standard multimedia analysis techniques with knowledge drawn from conceptual metadata provided by domain experts of a specialized scholarly domain, to learn a domain-specific multimedia ontology from a set of annotated examples. A standard Bayesian network learning algorithm that learns structure and parameters of a Bayesian network is extended to include media observables in the learning. An expert group provides domain knowledge to construct a basic ontology of the domain as well as to annotate a set of training videos. These annotations help derive the associations between high-level semantic concepts of the domain and low-level MPEG-7 based features representing audio-visual content of the videos. We construct a more robust and refined version of this ontology by learning from this set of conceptually annotated videos. To encode this knowledge, we use MOWL, a multimedia extension of Web Ontology Language (OWL) which is capable of describing domain concepts in terms of their media properties and of capturing the inherent uncertainties involved. We use the ontology specified knowledge for recognizing concepts relevant to a video to annotate fresh addition to the video database with relevant concepts in the ontology. These conceptual annotations are used to create hyperlinks in the video collection, to provide an effective video browsing interface to the user.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Automatic annotation of multimedia content by user clickthroughs: enhancing the performance of multimedia search engines</title>
	<abstract>Content-based multimedia retrieval is a very hot research topic, applicable to several domains. Traditional feature vector based retrieval methods cannot provide semantically meaningful results. Additionally manual annotation is both time/money consuming and user-dependent. To address these problems in this paper we present an approach to automatically annotate multimedia files by incorporating clickthrough data of search engines. In particular the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking, are analyzed in order to assign keywords to selected content. A query extension method is also proposed in order to agitate the pool of files and bring content with similar visual features to the surface. This is very important since users typically select only the first files of the ranking by clicking on them. The proposed method is feasible even for large sets of queries and features and theoretical results are verified in a controlled experiment, which shows that the method can effectively annotate multimedia files and significantly enhance the performance of multimedia search engines.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Learning ontology for personalized video retrieval</title>
	<abstract>This paper proposes a new method for using implicit user feedback from clickthrough data to provide personalized ranking of results in a video retrieval system. The annotation based search is complemented with a feature based ranking in our approach. The ranking algorithm uses belief revision in a Bayesian Network, which is derived from a multimedia ontology that captures the probabilistic association of a concept with expected video features. We have developed a content model for videos using discrete feature states to enable Bayesian reasoning and to alleviate on-line feature processing overheads. We propose a reinforcement learning algorithm for the parameters of the Bayesian Network with the implicit feedback obtained from the clickthrough data.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Investigating the usability of a mobile location-based annotation system</title>
	<abstract>We investigate the usability of MobiTOP (Mobile Tagging of Objects and People), a mobile location-based annotation system. MobiTOP allows users to annotate real world locations with both multimedia and textual content and concurrently, share the annotations among its users. Equipped with a new interface as well as additional functionality such as clustering of annotations and advanced search and filtering options, a usability evaluation of MobiTOP was conducted in the context of a travel companion for tourists. The results suggested the potential of the system in terms of functionality for mobile content sharing. Participants agree that the features in MobiTOP are generally usable as a content sharing tool. Implications and future work are also reported in this paper.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Relevance feedback strategies for artistic image collections tagging</title>
	<abstract>This paper provides an analysis on relevance feedback techniques in a multimedia system designed for the interactive exploration and annotation of artistic collections, in particular illuminated manuscripts. The relevance feedback is presented not only as a very effective technique to improve the performance of the system, but also as a clever way to increase the user experience, mixing the interactive surfing through the artistic content with the possibility to gather valuable information from the user, and consequently improving his retrieval satisfaction. We compare a modification of the Mean-Shift Feature Space Warping algorithm, as representative of the standard RF procedures, and a learning-based technique based on transduction, considered in order to overcome some limitation of the previous technique. Experiments are reported regarding the adopted visual features based on covariance matrices.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A usability study of a mobile content sharing system</title>
	<abstract>We investigate the usability of MobiTOP (Mobile Tagging of Objects and People), a mobile location-based content sharing system. MobiTOP allows users to annotate real world locations with both multimedia and textual content and concurrently, share the annotations among its users. In addition, MobiTOP provides additional functionality such as clustering of annotations and advanced search and filtering options. A usability evaluation of the system was conducted in the context of a travel companion for tourists. The results suggested the potential of the system in terms of functionality for mobile content sharing. Participants agreed that the features in MobiTOP were generally usable as a content sharing tool. Implications and future work are also reported in this paper.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Image retrieval based on a multipurpose watermarking scheme</title>
	<abstract>The rapid development of Internet and multimedia technologies has made copyright protection and multimedia retrieval be the two most important issues in the digital world. To solve these problems simultaneously, this paper presents a multipurpose watermarking scheme for image retrieval. First, several important features are computed offline for each image in the database. Then, the copyright, annotation and feature watermarks are offline embedded into all images in the database. During the online retrieval, the query image features are compared with the exacted features from each image in the database to find the similar images. The experimental results based on a database with 1000 images in 10 classes demonstrate the effectiveness of the proposed scheme.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Surfing on artistic documents with visually assisted tagging</title>
	<abstract>This paper describes a complete architecture for the interactive exploration and annotation of artistic collections. In particular the focus is on Renaissance illuminated manuscripts, which typically contain thousands of pictures, used to comment or embellish the manuscript Gothic text. The final aim is to create a human centered multimedia application allowing the non practitioners to enjoy these masterpieces and expert users to share their knowledge. The system is composed by a modern user interface for browsing, surfing and querying, an automatic segmentation module, to ease the initial picture extraction task, and a similarity based retrieval engine, used to provide visually assisted tagging capabilities. A relevance feedback procedure is included to further refine the results. Experiments are reported regarding the adopted visual features based on covariance matrices and the Mean Shift Feature Space Warping relevance feedback. Finally some hints on the user interface for museum installations are discussed.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>How do you feel about "dancing queen"?: deriving mood And theme annotations from user tags</title>
	<abstract>Web 2.0 enables information sharing, collaboration among users and most notably supports active participation and creativity of the users. As a result, a huge amount of manually created metadata describing all kinds of resources is now available. Such semantically rich user generated annotations are especially valuable for digital libraries covering multimedia resources such as music, where these metadata enable retrieval relying not only on content-based (low level) features, but also on the textual descriptions represented by tags. However, if we analyze the annotations users generate for music tracks, we find them heavily biased towards genre. Previous work investigating the types of user provided annotations for music tracks showed that the types of tags which would be really beneficial for supporting retrieval - usage (theme) and opinion (mood) tags - are often neglected by users in the annotation rocess. In this paper we address exactly this problem: in order to support users in tagging and to fill these gaps in the tag space, we develop algorithms for recommending mood and theme annotations. Our methods exploit the available user annotations, the lyrics of music tracks, as well as combinations of both. We also compare the results for our recommended mood / theme annotations against genre and style recommendations - a much easier and already studied task. Besides evaluating against an expert (AllMusic.com) ground truth, we evaluate the quality of our recommended tags through a Facebook-based user study. Our results are very promising both in comparison to experts as well as users and provide interesting insights into possible extensions for music tagging systems to support music search.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Personalized video adaptation based on video content analysis</title>
	<abstract>Personalized video adaptation is expected to satisfy individual users' needs on video content. Multimedia data mining plays a significant role of video annotation to meet users' preference on video content. In this paper, a comprehensive solution for personalized video adaptation is proposed based on video content mining. Video content mining targets both cognitive content and affective content. Cognitive content refers to those semantic events, which are very specific for the video domains. Sometimes, users might prefer "emotional decision" to select their interested video content. Therefore, we introduce affective content which causes audiences' strong reactions.
For cognitive content mining, features are extracted from multiple modalities. Machine learning module is further performed to get some middle-level features, such as specific audio sounds, semantic video shots and so on. Those middle-level features are used to detect cognitive content by using Hidden Markov Models. For affective content mining, affective content is detected with three affective levels: "low", "medium" and "high". Considering affective levels might have no sharp boundaries, fuzzy c mean clustering is used on low-level features to simulate user's perceptions.
The adaptation is later implemented based on MPEG-21 Digital Item Adaptation framework. One of the challenges is how to quantify users' preference on video content. Information Entropy (IE) and Membership Functions are calculated to decide priorities for resource allocation for cognitive content and affective content respectively.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Content-based mood classification for photos and music: a generic multi-modal classification framework and evaluation approach</title>
	<abstract></abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Timesheets.js: when SMIL meets HTML5 and CSS3</title>
	<abstract>In this paper, we explore different ways to publish multimedia documents on the web. We propose a solution that takes advantage of the new multimedia features of web standards, namely HTML5 and CSS3. While JavaScript is fine for handling timing, synchronization and user interaction in specific multimedia pages, we advocate a more generic, document-oriented alternative relying primarily on declarative standards: HTML5 and CSS3 complemented by SMIL Timesheets. This approach is made possible by a Timesheets scheduler that runs in the browser. Various applications based on this solution illustrate the paper, ranging from media annotations to web documentaries.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Concept based interactive retrieval for social environment</title>
	<abstract>Following the recent developments in social networking, there is an emerging interest to share experiences online with social peers through multimedia data. Consequently, exponential amount of multimedia information has been generated by everyday users and shared among social peers. As opposed to conventional digital archives, the user generated content archive does not confine to one particular domain and therefore semantic indexing of the content requires the creation of large number of training samples for each semantic query concept. Addressing this problem, we present an interactive multi-concept based browsing and retrieval framework using which users can construct high-level semantic queries based on mid-level primitive features. The proposed framework integrates innovative visualisation methodology developed for browsing, navigating and retrieving information from multimedia database. The framework is user centric and supports interactive formulation of high-level semantic queries for content retrieval using available content annotation. The performance of the proposed framework is evaluated using annotation based on automatic algorithms against Support Vector Machines, Multi-feature classification and particle swarm optimisation based relevance feedback techniques.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Browsing personal digital photograph collections with spatial and temporal based ontology and MPEG-7 dozen dimensional digital content architecture</title>
	<abstract>The current trend of image retrieval is to incorporate the image visual features used in Content Based Image Retrieval (CBIR) and semantics annotations used in Metadata Based Image Retrieval to enhance retrieval performance. Because of the pervasive of consumer imaging devices, building personal digital photograph libraries became an increasingly interested domain. Personal digital photograph collections have specific characteristics compare to general purpose image databases. Hence, annotation architecture specially designed for that plays an important role in building an interoperatable data repository for future indexing, browsing and retrieving purposes. We propose a MPEG- 7 based multimedia content description architecture, Dozen Dimensional Digital Content (DDDC), which annotates multimedia data with twelve main attributes regarding its semantic representation. In addition, we also proposed a machine-understandable "Spatial and Temporal Based Ontology" representation for the above DDDC semantics description to enable semi-automatic annotation process.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A video retrieval system for electrical safety education based on a mobile agent</title>
	<abstract>Recently, retrieval of various video data has become an important issue as more and more multimedia content services are being provided. To effectively deal with video data, a semantic-based retrieval scheme that allows for processing diverse user queries and saving them on the database is required. In this regard, this paper proposes a semantic-based video retrieval system that allows the user to search diverse meanings of video data for electrical safety-related educational purposes by means of automatic annotation processing. If the user inputs a keyword to search video data for electrical safety-related educational purposes, the mobile agent of the proposed system extracts the features of the video data that are afterwards learned in a continuous manner, and detailed information on electrical safety education is saved on the database. The proposed system is designed to enhance video data retrieval efficiency for electrical safety-related educational purposes.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Shiatsu: semantic-based hierarchical automatic tagging of videos by segmentation using cuts</title>
	<abstract>The recent dramatic widespread of multimedia content over the Internet and other media channels (such as television or mobile phone platforms) points the interest of media broadcasters to the topics of video retrieval and content browsing. Semantic indexing based on content is a good starting point for an effective retrieval system, since it allows an intuitive categorization of videos. However, the annotation process is usually done manually, leading to ambiguity, lack of information, and translation problems. In this paper, we propose SHIATSU, a novel technique for automatic video tagging which is based on shot boundaries detection and hierarchical annotation processes. Our shot detection module uses a simple yet efficient algorithm based on HSV histograms and edge features. The tagging module assigns semantic concepts to both shot sequences and whole videos, by exploiting visual features extracted from key frames. We present preliminary results of our technique on the Mammie platform video set by proving its effectiveness in real scenarios.</abstract>
	<search_task_number>3</search_task_number>
	<query>feature based Multimedia annotation</query>
	<relevance>1</relevance>
  </item>

  <item>
    <title>Home automation in the wild: challenges and opportunities</title>
	<abstract>Visions of smart homes have long caught the attention of researchers and considerable effort has been put toward enabling home automation. However, these technologies have not been widely adopted despite being available for over three decades. To gain insight into this state of affairs, we conducted semi-structured home visits to 14 households with home automation. The long term experience, both positive and negative, of the households we interviewed illustrates four barriers that need to be addressed before home automation becomes amenable to broader adoption. These barriers are high cost of ownership, inflexibility, poor manageability, and difficulty achieving security. Our findings also provide several directions for further research, which include eliminating the need for structural changes for installing home automation, providing users with simple security primitives that they can confidently configure, and enabling composition of home devices.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>User-configurable semantic home automation</title>
	<abstract>The ideas of smart home and home automation have been proposed for many years. However, when discussing homes of the future, related studies have usually focused on deploying various smart appliances (or devices) within a home environment and employing those appliances automatically by pre-defined procedures. The difficulties of supporting user-configurable automation are due to the complexity of various dynamic home environments. Moreover, within their home domains, users usually think semantically; for example, ''I want to turn off all the lights on the second floor''. This paper proposes a semantic home automation system, USHAS (User-configurable Semantic Home Automation System), which adopts Web Service and WSBPEL for executing automated process; OWL and OWL-S for defining home environments and service ontology; and a self-defined markup language, SHPL (Semantic Home Process Language), for describing semantic processes. </abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>BlogAlpha: home automation robot using ontology in home environment</title>
	<abstract>BlogAlpha is a home automation system that uses a home automation robot ApriAlpha to integrate the legacy appliances in a home. It provides a blog interface to receive the user's request remotely in a natural language and show the state of the home. A robot works as intelligent glue that connects and automates legacy appliances, allowing users to introduce an intelligent home environment in their home at much lower cost. BlogAlpha uses the ontologies about the commodities in a home, the locations where these are placed and the tasks the robot can achieve. By using these ontologies, the robot can achieve the proper action to respond to a wide variety of user requests.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Prototyping sensor-actuator networks for home automation</title>
	<abstract>Integrating actuators into sensor networks is often considered to be the next logical step in the evolution of wireless sensor networks. However, few practical examples of such sensor and actuator networks have been demonstrated so far. In this paper, we present a prototype system that supports the easy prototyping of such applications in the area of home automation. We demonstrate the utility of this system with a simple light control application built on top of it. We also report first experiences and insights gained with the help of real-world experiments.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The integration of home-automation and IPTV system and services</title>
	<abstract>In this paper we propose the integration of a home-automation system and its services (HASS) with an IPTV system and its services (IPTVSS) to enable convergence at the network-technology and user-interface levels. The convergence at the network-technology level was achieved by using the Internet protocol (IP), and therefore both integrated systems have to provide IP connectivity. To enable communication between the HASS and the IP-based services, we have implemented the WebService/Konnex gateway. The IPTVSS are IP based; therefore, an easy convergence at the network-technology level is possible. The convergence at the user-interface level was achieved by a hardware and software user-interface implementation. </abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>A framework for developing home automation systems: From requirements to code</title>
	<abstract>This article presents an integrated framework for the development of home automation systems following the model-driven approach. By executing model transformations the environment allows developers to generate executable code for specific platforms. The tools presented in this work help developers to model home automation systems by means of a domain specific language which is later transformed into code for home automation specific platforms. These transformations have been defined by means of graph grammars and template engines extended with traceability capabilities. Our framework also allows the models to be reused for different applications since a catalogue of requirements is provided. This framework enables the development of home automation applications with techniques for improving the quality of both the process and the models obtained. In order to evaluate the benefits of the approach, we conducted a survey among developers that used the framework. The analysis of the outcome of this survey shows which conditions should be fulfilled in order to increase reusability.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Using code of colors through ICT and home automation technologies in the housing environment context for persons with loss of autonomy</title>
	<abstract>The appropriation process of the environmental space parameters by elders can be done through the use of color codes that are meaningful to the persons with loss of autonomy. This approach is based upon the natural interpretation of communicating signs and signals by the brain that helps to answer more quickly to critical situation at different degrees. We illustrate first with the transposition of the color codes used in the automobile domain to the housing environment. By extension, we show that intelligent furniture can be designed through ICT and home automation to assist persons with cognitive troubles or motor disability. Finally, we describe the application done in a dare care center for persons with cognitive troubles. Colors of the rooms have been chosen to allow the patients to naturally identify the role of each room.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Smart Home Automation with Linux</title>
	<abstract>Linux users can now control their homes remotely! Are you a Linux user who has ever wanted to turn on the lights in your house, or open and close the curtains, while away on holiday? Want to be able to play the same music in every room, controlled from your laptop or mobile phone? Do you want to do these things without an expensive off-the-shelf kit? In Beginning Linux Home Automation, Steven Goodwin will show you how a house can be fully controlled by its occupants, all using open source software. From appliances to kettles to curtains, control your home remotely! What youll learn Control appliances like kettles and curtains both locally and remotely. Find and harness data sources to provide context-aware living. Hack/change existing hardware/software to better fit your needs. Integrate various technologies into a function greater than the whole. Set up a home network, for both network and audio/video traffic. Who is this book for? This book is for amateur and professional Linux users who want to control their homes and their gadgets!</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Sabbath day home automation: "it's like mixing technology and religion"</title>
	<abstract>We present a qualitative study of 20 American Orthodox Jewish families' use of home automation for religious purposes. These lead users offer insight into real-life, long-term experience with home automation technologies. We discuss how automation was seen by participants to contribute to spiritual experience and how participants oriented to the use of automation as a religious custom. We also discuss the relationship of home automation to family life. We draw design implications for the broader population, including surrender of control as a design resource, home technologies that support long-term goals and lifestyle choices, and respite from technology.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A MOM-based home automation platform</title>
	<abstract>While there have been many home networking technologies such as UPnP and INSTEON, appliances supporting different home networking technologies cannot collaborate to finish Home Automation (HA). Although many studies of interoperability among home networking technologies have been done, researches on further HA in heterogeneous environments are still lacking. This paper proposes the MOM-based Home Automation Platform (MHAP), which accomplishes event-driven HA in incompatible home networks.MHAPis independent of any home networking technology and integrates home networking technologies in the home gateway. For users, MHAP provides the easy-to-use and standardized way to configure complex HA scenarios by rules. Through introducing Message Oriented Middleware (MOM) and Open Service Gateway Initiative (OSGi), MHAP offers reliable automatic operations, fault tolerant and re-configurable HA, high extensibility and large scalable collaboration among appliances, other MHAP gateways and Internet services.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Scriptable sensor network based home-automation</title>
	<abstract>Today, proprietary home automation targets very specific applications which operate mostly on a cable based infrastructure. In contrast to that, our implementation builds a wireless ad-hoc multi-hop network based on the ESB sensor node platform from the FU-Berlin.

The nodes gather sensor readings in a home and transmit them to a central automation server. There, the readings are matched against a list of script statements. In case of a match, a specific action is performed. In this work we will show how the user can implement complex home automation applications optimized for his specific needs by defining very simple script statements. An important property of the system is also that the control of all home appliances is done by means of IR communication and Ethernet enabled multiple plugs. This way, the cooperation between manufacturers is no necessity in order to connect devices to the home automation network.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Maisons Vill'Âge®: Smart use of home automation for healthy aging</title>
	<abstract>As part of a PhD thesis at the Loria research center, National Polytechnic Institute of Lorraine (INPL), "Maisons Vill'âge®" a new concept of building smart home by integrating telehealthcare and home automation systems, is developed in France. The segment of population, we are targeting is the senior citizens. The healthcare system uses home automation sensors and other environmental sensors like bed and chair sensors to monitor the activity level of the elderly. Activity patterns are analyzed by an intelligent application which is based on Fuzzy Logic to find any unusual behavior. The system is also equipped with wireless medical sensors to monitor the health situation of the elderly; it uses also a wireless sensor network to detect falls. The system detects health abnormalities at an early stage through the frequent monitoring of physiological data. This paper presents a brief description of the system.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Zombee: a home automation prototype for retrofitted environments</title>
	<abstract>As home automation systems become more and more present, we propose a new solution by addressing some of the shortcomings of current solutions. We are developing an open system specially built for existing environments which need flexible automation functionalities. Our solution has the advantage of seamless integration in the existing home structure without important changes such as wiring when installing our system. The system is easy to operate and offers great flexibility in regular monitoring and controlling home automation tasks. Zombee brings a new approach by supporting the user to create complex automation programs, which are orchestrating device interactions tailored to the user's needs. These programs are run in background without any assistance from the user. In this manner, the user sets its own use-cases and can change the behavior of the system programmatically.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Multiagent system for home automation</title>
	<abstract>Smart-home conception has emerged in recent years and played a very important part in the formation of future houses. Since the beginning of the smart-home era, home automation benefits have never overshadowed the cost of such systems. One of these costs is that there is always the need for home inhabitants to program the system to perform daily tasks. In this paper we present prototype of a system that overcomes this problem by giving the home enough intelligence to adapt to its inhabitants life style without the need for the inhabitants to exercise authority. The system makes use of multi-agent and prediction techniques to provide intelligent smart-home appliances automation. Results indicate that the technique applied in this research not only makes the system very fast and accurate but also makes it portable and cost effective.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The architecture to implement the home automation systems with LabVIEWTM</title>
	<abstract>It becomes a topic that constructional environment could be controlled at any place and anytime in the ubiquitous era. It demands much expenses and technology to control everything. Therefore, we presented construct environment that could control home from outside using inexpensive USB web camera, we suggested that we could control it quickly by PDA. Proposed program could be used to many applications including home automation, monitor system and was verified through the experiment. [1] For implementation LabVIEWTM was chosen as software along with USB web camera and a wireless client (PDA).</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>SNMP for home automation</title>
	<abstract>Home and building automation applications impose the need for convergence, or at least interoperability, of data networks with intelligent building networks. This situation entails the use of a unified management system. In this paper we extend the traditional SNMP-based network management system paradigm by creating an integrated site network management system that contains data elements, telecommunication equipment and intelligent building devices. A possible architecture based on a proxy gateway is presented and analyzed. We suggest an implementation and potential applications.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>25 Home Automation Projects for the Evil Genius</title>
	<abstract>TURN YOUR HOME SWEET HOME INTO AN AUTOMATED, EVIL GENIUS PARADISE!

Your home may be your castle-but can it cook your dinner? Well, with the help of 25 Home Automation Projects for the Evil Genius, you can teach it to do just that, along with dozens of other affordable, enjoyable things that will transform your humble abode into a wickedly automated living environment.

But fear not-you don't need an engineering degree to complete the projects in this book. That's because technology maven Jerri L. Ledford skillfully provides you with a firm understanding of the basic wiring, networking, and equipment demands for home automation. She then leads you step by step through each application, offering clearly worded and heavily diagramed guidance that will truly satisfy your inner Evil Genius.

With the help of just a few household tools, you'll be able to bring info-age automation to:

    Indoor and outdoor lighting
    Security and surveillance
    Climate regulation
    Entertainment systems
    Personal reminders
    Plant care
    Remote monitoring of kids and pets
    Keyless entry
    Wireless TV
    And many more!

Plus, you'll gain access to discounts from a variety of home automation product manufacturers, to make your projects even more economical. With 25 Home Automation Projects for the Evil Genius, easy living is now automatic!

Table of contents

INTRODUCTION TO HOME AUTOMATION
Chapter 1: Lights Please: Indoor Lighting
Chapter 2: Intruder Alert: Outdoor Lighting
Chapter 3: Whole House Automation: Installing an X-10 System
Chapter 4: It's Getting Hot in Here: Control Your Climate
Chapter 5: Speak Up! Installing Voice Controls
Chapter 6: What's at the Door? Installing a Surveillance Camera
Chapter 7: Shhh...I'm on the Phone
Chapter 8: Music Everywhere: Audio Controls
Chapter 9: Is Anyone There? Installing an Occupancy Sensor
Chapter 10: Today Is Trash Day: Automating Reminders
Chapter 11: No More Dead Lawns: Automating Watering Systems
Chapter 12: Watching from Afar: Monitoring Kids and Pets from Work
Chapter 13: Hot and Ready: Preheat the Oven from Your Cell Phone
Chapter 14: Who Let the Dogs Out: Creating Your Own Robo-Dog
Chapter 15: For the Birds: Installing a Bird Feeder Cam
Chapter 16: No Keys Necessary: Installing a Keyless Entry
Chapter 17: Romantic Settings: Creating Grouped Tasks to Control Your Environment
Chapter 18: Hello Dave: Creating Automation Personalization
Chapter 19: TV Anywhere: Wireless Television Throughout Your House
Chapter 20: Adding Sound Effects to Your Home Automation
Chapter 21: Did I Close That Door? Installing an Automatic Garage Door Closer
Chapter 22: Automating Your Hot Tub
Chapter 23: Add a Little Humor to Your House
Chapter 24: Connecting to Your Favorite Entertainment
Chapter 25: Creating an Illusion of Occupancy: Vacation Controls
APPENDIX A: AUTOMATION CHECKLIST
APPENDIX B: HOME AUTOMATION RESOURCES
GLOSSARY
INDEX</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>HAAIS-DSL: DSL to develop home automation and ambient intelligence systems</title>
	<abstract>Domain Specific Language (DSL) is an emergent software engineering discipline that allows software architects to model systems based on the elements of a specific domain. Home Automation (HA) and Ambient Intelligence (AmI) are examples of specific domains and they are considered the key elements in the future of home development. However, software for these domains is usually hand coded based on embedded devices and specific implementation technologies and frameworks. In this paper we present a Model Driven Development (MDD) approach to develop software systems for HA and AmI. A Domain Specific Language has been designed to model the architecture of these kinds of systems. Then, taking as input the architecture models, a set of model transformations allows code and configuration generation for a specific device platform like KNX/EIB (European Installation Bus).</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Prototyping home automation wireless sensor networks with ASSL</title>
	<abstract>We target effective home automation based on wireless sensor networks. ASSL (Autonomic System Specification Language) is used to formally specify and generate prototype models for wireless sensor networks controlling a simulated virtual home environment. This approach allows for formal validation, experiments under simulated conditions, and smooth transition from a prototype system to a real one.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Tangible user interface design for home automation energy management appliances</title>
	<abstract>Home Automation System connects the digital technology and computers to the appliances at home and manages it. It's the frequency of use an position in the home that makes it important to have an aesthetic friendly user interface. There are numerous UI design cases of Display panel or the buttons. However, the usability or aesthetic sensibility design through using a variety of multi-layered emotional satisfaction of feeling that provides the interface design needs to be studied. This study explores the ways to satisfy the emotional needs of Home Automation System and Tangible User Interface (TUI).</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Home Automation Basics: Practical Applications Using Visual Basic 6</title>
	<abstract>From the Publisher:
Did the mailman come today or not? How many times did the kids run in and out today? How long does the air conditioner usually run? So many questions-all of which can be answered with a monitoring system as part of an automated home.

In Home Automation Basics, Thomas E. Leonik, P.E. shows you how to utilize Visual Basic 6 in a home-monitoring system. Some day soon, all the functions of your home will be automated by a Programmable Logic Controller (PLC), the backbone of all industrial controls in the world. Trailblazing toward that new era, Leonik demonstrates just how easy it is to establish ground-level communications with a PLC through Visual Basic.

Visual Basic 6 is a flexible programming language that operates through Microsoft(r) Windows. With the help of a companion CD-ROM, Home Automation Basics guides you through the programming and installation portions of this project, giving you the ability to monitor a variety of functions in your home or office, including:

    Front doorbell
    Door open/closed status
    Water-pump usage
    HVAC usage
    Mailbox status
    Temperature
    Water pressure</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Absolute beginner's guide to home automation</title>
	<abstract>Get the home of tomorrow, today! Absolute Beginner's Guide to Home Automation will help you turn your ordinary home into a high-tech haven. Want to schedule your lights to turn on while you're on vacation? Stuck late at work and want to start the roast you put in the crock pot this morning? You can make it all happen with the help of existing 110V electrical wiring in your home and this step-by-step tutorial. Through simple, do-it-yourself instructions, you will walk through the process of outfitting every room in your home with a network connection that you can control with a few clicks on your computer keyboard. Complete with illustrations and photographs, Absolute Beginner's Guide to Home Automation will have you riding the wave of the future in no time. </abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Home Automation with ZigBee</title>
	<abstract>This article discusses a topic of home automation, also called domotics, and provides an overview of state-of-the-art in communication protocols for this application area. Comparitive analysis for different networking standards and communication media is provided. Special focus is given to wireless technologies and advantages provided by ZigBee wireless networking standard for home automation solutions. ZigBee protocol implementation details are covered together with examples of possible applications in conjunction with Linux Open Source software platform.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Smart House: Artificially Intelligent Home Automation System</title>
	<abstract>Technologies like Home Automation Systems are still under considerations and have not been developed up to the level of ultimate maturity. Targeting the challenges of the proposition of a home automation system capable of controlling house's electric appliances and providing efficient security system, a thorough research has been conducted in fields of System Automation, Hardware Engineering, Software Engineering, Human Machine Interaction, Mobile Programming, Produce Line Architectures, Software Testing and Data Management Systems. Taking help from reviewed research and using our personal research and development experiences, we have proposed an approach i.e. Smart House. This book discusses in detail the way to achieve the set goals of this research along with the detailed presentation of system requirements, proposed approach, hardware and software designs, implemented prototype, testing, deployment, usage, benefits and limitations. The presented research and development has been performed as the part of author's bachelor project submitted in Fall 2002 at Punjab Institute of Computer Science, Faculty of Information Technology, University of Central Punjab, Pakistan.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Development of a Residential Gateway and a Service Server for Home Automation</title>
	<abstract>We have developed a residential gateway that centralizes device interfacing between the external Internet and internal devices as well as appliance networks. We have also proposed the concept of a service server to be implemented for a large scale of apartment complex or a wide area house. The primary components of the residential gateway to be implemented in this paper include a processor(Motorola MPC8240), persistent storage(flash RAM, extend storage device), networking modules (such as TCP/IP for Ethernet, ADSL), home networking(HomePNA, IEEE1394, PLC), device interfaces(serial or PCI), home automation, and telecommunication system (PSTN/SLT, VoIP, Video Communication), which are typically powered by a certain RTOS. Finally, we have test results validating the effectiveness of both the residential gateway and the service server.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Using Jini to Integrate Home Automation in a Distributed Software-System</title>
	<abstract>The last few years, a tendency arose to integrate various programmable devices through ever expanding computer networks. One particular domain in which this evolution stood out clearly is home automation. The EIB standard defines a home automation solution that consists of a network of cooperating components with little computational power. Bridging the EIB network with a computer network allows software to interact with these EIB components. However, past attempts to link EIB components with computer networks fell short in dynamism, automatism or user friendliness. In this paper we present a distributed software framework that overcomes these issues, and is capable of automatically generating a software model based on the configuration of the components on the EIB fieldbus. A framework that can be used in this perspective is Jini. Its excellent capabilities for dynamic reconfiguration and its proven deployment in domestic and office environments make it an appropriate candidate for supporting home automation systems.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A study of an embedded community network system in home automation</title>
	<abstract>This article proposes an embedded system of home automation in a community network The system includes a home security monitor and a home energy monitor. The home security monitor uses sensors and micro switches to monitor the status of the home; the home energy monitor uses a Hall sensor to oversee household energy consumption and the control circuits of home appliances. The system integrates every node subsystem. It is constructed on a net framework and is established by using a common gateway interface (CGI).</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Infostudio: teaching ambient display design using home automation</title>
	<abstract>The infostudio course unit introduced 3rd year undergraduate students to the design of ambient display as physical, human-scale installations that convey data-driven spatial experiences. Students developed ambient display installations in a common printer hub room that subtly reflected the electronic network traffic, human activities and environmental data within the adjacent computer labs in real time. The resulting prototypes explored how the combination of common networked home automation hardware controlling simple electrical devices and multiple multimedia projections can be used to convey real-time information through different human senses.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Assertion-based test oracles for home automation systems</title>
	<abstract>The Home Automation System (HAS) is a service-oriented application that facilitates the automation of a private home to improve the comfort and security of its residents. HAS is implemented using a service-oriented architecture. Many of the services in the HAS dynamically change their configuration during run-time. This occurs due to change in availability and bindings between services. Dynamic reconfigurations of services in the HAS presents several testing challenges, one being the specification of test oracles. In this paper, we give an approach for specifying test oracles for services in the HAS. We formally specify test oracles in the JML specification language. To verify service behavior in the presence of dynamic reconfigurations, we use mechanisms in the service architecture that notify dynamic changes along with run-time evaluation of JML specifications. We illustrate our approach using an example service in the H-Omega HAS developed on the OSGi™ and iPOJO service platform. To evaluate our approach, we developed a testing framework that allows for generation of tests with dynamic service reconfigurations. In addition, we seeded faults into the example service, and evaluated the effectiveness of the test oracles in revealing the faults using the generated tests.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Remote Control of Home Automation Systems with Mobile Devices</title>
	<abstract>Remote control based on mobile devices as mobile phones or PDA's, is considered more and more useful in many computerised applications. This paper deals with the implementation of functions, based on mobile devices, for the remote control of commercial home automation systems. Different solutions are considered and some problems concerning their implementation are discussed. A preliminary development of the interface used to control X10 modules or to interrogate a home database of the device state is here described. Some guide-lines for the interface design are also reported.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Dynamic scheduling of multi-media streams in home automation systems</title>
	<abstract>A new trend in home automation is to integrate audio en video applications. However in current domotics systems, these are usually conFigured statically. We implemented a home automation tool that dynamically (re)schedules bandwidth for different types of multimedia streams with different constraints toward latency and quality. The dynamic scheduling problem was implemented using distributed agents negotiating for bandwidth using the contract-net negotation protocol.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Multi-Agent System Theory for Modelling a Home Automation System</title>
	<abstract>A paradigm for modelling and analysing Home Automation Systems is introduced, based on Multi-Agent System theory. A rich and versatile environment for Home Automation System simulation is constructed for investigating the performances of the system. The exploitation of limited resources (electricity, gas, hot water) depends on behavioural parameters of the individual appliances. In order to deal with the problem of developing systematic design and validation procedures for control strategies, global performance indices for the system are introduced. Different strategies for allocating resources and establishing priorities in their use can therefore be tested and compared.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>An Integral and Networked Home Automation Solution for Indoor Ambient Intelligence</title>
	<abstract>We've long heard about the wide functionality that home automation technologies offer for improving our lives and securing our houses. However, price and an unstable domotics market have restricted such systems' deployment. Nowadays, companies offer too technology-dependent solutions that don't cover user demands completely. Meanwhile, research works focus on small innovations on specific parts of home automation systems, which don't consider integration and deployment issues to present practical designs. The system presented in this article considers user requirements, including novel advances, all in an integral home automation solution suitable for many services. The architecture's modular nature allows direct adaptation to specific cases using standard domotic technologies, for managing in-house devices, and a proposal of an IP-based network for connecting the main home automation module with other platform elements. A remote security system has been developed, and managing tasks are enabled via in-home control panels and an advanced 3D application for local and remote homeowner access. The system has been deployed on a prototype house, testing a wide set of domotic services. Moreover, the range of indoor pervasive applications has also been extended to e-health, elderly adaptation, greenhouse automation, and energy efficiency.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Peer-to-Peer Overlay Gateway Services for Home Automation and Management</title>
	<abstract>Recent advances in home networking devices and the increase in users connected to the internet, has allowed "home automation" to gain more attention. Home automation is the process of accessing and controlling home devices from remote locations across the Internet. Today, modern technology has reached a point in which computing devices are common in every facet of our life. We encounter numerous electronic devices within the home and office environment, as well as in public spaces such as shopping centres and airports. Computer technology and networking surround us wherever we are. We routinely use computer-based devices in our daily lives ranging from home appliances such as TVs, PCs, and Audio/Video devices to office appliances such as PDAs, and printers. All of these devices are physically located in different locations. Whilst networking has increased the opportunity to transfer data between devices, the process often remains intricate and unintuitive requiring careful configuration. In this paper we propose an architecture for accessing and controlling networked appliances in this way. We present a scenario to demonstrate our architecture showing how it can help in arranging daily home visit jobs such as engineer visits, receiving postal deliveries and so on, enabling the user to communicate with an engineer or postman while they are away from home.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Supervision and Regulation of Home Automation Systems with Smartphones</title>
	<abstract>Home automation and building services systems are integrated in many homes and buildings to meet the needs of comfort, security and efficiency of the customers. On the other hand, mobile devices such as handheld devices and smart phones are providing location-independent access to the Internet and local networks. This paper describes the requirements to combine these two mediums to supervise home automation systems with smart phones. It presents the design and implementation of an iPhone application that allows the retrofitting of mobile devices into an existing KNX home automation system without the need of specific hardware or skilled technicians. An approach to use configuration files, which arise during the installation of KNX home automation systems, for the initial setup of the mobile application is discussed.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Habitation: A Domain-Specific Language for Home Automation</title>
	<abstract>Developers need suitable tools to develop home automation systems while enhancing quality and productivity. One solution is to use domain-specific languages (DSLs) within a model-driven approach. The Habitation DSL provides a powerful visual development environment, including a catalog of reusable functional units and a set of home automation interconnection primitives. The model-driven approach offers mechanisms to automatically generate code to enhance the quality and portability of home automation systems. The result is an Eclipse-based tool whose usability the authors have validated in a case study.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A New Intelligent Remote Control System for Home Automation and Reduce Energy Consumption</title>
	<abstract>This paper presents the design and implementation of an internet-based smart remote control system for home automation, dedicated to power management that adapts power consumption to available power resources according to user comfort and cost criteria. Sensors and home appliances are connected to the designed and implemented control panel and then they are monitored and controlled from every corner of the world through the Internet cloud. The system is scalable and allows additional appliances to be added to it with no major changes to its core. New communication format is proposed to enable communication between the control panel and the server as well. To verify the principle operation of the design, some home applications are experimentally tested. Experimental results show the efficiency and accuracy of proposed intelligent control system in terms of energy saving and being user friendly.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Development of Remote-Controlled Home Automation System with Wireless Sensor Network</title>
	<abstract>A remote-controlled home automation system basing on the wireless sensor network, embedded system and GPRS was developed. This system allows the user to control the equipments in home, collect data about the appliance status and weather condition, and receive the alarm information of home intruder and fire through Chinese instant message of mobile service. The test result shows that the system can work according to deigned function. The advantages of this system are easy to set up, convince to use and interface friendly to Chinese people.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Wireless home automation networks: a survey of architectures and technologies</title>
	<abstract>Wireless home automation networks comprise wireless embedded sensors and actuators that enable monitoring and control applications for home user comfort and efficient home management. This article surveys the main current and emerging solutions that are suitable for WHANs, including ZigBee, Z-Wave, INSTEON, Wavenis, and IP-based technology.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Testing Resource Usage in Home Automation Systems</title>
	<abstract>The problem of resource management in Home Automation Systems (HAS) emerges in several recent contributions due to the growth of ecological concerns and the rapid development of automated home equipments. This paper presents an ongoing work on the subject, whose goal is to provide an approach to test the resource usage of HAS, including a test suite generation and execution method. The envisioned test case generation process is parametric in order to focus on certain testing objectives of interest for the system developer.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>The Gesture Pendant: A Self-illuminating, Wearable, Infrared Computer Vision System for Home Automation Control and Medical Monitoring</title>
	<abstract>In this paper, we present a wearable device for control of home automation systems via hand gestures. This solution has many advantages over traditional home automation interfaces in that those with loss of vision, motor skills, and mobility can use it. By combining other sources of context with the pendant, we can reduce the number and complexity of gestures while maintaining functionality. As users input gestures, the system can also analyze their movements for pathological tremors. This information can then be used for medical diagnosis, therapy, and emergency services. Currently, the Gesture Pendant can recognize control gestures with an accuracy of 95% and user-defined gestures with an accuracy of 97% it can detect tremors above 2HZ within plus or minus 0.1 Hz.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>A heuristic routing protocol for wireless sensor networks in home automation</title>
	<abstract>Although numerous routing protocols have been designed for wireless sensor networks, not all are suitable for wireless sensor networks in home automation (WSNHA). In this paper, we evaluate some popular wireless sensor network routing protocols for their suitability to WSNHA. We also propose a greedy-algorithm heuristic routing (GAHR) protocol and an A* algorithm for route finding. We compared the performance of this heuristic routing with the ad-hoc on-demand distance-vector protocol using a simulation. The simulation results showed that this routing protocol dramatically reduced the routing overhead and the average packet delay without impacting reliability, and demonstrated that WSNHA-GAHR adapts automatically to changes in the network topology.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Agenda Driven Home Automation - Towards High Level Context Aware Systems</title>
	<abstract>In this paper, we present a system that exploits the semantics of events that are traditionally stored in simple agendas to control appliances and other ordinary devices in the home.Our approach is based on representing the content of agendas using an expressive and semantic modeling language and on implementing the system on top of the Amigo Context Management Service. This approach makes it possible to automate the control of physical devices while taking into account the anticipation of future events, where control includes action triggering and self conguration. We have assessed our approach through the development and deployment of an agenda based home automation system that controls a variety of devices including heating systems, lights, and motorized window shutters.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Design and Implementation of Home Automation System</title>
	<abstract>This essay focuses on the overall framework and functions of home automation system (HAS) based on J2EE/J2ME platform. This system mainly fulfills a remote control on household electronics with the help of mobile phones. This essay also deals with the modeling of HAS with the use of UML technology. This essay analyzes and explains the designing plan of both the mobile phone user and the server terminal, thus, the essay provides a theory which can be referred to in the HAS information construction of our country.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>ITU G.hn Concept and Home Automation</title>
	<abstract>For long time, many in the buildings industry have been looking for a day when home automation systems would become fully integrated with communication and human-interface practices, with standards widely employed for information technology. Now the time is coming in the form of the ITU Recommendations G.hn.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>GENIO: an ambient intelligence application in home automation and entertainment environment</title>
	<abstract>As one of the main aims of "Ambient Intelligence" is natural human interaction with the environment and one the most suitable is the home environment. Fagor has been working several years to develop a power-line network where all its household appliances, security sensors and actuators, heating systems and antiintrusion systems are connected and managed by a central controller named Maior-Domo. As a result of GENIO project, the user can dialog with his home and asks the services and functionalities he wants by talking as he was talking to a friend. The controller Maior-Domo has a human representation that the user can see and can interact with. When the user talks the Maior-Domo extracts the different commands from those vocal orders and controls the home devices. These orders are not specific commands that the user has to learn but natural speaking language without any need of learning. In the same way, any event or information from any device of the network is transmitted to the user by voice. In order to achieve a demonstrator of this Ambient Intelligence application, a real kitchen and sitting room have been built where the users can command the home talking naturally (in Spanish). Possible actions are: reading e-mails, programming the washing machine, checking the goods in the fridge, creating the shopping list, doing shopping with a PDA in the supermarket, activating the dishwasher, being guided on how to prepare a recipe for the oven checking if there are the needed goods to do it, listening some music stored at home, watching some photos, watching some selected video and so on. Every user has a wireless microphone in his/her shirt's pocket. This microphone captures his/her voice and all the sounds around him/her and sends them to a developed board which filters the voice frequency range from other sounds. From here the voice recognition system "understands" the pronounced sentence and process it. A quite extended number of sentences, called grammar, make up the possible dialogue between the person and the whole system. The user can address the whole system in different ways using a lot of expressions, talking naturally and spontaneously and dialoguing to the home. The defined grammar is so extended that almost the total speaker independence has been achieved.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Building a home automation and security system with python</title>
	<abstract>Want to use Python and cheap hardware to feel more secure about your home?</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>0</relevance>
  </item>
  <item>
    <title>Ontology-based expert system for home automation controlling</title>
	<abstract>In the present paper we describe the model process of an ontologybased expert system for the control of domotic installations and SWRL rules management. From the domotic system database where the attribute values of each device are stored, a background process converts these values into ontology's instances representing the system physic devices. From these instances, a software application -known as DomoRules- allows creating production rules in SWRL language that will be useful to regulate the system behaviour. IntelliDomo draws inferences from the ontology and the SWRL rules by using parameters previously indicated by the user, so the state of the physic devices in the domotic system can be modified in real time.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Improving dialogue systems in a home automation environment</title>
	<abstract>In this paper, a task of human-machine interaction based on speech is presented. The specific task consists on the use and control of a set of home appliances through a turn-based dialogue system. This work focuses on the first part of the dialogue system, the Automatic Speech Recognition (ASR) system. Two lines of work are taken into account to improve the performance of the ASR system. On one hand, the acoustic modeling required for the ASR is improved via Speaker Adaptation techniques. On the other hand, the Language Modeling in the system is improved by the use of class-based Language Models. The results show the good performance of both techniques to improve the ASR results, as the Word Error Rate (WER) drops from 5.81% using a close-talk microphone to a 0.99% and from 14.53% using a lapel microphone to a 1.52%. Also, an important reduction is achieved in terms of the Category Error Rate (CER), which measures the ability of the ASR system to extract the semantic information of the uttered sentence, dropping from 6.13% and 15.32% to 1.29% and 1.32% for the two microphones used in the experiments.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
  <item>
    <title>Making Home Automation Communications Secure</title>
	<abstract>Long the futuristic domain of hobbyists, home automation is now moving to the mainstream. This domain involves three significant technological developments:Focused subsystems consist of specific home features that use local information to automate desired performance, such as programmable thermostats that change home temperature based on a time schedule.Integrated whole-home behavior lets users combine safety, comfort, health, information, and entertainment needs into one system that, for example, can change environmental settings based on variable home occupant activities rather than a fixed time schedule.Distributed-home-automation applications, enabled by widespread adoption of the Internet, run substantially outside the home, eliminating the need for a PC and making these applications easy to upgrade or tailor for specific individuals and markets.Although these new communications technologies offer numerous benefits, they also open home automation to many security threats. To protect against these threats within the limited resources of a typical home automation system, the authors have developed a family of products based on Honeywell's Global Home Server, a remote Web site that provides secure Internet access and other services to client installations.The GHS system is now operational, with initial product deployments in both the United States and Europe. Expanding on this work, the authors are developing security products that use different media and processors, can function in aircraft as well as in homes, and run various novel applications.</abstract>
	<search_task_number>11</search_task_number>
	<query>Home Automation</query>
	<relevance>1</relevance>
  </item>
</results>
